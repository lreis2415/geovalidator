[
{"syntax": "ExportMetadata_conversion (Source_Metadata, Translator, {Output_File})", "name": "Export Metadata (Conversion)", "description": "Updates metadata to contain the most current properties of the ArcGIS item before processing the metadata and finally exporting it to an XML file that conforms to a standard metadata format.  Initial processing is performed to produce the best results when exporting information to a standard metadata format. Afterwards, the metadata is exported using the  ESRI Metadata Translator . ", "example": {"title": "ExportMetadata Example (Python Window)", "description": "Updates and exports ArcGIS metadata to an XML file that is formatted correctly for the ISO 19139 metadata standard.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" #set local variables dir = arcpy.GetInstallInfo ( \"desktop\" )[ \"InstallDir\" ] translator = dir + \"Metadata/Translator/ESRI_ISO2ISO19139.xml\" arcpy.ExportMetadata_conversion ( \"data.gdb/roads\" , translator , \"roads_19139.xml\" )"}, "usage": ["A new metadata translator is provided with the current release of ", "ArcGIS for Desktop", "\u2014the ArcGIS to ISO 19139 translator, ", "ARCGIS2ISO19139.xml", ". All ISO-based metadata styles have been updated to use this translator. Any geoprocessing models or Python scripts that export metadata to the ISO 19139 format should be updated to use this translator as well. ", "The earlier ArcGIS or ESRI-ISO to ISO 19139 translator, ", "ESRI_ISO2ISO19139.xml", ", continues to be provided; however, there are several known issues with this translator that can't be resolved.  Any existing 9.3.1 metadata in the ESRI-ISO format must be upgraded to the ArcGIS metadata format to successfully export the item's metadata content to the ISO 19139 format with the ArcGIS to ISO 19139 translator, ", "ARCGIS2ISO19139.xml", ".", "Learn more about upgrading metadata", " ", "The output files produced by this tool can't be stored in a geodatabase. If the ", "Current_workspace", " environment is set to a geodatabase, the output files will be stored in a different location, as described below.", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", "."], "parameters": [{"name": "Source_Metadata", "isOptional": false, "description": "The item whose metadata will be converted or a stand-alone XML file that will be converted. ", "dataType": "Data Element; Layer"}, {"name": "Translator", "isOptional": false, "description": "An XML file that defines the conversion that will be performed. The translator files provided with ArcGIS for Desktop can be found in the <ArcGIS Installation Location>\\Metadata\\Translator folder. The following translators are provided: ARCGIS2FGDC.xml \u2014 Translates content stored in the ArcGIS metadata format to the FGDC CSDGM XML format. This translator is used by default when you export metadata from the Description tab using the FGDC CSDGM Metadata style. Metadata is converted using an XSLT transformation and won't produce a log file. ARCGIS2ISO19139.xml \u2014 Translates content stored in the ArcGIS metadata format to the ISO 19139 XML format. This translator is used by default when you export metadata from the Description tab using any of the ISO-based metadata styles. It is the preferred translator for exporting metadata to the ISO 19139 XML format. Metadata is converted using an XSLT transformation and won't produce a log file. ESRI_ISO2ISO19139.xml \u2014 Translates content stored in either the ArcGIS metadata format or the ESRI-ISO metadata format to the ISO 19139 XML format. This translator is provided for backwards compatibility to support existing models and Python scripts. It has some known limitations with exporting metadata to the ISO 19139 XML format. Use the ARCGIS2ISO19139.xml translator instead. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ESRI_ISO.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ArcGIS metadata format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . This translator is used when you import FGDC-formatted metadata by running the Import Metadata tool with the FROM_FGDC type and when you upgrade metadata by running the Upgrade Metadata tool with the FGDC_TO_ARCGIS type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ISO19139.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ISO 19139 XML format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. ISO19139_2ESRI_ISO.xml \u2014 Translates content stored in the ISO 19139 XML format to the ArcGIS metadata format. This translator is used when you import ISO 19139-formatted metadata by running the Import Metadata tool with the FROM_ISO_19139 type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine.", "dataType": "File"}, {"name": "Output_File", "isOutputFile": true, "isOptional": true, "description": "A stand-alone XML file that will be created containing the converted metadata. ", "dataType": "File"}]},
{"syntax": "ESRITranslator_conversion (source, translator, {output}, {logfile})", "name": "Esri Metadata Translator (Conversion)", "description": "Uses the ArcGIS metadata translation engine or an  XSLT  transformation to export metadata content from ArcGIS to a stand-alone metadata XML file. The exported metadata will be formatted to satisfy the metadata standard associated with the specified translation.  Metadata for items in ArcGIS is stored in the ArcGIS metadata format. Use the ArcGIS to translations to export ArcGIS metadata to another metadata XML format. For example, do this  to share information outside of ArcGIS by publishing it to a metadata catalog. Different metadata catalogs accept information in different XML formats. ArcGIS metadata can be exported to different formats  if you must publish your information to metadata catalogs with different requirements. When using a to ISO 19139 translation, the exported metadata will be formatted following the rules defined in ISO standard 19139,  Geographic information \u2014 Metadata \u2014 XML schema implementation , and its associated XML Schemas. The same translation is used to export metadata according to an ISO metadata profile. The translation accounts for the metadata style selected in ArcGIS. For example, when the selected metadata style is  North American Profile of ISO 19115:2003 \u2013 Geographic information \u2013 Metadata , the output file will be formatted appropriately for that ISO metadata  profile. When using the ArcGIS to FGDC translation, the exported metadata will be formatted following the  Federal Geographic Data Committee (FGDC)   Content Standard for Digital Geospatial Metadata (CSDGM)  XML format. The resulting file can be published to  geodata.gov , for example. FGDC to translations handle information in the item's metadata which is formatted according to the FGDC CSDGM XML format. This content appears under the  FGDC Metadata (read-only)  heading in the  Description  tab  when you are using a metadata style that gives you full access to the item's metadata. This content may have been provided with the current release of  ArcGIS for Desktop  using the  FGDC metadata editor add-in  or using the FGDC metadata editor provided with ArcGIS Desktop 9.3.1 or earlier releases. The to ArcGIS translations convert other metadata XML formats to the ArcGIS metadata format. This is an important step in the process of importing metadata that exists in another format to an ArcGIS item; however, several additional steps are also required to achieve the best results. Use the  Import Metadata  tool with the appropriate translation to complete this task instead.", "example": {"title": "Export ArcGIS metadata", "description": "Exports ArcGIS metadata to an XML file that is formatted correctly for the ISO 19139 metadata standard.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" #set local variables dir = arcpy.GetInstallInfo ( \"desktop\" )[ \"InstallDir\" ] translator = dir + \"Metadata/Translator/ESRI_ISO2ISO19139.xml\" arcpy.ESRITranslator_conversion ( \"locations.shp\" , translator , \"locations_19139.xml\" , \"locations_19139.txt\" )"}, "usage": ["A new metadata translator is provided with the current release of ", "ArcGIS for Desktop", "\u2014the ArcGIS to ISO 19139 translator, ", "ARCGIS2ISO19139.xml", ". All ISO-based metadata styles have been updated to use this translator. Any geoprocessing models or Python scripts that export metadata to the ISO 19139 format should be updated to use this translator as well. ", "The earlier ArcGIS or ESRI-ISO to ISO 19139 translator, ", "ESRI_ISO2ISO19139.xml", ", continues to be provided; however, there are several known issues with this translator that can't be resolved.  Any existing 9.3.1 metadata in the ESRI-ISO format must be upgraded to the ArcGIS metadata format to successfully export the item's metadata content to the ISO 19139 format with the ArcGIS to ISO 19139 translator, ", "ARCGIS2ISO19139.xml", ".", "Learn more about upgrading metadata", " ", "When metadata is translated to the ArcGIS metadata format using a to ArcGIS translation, the original document is included in the resulting metadata as an enclosure. You can access the original document from the ", "Metadata Properties", " dialog box in ", "ArcCatalog", ".", "The metadata content validation messages provided by this tool account for detailed conditions documented in the metadata standard that can't be tested using XML Schema validation. For example, in ISO 19115-based metadata, the value of the metadata hierarchy level element determines if a topic category is required or not; this condition can't be tested using an XML Schema. ", "A topic category is required if the  item contains spatial data. If the item is identified as spatial data but a topic  category is missing, the validation error reported is ", "Validation failed: em:Resource($a) --> es:not(em:Dataset($a)); em:identificationInfo($a, $b), em:topicCategory($b, $c), em:lookupCodeName($c, gmxRes:MD_TopicCategoryCode)", ". This statement means that the item has to either not be a dataset or a valid topic category code must be provided, and in this case neither is true.", "The output files produced by this tool can't be stored in a geodatabase. If the ", "Current_workspace", " environment is set to a geodatabase, the output files will be stored in a different location, as described below.", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", ".", "Each Translator, except the ArcGIS to FGDC and ArcGIS to ISO 19139 translators, references files containing validation rules. The message ", "Starting validation", " indicates validation is beginning using the rules in the specified file. The validation rules defined in ", "ISO19139_min_schema.txt", " cover the minimum mandatory requirements for ISO metadata, and the rules defined in ", "ESRI-ISO_schema.txt", " address the remaining metadata elements.  ", "  These rules identify if mandatory elements have been provided, if conditional elements are required and provided, and if the value provided in a metadata element is of the correct data type. Validation rules can't detect cardinality issues such as only one purpose element is allowed but two have been provided. Any validation issues found are reported as warnings in the tool's messages. For example, a metadata contact is required in ISO metadata. The following warning indicates complete metadata contact information hasn't been provided: ", "Validation failed: em:Metadata($a) --> em:contact($a, $b), em:CI_ResponsibleParty($b)", "; that is, a metadata contact hasn't been provided at all, or the contact information provided doesn't follow the rules for contacts defined in the CI_ResponsibleParty information class in the ISO 19115 metadata standard.", "After validation the translator generates the output metadata XML file. Cardinality rules are tested at this time. If more than the expected number of elements exist, none of them will be written to the output metadata XML file.   Some validation issues prevent the translator from generating valid output XML and will also be reported as warnings in this phase. For example, if a valid metadata contact hasn't been provided, this information can't be included in the output: ", "Error [InvalidForSome]: <MD_Metadata> for-some clause: em:contact(v:Metadata, v:Object) <contact>", "; that is, one or more contact elements could not be added to the MD_Metadata element in the output XML file. This message is a warning even though it begins with the word error.", "The Translator XML files that configure a translation are provided with ", "ArcGIS for Desktop", " in the installation location in the ", "Metadata\\Translator", " folder. If you open one of these files in an XML editor you'll see the translator element has an attribute stopOnErrors. This attribute can have one of three values: yes, no, or any. When including this tool in a model, you may prefer to copy one of the default Translators and change the stopOnErrors value to produce the desired behavior."], "parameters": [{"name": "source", "isOptional": false, "description": "The item whose metadata will be converted or a stand-alone XML file that will be converted. ", "dataType": "Data Element; Layer"}, {"name": "translator", "isOptional": false, "description": "An XML file that defines the conversion that will be performed. The translator files provided with ArcGIS for Desktop can be found in the <ArcGIS Installation Location>\\Metadata\\Translator folder. The following translators are provided: A translator file must be specified. This tool does not have a default value for this parameter. ARCGIS2FGDC.xml \u2014 Translates content stored in the ArcGIS metadata format to the FGDC CSDGM XML format. This translator is used by default when you export metadata from the Description tab using the FGDC CSDGM Metadata style. Metadata is converted using an XSLT transformation and won't produce a log file. ARCGIS2ISO19139.xml \u2014 Translates content stored in the ArcGIS metadata format to the ISO 19139 XML format. This translator is used by default when you export metadata from the Description tab using any of the ISO-based metadata styles. It is the preferred translator for exporting metadata to the ISO 19139 XML format. Metadata is converted using an XSLT transformation and won't produce a log file. ESRI_ISO2ISO19139.xml \u2014 Translates content stored in either the ArcGIS metadata format or the ESRI-ISO metadata format to the ISO 19139 XML format. This translator is provided for backwards compatibility to support existing models and Python scripts. It has some known limitations with exporting metadata to the ISO 19139 XML format. Use the ARCGIS2ISO19139.xml translator instead. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ESRI_ISO.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ArcGIS metadata format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . This translator is used when you import FGDC-formatted metadata by running the Import Metadata tool with the FROM_FGDC type and when you upgrade metadata by running the Upgrade Metadata tool with the FGDC_TO_ARCGIS type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ISO19139.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ISO 19139 XML format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. ISO19139_2ESRI_ISO.xml \u2014 Translates content stored in the ISO 19139 XML format to the ArcGIS metadata format. This translator is used when you import ISO 19139-formatted metadata by running the Import Metadata tool with the FROM_ISO_19139 type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine.", "dataType": "File"}, {"name": "output", "isOptional": true, "description": "A stand-alone XML file that will be created containing the converted metadata. To check for problems in the metadata using the Esri Metadata Translator's translation engine and not produce an output XML file, provide the pound sign ( # ) instead of a file name. ", "dataType": "File"}, {"name": "logfile", "isOptional": true, "description": "A text file that will be created listing the warnings and errors that occurred during the conversion process. To export metadata without producing a log file, provide the pound sign ( # ) instead of a file name. A log file will not be created when using the ArcGIS to FGDC translation even if a log file name is provided. ", "dataType": "File"}]},
{"syntax": "WFSToFeatureClass_conversion (input_WFS_server, WFS_feature_type, out_path, out_name)", "name": "WFS To Feature Class (Conversion)", "description": "Imports a feature type from a web feature service (WFS) to a feature class in a geodatabase.   Learn more about WFS support in ArcGIS", "example": {"title": null, "description": null, "code": ""}, "usage": ["After setting the URL for the WFS server, all feature types published from the server are listed. Examples can include WFS feature types for schools, roads and parcels. One of these is then selected and an output location and feature class name are specified. The output location can be the root level of a geodatabase or a feature dataset within a geodatabase.", "By default all features from the WFS source are added to the feature class. The extent environment setting can be used to limit the features to just those that intersect a user-defined extent. You can also specify an output config keyword and output spatial grids using the geodatabase settings section of the environment settings."], "parameters": [{"name": "input_WFS_server", "isInputFile": true, "isOptional": false, "description": "The URL of the source WFS service. T ", "dataType": "String"}, {"name": "WFS_feature_type", "isOptional": false, "description": "The list of feature types published from the WFS service. ", "dataType": "String"}, {"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The output location can be the root level of a geodatabase or a feature dataset within a geodatabase. If the output location is a feature dataset, the coordinates are converted from the source coordinate system to the coordinate system of the feature dataset. ", "dataType": "Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the feature class to create within the output location. The name must not already exist in the geodatabase. By default, the feature type name is used. ", "dataType": "String"}]},
{"syntax": "RasterToVideo_conversion (input_folder, out_video_file, {image_format}, {codec}, {duration_method}, {time}, {quality})", "name": "Raster To Video (Conversion)", "description": "Creates a video file from a set of images. ", "example": {"title": null, "description": "RasterTo Video Python example", "code": "# Name: RasterToVideo_AVI.py # Description: Creates an AVI video from a folder of images # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"c:/data\" #Set local variables inFolder = \"images\" outputVideoFile = \"outputVideo.avi\" #Execute RaserToVideo arcpy.RasterToVideo_conversion ( inFolder , outputVideoFile , \"BMP\" , \"Microsoft Video 1\" , \"FRAME_RATE\" , \"2\" , \"90\" )"}, "usage": [" Keeping in mind that the images are read alphabetically, you can insert images in the folder of images such as a title image for your video. For example, if the folder of images has images with names ", "image_1.bmp", ", ", "image_2.bmp", ", and ", "image_20.bmp", ", you can add an image named ", "_image_1.bmp", ". Since the tool reads the images in an alphabetical order,", "_image_1.bmp", " will appear before ", "image_1.bmp", " in the output video.", "The resolution (width x height) of the output video is based on the resolution of the first image in the folder of images supplied to the tool. If the first image has a smaller or larger resolution than the others, the rest of the images will be stretched to fit the resolution of the first image. So try to keep the resolution of the images in the folder consistent. ", "Since AVI and QuickTime formats are supported on Windows only, this tool will not work on a non-Windows platform. "], "parameters": [{"name": "input_folder", "isInputFile": true, "isOptional": false, "description": " The folder containing the images. The images in the folder should be of the same type (BMP or JPEG). ", "dataType": "Folder"}, {"name": "out_video_file", "isOutputFile": true, "isOptional": false, "description": "The output video file (*.avi or *.mov). ", "dataType": "File"}, {"name": "image_format", "isOptional": true, "description": " The format of the images files in the folder. The output video will be created using the images of the chosen format. BMP \u2014 Windows Bitmap (*.bmp) JPG \u2014 JPEG (*.jpg) ", "dataType": "String"}, {"name": "codec", "isOptional": true, "description": " The codec used for compressing the frames while writing the video file. The list of codecs can vary on different machines. ", "dataType": "String"}, {"name": "duration_method", "isOptional": true, "description": " The method to be used for defining the output video duration. FRAME_RATE \u2014 Defines the duration of the output video based on the number of frames per second TIME \u2014 Defines the duration of the output video in seconds", "dataType": "String"}, {"name": "time", "isOptional": true, "description": " Duration of the video to be output. ", "dataType": "Double"}, {"name": "quality", "isOptional": true, "description": " The quality of the output video. The video can be exported at different qualities ranging from 1 to 100. The default value is 100. ", "dataType": "Long"}]},
{"syntax": "RasterToPolyline_conversion (in_raster, out_polyline_features, {background_value}, {minimum_dangle_length}, {simplify}, {raster_field})", "name": "Raster to Polyline (Conversion)", "description": "Converts a raster dataset to polyline features.", "example": {"title": "RasterToPolyline example (Python window)", "description": "Converts a raster dataset to polyline features.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.RasterToPolyline_conversion ( \"flowstr\" , \"c:/output/streams.shp\" , \"ZERO\" , 50 , \"SIMPLIFY\" )"}, "usage": ["The input raster can have any valid cell size greater than 0, and may be any valid integer raster dataset.", "The ", "Field", " parameter allows you to choose which attribute field of the input raster dataset will become an attribute in the output feature class. If a field is not specified, the cell values of the input raster (the ", "VALUE", " field) will become a column with the heading ", "Grid_code", " in the attribute table of the output feature class."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. The raster must be integer type. ", "dataType": "Raster Layer"}, {"name": "out_polyline_features", "isOutputFile": true, "isOptional": false, "description": "The output feature class that will contain the converted polylines. ", "dataType": "Feature Class"}, {"name": "background_value", "isOptional": true, "description": "Specifies the value that will identify the background cells. The raster dataset is viewed as a set of foreground cells and background cells. The linear features are formed from the foreground cells. ZERO \u2014 The background is composed of cells of zero or less or NoData. All cells with a value greater than zero are considered a foreground value. NODATA \u2014 The background is composed of NoData cells. All cells with valid values belong to the foreground.", "dataType": "String"}, {"name": "minimum_dangle_length", "isOptional": true, "description": "Minimum length of dangling polylines that will be retained. The default is zero. ", "dataType": "Double"}, {"name": "simplify", "isOptional": true, "description": "Simplifies a line by removing small fluctuations or extraneous bends from it while preserving its essential shape. SIMPLIFY \u2014 The polylines will be simplified. This is the default. NO_SIMPLIFY \u2014 The polylines will not be simplified. ", "dataType": "Boolean"}, {"name": "raster_field", "isOptional": true, "description": "The field used to assign values from the cells in the input raster to the polyline features in the output dataset. It can be an integer or a string field. ", "dataType": "Field"}]},
{"syntax": "RasterToPolygon_conversion (in_raster, out_polygon_features, {simplify}, {raster_field})", "name": "Raster to Polygon (Conversion)", "description": "Converts a raster dataset to polygon features.", "example": {"title": "RasterToPolygon example (Python window)", "description": "Converts a raster dataset to polygon features.\r\n", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.RasterToPolygon_conversion ( \"zone\" , \"c:/output/zones.shp\" , \"NO_SIMPLIFY\" , \"VALUE\" )"}, "usage": ["The input raster can have any cell size and must be a valid integer raster dataset.", "The ", "Field", " parameter allows you to choose which attribute field of the input raster dataset will become an attribute in the output feature class. If a field is not specified, the cell values of the input raster (the ", "VALUE", " field) will become a column with the heading ", "Grid_code", " in the attribute table of the output feature class."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. The raster must be integer type. ", "dataType": "Raster Layer"}, {"name": "out_polygon_features", "isOutputFile": true, "isOptional": false, "description": "The output feature class that will contain the converted polygons. ", "dataType": "Feature Class"}, {"name": "simplify", "isOptional": true, "description": "Determines if the output polygons will be smoothed into simpler shapes or conform to the input raster's cell edges. SIMPLIFY \u2014 The polygons will be smoothed into simpler shapes. This is the default. NO_SIMPLIFY \u2014 The edge of the polygons will conform to the input raster's cell edges.", "dataType": "Boolean"}, {"name": "raster_field", "isOptional": true, "description": "The field used to assign values from the cells in the input raster to the polygons in the output dataset. It can be an integer or a string field. ", "dataType": "Field"}]},
{"syntax": "RasterToPoint_conversion (in_raster, out_point_features, {raster_field})", "name": "Raster to Point (Conversion)", "description": "Converts a raster dataset to point features.", "example": {"title": "RasterToPoint example (Python window)", "description": "Converts a raster dataset to point features.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.RasterToPoint_conversion ( \"source.img\" , \"c:/output/source.shp\" , \"VALUE\" )"}, "usage": ["For each cell of the input raster dataset, a point will be created in the output feature class. The points will be positioned at the centers of cells that they represent. The NoData cells will not be transformed into points.", "The input raster can have any cell size and may be any valid raster dataset.", "The ", "Field", " parameter allows you to choose which attribute field of the input raster dataset will become an attribute in the output feature class. If a field is not specified, the cell values of the input raster (the ", "VALUE", " field) will become a column with the heading ", "Grid_code", " in the attribute table of the output feature class."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. The raster can be integer or floating-point type. ", "dataType": "Raster Layer"}, {"name": "out_point_features", "isOutputFile": true, "isOptional": false, "description": "The output feature class that will contain the converted points. ", "dataType": "Feature Class"}, {"name": "raster_field", "isOptional": true, "description": "The field to assign values from the cells in the input raster to the points in the output dataset. It can be an integer, floating point, or string field. ", "dataType": "Field"}]},
{"syntax": "RasterToFloat_conversion (in_raster, out_float_file)", "name": "Raster to Float (Conversion)", "description": "Converts a raster dataset to a file of binary floating-point values representing raster data.", "example": {"title": "RasterToFloat example (Python window)", "description": "Converts a raster dataset to a file of binary floating-point values representing raster data.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" inRaster = \"elevation\" outFloat = \"c:/output/elevation.flt\" arcpy.RasterToFloat_conversion ( \"elevation\" , \"c:/output/elevation.flt\" )"}, "usage": ["The input raster dataset can be any valid raster dataset.", "Two outputs are created: an IEEE floating-point format 32-bit signed binary file with a ", ".flt", " extension and an ASCII header file with a ", ".hdr", " extension. Both will use the same output floating-point raster file name.", "The ASCII file consists of header information containing a set of keywords.", "The format of the file in general  is:", "The definitions of the keywords are as follows:", "NCOLS", " and ", "NROWS", " are the number of columns and rows in the raster defined by the binary file.", "XLLCORNER", " and ", "YLLCORNER", " are the coordinates of the lower left corner of the lower left cell.", "The use of ", "XLLCENTER", " and ", "YLLCENTER", " is not supported by ", "Raster to Float", ".", "CELLSIZE", " is the cell size of the raster.", "NODATA_VALUE", " is the value that is to represent NoData cells.", "BYTEORDER", " represents how multibyte binary numbers are stored on the system on which the binary file was generated. On Intel-based systems, the byte order is ", "LSBFIRST", " (also known as Big Endian). On most other architectures (all UNIX systems  except Alpha, and older Macintosh  with Motorola CPUs), the byte order is ", "MSBFIRST", " (also known as Little Endian).", "The ", "NODATA_VALUE", " is the value in the output file assigned to those cells in the input raster that contain NoData. This value is normally reserved for those cells whose true value is unknown.", "By default, NoData values on the input raster will have a value of -9999 in the output float file. If you want to use another value to represent NoData, a procedure similar to the following can be applied:", "This tool only writes the origin as the lower left corner of the lower left cell. The ", "Float to Raster", " tool also supports the origin as the center of the lower left cell."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. The raster can be integer or floating-point type. ", "dataType": "Raster Layer"}, {"name": "out_float_file", "isOutputFile": true, "isOptional": false, "description": "The output floating-point raster file. The file name must have a .flt extension. ", "dataType": "File"}]},
{"syntax": "RasterToASCII_conversion (in_raster, out_ascii_file)", "name": "Raster to ASCII (Conversion)", "description": "Converts a raster dataset to an ASCII text file representing raster data.", "example": {"title": "RasterToASCII example (Python window)", "description": "Converts a raster dataset to an ASCII file representing raster data.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.RasterToASCII_conversion ( \"elevation\" , \"c:/output/sa500.asc\" )"}, "usage": ["The input raster dataset can be any valid raster dataset.", "The structure of the ASCII file consists of header information containing a set of keywords, followed by cell values in row-major order.", "The format of the file in general  is:", "The definitions of the keywords are as follows:", "NCOLS", " and ", "NROWS", " are the number of columns and rows in the raster defined by the ASCII file.", "XLLCORNER", " and ", "YLLCORNER", " are the coordinates of the lower left corner of the lower left cell.", "CELLSIZE", " is the cell size of the raster.", "NODATA_VALUE", " is the value that is to represent NoData cells.", "In the data stream of cell values, row 1 of the data is the top of the raster, row 2 is just under row 1, and so on.", "An example of an ASCII raster file is:", "The ", "NODATA_VALUE", " is the value in the ASCII file that will  represent cells that are NoData in the input raster. This value is normally reserved for those cells whose true value is unknown.", "The end of each row of data from the raster is terminated with a carriage return character in the file.", "Both integer and floating point rasters can be converted to an ASCII raster file.", "This tool only writes the origin as the lower left corner of the lower left cell.  The ", "ASCII to Raster", " tool also supports the origin as the center of the lower left cell."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. The raster can be integer or floating-point type. ", "dataType": "Raster Layer"}, {"name": "out_ascii_file", "isOutputFile": true, "isOptional": false, "description": "The output ASCII raster file. ", "dataType": "File"}]},
{"syntax": "PolygonNeighbors_analysis (in_features, out_table, {in_fields}, {area_overlap}, {both_sides}, {cluster_tolerance}, {out_linear_units}, {out_area_units})", "name": "Polygon Neighbors (Analysis)", "description": "\r\nCreates a table with statistics based on polygon contiguity (overlaps, coincident edges, or nodes). Learn more about how Polygon Neighbors works", "example": {"title": "PolygonNeighbors example (Python window)", "description": "\r\nFind each electoral districts neighbors in the province of Nova Scotia.", "code": "import arcpy , sys arcpy.env.overwriteOutput = True arcpy.MakeFeatureLayer_management ( r\"C:\\Data\\Canada\\CanadaElecDist.shp\" , r\"Canada_ElectoralDist\" ) arcpy.SelectLayerByAttribute_management ( r\"Canada_ElectoralDist\" , \"NEW_SELECTION\" , \" \\\" PROVCODE \\\"  = 'NS'\" ) print \"Selected feature count: \" + str ( arcpy.GetCount_management ( r\"Canada_ElectoralDist\" )) arcpy.PolygonNeighbors_analysis ( r\"Canada_ElectoralDist\" , \"ENNAME\" , r\"C:\\Data\\Output\\NS_elec_neigh.dbf\" ) print arcpy.GetMessages ()"}, "usage": ["The tool analyzes polygon contiguity, summarizing the ", "Only first-order contiguity is analyzed and reported by the tool;  relationships beyond that are not examined;  that is, neighbors of neighbors (second-order contiguity) are not examined.", "The ", "Report By Field(s)", "  parameter (", "in_fields", " in scripting) is used to identify unique polygons or polygon groups and report their neighboring information by the polygons or polygon groups.   To find the neighbors of each individual polygon in the input, specify one or more input fields that result in a unique value or set of values (in the case of using multiple fields) for every polygon.", "If the specified fields identify unique polygon groups, the neighboring information is summarized and reported by the groups.", "  See ", "How Polygon Neighbors works", " for details on the use of this parameter.", "Include area overlap", " can be used if you want area overlapping relationships to be analyzed.   When ", "Include area overlap", " is selected, the output table contains a field called AREA that holds the area of overlap for the overlapping neighbor being analyzed.  If no overlap is found, AREA is 0.  ", "Calculating area overlapping relationships is a high-resource operation, and performance could be much slower than determining coincident edge and node neighbor relationships. If you know your data has no overlapping features or you are not concerned about analyzing overlapping neighbors, be sure the ", "Include area overlap", " check box is unchecked on the tool dialog box or the ", "area_overlap", " parameter is set to NO_AREA_OVERLAP in your script.", "The ", "Include area overlap", " check box must be checked in the dialog box, or the ", "area_overlap", " parameter must be set to AREA_OVERLAP in a script in order to obtain records for neighbors that are completely contained by a source polygon.  If you do not have the ", "Include area overlap", " check box checked, the output table will not contain records for neighbors that are completely contained in a source polygon.", "There is no entry in the output table for features that are not neighbors.", "The ", "Include both sides of neighbor relationship", " check box on the tool dialog box (the ", "both_sides", " parameter in scripting) is used to control the relationships included in the output.  To report all contiguity relationships, including reciprocal relationships, check ", "Include both sides of neighbor relationship", " (", "both_sides", " = \"BOTH_SIDES\").  For example, if OID1 is a neighbor of OID2, an entry is written to the output table for OID1 having a neighbor OID2 and for OID2 having a neighbor OID1.  If you only want the first side of the relationship, uncheck ", "Include both sides of neighbor relationship", ". Using the example above, but with ", "Include both sides of neighbor relationship", " unchecked, only the entry for OID1 having a neighbor OID2 is entered into the output table.", "Output Linear Units", " specifies the units to use for shared boundary length between neighbors. The default is to use the same units as defined by the input feature coordinate system.", " ", "Output Area Units", " is only used when the ", "Area Overlap", " parameter is checked (", "area_overlap", " = \" = \"AREA_OVERLAP\"). When", " Area Overlap", " is checked, the units used to calculate the area overlap of neighbors are specified in the  ", "Output Area Units", " parameter.  The default is to use the same units as defined by the input feature's coordinate system.", "The output table contains the following fields:", "If there is a selection set on the input features, only selected features are analyzed."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input polygon features. ", "dataType": "Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": " The output table. ", "dataType": "Table"}, {"name": "in_fields", "isInputFile": true, "isOptional": false, "description": " Input attribute field or fields used to identify unique polygons or polygon groups and represent them in the output. ", "dataType": "Field"}, {"name": "area_overlap", "isOptional": true, "description": "Determines if overlapping polygons will be analyzed and reported in the output. NO_AREA_OVERLAP \u2014 Overlapping relationships will not be analyzed and included in the output. This is the default. AREA_OVERLAP \u2014 Overlapping relationships will be analyzed and included in the output.", "dataType": "Boolean"}, {"name": "both_sides", "isOptional": true, "description": "Determines if both sides of neighbor relationships will be included in the output. BOTH_SIDES \u2014 For a pair of neighboring polygons, report both neighboring information of one polygon being the source and the other being the neighbor and vice versa. This is the default. NO_BOTH_SIDES \u2014 For a pair of neighboring polygons, only report neighboring information of one polygon being the source and the other being the neighbor. Do not include the reciprocal relationship.", "dataType": "Boolean"}, {"name": "cluster_tolerance", "isOptional": true, "description": " The minimum distance between coordinates before they are considered equal. By default, this is the XY Tolerance of the input features. ", "dataType": "Linear unit"}, {"name": "out_linear_units", "isOutputFile": true, "isOptional": true, "description": " Units used to report the total length of the coincident edge between neighboring polygons. The default is the input feature units. ", "dataType": "String"}, {"name": "out_area_units", "isOutputFile": true, "isOptional": true, "description": "Units used to report the area overlap of neighboring polygons. The default is the input feature units. This parameter is only enabled when the area_overlap parameter is set to AREA_OVERLAP. ", "dataType": "String"}]},
{"syntax": "TabulateIntersection_analysis (in_zone_features, zone_fields, in_class_features, out_table, {class_fields}, {sum_fields}, {xy_tolerance}, {out_units})", "name": "Tabulate Intersection (Analysis)", "description": "\r\nComputes the intersection between two feature classes and cross-tabulates the area, length, or count of the  intersecting features.", "example": {"title": "TabulateIntersection example 1 (Python window)", "description": "Using TabulateIntersection in the Python window to find the area of each vegetation type in each zone.", "code": "arcpy.TabulateIntersection_analysis ( \"Zones\" , \"zone_id\" , \"Vegetation\" , r\"C:\\Esri\\veganalysis.gdb\\vegtypeAreas\" , \"VEGTYPE\" )"}, "usage": ["\r\nA zone is comprised of all features in the ", "Input Zone Features", " that have the same values in the ", "Zone Field(s)", ". Similarly, a class is comprised of all features in the ", "Input Class Features", " that have the same values in the ", "Class Field(s)", ". Features do not have to be contiguous to be in the same zone or class. This tool calculates how much of the zone is intersected by each class (area and percentage of zone area).\r\n", "If no ", "Class Field(s)", " is specified, all features in the ", "Input Class Features", " will be considered a single class.  The ", "Output Table", " will contain one record for each zone.", "If ", "Class Field(s)", " is specified, the ", "Output Table", " will contain ", "n", " records for each zone, where ", "n", " is the number of classes within that zone.  For example, if a zone contains four classes, the ", "Output Table", " will have four records for that zone.", "Numeric attributes from the ", "Input Class Features", " can be summed by zone using the ", "Sum Fields", " parameter.  The sum values for a  class represent a  proportion of the sum values based on the percentage of the class intersecting the zone (similar to how a ", "Ratio Policy", " works).", "Only fields with absolute  values (not relative, normalized values such as percentages or densities) should be used as ", "Sum Fields", ", since the values may be split and apportioned to different zones.", "Using higher dimension ", "Input Class Features", " than the ", "Input Zone Features", " is not supported. Unsupported combinations:", "When the ", "Input Zone Features", " and ", "Input Class Features", " are polygons, the output table statistics will be based on area calculations.", "When the ", "Input Class Features", " are lines, the output table statistics will be based on linear calculations.", "When the ", "Input Class Features", " are points, the output table statistics will be based on feature count.", "When the ", "Input Zone Features", " and the ", "Input Class Features", " are the same dimension (both polygon, both line, or both point), the output PERCENTAGE field records the percentage of the zone feature that  is intersected by the class.  ", "If the ", "Input Zone Features", " and the ", "Input Class Features", " are different dimensions (polygon zone with line class, polygon zone with point class, or line zone with point class), the output PERCENTAGE field records the percentage of the class intersecting the zone polygon.   ", "It is possible for the PERCENTAGE field to record a percent value greater than 100 percent when there are overlapping features in the ", "Input Zone Features", " or the ", "Input Class Features", ".", "The AREA field is included in the output table only when the ", "Input Zone Features", " and ", "Input Class Features", " are polygon.  The AREA field contains the  area of the ", "Input Zone Features", " that the ", "Input Class Features", " intersect.", "A LENGTH field is included in the output table when the ", "Input Class Features", " are lines. The LENGTH field contains the length of intersection between the ", "Input Zone Features", " and the ", "Input Class Features", ".     ", "A PNT_COUNT field is included in the output table when the ", "Input Class Features", " are points.  The PNT_COUNT field contains the count of the number of ", "Input Class Features", " points that intersect the ", "Input Zone Features", ".", "When using feature layers, if any features are selected, only the selected features are used in calculations.", "Determining the intersection of zone and class features is done following the same rules as the ", "Intersect", " tool.", "Use the ", "Pivot Table", " tool to transform the ", "Output Table", " into a table that contains one record for each zone with class attributes as separate attribute fields. Fill in the parameters for the ", "Pivot Table", " tool as follows:"], "parameters": [{"name": "in_zone_features", "isInputFile": true, "isOptional": false, "description": "The features used to identify zones. ", "dataType": "Feature Layer"}, {"name": "zone_fields", "isOptional": false, "description": " The attribute field or fields that will be used to define zones. ", "dataType": "Field"}, {"name": "in_class_features", "isInputFile": true, "isOptional": false, "description": " The features used to identify classes. ", "dataType": "Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": " The table that will contain the cross-tabulation of intersections between zones and classes. ", "dataType": "Table"}, {"name": "class_fields", "isOptional": false, "description": " The attribute field or fields used to define classes. ", "dataType": "Field"}, {"name": "sum_fields", "isOptional": false, "description": " The fields to sum from the Input Class Features . ", "dataType": "Field"}, {"name": "xy_tolerance", "isOptional": true, "description": " The distance that determines the range in which features or their vertices are considered equal. By default, this is the XY Tolerance of the Input Zone Features . ", "dataType": "Linear Unit"}, {"name": "out_units", "isOutputFile": true, "isOptional": true, "description": " Units to be used to calculate area or length measurements. Setting Output Units when the Input Class Features are points is not supported. ", "dataType": "String"}]},
{"syntax": "Statistics_analysis (in_table, out_table, statistics_fields, {case_field})", "name": "Summary Statistics (Analysis)", "description": "Calculates summary statistics for field(s) in a table.", "example": {"title": "Statistics Example (Python Window)", "description": "The following Python window script demonstrates how to use the Statistics tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Habitat_Analysis.gdb\" arcpy.Statistics_analysis ( \"futrds\" , \"C:/output/output.gdb/stats\" , [[ \"Shape_Length\" , \"SUM\" ]], \"NM\" )"}, "usage": ["The ", "Output Table", " will consist of fields containing the result of the statistical operation.", "The following statistical operations are available with this tool: Sum, Mean, Maximum, Minimum, Range, Standard Deviation, Count, First, and Last. The Median operation is not available.", "A field will be created for each statistic type using the following naming convention: SUM_FIELD, MAX_FIELD, MIN_FIELD, RANGE_FIELD, STD_FIELD, FIRST_FIELD, LAST_FIELD, COUNT_FIELD. The field name is truncated to 10 characters when the Output Table is a dBASE table.", "If a ", "Case field", " is specified, statistics will be calculated separately for each unique attribute value. The ", "Output Table", " will contain only one record if no ", "Case field", " is specified. If one is specified, there will be one record for each ", "Case field", " value.", "Null values are excluded from all statistical calculations. For example, the AVERAGE of 10, 5 and NULL is 7.5 ((10+5)/2). The COUNT tool returns the number of values included in the statistical calculation, which in this case is 2.", "The ", "Statistics Field(s)", " parameter ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Statistics Field(s)", " parameter may not be populated with field names. The ", "Add Field", " button allows you to add expected field(s) so you can complete the Summary Statistics dialog and continue to build your model.", "When using layers, only the currently selected features are used to calculate statistics."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input table containing the field(s) that will be used to calculate statistics. The input can be an INFO table, a dBASE table, an OLE DB table, a VPF table, or a feature class. ", "dataType": "Table View; Raster Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output dBASE or geodatabase table that will store the calculated statistics. ", "dataType": "Table"}, {"name": "statistics_fields", "isOptional": false, "description": "The numeric field containing attribute values used to calculate the specified statistic. Multiple statistic and field combinations may be specified. Null values are excluded from all statistical calculations. The Add Field button, which is used only in ModelBuilder, allows you to add expected field(s) so you can complete the dialog box and continue to build your model. Available statistics types are: SUM\u2014Adds the total value for the specified field. MEAN\u2014Calculates the average for the specified field. MIN\u2014Finds the smallest value for all records of the specified field. MAX\u2014Finds the largest value for all records of the specified field. RANGE\u2014Finds the range of values (MAX minus MIN) for the specified field. STD\u2014Finds the standard deviation on values in the specified field. COUNT\u2014Finds the number of values included in statistical calculations. This counts each value except null values. To determine the number of null values in a field, use the COUNT statistic on the field in question, and a COUNT statistic on a different field which does not contain nulls (for example, the OID if present), then subtract the two values. FIRST\u2014Finds the first record in the Input Table and uses its specified field value. LAST\u2014Finds the last record in the Input Table and uses its specified field value.", "dataType": "Value Table"}, {"name": "case_field", "isOptional": false, "description": "The fields in the Input Table used to calculate statistics separately for each unique attribute value (or combination of attribute values when multiple fields are specified). ", "dataType": "Field"}]},
{"syntax": "Frequency_analysis (in_table, out_table, frequency_fields, {summary_fields})", "name": "Frequency (Analysis)", "description": "Reads a table and a set of fields and creates a new table containing unique field values and the number of occurrences of each unique field value.", "example": {"title": "Frequency Example (Python Window)", "description": "The following Python window script demonstrates how to use the Frequency function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Portland.gdb/Taxlots\" arcpy.Frequency_analysis ( \"taxlots\" , \"C:/output/output.gdb/tax_frequency\" ,[ \"YEARBUILT\" , \"COUNTY\" ], [ \"LANDVAL\" , \"BLDGVAL\" , \"TOTALVAL\" ])"}, "usage": ["The output table will contain the field ", "Frequency", " and the specified frequency field(s) and summary field(s).", "The output table will contain the frequency calculation for each attribute value combination of the specified frequency field(s).", "If a summary field is specified, the unique attribute values of the frequency calculation are summarized by the numeric attribute values of each summary field.", "When using layers, only the currently selected features are used in calculations."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table containing the field(s) that will be used to calculate frequency statistics. This table can be an INFO or OLE DB table, a dBASE or a VPF table, or a feature class table. ", "dataType": "Table View; Raster Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table that will store the calculated frequency statistics. ", "dataType": "Table"}, {"name": "frequency_fields", "isOptional": false, "description": "The attribute field or fields that will be used to calculate frequency statistics. ", "dataType": "Field"}, {"name": "summary_fields", "isOptional": false, "description": "The attribute field or fields to sum and add to the output table. Null values are excluded from this calculation. ", "dataType": "Field"}]},
{"syntax": "PointDistance_analysis (in_features, near_features, out_table, {search_radius})", "name": "Point Distance (Analysis)", "description": "Determines the distances from input point features to all points in the near features within a specified search radius.", "example": {"title": "PointDistance example 1 (Python window)", "description": "The following Python interactive window script demonstrates how to use the PointDistance function in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/pointdistance.gdb\" arcpy.PointDistance_analysis ( \"police_stations\" , \"crime_location\" , \"crime_distances\" )"}, "usage": ["The tool creates a table with distances between two sets of points. if the default search radius is used, distances from all input points to all near points are calculated. The output table can be quite large. For example, if both input and near features have 1,000 points each, then the output table can contain one  million records.", "Use a meaningful search radius to limit the size of the output and improve tool performance. The output table contains only those records that have a near point within the search radius.", "The results are recorded in the output table containing the following information:", "Both ", "Input Features", " and ", "Near Features", " can be the same dataset. In that case, when the input and near features are the same record, that result will be skipped so as not to report that each feature is 0 units from itself."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The point features from which distances to the near features will be calculated. ", "dataType": "Feature Layer"}, {"name": "near_features", "isOptional": false, "description": "The points to which distances from the input features will be calculated. Distances between points within the same feature class or layer can be determined by specifying the same feature class or layer for the input and near features. ", "dataType": "Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table containing the list of input features and information about all near features within the search radius. If a search radius is not specified, distances from all input features to all near features are calculated. ", "dataType": "Table"}, {"name": "search_radius", "isOptional": true, "description": "Specifies the radius used to search for candidate near features. The near features within this radius are considered for calculating the nearest feature. If no value is specified (that is, the default (empty) radius is used) all near features are considered for calculation. The unit of search radius defaults to units of the input features. The units can be changed to any other unit. However, this has no impact on the units of the output DISTANCE field which is based on the units of the coordinate system of the input features. ", "dataType": "Linear unit"}]},
{"syntax": "Near_analysis (in_features, near_features, {search_radius}, {location}, {angle})", "name": "Near (Analysis)", "description": "Determines the distance from each feature in the input features to the nearest feature in the near features, within the search radius.", "example": {"title": "Near example 1 (Python window)", "description": "The following Python interactive window script demonstrates how to use the Near function in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/city.gdb\" ## find the nearest road from each house arcpy.Near_analysis ( 'houses' , 'roads' )"}, "usage": ["The following two fields will be added to the attribute table of the input features. The field values are updated if the fields already exist.", "The values for NEAR_FID and NEAR_DIST will be -1 if no feature is found within the search radius.", "Optionally, NEAR_X, NEAR_Y, NEAR_ANGLE, and NEAR_FC fields can also be added to the attribute table of input features as explained in the   near features and optional parameters entries. The value of a field is updated if the fields already exist. If no feature is found within the search radius the values of these fields will be -1 for NEAR_X and NEAR_Y, 0 for NEAR_ANGLE, and null for NEAR_FC.", "Both input features and near features can be point, multipoint, line, or polygon.", "The ", "Near Features", " can include one or more feature classes of different shape types.", "The same dataset can be used as both  ", "Input Features", " and ", "Near Features", ". When an input feature's nearest feature is itself (NEAR_DIST is 0), this feature is ignored from the calculation and the next nearest feature is searched.", "The input features can be a layer on which you have performed a selection. The selected features will be used and updated during the execution of the tool. The remaining features will have the values of the newly created fields (such as NEAR_FID and NEAR_DIST) set to -1.", "The distances calculated by this tool are in the unit of the coordinate system of the input features. If your input is in a geographic coordinate system and you want output distances to be measured in a linear unit (as opposed to decimal degrees), you must first project your input to a projected coordinate system using the Project tool. For best results, use an equidistant projection or a projection intended for your study area (UTM, for example).", "When more than one near features has the same shortest distance from an input feature, one of them is randomly chosen as the nearest feature."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features that can be point, polyline, polygon, or multipoint type. ", "dataType": "Feature Layer"}, {"name": "near_features", "isOptional": false, "description": "Value used to find the nearest features from input features. There can be one or more entries of near features; each entry can be of point, polyline, polygon, or multipoint type. When multiple entries of near features are specified, a new field, NEAR_FC, is added to the input table to store the paths of the source feature class that contains the nearest features. ", "dataType": "Feature Layer"}, {"name": "search_radius", "isOptional": true, "description": "Specifies the radius used to search for candidate near features. The near features within this radius are considered for calculating the nearest feature. If no value is specified, that is, the default (empty) radius is used, all near features are considered for calculation. The unit of the search radius defaults to the units of the coordinate system of the input features. The units can be changed to any other unit. However, this has no impact on the units of NEAR_DIST which is based on the units of the coordinate system of the input features. ", "dataType": "Linear unit"}, {"name": "location", "isOptional": true, "description": "Specifies whether x- and y-coordinates of the nearest location of the near feature will be written to new fields NEAR_X and NEAR_Y, respectively. NO_LOCATION \u2014 Specifies that the x- and y-coordinates of the nearest location will not be written. This is the default. LOCATION \u2014 Specifies that the x- and y-coordinates of the nearest location will be written to NEAR_X and NEAR_Y fields.", "dataType": "Boolean"}, {"name": "angle", "isOptional": true, "description": "Specifies whether the near angle values in decimal degrees will be calculated and written to a new field, NEAR_ANGLE. A near angle measures from the x-axis (horizontal axis) to the direction of the line connecting an input feature to its nearest feature at their closest locations, and it is within the range of 0 to 180 or 0 to -180 decimal degrees - 0 to the east, 90 to the north, 180 (-180\u00b0) to the west, and -90 to the south. NO_ANGLE \u2014 Specifies that the near angle values will not be written. This is the default. ANGLE \u2014 Specifies that the near angle values will be written to the NEAR_ANGLE field.", "dataType": "Boolean"}]},
{"syntax": "MultipleRingBuffer_analysis (Input_Features, Output_Feature_class, Distances, {Buffer_Unit}, {Field_Name}, {Dissolve_Option}, {Outside_Polygons_Only})", "name": "Multiple Ring Buffer (Analysis)", "description": "Creates multiple buffers at specified distances around the input features. These buffers can optionally be merged and dissolved using the buffer distance values to create non-overlapping buffers.", "example": {"title": "MultipleRingBuffer Example (Python Window)", "description": "The following Python window script demonstrates how to use the MultipleRingBuffer tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/airport.gdb\" arcpy.MultipleRingBuffer_analysis ( \"schools\" , \"c:/output/output.gdb/multibuffer1\" , [ 10 , 20 , 30 ], \"meters\" , \"\" , \"ALL\" )"}, "usage": ["The ", "Buffer Unit", " parameter is ignored if the ", "Input Features", " do not have a defined spatial reference.", "If the ", "Dissolve Option", " ", "ALL", " is used, the output feature class will contain one feature for each distance specified in the ", "Distances", " parameter (all buffers the same distance from the Input Features will be dissolved).", "The tool creates intermediate data which is written to the location specified in the ", "Scratch_workspace", " environment setting. If the Scratch workspace environment is not set, the temporary data is written to the location defined by the TEMP system variable.", "This tool is a Python script tool. The script can be viewed by right-clicking the tool and selecting ", "Edit", ".", "For more information about buffers, see the ", "Buffer", " tool."], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": "The input point, line, or polygon features to be buffered. ", "dataType": "Feature Layer"}, {"name": "Output_Feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class that will contain multiple buffers. ", "dataType": "Feature Class"}, {"name": "Distances", "isOptional": false, "description": "The list of buffer distances. ", "dataType": "Double"}, {"name": "Buffer_Unit", "isOptional": true, "description": "The linear unit to be used with the Distance values. If the units are not specified, or entered as 'Default', the linear unit of the input features' spatial reference is used. If the Buffer Unit is specified as 'Default' and the Output Coordinate System geoprocessing environment has been set, its linear unit will be used. ", "dataType": "String"}, {"name": "Field_Name", "isOptional": true, "description": "The name of the field in the output feature class that stores the buffer distance used to create each buffer feature. If no name is specified, the default field name is 'distance'. This field will be of type Double. ", "dataType": "String"}, {"name": "Dissolve_Option", "isOptional": true, "description": "Determines if buffers will be dissolved to resemble rings around the input features. ALL \u2014 Buffers will be rings around the input features that do not overlap (think of these as rings or donuts around the input features). The smallest buffer will cover the area of its input feature plus the buffer distance, and subsequent buffers will be rings around the smallest buffer which do not cover the area of the input feature or smaller buffers. All buffers of the same distance will be dissolved into a single feature. This is the default. NONE \u2014 All buffer areas will be maintained regardless of overlap. Each buffer will cover its input feature plus the area of any smaller buffers.", "dataType": "String"}, {"name": "Outside_Polygons_Only", "isOptional": true, "description": "Valid only for polygon input features. FULL \u2014 Buffers will overlap or cover the input features. This is the default. OUTSIDE_ONLY \u2014 Buffers will be rings around the input features, and will not overlap or cover the input features (the area inside the input polygon will be erased from the buffer).", "dataType": "Boolean"}]},
{"syntax": "GenerateNearTable_analysis (in_features, near_features, out_table, {search_radius}, {location}, {angle}, {closest}, {closest_count})", "name": "Generate Near Table (Analysis)", "description": "Determines the distances from each feature in the input features to one or more nearby features in the near features, within the search radius. The results are recorded in the output table.", "example": {"title": "GenerateNearTable example 1 (Python window)", "description": "The following snippet demonstrates how to use the GenerateNearTable function in the Python window.", "code": "import arcpy arcpy.env.workspace = \"C:/data/input/gnt.gdb\" arcpy.GenerateNearTable_analysis ( \"campsites\" , [ \"parks\" , \"trails\" ], \"better_sites\" )"}, "usage": ["This tool behaves the same as the ", "Near", " tool. However, instead of updating the input features, it creates a new output table. Moreover, it can find as many near features as specified by the ", "Maximum number of closest matches", " parameter.", "The output table contains three fields\u2014IN_FID, NEAR_FID, and NEAR_DIST\u2014by default. Additional fields are added to output depending on the optional parameters selected as explained in the parameter entry.", "The output table can be joined back to the input feature class or a near feature class using the IN_FID or NEAR_FID fields.", "Both input features and near features can be point, multipoint, line, or polygon.", "The default option for this tool is to find the distance from each input feature to the closest near feature. Choose the ALL option; that is, uncheck the ", "Find only closest feature", " parameter to create a table containing the distance between all input to all near features.", "The values for NEAR_FID and NEAR_DIST will be -1 if no feature is found within the search radius.", "If no value for ", "Search Radius", " is specified, a radius is used large enough so that all near features can be incorporated in the distance calculation. If the default search radius is used (no radius is specified) the output table can be quite large. For example, calculating distances from 1,000 points in one feature class to 1,000 points in another feature class can produce an output table containing 1 million records. Use the search radius to limit the number of output records.", "Both ", "Input Features", " and ", "Near Features", " can be the same dataset. In that case, when the input and near features are the same record, that result will be skipped so as not to report that each feature is 0 units from itself.", "The input features can be a layer on which you have performed a selection. The selected features will be used and updated during the execution of the tool. The remaining features will have the values of the newly created fields (such as NEAR_FID and NEAR_DIST) set to -1.", "The distances calculated by this tool are in the unit of the coordinate system of the input features. If your input is in a geographic coordinate system and you want output distances to be measured in a linear unit (as opposed to decimal degrees), you must first project your input to a projected coordinate system using the Project tool. For best results, use an equidistant projection or a projection intended for your study area (UTM, for example)."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features that can be point, polyline, polygon, or multipoint type. ", "dataType": "Feature Layer"}, {"name": "near_features", "isOptional": false, "description": "Value used to find the nearest features from input features. There can be one or more entries of near features; each entry can be of point, polyline, polygon, or multipoint type. When multiple entries of near features are specified, a new field, NEAR_FC, is added to the input table to store the paths of the source feature class that contains the nearest features. ", "dataType": "Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output table that will contain the proximity information\u2014such as IN_FID, NEAR_FID, and NEAR_DIST\u2014and other attributes\u2014such as location (NEAR_X, NEAR_Y) and angle (NEAR_ANGLE)\u2014of the near feature and the NEAR_FC, if necessary. ", "dataType": "Table"}, {"name": "search_radius", "isOptional": true, "description": "Specifies the radius used to search for candidate near features. The near features within this radius are considered for calculating the nearest feature. If no value is specified, that is, the default (empty) radius is used, all near features are considered for calculation. The unit of the search radius defaults to the units of the coordinate system of the input features. The units can be changed to any other unit. However, this has no impact on the units of NEAR_DIST which is based on the units of the coordinate system of the input features. ", "dataType": "Linear unit"}, {"name": "location", "isOptional": true, "description": "Specifies whether x- and y-coordinates of the nearest location of the near feature will be written to new fields NEAR_X and NEAR_Y, respectively. NO_LOCATION \u2014 Specifies that the x- and y-coordinates of the nearest location will not be written. This is the default. LOCATION \u2014 Specifies that the x- and y-coordinates of the nearest location will be written to NEAR_X and NEAR_Y fields.", "dataType": "Boolean"}, {"name": "angle", "isOptional": true, "description": "Specifies whether the near angle values in decimal degrees will be calculated and written to a new field, NEAR_ANGLE. A near angle measures from the x-axis (horizontal axis) to the direction of the line connecting an input feature to its nearest feature at their closest locations, and it is within the range of 0 to 180 or 0 to -180 decimal degrees - 0 to the east, 90 to the north, 180 (-180\u00b0) to the west, and -90 to the south. NO_ANGLE \u2014 Specifies that the near angle values will not be written. This is the default. ANGLE \u2014 Specifies that the near angle values will be written to the NEAR_ANGLE field.", "dataType": "Boolean"}, {"name": "closest", "isOptional": true, "description": "Determines whether to locate and return only the closest features or all the features within the search radius. CLOSEST \u2014 Locate and return only the closest features from the near features to the input features within the search radius. This is the default. ALL \u2014 Locate and return all features from the near features to the input features within the search radius.", "dataType": "Boolean"}, {"name": "closest_count", "isOptional": true, "description": "Find only the specified number of closest features. This parameter will not be used if the Find only closest feature option is checked. ", "dataType": "Long"}]},
{"syntax": "CreateThiessenPolygons_analysis (in_features, out_feature_class, {fields_to_copy})", "name": "Create Thiessen Polygons (Analysis)", "description": "Creates Thiessen polygons from point input features. Each Thiessen polygon contains only a single point input feature. Any location within a Thiessen polygon is closer to its associated point than to any other point input feature.", "example": {"title": "CreateThiessenPolygons Example (Python Window)", "description": "The following Python window script demonstrates how to use the CreateThiessenPolygons tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/data.gdb\" arcpy.CreateThiessenPolygons_analysis ( \"schools\" , \"c:/output/output.gdb/thiessen1\" , \"ALL\" )"}, "usage": ["This tool is used to divide the area covered by the point input features into Thiessen or proximal zones. These zones represent full areas where any location within the zone is closer to its associated input point than to any other input point.", "The theoretical background for creating Thiessen polygons is as follows:", "Thiessen proximal polygons are constructed as follows:", "The outside boundary of the output Thiessen polygon feature class is the extent of the point input features plus an additional 10%. If the ", "Extent", " environment is set to a specific extent window, this tool  tool will use the environment setting to set its outside boundary.", "This tool may produce unexpected results with data in a geographic coordinate system since the Delaunay triangulation method used by the tool works best with data in a projected coordinate system. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The point input features from which Thiessen polygons will be generated. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class containing the Thiessen polygons that are generated from the point input features. ", "dataType": "Feature Class"}, {"name": "fields_to_copy", "isOptional": true, "description": "Determines which attributes from the point input features will be transferred to the output feature class. ONLY_FID \u2014 Only the FID field from the input features will be transferred to the output feature class. This is the default. ALL \u2014 All attributes from the input features will be transferred to the output feature class. ", "dataType": "String"}]},
{"syntax": "Buffer_analysis (in_features, out_feature_class, buffer_distance_or_field, {line_side}, {line_end_type}, {dissolve_option}, {dissolve_field})", "name": "Buffer (Analysis)", "description": "Creates buffer polygons around input features to a specified distance. \r\n Learn more about how Buffer works \r\n", "example": {"title": "Buffer example (Python window)", "description": "The following Python window script demonstrates how to use the Buffer tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.Buffer_analysis ( \"roads\" , \"C:/output/majorrdsBuffered\" , \"100 Feet\" , \"FULL\" , \"ROUND\" , \"LIST\" , \"Distance\" )"}, "usage": [" If buffering a ", "projected", " feature class that has features covering a large region, or you are using a very large buffer distance, ", "distortions", " in the projection can cause inaccurate buffers to be produced. You can completely avoid distortion when buffering by using a feature class that has a ", "geographic coordinate system", " and specifying a ", "Buffer Distance", " in linear units (meters, feet, and so forth, as opposed to angular units such as degrees).  When this combination of inputs is used, the tool will generate true ", "geodesic", " buffers that accurately represent distances on Earth's surface. Geodesic buffers may appear unusual on a flat map, but when displayed on a globe these buffers will look correct (you can use the ", "ArcGlobe", " or ", "ArcGIS Explorer", " applications to view geographic data on a three-dimensional globe). For more information, see ", "How Buffer works", ".", "You can change the coordinate system of a feature class using the ", "Project", " tool, or you can set the ", "Output Coordinate System", " geoprocessing environment before executing the ", "Buffer", " tool, and this coordinate system will be used in creating buffers.", "Improve the accuracy of buffers created with projected inputs by using a projection that minimizes distance distortion, such as an ", "Equidistant Conic", " or an ", "Azimuthal Equidistant", " projection, and is geographically appropriate for your input.  ", "When buffering features in a projected coordinate system with output to a geodatabase feature class, the geometries created will often contain ", "circular arc", " segments, especially when buffering points.  If these buffers that contain circular arcs are re-projected to a different coordinate system, the location and size of the original buffers will be transformed, but the shape of the buffers will not change, causing the re-projected buffers to no longer accurately represent the area covered by the original buffer. If you wish to re-project buffers containing circular arcs, first use the ", "Densify", " tool to convert circular arc segments to straight lines, then re-project the buffers.", "The output feature class will have a field, ", "BUFF_DIST", ", that contains the buffer distance used to buffer each feature, in the linear unit of the input features' coordinate system. If a  ", "Dissolve Type", " of ", "ALL", " or ", "LIST", " is used, the output will not have this field.", "When buffering polygon features, negative buffer distances can be used to create buffers inside the polygon features. Using a negative buffer distance will reduce the polygons' boundaries by the distance specified.", "If the negative buffer distance is large enough to collapse the polygon to nothing, a null geometry will be generated. A warning message will be given, and any null geometry features will not be written to the output feature class.", "If a field from the Input Features is used to obtain  buffer distances, the field's values can be either a number (", "5", ") or a number with a valid linear unit (", "5 Kilometers", "). If a field value is simply a number, it is assumed that the distance is in the linear unit of the Input Features' spatial reference (unless the Input Features are in a geographic coordinate system, in which case, the value is assumed to be in ", "meters", ").  If the linear unit specified in the field values is invalid or not recognized, the linear unit of the input features' spatial reference will be used by default.", "The ", "Dissolve Field(s)", " parameter ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run or its derived data does not exist, the ", "Dissolve Field(s)", " parameter may not be populated with field names. The Add Field button allows expected fields to be added to the ", "Dissolve Field(s)", " list in order to complete the Buffer tool dialog box.", "Side Type", " (line_side) options LEFT, RIGHT, and OUTSIDE_ONLY and the ", "End Type", " (line_end_type) option FLAT are only available with an ", "Advanced", " license."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point, line, or polygon features to be buffered. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class containing the output buffers. ", "dataType": "Feature Class"}, {"name": "buffer_distance_or_field", "isOptional": false, "description": "The distance around the input features that will be buffered. Distances can be provided as either a value representing a linear distance or as a field from the input features that contains the distance to buffer each feature. If linear units are not specified or are entered as Unknown, the linear unit of the input features' spatial reference is used. When specifying a distance in scripting, if the desired linear unit has two words, like Decimal Degrees, combine the two words into one (for example, '20 DecimalDegrees'). ", "dataType": "Linear unit ;Field"}, {"name": "line_side", "isOptional": true, "description": "The side(s) of the input features that will be buffered. This optional parameter is not available with a Basic or Standard license. FULL \u2014 For line input features, buffers will be generated on both sides of the line. For polygon input features, buffers will be generated around the polygon and will contain and overlap the area of the input features. For point input features, buffers will be generated around the point. This is the default. LEFT \u2014 For line input features, buffers will be generated on the topological left of the line. This option is not valid for polygon input features. RIGHT \u2014 For line input features, buffers will be generated on the topological right of the line. This option is not valid for polygon input features. OUTSIDE_ONLY \u2014 For polygon input features, buffers will be generated only outside the input polygon (the area inside the input polygon will be erased from the output buffer). This option is not valid for line input features.", "dataType": "String"}, {"name": "line_end_type", "isOptional": true, "description": "The shape of the buffer at the end of line input features. This parameter is not valid for polygon input features. This optional parameter is not available with a Basic or Standard license. ROUND \u2014 The ends of the buffer will be round, in the shape of a half circle. This is the default. FLAT \u2014 The ends of the buffer will be flat, or squared, and will end at the endpoint of the input line feature. ", "dataType": "String"}, {"name": "dissolve_option", "isOptional": true, "description": "Specifies the dissolve to be performed to remove buffer overlap. NONE \u2014 An individual buffer for each feature is maintained, regardless of overlap. This is the default. ALL \u2014 All buffers are dissolved together into a single feature, removing any overlap. LIST \u2014 Any buffers sharing attribute values in the listed fields (carried over from the input features) are dissolved. ", "dataType": "String"}, {"name": "dissolve_field", "isOptional": false, "description": "The list of field(s) from the input features on which to dissolve the output buffers. Any buffers sharing attribute values in the listed fields (carried over from the input features) are dissolved. ", "dataType": "Field"}]},
{"syntax": "Update_analysis (in_features, update_features, out_feature_class, {keep_borders}, {cluster_tolerance})", "name": "Update (Analysis)", "description": "Computes a geometric intersection of the  Input Features  and  Update Features . The attributes and geometry of the input features are updated by the update features in the output feature class.", "example": {"title": "Update Example (Python Window)", "description": "The following Python window script demonstrates how to use the Update function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Update_analysis ( \"city_lots.shp\" , \"data.gdb/flood_levels\" , \"data.gdb/low_lots\" , \"DROP_BORDER\" , 0.0003 )"}, "usage": ["The ", "Input Features", " must be of type polygon.", "The input feature class will not be modified by this tool. The results of the tool will be written to a new feature class.", "The ", "Update Features", " must be polygon.", "The input feature class and update feature class field names must match.", "If the update feature class is missing one (or many) of the fields that are present in the input feature class, the input feature class field value for the missing field(s) will be removed from the output feature class. ", "If the ", "Borders", " parameter is unchecked in the dialog box (or set to NO_BORDERS in scripting), the polygon boundaries along the outer edge of the update features will be dropped. Even though the outer boundaries of some update polygons are dropped, the attributes of the update features that overlap input features will be assigned to the polygons in the output feature class.", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "This tool may generate multipart features in the output even if all inputs were single part. If multipart features are not desired, use the ", "Multipart to Singlepart tool", " on the output feature class."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. Geometry type must be polygon. ", "dataType": "Feature Layer"}, {"name": "update_features", "isOptional": false, "description": "The features that will be used to update the Input Features. Geometry type must be polygon. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to contain the results. Do not set this to be the same as the Input Features. ", "dataType": "Feature Class"}, {"name": "keep_borders", "isOptional": true, "description": "Specifies whether the boundary of the update polygon features will be kept. BORDERS \u2014 The outside border of the Update Features will be kept in the Output Feature Class. This is the default option. NO_BORDERS \u2014 The outside border of the Update Features are dropped after they are inserted into the Input Features. Item values of the Update Features take precedence over Input Features attributes. ", "dataType": "Boolean"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in X or Y (or both). ", "dataType": "Linear unit"}]},
{"syntax": "Union_analysis (in_features, out_feature_class, {join_attributes}, {cluster_tolerance}, {gaps})", "name": "Union (Analysis)", "description": "Computes a geometric union of the input features. All features and their attributes will be written to the output feature class. \r\n Learn more about how Union works \r\n\r\n\r\n", "example": {"title": "Union Example (Python Window)", "description": "The following Python window script demonstrates how to use the Union function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data/gdb\" arcpy.Union_analysis ([ \"well_buff50\" , \"stream_buff200\" , \"waterbody_buff500\" ], \"water_buffers\" , \"NO_FID\" , 0.0003 ) arcpy.Union_analysis ([[ \"counties\" , 2 ],[ \"parcels\" , 1 ],[ \"state\" , 2 ]], \"state_landinfo\" )"}, "usage": ["All input ", "feature classes", " and ", "feature layers", " must have polygon geometry.", "The ", "Allow Gaps", " parameter can be used with the ALL or ONLY_FID settings on the ", "Join Attribute", " parameter. This allows for identification of resulting areas that are completely enclosed by the resulting polygons. The FID attributes for these GAP features will all be -1.", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "This tool may generate multipart features in the output even if all inputs were single part. If multipart features are not desired, use the ", "Multipart to Singlepart tool", " on the output feature class.", "With ", "ArcGIS for Desktop Basic", " and ", "Standard", " licenses, the number of input feature classes or layers is limited to two."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "A list of the input feature classes or layers. When the distance between features is less than the cluster tolerance, the features with the lower rank will snap to the feature with the higher rank. The highest rank is one. All of the input features must be polygons. ", "dataType": "Value Table"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class that will contain the results. ", "dataType": "Feature Class"}, {"name": "join_attributes", "isOptional": true, "description": "Determines which attributes from the input features will be transferred to the output feature class. ALL \u2014 All of the attributes from the input features will be transferred to the output feature class. This is the default. NO_FID \u2014 All of the attributes except the FID from the input features will be transferred to the output feature class. ONLY_FID \u2014 Only the FID field from the input features will be transferred to the output feature class. ", "dataType": "String"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in X or Y (or both). ", "dataType": "Linear unit"}, {"name": "gaps", "isOptional": true, "description": "Gaps are areas in the output feature class that are completely enclosed by other polygons. This is not invalid, but it may be desirable to identify these for analysis. To find the gaps in the output, set this option to NO_GAPS, and a feature will be created in these areas. To select these features, query the output feature class based on all the input feature's FID values being equal to -1. GAPS \u2014 No feature will be created for areas in the output that are completely enclosed by polygons. This is the default. NO_GAPS \u2014 A feature will be created for the areas in the output that are completely enclosed by polygons. This feature will have blank attributes. ", "dataType": "Boolean"}]},
{"syntax": "SymDiff_analysis (in_features, update_features, out_feature_class, {join_attributes}, {cluster_tolerance})", "name": "Symmetrical Difference (Analysis)", "description": " Features or portions of features in the input and update features that do not overlap will be written to the output feature class.", "example": {"title": "SymDiff example (Python window)", "description": "The following Python window script demonstrates how to use the SymDiff function in immediate mode:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.SymDiff_analysis ( \"climate.shp\" , \"elevlt250.shp\" , \"C:/output/symdiff.shp\" , \"ALL\" , 0.001 )"}, "usage": ["The input and update ", "feature class", " or ", "feature layer", " must be of the same geometry type.", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "This tool may generate multipart features in the output even if all inputs were single part. If multipart features are not desired, use the ", "Multipart to Singlepart tool", " on the output feature class."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "update_features", "isOptional": false, "description": "The update feature class or layer. Geometry type must be the same geometry type as the input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to which the results will be written. ", "dataType": "Feature Class"}, {"name": "join_attributes", "isOptional": true, "description": "Determines which attributes will be transferred to the output feature class. ALL \u2014 All the attributes from the input features and update features will be transferred to the output. This is the default. NO_FID \u2014 All the attributes except the FID input features and update features will be transferred to the output. ONLY_FID \u2014 Only the FID from the input features and update features will be transferred to the output. ", "dataType": "String"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in x or y (or both). ", "dataType": "Linear unit"}]},
{"syntax": "SpatialJoin_analysis (target_features, join_features, out_feature_class, {join_operation}, {join_type}, {field_mapping}, {match_option}, {search_radius}, {distance_field_name})", "name": "Spatial Join (Analysis)", "description": "Joins attributes from one feature to another\r\nbased on the spatial relationship. The target features and the joined attributes from the join features are written to the output feature class.", "example": {"title": "SpatialJoin example 1 (Python window)", "description": "The following script demonstrates how to use the SpatialJoin function in a Python window.", "code": "import arcpy target_features = \"C:/data/usa.gdb/states\" join_features = \"C:/data/usa.gdb/cities\" out_feature_class = \"C:/data/usa.gdb/states_cities\" arcpy.SpatialJoin_analysis ( target_features , join_features , out_feature_class )"}, "usage": ["A spatial join involves matching rows from the ", "Join Features", " to the ", "Target Features", " based on their relative spatial locations.", "By default, all attributes of the join features are appended to attributes of the target features and copied over to the output feature class. You can define which of the attributes will be written to the output by manipulating them in the ", "Field Map of Join Features", " parameter.", "Two new fields, Join_Count and TARGET_FID, are always added to the output feature class. Join_Count indicates how many join features match each target feature (TARGET_FID).", "Another new field, JOIN_FID, is added to the output when JOIN_ONE_TO_MANY is specified in the ", "Join Operation", " parameter.", "When the ", "Join Operation", " parameter is JOIN_ONE_TO_MANY, there can be more than one row in the output feature class for each target feature. The JOIN_FID field makes it easier to determine which feature is joined to which target feature (TARGET_FID). A value of -1 for JOIN_FID field means no feature meets the specified spatial relationship with the target feature. ", "All input target features are written to the output feature class only if:", "Merge rules specified in the ", "Field Map of Join Features", " parameter only apply to attributes from the join features and when more than one feature is matched to a target feature (when Join_Count > 1). For example, if three features with DEPTH attribute values of 15.5, 2.5, and 3.3 are joined, and a merge rule of Mean is applied, the output field will have a value of 6.1. Null values in join fields are ignored for statistic calculation. For example, 15.5, <null>, and 2.5 will result in 9.0 for Mean and 2 for Count.", "When the ", "Match Option", " is set to CLOSEST, it is possible that two or more join features are at the same distance from the target feature.  When this situation occurs, one of the join features is randomly selected as the matching feature (the join feature's FID does not influence this random selection).  If you want to find the 2", "nd", ", 3", "rd", ", or ", "N", "th", " closest feature, use the ", "Generate Near Table", " tool.", "If a join feature has a spatial relationship with multiple target features, then it is counted as many times as it is matched against the target feature. For example, if a point is WITHIN three polygons, then the point is counted three times, once for each polygon."], "parameters": [{"name": "target_features", "isOptional": false, "description": "Attributes of the target features and the attributes from the joined features are transferred to the output feature class. However, a subset of attributes can be defined in the field map parameter. The target features can be any spatial data source supported by ArcGIS. ", "dataType": "Feature Layer"}, {"name": "join_features", "isOptional": false, "description": "The attributes from the join features are joined to the attributes of the target features. See the explanation of the Join Operation parameter for details on how the aggregation of joined attributes are affected by the type of join operation. The join features can be any spatial data source supported by ArcGIS. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "A new feature class containing the attributes of the target and join features. By default, all attributes of target features and the attributes of the joined features are written to the output. However, the set of attributes to be transferred can be controlled by the field map parameter. ", "dataType": "Feature Class"}, {"name": "join_operation", "isOptional": true, "description": "Determines how joins between the target features and join features will be handled in the output feature class if multiple join features are found that have the same spatial relationship with a single target feature. JOIN_ONE_TO_ONE \u2014 If multiple join features are found that have the same spatial relationship with a single target feature, the attributes from the multiple join features will be aggregated using a field map merge rule. For example, if a point target feature is found within two separate polygon join features, the attributes from the two polygons will be aggregated before being transferred to the output point feature class. If one polygon has an attribute value of 3 and the other has a value of 7, and a Sum merge rule is specified, the aggregated value in the output feature class will be 10. The JOIN_ONE_TO_ONE option is the default. JOIN_ONE_TO_MANY \u2014 If multiple join features are found that have the same spatial relationship with a single target feature, the output feature class will contain multiple copies (records) of the target feature. For example, if a single point target feature is found within two separate polygon join features, the output feature class will contain two copies of the target feature: one record with the attributes of one polygon, and another record with the attributes of the other polygon.", "dataType": "String"}, {"name": "join_type", "isOptional": true, "description": "Determines if all target features will be maintained in the output feature class (known as outer join), or only those that have the specified spatial relationship with the join features (inner join). KEEP_ALL \u2014 All target features will be maintained in the output (outer join). This is the default. KEEP_COMMON \u2014 Only those target features that have the specified spatial relationship with the join features will be maintained in the output feature class (inner join). For example, if a point feature class is specified for the target features, and a polygon feature class is specified for the join features, with a Match Option of WITHIN, the output feature class will only contain those target features that are within a polygon join feature; any target features not within a join feature will be excluded from the output.", "dataType": "Boolean"}, {"name": "field_mapping", "isOptional": true, "description": "Controls what attribute fields will be in the output feature class. The initial list contains all the fields from both the target features and the join features. Fields can be added, deleted, renamed, or have their properties changed. The selected fields from the target features are transferred as is, but selected fields from the join features can be aggregated by a merge rule. For details on field mapping, see Using the field mapping control and Mapping input fields to output fields . Multiple fields and statistic combination may be specified. ", "dataType": "Field Mappings"}, {"name": "match_option", "isOptional": true, "description": "Defines the criteria used to match rows. The match options are: INTERSECT \u2014 The features in the join features will be matched if they intersect a target feature. This is the default. INTERSECT_3D \u2014 The features in the join features will be matched if they intersect a target feature in three-dimensional space (x, y, and z). WITHIN_A_DISTANCE \u2014 The features in the join features will be matched if they are within a specified distance of a target feature. Specify a distance in the Search Radius parameter. WITHIN_A_DISTANCE_3D \u2014 The features in the join features will be matched if they are within a specified distance of a target feature in three-dimensional space. Specify a distance in Search Radius parameter. CONTAINS \u2014 The features in the join features will be matched if a target feature contains them. The target features must be polygons or polylines. For this option, the target features cannot be points, and the join features can only be polygons when the target features are also polygons. COMPLETELY_CONTAINS \u2014 The features in the join features will be matched if a target feature completely contains them. Polygon can completely contain any feature. Point cannot completely contain any feature, not even a point. Polyline can completely contain only polyline and point. CONTAINS_CLEMENTINI \u2014 This spatial relationship yields the same results as COMPLETELY_CONTAINS with the exception that if the join feature is entirely on the boundary of the target feature (no part is properly inside or outside) the feature will not be matched. CLEMENTINI defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty. WITHIN \u2014 The features in the join features will be matched if a target feature is within them. It is opposite to CONTAINS. For this option, the target features can only be polygons when the join features are also polygons. Point can be join feature only if point is target. COMPLETELY_WITHIN \u2014 The features in the join features will be matched if a target feature is completely within them. This is opposite to COMPLETELY_CONTAINS. WITHIN_CLEMENTINI \u2014 The result will be identical to WITHIN except if the entirety of the feature in the join features is on the boundary of the target feature, the feature will not be matched. CLEMENTINI defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty. ARE_IDENTICAL_TO \u2014 The features in the join features will be matched if they are identical to a target feature. Both join and target feature must be of same shape type\u2014point-to-point, line-to-line, and polygon-to-polygon. BOUNDARY_TOUCHES \u2014 The features in the join features will be matched if they have a boundary that touches a target feature. The join and target features must be lines or polygons. Additionally, the feature in the join features must be either outside or completely inside of the target polygon. SHARE_A_LINE_SEGMENT_WITH \u2014 The features in the join features will be matched if they share a line segment with a target feature. The join and target features must be lines or polygons. CROSSED_BY_THE_OUTLINE_OF \u2014 The features in the join features will be matched if a target feature is crossed by their outline. The join and target features must be lines or polygons. If polygons are used for the join or target features, the polygon's boundary (line) will be used. Lines that cross at a point will be matched, not lines that share a line segment. HAVE_THEIR_CENTER_IN \u2014 The features in the join features will be matched if a target feature's center falls within them. The center of the feature is calculated as follows: for polygon and multipoint the geometry's centroid is used, and for line input the geometry's midpoint is used. CLOSEST \u2014 The feature in the join features that is closest to a target feature is matched. See the usage tip for more information.", "dataType": "String"}, {"name": "search_radius", "isOptional": true, "description": "Join features within this distance of a target feature will be considered for the spatial join. A search radius is only valid when the spatial relationship (Match Option) INTERSECT or CLOSEST is specified. Using a search radius of 100 meters with the spatial relationship INTERSECT is the equivalent of saying: if a join feature is within 100 meters of a target feature, consider the join feature to be a match. ", "dataType": "Linear unit"}, {"name": "distance_field_name", "isOptional": true, "description": "The name of a field to be added to the output feature class, which contains the distance between the target feature and the closest join feature. This option is only valid when the spatial relationship (Match Option) CLOSEST is specified. The value of this field is -1 if no feature is matched within a search radius. If no field name is specified, the field will not be added to the output feature class. ", "dataType": "String"}]},
{"syntax": "Intersect_analysis (in_features, out_feature_class, {join_attributes}, {cluster_tolerance}, {output_type})", "name": "Intersect (Analysis)", "description": "Computes a geometric intersection of the input features. Features or portions of features which overlap in all layers and/or feature classes will be written to the output feature class. \r\n Learn more about how Intersect works \r\n", "example": {"title": "Intersect Example (Python Window)", "description": "The following Python window script demonstrates how to use the Intersect function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/RedRiver_basin.gdb\" arcpy.Intersect_analysis ([ \"vegetation_stands\" , \"road_buffer200m\" , \"water_buffer100\" ], \"mysites\" , \"ALL\" , \"\" , \"\" ) arcpy.Intersect_analysis ([[ \"vegetation_stands\" , 2 ], [ \"road_buffer200m\" , 1 ], [ \"water_buffer100\" , 2 ]], \"mysites_ranked\" , \"ALL\" , \"\" , \"\" )"}, "usage": ["Input Features", " must be simple features: point, multipoint, line, or polygon. They cannot be complex features such as annotation features, dimension features, or network features.", "If the inputs have different geometry types (that is, line on poly, point on line, and so on), the ", "Output Feature Class", " geometry type will default to be the same as the", " Input Features", " with the lowest dimension geometry. For example, if one or more of the inputs is of type point, the default output will be point; if one or more of the inputs is line, the default output will be line; and if all inputs are polygon, the default output will be polygon.", "The ", "Output Type", " can be that of the ", "Input Features", " with the lowest dimension geometry or lower. For example, if all the inputs are polygons, the output could be polygon, line, or point. If one of the inputs is of type line and none are points, the output can be line or point. If any of the inputs are point, the ", "Output Type", " can only be point.", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "With ", "ArcGIS for Desktop Basic", " and ", "Standard", " licenses, the number of input feature classes or layers is limited to two."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "A list of the input feature classes or layers. When the distance between features is less than the cluster tolerance, the features with the lower rank will snap to the feature with the higher rank. The highest rank is one. For more information, see Priority ranks and Geoprocessing tools . ", "dataType": "Value Table"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "join_attributes", "isOptional": true, "description": "Determines which attributes from the Input Features will be transferred to the Output Feature Class. ALL \u2014 All the attributes from the Input Features will be transferred to the Output Feature Class. This is the default. NO_FID \u2014 All the attributes except the FID from the Input Features will be transferred to the Output Feature Class. ONLY_FID \u2014 Only the FID field from the Input Features will be transferred to the Output Feature Class. ", "dataType": "String"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in X or Y (or both). ", "dataType": "Linear unit"}, {"name": "output_type", "isOutputFile": true, "isOptional": true, "description": "Choose what type of intersection you want to find. INPUT \u2014 The intersections returned will be the same geometry type as the Input Features with the lowest dimension geometry. If all inputs are polygons, the output feature class will contain polygons. If one or more of the inputs are lines and none of the inputs are points, the output will be line. If one or more of the inputs are points, the output feature class will contain points. This is the default. LINE \u2014 Line intersections will be returned. This is only valid if none of the inputs are points. POINT \u2014 Point intersections will be returned. If the inputs are line or polygon, the output will be a multipoint feature class. ", "dataType": "String"}]},
{"syntax": "Identity_analysis (in_features, identity_features, out_feature_class, {join_attributes}, {cluster_tolerance}, {relationship})", "name": "Identity (Analysis)", "description": "Computes a geometric intersection of the input features and identity features. The input features or portions thereof that overlap identity features will get the attributes of those identity features.", "example": {"title": "Identity example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Identity function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data/data.gdb\" arcpy.Identity_analysis ( \"wells\" , \"counties\" , \"wells_w_county_info\" )"}, "usage": [" Input Features", " can be point, multipoint, line, or polygon. The inputs cannot be annotation features, dimension features, or network features.", "Identity Features", " must be polygons or have the same geometry type as the ", "Input Features", ".", "When using this tool with point input and  polygon ", "Identity Features", ", points that fall directly on a polygon boundary will be added to the output twice, once for each polygon that is part of the boundary. Running the ", "Intersect tool", " on the output of this scenario will identify the duplicate points so you can decide which one to keep.", "When the ", "Input Features", " are lines and the ", "Identity Features", " are polygons, and the ", "Keep relationships", " parameter is checked (", "relationship", " set to KEEP_RELATIONSHIPS), the output line feature class will have two additional fields, LEFT_poly and RIGHT_poly.  These  fields contain the feature ID  of the ", "Identity Features", "  on the left and right side of the line feature.  ", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "This tool may generate multipart features in the output even if all inputs were single part. If multipart features are not desired, use the ", "Multipart to Singlepart tool", " on the output feature class."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "identity_features", "isOptional": false, "description": "The identity feature class or layer. Must be polygons or the same geometry type as the input features. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class that will be created and to which the results will be written. ", "dataType": "Feature Class"}, {"name": "join_attributes", "isOptional": true, "description": "Determines what attributes will be transferred to the output feature class. ALL \u2014 All the attributes (including FIDs) from the input features, as well as the identity features, will be transferred to the output features. This is the default. NO_FID \u2014 All the attributes except the FID from the input features and identity features will be transferred to the output features. ONLY_FID \u2014 All the attributes from the input features but only the FID from the identity features will be transferred to the output features. ", "dataType": "String"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in X or Y (or both). ", "dataType": "Linear unit"}, {"name": "relationship", "isOptional": true, "description": "Determines if additional spatial relationships between the in_features and the identity_features are to be written to the output. This only applies when the in_features are lines and the identity_features are polygons. NO_RELATIONSHIPS \u2014 No additional spatial relationship will be determined. KEEP_RELATIONSHIPS \u2014 The output line features will contain two additional fields, LEFT_poly and RIGHT_poly. These fields contain the feature ID of the identity_features on the left and right side of the line feature.", "dataType": "Boolean"}]},
{"syntax": "Erase_analysis (in_features, erase_features, out_feature_class, {cluster_tolerance})", "name": "Erase (Analysis)", "description": "Creates a  feature class  by overlaying the  Input Features  with the polygons of the  Erase Features . Only those portions of the input features falling outside the erase features outside boundaries are copied to the output feature class.", "example": {"title": "Erase example (Python Window)", "description": "The following Python window script demonstrates how to use the Erase function in immediate mode.", "code": ""}, "usage": ["Input Feature", " geometries ", "coincident", " with ", "Erase Feature", " geometries will be removed.", "The ", "Erase Features", " can be point, line, or polygon as long as the ", "Input Feature", " is of the same or lesser order feature type. A polygon erase feature can be used to erase polygons, lines, or points from the input features; a line erase feature can be used to erase lines or points from the input features; a point erase feature can be used to erase points from the input features.", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "This tool may generate multipart features in the output even if all inputs were single part. If multipart features are not desired, use the ", "Multipart to Singlepart tool", " on the output feature class."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "erase_features", "isOptional": false, "description": "The features to be used to erase coincident features in the input. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class that will contain only those Input Features that are not coincident with the Erase Features. ", "dataType": "Feature Class"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in X or Y (or both). ", "dataType": "Linear unit"}]},
{"syntax": "TableSelect_analysis (in_table, out_table, {where_clause})", "name": "Table Select (Analysis)", "description": "Selects table records matching a Structured Query Language (SQL) expression and writes them to an output table.", "example": {"title": "Table Select Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Table Select function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.TableSelect_analysis ( \"majorrds.shp\" , \"C:/output/majorrdsCl4.shp\" , '\"CLASS\" =  \\' 4 \\' ' )"}, "usage": ["The input can be an INFO, dBASE, or geodatabase table, a feature class, table view, or VPF dataset.", " The ", "Expression", " parameter can be created with the ", "Query Builder", " or simply typed in. For details on the expression syntax see ", "Building an SQL Expression", " or ", "SQL Reference", ".", "  If a table view is used for ", "Input Table", " and no expression is entered, only the selected records are written to the output table. If a table view is used for input\r\ntable and an expression is entered, the expression is only executed\r\nagainst the selected records, and the expression-based subset of\r\nthe selected set is written to the output table.", "If you want to create a table from the table view's selected set of records, use the ", "Copy Rows", " (management) tool."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table whose records matching the specified expression will be written to the output table. ", "dataType": "Table View; Raster Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output table containing records from the input table that match the specified expression. ", "dataType": "Table"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of records. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, coverages, or dBASE or INFO tables, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}]},
{"syntax": "Split_analysis (in_features, split_features, split_field, out_workspace, {cluster_tolerance})", "name": "Split (Analysis)", "description": "Splitting the  Input Features  creates a subset of multiple output feature classes. The Split Field's unique values form the names of the output  feature classes . These are saved in the target  workspace .", "example": {"title": "Split Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Split tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data\" arcpy.Split_analysis ( \"Habitat_Analysis.gdb/vegtype\" , \"climate.shp\" , \"Zone\" , \"C:/output/Output.gdb\" , \"1 Meters\" )"}, "usage": ["The ", "Split Features", " dataset must be polygons.", "The ", "Split Field", " data type must be character. Its unique values form the names of output feature classes.", "The split field's unique values must start with a valid character. If the target workspace is a file, personal, or ArcSDE geodatabase, the field's values must begin with a letter. Field values that begin with a number as in \"350 degrees\" cause an error. Exception: Shapefile names can begin with a number, and a folder target workspace allows field values that begin with a number.", "The target workspace must already exist.", "The total number of output feature classes equals the number of unique ", "Split Field", " values and the extent of overlay of the ", "Input Features", " with the ", "Split Features", ".", "The feature attribute table of each output feature class contains the same fields as the ", "Input Features", " attribute table.", "Annotation", " features are split and saved in the output features based on the ", "Split Features", " polygon in which the lower left starting point of the annotation string falls.", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The features to be split. ", "dataType": "Feature Layer"}, {"name": "split_features", "isOptional": false, "description": "The features containing a tabular field whose unique values are used to split the Input Features and provide the output feature classes' names. ", "dataType": "Feature Layer"}, {"name": "split_field", "isOptional": false, "description": "The character field used to split the Input Features. This field's values identify the Split Features used to create each output feature class. The Split Field's unique values provide the output feature classes' names. ", "dataType": "Field"}, {"name": "out_workspace", "isOutputFile": true, "isOptional": false, "description": "The workspace where the output feature classes are stored. ", "dataType": "Workspace ; Feature Dataset"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in X or Y (or both). Set the value to be higher for data that has less coordinate accuracy and lower for datasets with extremely high accuracy. ", "dataType": "Linear unit"}]},
{"syntax": "Select_analysis (in_features, out_feature_class, {where_clause})", "name": "Select (Analysis)", "description": "Extracts features from an input  feature class  or input  feature layer , typically using a select or Structured Query Language (SQL) expression and stores them in an output feature class.", "example": {"title": "Select Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"c:/basedata/roads.gdb\" arcpy.Select_analysis ( \"nfroads\" , \"paved\" , '[ROAD_CLASS] = \"PAVED\"' )"}, "usage": ["The select or SQL expression gets built with the ", "Query Builder", ", or is simply typed in. For details on the expression syntax see ", "Building an SQL Expression", " or ", "SQL Reference", ".", "If a layer is used for ", "Input Features", " and no expression is entered, only the selected features are written to the output feature class. If a layer is used for Input\r\nFeatures and an expression is entered, the expression is only executed\r\nagainst the selected features, and the expression-based subset of\r\nthe selected set is written to the output feature class.", "If you want to create a feature class from the selected set of features in a layer, use the ", "Copy_Features", " tool."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer from which features are selected. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to be created. If no expression is used, it contains all input features. ", "dataType": "Feature Class"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of features. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, or coverages, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}]},
{"syntax": "Clip_analysis (in_features, clip_features, out_feature_class, {cluster_tolerance})", "name": "Clip (Analysis)", "description": "Extracts input features that overlay the clip features. Use this tool to cut out a piece of one feature class using one or more of the features in another feature class as a cookie cutter. This is particularly useful for creating a new feature class\u2014also referred to as study area or area of interest (AOI)\u2014that contains a geographic subset of the features in another, larger feature class.", "example": {"title": "Clip example (Python window)", "description": "The following Python window script demonstrates how to use the Clip function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Clip_analysis ( \"majorrds.shp\" , \"study_quads.shp\" , \"C:/output/studyarea.shp\" )"}, "usage": ["The ", "Clip Features", " can be points, lines, and polygons, depending on the ", "Input Features", " type. ", "The ", "Output Feature Class", " will contain all the attributes of the ", "Input Features", ".", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "Line features clipped by polygon features:", "Point features clipped by polygon features:", "Line features clipped with line features:", "Point features clipped with point features:", "Attribute values from the input feature classes will be copied to the output feature class.  However, if the input is a layer or layers created by the ", "Make Feature Layer", " tool and a field's ", "Use Ratio Policy", " is checked, then a ratio of the input attribute value is calculated for the output attribute value.    When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value  is based on the ratio in which the input feature  geometry was divided. For example, If the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. ", "Use Ratio Policy", " only applies to numeric field types. ", "Geoprocessing tools do not honor geodatabase feature class or table field ", "split policies", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The features to be clipped. ", "dataType": "Feature Layer"}, {"name": "clip_features", "isOptional": false, "description": "The features used to clip the input features. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to be created. ", "dataType": "Feature Class"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates as well as the distance a coordinate can move in X or Y (or both). Set the value to be higher for data with less coordinate accuracy and lower for data with extremely high accuracy. ", "dataType": "Linear unit"}]},
{"syntax": "CADtoGeodatabase_conversion (input_cad_datasets, out_gdb_path, out_dataset_name, reference_scale, {spatial_reference})", "name": "CAD to Geodatabase (Conversion)", "description": "Reads a CAD dataset and creates  feature classes  of the drawing. The feature classes are written to a geodatabase  feature dataset .", "example": {"title": null, "description": null, "code": "#Name: CadtoGeodatabase.py # Description: Create a feature dataset # Import system modules import arcpy from arcpy import env # Set workspace env.workspace = \"C:/data\" # Set local variables input_cad_dataset = \"C:/data/City.DWG\" out_gdb_path = \"C:/data/HabitatAnalysis.gdb\" out_dataset_name = \"analysisresults\" reference_scale = \"1000\" spatial_reference = \"NAD_1983_StatePlane_California_VI_FIPS_0406_Feet\" # Create a FileGDB for the fds arcpy.CreateFileGDB_management ( \"C:/data\" , \"HabitatAnalysis.gdb\" ) # Execute CreateFeaturedataset  arcpy.CadToGeodatabase_conversion ( input_cad_dataset , out_gdb_path , out_dataset_name , reference_scale )"}, "usage": ["This tool creates a new feature dataset in an existing geodatabase. When run from ArcMap the results are added as a group layer.", "This tool creates feature class annotation from CAD text.", " The input features must be a CAD file. The CAD file will contain all the available homogenous geometries. ", "The input parameter will accept CAD data from multiple formats (DWG, DXF, and DGN) in one operation.", "If a DWG is used as input it may contain additional CAD-defined feature classes that conform to the ESRI specification document ", "Mapping Specification for CAD", ". These are subsets of the original homogenous geometries with entity-linked attributes that also import to the geodatabase as feature attributes.", "Feature class names must be unique for the entire geodatabase or the tool will fail.", "All inputs are combined into a single output CAD dataset which will contain the standard point, line, polygon, annotation  feature classes in addition to any CAD-defined feature classes that may exist.", "This tool only outputs a dataset to any geodatabase along with ArcSDE instances.", "If a projection file exists for the input CAD file, it will automatically populate the spatial reference parameter with the projection information. If multiple CAD files are used as inputs, the spatial reference will be taken from the first CAD file with valid projection information.", "If a universal projection (", "esri_cad.prj", ") file exists in the directory, the projection information will be taken from the universal projection file if a coordinate system is not defined for the first CAD file.", "If a world file exists for the input CAD file, it will automatically perform the transformation.", "If a universal world (", "esri_cad.wld", ") file exists in the directory, the transformation will be applied to each CAD dataset in the list that does not have a companion world file.", "If a DGN file has multiple models, be sure the first model has the largest domain. This tool calculates the domain for the entire DGN file from the first model. If this is not the case, be sure to expand the domain in your first model to be large enough so all will fit.", "Choose a reference scale that is roughly equal to the scale at which the annotation will normally be displayed.", "Should you only require a single feature class from the CAD feature classes, then use a geoprocessing tool, such as ", "Feature Class To Feature Class", ". "], "parameters": [{"name": "input_cad_datasets", "isInputFile": true, "isOptional": false, "description": " The collection of CAD files to convert to geodatabase features. ", "dataType": "CAD Drawing Dataset"}, {"name": "out_gdb_path", "isOutputFile": true, "isOptional": false, "description": " The ArcSDE, file, or personal geodatabase where the Output Feature Dataset will be created. The target geodatabase must already exist. ", "dataType": "Workspace"}, {"name": "out_dataset_name", "isOutputFile": true, "isOptional": false, "description": " The name of the feature dataset to be created. ", "dataType": "String"}, {"name": "reference_scale", "isOptional": false, "description": " Enter the scale to use as a reference for the annotation. This sets the scale to which all symbol and text sizes in the annotation will be made relative. ", "dataType": "Double"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output feature dataset. If you wish to control other aspects of the spatial reference (i.e., the xy, z, m domains, resolutions, tolerances), use the relevant environments. ", "dataType": "Spatial Reference"}]},
{"syntax": "TableToDBASE_conversion (input_table, output_folder)", "name": "Table to dBASE (Conversion)", "description": "Converts one or more tables to dBASE tables in an output folder.", "example": {"title": "TableToDBASE Example (Python Window)", "description": "The following Python window script demonstrates how to use the TableToDBASE function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Habitat_Analysis.gdb\" arcpy.TableToDBASE_conversion ([ \"vegtype\" , \"futrds\" ], \"C:/output\" )"}, "usage": ["This tool supports the following table formats as input: ", "For file input (", ".csv", " or ", ".txt", "), the first row of the input file is used as the field names on the output table.  Field names cannot contain spaces or special characters (such as ", "$", " or ", "*", "), and you will  receive an error if the first row of the input file contains spaces or special characters.", "Learn more about table formats supported in ArcGIS", "The name of the output dBASE tables will be based on the name of the input table. To control the output name and for additional conversion options use the ", "Table to Table", " tool.", "This tool can be used to export an ArcGIS table to a dBASE table (", ".dbf", ") that can be read and edited in Microsoft Excel.", "The ", "Copy Rows", " and ", "Table To Table", " tools can also be used to convert a table to a dBASE file.", "If the name of the output table already exists in the output folder, a number will be appended to the end of the name to make it unique (for example, OutputTbl_1.dbf)."], "parameters": [{"name": "input_table", "isInputFile": true, "isOptional": false, "description": "The list of tables to be converted to dBASE. ", "dataType": "Table View"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": "The destination folder where the output dBASE table(s) will be placed. ", "dataType": "Folder"}]},
{"syntax": "FeatureclassToCoverage_conversion (in_features, out_cover, {cluster_tolerance}, {precision})", "name": "Feature Class To Coverage (Conversion)", "description": "Creates a single  ArcInfo Workstation  coverage from one or more input  feature classes  or  layers .", "example": {"title": "FeatureClassToCoverage Example (Python Window)", "description": "The following Python window script demonstrates how to use the FeatureclassToCoverage function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.FeatureclassToCoverage_conversion ( [[ \"Montgomery.gdb/Landbase/parcels\" , \"REGION\" ]], \"C:/output/mont_parcel\" , \"\" , \"\" )"}, "usage": ["The ", "cluster tolerance", " acts the same as the ", "fuzzy tolerance", " in ", "ArcInfo Workstation", ". The fuzzy tolerance of the output coverage will be the same as the cluster tolerance specified when executing this tool. If no cluster tolerance is specified, a default is calculated.", "Coverages do not support null geometries. Null geometries will be dropped during conversion.", "When one line feature class is entered as input, you can choose to build a coverage for arcs or ", "routes", ". If the output contains a route subclass, it will be prefixed by ROUTE (for example, ROUTE.streets). The default type is ROUTE.", "When one polygon feature class is entered as input, you can choose to build a coverage for polygons or ", "regions", ". If the output contains a region subclass, it will be prefixed by REGION (for example, REGION.parcels). The default type is REGION.", "The default precision of the output will be DOUBLE.", "It is suggested you run the ", "Create Labels", " tool after successfully executing this tool to ensure all polygon features have an accurate label.", "If one point feature class and one polygon feature class are entered as inputs, the point feature class can represent labels for the output coverage. To do this, choose LABEL as the type of features for the input points.", "When more than two line feature classes or layers are entered, only one of the feature classes can have arcs built in the output coverage. The remaining feature classes will contain routes.", "When more than two polygon feature classes or layers are entered, only one of the feature classes can have polygons built in the output coverage. The remaining feature classes will contain regions.", "When converting a polygon feature class with overlapping polygons, discontiguous polygons, or \"donut holes\", you should convert the feature class to a region coverage.  If you convert such a feature class to a polygon coverage, any overlapping polygons, discontinuous polygons, or \"donut holes\" will be lost or changed because  those types of geometries are not permitted in polygon coverages.", " In a line  feature class, there is no effective limit to the number of vertices that can exist in a single line feature. In the ", "ArcInfo Workstation", " coverage data model, a line is defined by 500 vertices. At the 500th vertex, the vertex automatically becomes a node, and a new line is started. When a line  feature class is converted to a coverage, the attributes of the lines are stored in the ROUTE subclass. In order to transfer those attributes to the individual lines in the coverage, you can run the ArcInfo Workstation command ROUTEARC at the ", "ArcInfo Workstation", " command line. This command transfers the attribute from the route, with its' unlimited vertices, to each segment of the entire line in the coverage, each of which is 500 vertices long."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature classes or layers used to create a single ArcInfo Workstation coverage, including the type of features of which the coverage will be composed. POINT \u2014 LABEL \u2014 NODE \u2014 ARC \u2014 ROUTE \u2014 POLYGON \u2014 REGION \u2014 ANNO \u2014", "dataType": "Value Table"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage to be created. ", "dataType": "Coverage"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates (nodes and vertices) as well as the distance a coordinate can move in X or Y (or both). You can set the value to be higher for data that has less coordinate accuracy and lower for datasets with extremely high accuracy. ", "dataType": "Linear unit"}, {"name": "precision", "isOptional": true, "description": "The precision of the output coverage. DOUBLE \u2014 The out_cover will have double precison . This is the default. SINGLE \u2014 The out_cover will have single precision . ", "dataType": "Boolean"}]},
{"syntax": "SetCADAlias_conversion (input_table, field_info)", "name": "Set CAD Alias (Conversion)", "description": "Renames one or more existing field name aliases by matching columns from the input table with a list of CAD-specific fields with the appropriate name recognized by the Export To CAD tool.", "example": {"title": null, "description": null, "code": "# Name: SetCadAlias.py # Description: create aliases # Author: ESRI # Import system modules import arcpy from arcpy import env # Set workspace env.workspace = \"c:/Workspace/network.mdb\" arcpy.toolbox = \"conversion\" arcpy.setcadalias ( \"primary_roads\" , \"area area; name layer\" )"}, "usage": ["The CAD-specific fields in the matching list have the appropriate name and type recognized by the Export To CAD tool. Typically, values already exist in these fields to derive the various CAD entity properties of CAD objects created by the Export To CAD tool.", "If a feature class intended for export already contains values useful for driving CAD properties, such as layer name, but the fields have different names, assigning a CAD field alias on that table using the Assign CAD Alias tool is an efficient way to have the Export To CAD tool recognize those values as CAD properties.", "To permanently set CAD-specific aliases on an input feature class, use the Set CAD Alias tool, since it has a predefined alias list. To create a temporary alias for a feature layer, use the Make Feature Layer tool in ModelBuilder, which gives you the flexibility to change the alias of the field name for the feature layer.", "Using the Add CAD fields tool to build a lookup table is perhaps a better way to create a repeatable system of exporting feature data to CAD files according to some established drawing standard.", "These CAD-specific fields are used to specify the various CAD graphic properties and other forms of CAD attribution. A similar tool, Add CAD Fields, is used to add fields to a table for the same export purpose. Assigning a Field Alias differs from Add CAD Fields in that existing fields that may already contain valid values to describe CAD properties in Export To CAD are renamed with an alias rather than adding a field and calculating values.", "Shapefiles are not a valid input to this function, since they cannot maintain aliases for fields. If you need to use a shapefile as input, convert the shapefile to a layer file. Layer files or feature classes from a personal geodatabase or SDE geodatabase are valid inputs to this tool.", "This tool overwrites the input, so be sure to make a backup of the original data."], "parameters": [{"name": "input_table", "isInputFile": true, "isOptional": false, "description": "The feature class, feature layer, table, or table view to which you would like to assign CAD-specific field name aliases. ", "dataType": "Table View"}, {"name": "field_info", "isOptional": false, "description": "Field info relating a list of aliases matched to the list of existing fields in the input table. ", "dataType": "Field Info"}]},
{"syntax": "ExportCAD_conversion (in_features, Output_Type, Output_File, {Ignore_FileNames}, {Append_To_Existing}, {Seed_File})", "name": "Export To CAD (Conversion)", "description": "Creates one or more CAD drawings based on the values contained in one or more input feature classes or feature layers and supporting tables.", "example": {"title": null, "description": null, "code": "# Name: ExportToCAD.py # Description: Create an AutoCAD DWG # Import system modules import arcpy from arcpy import env # Set workspace env.workspace = \"C:/data\" # Set local variables in_features = \"C:/data/EditorTutorial.gdb/StudyArea/Buildings\" output_type = \"DWG_R2010\" output_file = \"c:/data/Buildings.dwg\" try : # Process: Export to CAD arcpy.ExportCAD_conversion ( in_features , output_type , output_file , \"USE_FILENAMES_IN_TABLES\" , \"OVERWRITE_EXISTING_FILES\" , \"\" ) except : # If an error occurs while running a tool print the message print arcpy.GetMessages ()"}, "usage": ["This tool creates DWG, DXF, or DGN CAD files.", "Feature classes, feature layers, and shapefiles are valid inputs to this tool.", "This tool exports annotation feature classes to CAD files as text. Conversely, the ", "Import CAD Annotation", " tool creates annotation feature classes from text in CAD files.", "Default output creates a drawing layer for each input feature class or layer unless otherwise specified with a seed file and/or key named fields.", "Default output generates entities using the  default  properties of the output CAD file format unless otherwise specified with a seed file and/or key named fields.", "Output to DWG/DXF format writes the coordinate system to the drawing's named object dictionary using a well-known text (WKT) string, as defined by the Mapping Specification for CAD. This nongraphic data is recognized by ", "ArcGIS for Desktop", " and it can be edited in the AutoCAD application (version 2007 or higher) using the Esri plug-in ArcGIS for AutoCAD. ", "Output to DWG/DXF (version 2007 or higher) writes feature class schema for each feature class to the drawing\u2019s named object dictionary using xrecords, as defined by the Mapping Specification for CAD. This nongraphic data is recognized by ", "ArcGIS for Desktop", " and it can be edited in the AutoCAD application using the Esri plug-in ArcGIS for AutoCAD.", "Output to DWG/DXF (version 2007 or higher) exports feature attributes as entity-linked attributes to the drawing's extension dictionary referenced by the entity, as defined by the Mapping Specification for CAD. This nongraphic data is recognized by ", "ArcGIS for Desktop", " and it can be edited in the AutoCAD application using the Esri plug-in ArcGIS for AutoCAD.", "This tool will not export coverage annotation to any CAD format.", "Seed files and key named fields can be used to control layers, graphic properties such as color, and AutoCAD block definitions in the output CAD files.", "To add key named fields to the input feature class, use the ", "Add Field", " tool.  To add multiple fields at one time by functional category, use the  ", "Add CAD Fields", " tool.  ", "The RefName field in the input feature class specifies which seed file block or cell names to use. Attribute tag elements defined for an AutoCAD block definition  are populated with attribute information if the tag name matches a field name.", "The DocPath field in the input feature class is used to direct the output to one or more CAD drawing files. If the parameter ", "Ignore Paths in Tables", " is checked (enabled) on the tool dialog box, this field is ignored.", "The LTScale field is used to adjust the line weight in DGN files.", "The LineWt field is used to adjust the line weight in DWG files."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "A collection of feature classes and/or feature layers whose geometry will be exported to one or more CAD files. ", "dataType": "Feature Layer"}, {"name": "Output_Type", "isOutputFile": true, "isOptional": false, "description": "The CAD platform and file version of the output files. This value overrides any output_type values contained in the keyname column or alias column CADFile_type. Types include DGN_V8, DWG_R14, DWG_R2000, DWG_R2004, DWG_R2005, DWG_R2007, DWG_R2010, DXF_R14, DXF_R2000, DXF_R2004, DXF_R2005, DXF_R2007, and DXF_R2010. ", "dataType": "String"}, {"name": "Output_File", "isOutputFile": true, "isOptional": false, "description": "The path of the desired output CAD drawing file. This name overrides any drawing name information included in the input features columns or alias columns named DrawingPathName. ", "dataType": "CAD Drawing Dataset"}, {"name": "Ignore_FileNames", "isOptional": true, "description": "Allows the function to ignore or use the paths in the DrawingPathName. This allows the function to output CAD entities to specific drawings or ignore this and add to one CAD file. IGNORE_FILENAMES_IN_TABLES \u2014 Ignores the paths in the document entity fields and allows the output of all entities to a single CAD file. USE_FILENAMES_IN_TABLES \u2014 Allows the paths in the document entity fields to be used and have each entity's path used so that each CAD part will be written to a separate file. This is the default. ", "dataType": "Boolean"}, {"name": "Append_To_Existing", "isOptional": true, "description": "Allows the output to append to an existing CAD file. This lets you add information to a CAD file on disk. APPEND_TO_EXISTING_FILES \u2014 Allows the output file content to be added to an existing output CAD file. The existing CAD file content will not be lost. OVERWRITE_EXISTING_FILES \u2014 The output file content will overwrite the existing CAD file content. This is the default. ", "dataType": "Boolean"}, {"name": "Seed_File", "isOptional": true, "description": "An existing CAD drawing whose contents and document and layer properties will be used for all new output CAD files. The CAD platform and format version of the seed file overrides the value specified by the Output_Type parameter. If appending to existing CAD files, the seed drawing is ignored. ", "dataType": "CAD Drawing Dataset"}]},
{"syntax": "CreateCADXData_conversion (In_table, Fields, RegApp, Output_Type)", "name": "Create CAD XData (Conversion)", "description": "Creates an attribute field called 'XDList' in the Input Table.", "example": {"title": null, "description": null, "code": "# Name: ExportXData.py # Description: Creates XData from a feature class and exports it to a CAD file with the Extended Entity Data. # Author: ESRI # Import system modules import arcpy from arcpy import env # Local variables... Roads2 = \"C:/Test_data/Personal GDB/OrangeCounty.mdb/Roads\" OrangeCounty_mdb = \"C:/Test_data/Personal GDB/OrangeCounty.mdb\" Roads = \"C:/Test_data/Personal GDB/OrangeCounty.mdb/Roads\" roads_dwg = \"C:/Test_data/CAD/roads.dwg\" # Process: Select Data... arcpy.SelectData_management ( OrangeCounty_mdb , \"Roads\" , ) # Process: Create CAD XData... arcpy.CreateCADXData_conversion ( Roads , \"'MINOR2';'MAJOR3';'MINOR3';'MAJOR4';'MINOR4';'DESCRIPT';'Shape_Length'\" , \"ArcGIS\" , \"ADE\" , ) # Process: Export to CAD... arcpy.ExportCAD_conversion ( \"'C:/Test_data/Personal GDB/OrangeCounty.mdb/Roads'\" , \"DWG-R2007\" , roads_dwg , \"USE_FILENAMES_IN_TABLES\" , \"OVERWRITE_EXISTING_FILES\" , \"\" )"}, "usage": ["All input feature classes and/or feature layers are valid inputs to this tool.", "XData is only read by AutoCAD.", "The XDList field that is created by this function is read by the tool Expor to CAD when exported to AutoCAD .", "Extended Entity Data, though, allows you to attach up to 16K of information to each and every entity in the drawing.", "You can also keep the XData separate from other information because it uses a uniquely registered name.", "If your function fails with \"Output exceeds field length\", then include less fields in your XData string. The XData field that is added has a maximum field length of 2049 characters."], "parameters": [{"name": "In_table", "isInputFile": true, "isOptional": false, "description": "This table contains the fields used to generate an XML string of values recognized by the Export to CAD tool. This enables XData to be created on output objects, based on values of fields from the Input Table. ", "dataType": "Table View"}, {"name": "Fields", "isOptional": false, "description": "The selected field values for the encoded XData. The Add Field button, which is used only in ModelBuilder, allows you to add expected field(s) so that you can complete the dialog and continue to build your model. ", "dataType": "Field"}, {"name": "RegApp", "isOptional": false, "description": "The AutoCAD extended entity data registered application name. Required by only AutoCAD to identify the XData. This application name is an identifier of the newly created XData. ", "dataType": "String"}, {"name": "Output_Type", "isOutputFile": true, "isOptional": false, "description": "Format of the AutoCAD XData. ADE \u2014 The XData string that contains a tag, type, and value in a single string. TRADITIONAL \u2014 Single-typed value without tag names. ", "dataType": "String"}]},
{"syntax": "AddCADFields_conversion (input_table, Entities, {LayerProps}, {TextProps}, {DocProps}, {XDataProps})", "name": "Add CAD Fields (Conversion)", "description": " Adds several reserved CAD fields in one step. Fields created by this tool are used by the  Export To CAD  tool to generate CAD entities with specific properties.   After executing this tool, you must calculate or type the appropriate field values.", "example": {"title": null, "description": null, "code": "# Name: AddCADFields.py # Description: Add reserved CAD fields to attribute table for use with Export To CAD tool # Author: Esri # 10/28/2009  # Import system modules import arcpy from arcpy import env # Set workspace env.workspace = \"C:/data\" # Set local variables input_table = \"C:/data/EditorTutorial.gdb/StudyArea/Buildings\" try : # Process: Add CAD Fields arcpy.AddCADFields_conversion ( input_table , \"ADD_ENTITY_PROPERTIES\" , \"ADD_LAYER_PROPERTIES\" , \"NO_TEXT_PROPERTIES\" , \"NO_DOCUMENT_PROPERTIES\" , \"NO_XDATA_PROPERTIES\" ) except : # If an error occurs while running a tool print the message print arcpy.GetMessages ()"}, "usage": ["All input ", "feature classes", " and/or ", "feature layers", " are valid inputs to this tool.", "You can add fields to a table other than a feature class and join the tables using a field that exists in both tables.  Using a lookup table can be an efficient way to standardize and reuse common CAD property values.", "If the input is a table view or a feature layer with a joined table, the fields are only added to the base table (not to the joined table)."], "parameters": [{"name": "input_table", "isInputFile": true, "isOptional": false, "description": "Input table, feature class, or shapefile that will have the CAD-specific fields added to it ", "dataType": "Table View"}, {"name": "Entities", "isOptional": false, "description": "Adds the list of CAD-specific Entity property fields to the input table ADD_ENTITY_PROPERTIES \u2014 Adds the list of CAD-specific Entity property fields to the input table NO_ENTITY_PROPERTIES \u2014 Does not add the list of CAD-specific Entity property fields to the input table", "dataType": "Boolean"}, {"name": "LayerProps", "isOptional": true, "description": "Adds the list of CAD-specific Layer property fields to the input table ADD_LAYER_PROPERTIES \u2014 Adds the list of CAD-specific Layer property fields to the input table NO_LAYER_PROPERTIES \u2014 Does not add the list of CAD-specific Layer property fields to the input table", "dataType": "Boolean"}, {"name": "TextProps", "isOptional": true, "description": "Adds the list of CAD-specific Text property fields to the input table ADD_TEXT_PROPERTIES \u2014 Adds the list of CAD-specific Text property fields to the input table NO_TEXT_PROPERTIES \u2014 Does not add the list of CAD-specific Text property fields to the input table", "dataType": "Boolean"}, {"name": "DocProps", "isOptional": true, "description": "Adds the list of CAD-specific Document property fields to the input table ADD_DOCUMENT_PROPERTIES \u2014 Adds the list of CAD-specific Document property fields to the input table NO_DOCUMENT_PROPERTIES \u2014 Does not add the list of CAD-specific Document property fields to the input table", "dataType": "Boolean"}, {"name": "XDataProps", "isOptional": true, "description": "Adds the list of CAD-specific XData property fields to the input table ADD_XDATA_PROPERTIES \u2014 Adds the list of CAD-specific XData property fields to the input table NO_XDATA_PROPERTIES \u2014 Does not add the list of CAD-specific XData property fields to the input table", "dataType": "Boolean"}]},
{"syntax": "XSLTransform_conversion (source, xslt, output, {xsltparam})", "name": "XSLT Transformation (Conversion)", "description": "Uses the .NET 3.5 XML software to transform an ArcGIS item's metadata or any XML file using an XSLT 1.0 stylesheet and save the result to an XML file. XSLT stylesheets can be used to perform a variety of modifications to ArcGIS metadata or an XML file. Several XSLT stylesheets are provided with  ArcGIS for Desktop . They can be found in the  <ArcGIS Installation Location>\\Metadata\\Stylesheets  folder. These stylesheets are described in the tables below.  The following stylesheets produce the HTML documents that are used to display information about an item in the  Description  tab . They extract content from an item's metadata and add HTML formatting instructions to it. These stylesheets import many XSLT templates from other files in the   ArcGIS_Imports  folder; the imported templates perform most of the work. If you are interested in creating custom stylesheets for display, you can learn more about these stylesheets by reading the documentation provided with the  ArcGIS Metadata Toolkit . The following stylesheets, provided in the  <ArcGIS Installation Location>\\Metadata\\Stylesheets\\gpTools  folder, take an item's metadata, process it, then save the resulting XML document to a new XML file. The goal of this operation might be to filter an item's metadata before using it outside of ArcGIS. Or, the goal might be to alter the item's metadata; in this case,  the resulting XML file can be saved back to the original item as its metadata using the  Metadata Importer  tool.  A model or script can be created  that first runs this tool with a stylesheet to update the metadata content, then immediately saves the changes back to the item. The following stylesheets are provided with  ArcGIS for Desktop  to perform some well-defined metadata tasks.  Some of the stylesheets provided with  ArcGIS for Desktop  are used by the metadata geoprocessing tools to perform portions of the importing, exporting, and upgrading processes:   You can create your own XSLT stylesheets to perform tasks using the provided stylesheets as examples. For example, you might write a stylesheet to:  XSLT stylesheets that modify ArcGIS metadata should not remove information in the Esri and Binary metadata elements except if the output XML will be used outside of ArcGIS. ", "example": {"title": "Export metadata to an HTML file", "description": "Uses the ", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" #set local variables dir = arcpy.GetInstallInfo ( \"desktop\" )[ \"InstallDir\" ] xslt = dir + \"Metadata/Stylesheets/ArcGIS.xsl\" arcpy.XSLTransform_conversion ( \"vegetation\" , xslt , \"vegetation.html\" , \"#\" )"}, "usage": ["The output file produced by an XSLT stylesheet may be formatted as XML, HTML, or text, for example. The default output file name that will be provided by this tool will have a ", ".xml", " file extension. If the XSLT stylesheet you are using does not produce an XML file you should provide a file name with a more appropriate file extension.", "This tool can't process an ArcGIS item's metadata or a stand-alone metadata XML file using the XSL stylesheets provided with ArcGIS Desktop 9.3.1 and earlier versions because they don't use XSLT 1.0 technology. They use an older technology that is not supported by the .NET 3.5 Framework. These XSL stylesheets also can't  be used with the ArcGIS metadata editor. ", "The ", "XSLT Parameter", " can be used to pass a string or an XML file name to an XSLT stylesheet which can then use it to modify an item's metadata. For example, an XSLT stylesheet could take a phone number passed in as a string and update all phone numbers in the metadata. If an XML file is passed in with this parameter, an XSLT stylesheet can merge the information it contains with the source metadata. ", "Refer to the ", "add unique identifier.xslt", " file provided with ArcGIS for an example of how to pass in a value and ", "merge upgraded FGDC with existing.xslt", " for an example of merging information in a separate XML file with an ArcGIS item's metadata.", "If you want to create a custom XSLT stylesheet to display or export metadata or XML files in ArcGIS, refer to the Microsoft documentation for the .NET 3.5 Framework for a complete list of the XSLT and XPath elements, functions and syntax that it supports. In addition to some standard XSLT functions, some .NET-specific functions are also available.", "If you want to use your custom stylesheet both inside and outside ArcGIS you will have the best chance of success if you limit yourself to using XSLT 1.0 and XPath 1.0 elements, functions, and syntax. Non-Microsoft applications are unlikely to support any .NET-specific  XSLT and XPath functions. ", "XSLT stylesheets that are exclusively used with this geoprocessing tool and the ArcGIS metadata editor can use some custom XSLT functions provided with ArcGIS to improve metadata handling and display. To use these functions the XSLT stylesheet must reference the URI of the ArcGIS XSLT function namespace ", "http://www.esri.com/metadata/", ". For example, ", "<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" xmlns:esri=\"http://www.esri.com/metadata/\">", ". ", "The ArcGIS XSLT functions must be prefaced by the prefix assigned to the ArcGIS XSLT function namespace. If the ArcGIS XSLT function namespace is assigned the prefix ", "esri", ", as in the above example, the GuidGen() function would be used like this: ", "<xsl:value-of select=\"esri:GuidGen()\" />", ".", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", ".", "If an ArcGIS item has metadata that was created in ArcGIS Desktop 9.3.1 or earlier using the FGDC metadata editor or in the current version of ", "ArcGIS for Desktop", " using the ", "FGDC metadata editor add-in", " and the item's metadata has been upgraded to ArcGIS metadata, the ArcGIS metadata will continue to include the original FGDC metadata XML elements. The metadata will also include other ArcGIS metadata elements. ", "You can export the original FGDC metadata elements using this tool with the ", "_MPXML2.xsl", " file provided with ArcGIS. In the example above, this file would only export the item's original FGDC-format metadata content\u2014the information displayed under the ", "FGDC Metadata (read-only)", " heading in the ", "Description", " tab", ". The FGDC elements will be ordered correctly in the output XML file.", "The exported file can be validated with the FGDC XML Schema or DTD, or the USGS metadata utility known as mp. The ", "USGS MP Metadata Translator", " performs this operation internally before processing the item's FGDc-format metadata content.", "Metadata that is published to an external website in HTML format will be indexed by Internet search engines such as Google if the website exposes its content correctly."], "parameters": [{"name": "source", "isOptional": false, "description": "The item whose metadata will be converted or a stand-alone XML file that will be converted. ", "dataType": "Data Element; Layer"}, {"name": "xslt", "isOptional": false, "description": "A W3C-compliant XSLT 1.0 stylesheet file that defines the transformation that will be performed. Several stylesheets are provided with ArcGIS and are available in the <ArcGIS Installation Location>\\Metadata\\Stylesheets folder. ", "dataType": "File"}, {"name": "output", "isOptional": false, "description": "A file that will be created containing the converted metadata. The type of file created depends on the output method specified in the XSLT stylesheet. ", "dataType": "File"}, {"name": "xsltparam", "isOptional": true, "description": "An XML file or string that will be passed to the XSLT stylesheet. To capture this parameter in the XSLT stylesheet, add <xsl:param name=\"gpparam\" /> to the top of the XSLT stylesheet after the xsl:output element and before the first xsl:template element. See add unique identifier.xslt for an example. ", "dataType": "File; String"}]},
{"syntax": "XMLSchemaValidator_conversion (source, schemaurl, {nsuri})", "name": "XML Schema Validation (Conversion)", "description": " Uses the .NET 3.5 Framework's XML software to validate an ArcGIS item's metadata or any XML file. The XML is checked to see  if it follows the structure and content rules outlined by an XML schema. Schemas written using the DTD or W3C XML Schema languages may be used with this tool.  ArcGIS metadata is not formatted in a manner that can be directly validated against an XML schema. However, other metadata geoprocessing tools can export ArcGIS metadata to XML files that are formatted to follow a metadata standard's XML Schema or DTD; use this tool to validate the exported XML file or a stand-alone metadata XML file that is already formatted correctly for a metadata standard's XML schema. If the metadata or XML file is not valid for the specified XML Schema or DTD, the warnings or errors returned by the XML software will appear in the tool's messages. ", "example": {"title": "Validate an ISO 19139 metadata XML file using an XML Schema", "description": "Validates a stand-alone XML file containing ISO 19139-formatted metadata using the online ISO 19139 XML Schema. These XML Schemas require you to specify the XML namespace you want to validate.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" #set local variables schema = \"http://www.isotc211.org/schemas/2005/gmd/metadataEntity.xsd\" namespace = \"http://www.isotc211.org/2005/gmd\" arcpy.XMLSchemaValidator_conversion ( \"metadata_19139.xml\" , schema , namespace )"}, "usage": ["Some XML Schemas require you to specify the XML namespace you want to validate, while others do not. If a value is provided in the Namespace URI parameter when none is required, any warnings or errors reported will be related to not finding the correct information in the XML Schema and won't identify whether the XML document is valid.", "For example, if you try to validate an XML document that follows the ", "Federal Geographic Data Committee (FGDC)", " ", "Content Standard for Digital Geospatial Metadata (CSDGM)", " using the XML Schema ", "http://www.fgdc.gov/schemas/metadata/fgdc-std-001-1998.xsd", ", you should not provide a value in the Namespace URI parameter. ", "When validating an XML document using a DTD, the name of the element that will be validated must be provided in the Namespace URI parameter. For example, if a DOCTYPE declaration were added to an XML document referencing an external DTD, that syntax might look like this: ", "<!DOCTYPE metadata SYSTEM \"http://www.fgdc.gov/schemas/metadata/fgdc-std-001-1998.dtd\">", ". To validate an XML document using this same external DTD with this tool, you would provide the value ", "http://www.fgdc.gov/schemas/metadata/fgdc-std-001-1998.dtd", " in the Schema URL parameter. Provide the name of the root element in the XML document in the Namespace URI parameter: ", "metadata", ". The root element's content is validated.", "If the XML document being validated contains an embedded reference to an external XML Schema or XML DTD, that schema will not be used for validation. The schema to use for validation and the namespace or root element to validate must be explicitly specified in this tool's Schema URL and Namespace URI parameters.", "The message ", "Adding schemas...", " indicates the schema is loading. All errors or warnings found in the schema will be reported here. ", "The message ", "Reading document...", " indicates when validation of the metadata or XML file begins. All errors or warnings found in the metadata or XML file are reported  here.", "If you have metadata XML files formatted to follow the ISO 19139 metadata standard, ", "Geographic information \u2014 Metadata \u2014 XML schema implementation", ", and you validate them against the 2005 version of the XML Schemas provided with this standard, the tool reports an issue with the ", "XML Schema files", ". This warning is only reported by the .NET 3.5 Framework's XML software and appears in the tool's messages as follows: ", "validation warning (423:8): Empty choice cannot be satisfied if 'minOccurs' is not equal to 0.", " The warning indicates there is a problem in the 8th character on line 423; it occurs in the coverage.xsd XML Schema. Other XML parsers don't report any problems with the same XML Schemas.", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", ".", " When using this tool in ModelBuilder, the ", "Output Metadata", " parameter is derived from the ", "Source Metadata", " parameter."], "parameters": [{"name": "source", "isOptional": false, "description": "The item whose metadata will be validated or a stand-alone XML file that will be validated. ", "dataType": "Data Element; Layer"}, {"name": "schemaurl", "isOptional": false, "description": "The XML Schema or XML DTD that describes the structure and content of a valid XML document. ", "dataType": "String"}, {"name": "nsuri", "isOptional": true, "description": "The XML namespace that will be validated for an XML Schema, if appropriate, or the root element of the document for an XML DTD. If this value is inappropriate for the XML Schema being used, provide the pound sign ( # ) instead of a namespace URI. ", "dataType": "String"}]},
{"syntax": "ValidateMetadataMultiple_conversion (Source_Metadata, Translator, Output_Folder, Schema_URL, {Namespace_URI})", "name": "Validate Metadata Multiple (Conversion)", "description": "Exports metadata for many ArcGIS items to a designated folder, then validates the exported files. This tool is a model that uses  Validate Metadata  to validate metadata for many ArcGIS items. ", "example": {"title": "Validate metadata for many ArcGIS items", "description": "Exports metadata for many ArcGIS items to the specified folder, then validates the exported files.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/yellowstone.gdb\" #set local variables sources = \"roads;water;lakes;rivers;streams\" dir = arcpy.GetInstallInfo ( \"desktop\" )[ \"InstallDir\" ] translator = dir + \"Metadata/Translator/ESRI_ISO2ISO19139.xml\" schema = \"http://www.isotc211.org/schemas/2005/gmd/metadataEntity.xsd\" namespace = \"http://www.isotc211.org/2005/gmd\" arcpy.ValidateMetadataMultiple_conversion ( sources , translator , \"c:/data/19139metadata\" , schema , namespace )"}, "usage": ["The output XML files are named by appending ", "_export.xml", " to the item's name and are stored in the specified folder."], "parameters": [{"name": "Source_Metadata", "isOptional": false, "description": "The item whose metadata will be validated or a stand-alone XML file that will be validated. ", "dataType": "Data Element"}, {"name": "Translator", "isOptional": false, "description": "An XML file that defines the conversion that will be performed. The translator files provided with ArcGIS for Desktop can be found in the <ArcGIS Installation Location>\\Metadata\\Translator folder. The following translators are provided: ARCGIS2FGDC.xml \u2014 Translates content stored in the ArcGIS metadata format to the FGDC CSDGM XML format. This translator is used by default when you export metadata from the Description tab using the FGDC CSDGM Metadata style. Metadata is converted using an XSLT transformation and won't produce a log file. ARCGIS2ISO19139.xml \u2014 Translates content stored in the ArcGIS metadata format to the ISO 19139 XML format. This translator is used by default when you export metadata from the Description tab using any of the ISO-based metadata styles. It is the preferred translator for exporting metadata to the ISO 19139 XML format. Metadata is converted using an XSLT transformation and won't produce a log file. ESRI_ISO2ISO19139.xml \u2014 Translates content stored in either the ArcGIS metadata format or the ESRI-ISO metadata format to the ISO 19139 XML format. This translator is provided for backwards compatibility to support existing models and Python scripts. It has some known limitations with exporting metadata to the ISO 19139 XML format. Use the ARCGIS2ISO19139.xml translator instead. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ESRI_ISO.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ArcGIS metadata format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . This translator is used when you import FGDC-formatted metadata by running the Import Metadata tool with the FROM_FGDC type and when you upgrade metadata by running the Upgrade Metadata tool with the FGDC_TO_ARCGIS type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ISO19139.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ISO 19139 XML format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. ISO19139_2ESRI_ISO.xml \u2014 Translates content stored in the ISO 19139 XML format to the ArcGIS metadata format. This translator is used when you import ISO 19139-formatted metadata by running the Import Metadata tool with the FROM_ISO_19139 type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine.", "dataType": "File"}, {"name": "Output_Folder", "isOutputFile": true, "isOptional": false, "description": "An existing folder where the output XML files containing the converted metadata will be stored. ", "dataType": "Folder"}, {"name": "Schema_URL", "isOptional": false, "description": "The XML Schema or XML DTD that describes the structure and content of a valid XML document. ", "dataType": "String"}, {"name": "Namespace_URI", "isOptional": true, "description": "The XML namespace that will be validated for an XML Schema, if appropriate, or the root element of the document for an XML DTD. If this value is inappropriate for the XML Schema being used, provide the pound sign ( # ) instead of a namespace URI. ", "dataType": "String"}]},
{"syntax": "ValidateMetadata_conversion (Source_Metadata, Translator, Schema_URL, {Namespace_URI}, Output_File)", "name": "Validate Metadata (Conversion)", "description": " Exports metadata to a standard metadata format then validates the exported file.  ArcGIS metadata can't be directly validated against a metadata standard's XML schema. This tool validates ArcGIS metadata by first exporting it to an XML file that is formatted to follow a metadata standard's XML schema using  Export Metadata , then validating the exported file using  XML Schema Validation . Only use this tool to validate ArcGIS metadata, not a stand-alone XML file that is already formatted correctly for a metadata standard's XML format. Any validation errors and warnings will be reported in the tool's messages. ", "example": {"title": "Validate metadata for an ArcGIS item", "description": "Exports ArcGIS metadata to an XML file and validates the exported file using an XML Schema.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" #set local variables dir = arcpy.GetInstallInfo ( \"desktop\" )[ \"InstallDir\" ] translator = dir + \"Metadata/Translator/ESRI_ISO2ISO19139.xml\" schema = \"http://www.isotc211.org/schemas/2005/gmd/metadataEntity.xsd\" namespace = \"http://www.isotc211.org/2005/gmd\" arcpy.ValidateMetadata_conversion ( \"vegtype.dbf\" , translator , schema , namespace , \"vegtype_19139.xml\" )"}, "usage": ["Some XML Schemas require you to specify the XML namespace you want to validate, while others do not. If a value is provided in the Namespace URI parameter when none is required, any warnings or errors reported will be related to not finding the correct information in the XML Schema and won't identify whether the XML document is valid.", "For example, if you try to validate an XML document that follows the ", "Federal Geographic Data Committee (FGDC)", " ", "Content Standard for Digital Geospatial Metadata (CSDGM)", " using the XML Schema ", "http://www.fgdc.gov/schemas/metadata/fgdc-std-001-1998.xsd", ", you should not provide a value in the Namespace URI parameter. ", "When validating an XML document using a DTD, the name of the element that will be validated must be provided in the Namespace URI parameter. For example, if a DOCTYPE declaration were added to an XML document referencing an external DTD, that syntax might look like this: ", "<!DOCTYPE metadata SYSTEM \"http://www.fgdc.gov/schemas/metadata/fgdc-std-001-1998.dtd\">", ". To validate an XML document using this same external DTD with this tool, you would provide the value ", "http://www.fgdc.gov/schemas/metadata/fgdc-std-001-1998.dtd", " in the Schema URL parameter. Provide the name of the root element in the XML document in the Namespace URI parameter: ", "metadata", ". The root element's content is validated.", "If the XML document being validated contains an embedded reference to an external XML Schema or XML DTD, that schema will not be used for validation. The schema to use for validation and the namespace or root element to validate must be explicitly specified in this tool's Schema URL and Namespace URI parameters.", "Specify an output file if you want to keep the exported metadata file. For example, after checking that the exported file validated successfully you might publish it to a metadata catalog. ", "The output files produced by this tool can't be stored in a geodatabase. If the ", "Current_workspace", " environment is set to a geodatabase, the output files will be stored in a different location, as described below."], "parameters": [{"name": "Source_Metadata", "isOptional": false, "description": "The item whose metadata will be validated or a stand-alone XML file that will be validated. ", "dataType": "Data Element; Layer"}, {"name": "Translator", "isOptional": false, "description": "An XML file that defines the conversion that will be performed. The translator files provided with ArcGIS for Desktop can be found in the <ArcGIS Installation Location>\\Metadata\\Translator folder. The following translators are provided: ARCGIS2FGDC.xml \u2014 Translates content stored in the ArcGIS metadata format to the FGDC CSDGM XML format. This translator is used by default when you export metadata from the Description tab using the FGDC CSDGM Metadata style. Metadata is converted using an XSLT transformation and won't produce a log file. ARCGIS2ISO19139.xml \u2014 Translates content stored in the ArcGIS metadata format to the ISO 19139 XML format. This translator is used by default when you export metadata from the Description tab using any of the ISO-based metadata styles. It is the preferred translator for exporting metadata to the ISO 19139 XML format. Metadata is converted using an XSLT transformation and won't produce a log file. ESRI_ISO2ISO19139.xml \u2014 Translates content stored in either the ArcGIS metadata format or the ESRI-ISO metadata format to the ISO 19139 XML format. This translator is provided for backwards compatibility to support existing models and Python scripts. It has some known limitations with exporting metadata to the ISO 19139 XML format. Use the ARCGIS2ISO19139.xml translator instead. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ESRI_ISO.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ArcGIS metadata format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . This translator is used when you import FGDC-formatted metadata by running the Import Metadata tool with the FROM_FGDC type and when you upgrade metadata by running the Upgrade Metadata tool with the FGDC_TO_ARCGIS type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ISO19139.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ISO 19139 XML format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. ISO19139_2ESRI_ISO.xml \u2014 Translates content stored in the ISO 19139 XML format to the ArcGIS metadata format. This translator is used when you import ISO 19139-formatted metadata by running the Import Metadata tool with the FROM_ISO_19139 type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine.", "dataType": "File"}, {"name": "Schema_URL", "isOptional": false, "description": "The XML Schema or XML DTD that describes the structure and content of a valid XML document. ", "dataType": "String"}, {"name": "Namespace_URI", "isOptional": true, "description": "The XML namespace that will be validated for an XML Schema, if appropriate, or the root element of the document for an XML DTD. If this value is inappropriate for the XML Schema being used, provide the pound sign ( # ) instead of a namespace URI. ", "dataType": "String"}, {"name": "Output_File", "isOutputFile": true, "isOptional": false, "description": "A stand-alone XML file that will be created containing the converted metadata. ", "dataType": "File"}]},
{"syntax": "USGSMPTranslator_conversion (source, {config}, {conversion}, {output}, {errors})", "name": "USGS MP Metadata Translator (Conversion)", "description": "Uses the USGS metadata parser utility, known as mp, to export or validate  FGDC  metadata content. The mp utility is created and maintained by the USGS for managing metadata that follows the FGDC  Content Standard for Digital Geospatial Metadata (CSDGM) . A version of mp is provided with ArcGIS.", "example": {"title": "Export an ArcGIS item's existing FGDC metadata", "description": "Exports information in an ArcGIS item's metadata that is formatted according to the FGDC CSDGM standard to an HTML file.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.USGSMPTranslator_conversion ( \"yellowstone.mxd\" , \"#\" , \"HTML\" , \"yellowstone_map.html\" , \"yellowstone_map.log\" )"}, "usage": ["Documentation for the mp utility can be found on the ", "USGS metadata tools Web site", ".", "FGDC metadata elements, if they exist in ArcGIS metadata, will not be in the correct order as specified by the FGDC CSDGM rules. If the mp utility is used independently to validate or export ArcGIS metadata, mp will record warnings in the log file indicating the elements are out of order. This tool internally processes ArcGIS metadata using the ", "_MPXML2.xsl", " stylesheet in the   ", "<ArcGIS Installation Location>\\Metadata\\Stylesheets", "  folder\r\nto select only FGDC metadata elements and order them correctly before using mp to export or validate the resulting XML file. ", "When using the tool dialog, the default ", "Output File", " name will have a file extension that is appropriate for the conversion type that is selected at the time that the source metadata is defined. If you change the conversion type after specifying the source, delete the default output file name and a new file name will be generated with an appropriate file extension. Or, you can change the output file extension manually.", "For all conversion types, the source metadata will be validated according to the FGDC CSDGM rules and any warnings or errors regarding the metadata content will be reported in the tool's messages. If a ", "Log File", " is specified, the same warnings or errors will be saved to the specified file. If the \"none\" conversion is specified, only the Log File will be created. A conversion won't be performed and an output file won't be generated.", "The output files produced by this tool can't be stored in a geodatabase. If the ", "Current_workspace", " environment is set to a geodatabase, the output files will be stored in a different location, as described below.", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", "."], "parameters": [{"name": "source", "isOptional": false, "description": "The item whose metadata will be converted or a stand-alone XML file that will be converted. ", "dataType": "Data Element; Layer"}, {"name": "config", "isOptional": true, "description": "A file that defines custom parameters that mp will consider when processing the metadata. To export metadata without using a configuration file, provide # instead of a file name. ", "dataType": "File"}, {"name": "conversion", "isOptional": true, "description": "The type of conversion that will take place. By default, the \"XML\" conversion will be performed. none \u2014 No conversion will be performed. XML \u2014 An XML file will be created as output. HTML \u2014 An HTML file will be created as output. TEXT \u2014 A text file will be created as output. FAQ \u2014 An HTML file in FAQ format will be created as output. SGML \u2014 An SGML file will be created as output. DIF \u2014 A text file in DIF format will be created as output.", "dataType": "String"}, {"name": "output", "isOptional": true, "description": "A file that will be created containing the converted metadata. The type of file created is defined by the conversion type. To check for problems in the FGDC metadata using mp and not produce a output file, provide # instead of a file name. ", "dataType": "File"}, {"name": "errors", "isOptional": true, "description": "A text file that will be created listing the warnings and errors that occurred during the conversion process. To export metadata without producing a log file, provide the pound sign ( # ) instead of a file name. ", "dataType": "File"}]},
{"syntax": "UpgradeMetadata_conversion (Source_Metadata, Upgrade_Type)", "name": "Upgrade Metadata (Conversion)", "description": " Updates an ArcGIS item's metadata or a stand-alone XML file to the current ArcGIS metadata format.  The current release of ArcGIS will only maintain information in the ArcGIS metadata format. For example, if an ArcGIS item has metadata in another format it must be upgraded to ArcGIS metadata before ArcGIS will automatically update it with the item's current properties; the item's properties are recorded in ArcGIS metadata elements. Upgrading metadata for the current release of ArcGIS will not change the existing metadata except to add ArcGIS metadata alongside the existing information. The existing metadata will remain unchanged.  Learn more about upgrading metadata  ", "example": {"title": "Upgrade FGDC metadata to ArcGIS metadata", "description": "Upgrades information in an ArcGIS item's metadata formatted following the FGDC CSDGM standard to ArcGIS metadata.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.UpgradeMetadata_conversion ( \"locationMap.mxd\" , \"FGDC_TO_ARCGIS\" )"}, "usage": ["When you view an ArcGIS item's metadata or a stand-alone metadata XML file in the ", "Description tab", ", a notification will prompt you if an item has FGDC-format metadata that hasn't been upgraded yet; clicking Yes will open this tool with the Upgrade Type set to FGDC_TO_ARCGIS. Also, an ", "Upgrade", " button may be available that will run  this tool. If you have many items or metadata XML files you want to upgrade, run this tool as a ", "batch process", " to upgrade all your metadata at once.", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", ".", " When using this tool in ModelBuilder, the ", "Output Metadata", " parameter is derived from the ", "Source Metadata", " parameter.", "If you do not  have write access to the ArcGIS item or its metadata or the stand-alone metadata XML file that you are trying to modify, this tool will complete successfully, but the item's original metadata will remain unchanged."], "parameters": [{"name": "Source_Metadata", "isOptional": false, "description": "The item whose metadata will be upgraded, or a stand-alone XML file that will be upgraded. ", "dataType": "Data Element; Layer"}, {"name": "Upgrade_Type", "isOptional": false, "description": "The type of conversion that will take place. An upgrade type must be specified; otherwise, no conversion will be performed. ESRIISO_TO_ARCGIS \u2014 Upgrades ESRI-ISO-format metadata. ESRI-ISO metadata is typically created with the ISO metadata editing wizard provided with ArcGIS Desktop 9.3.1 and earlier releases. FGDC_TO_ARCGIS \u2014 Upgrades FGDC-format metadata. For example, FGDC metadata may have been created in ArcGIS Desktop 9.3.1 with the FGDC metadata editor. FGDC metadata may have been created outside ArcGIS.", "dataType": "String"}]},
{"syntax": "SynchronizeMetadata_conversion (source, synctype)", "name": "Synchronize Metadata (Conversion)", "description": " Automatically updates an ArcGIS item's metadata with the current properties of the item.  For example, if the metadata describes the item as having one projection but the item's projection has changed since the last automatic update, the old projection information in the metadata will be replaced with the new projection information. ", "example": {"title": "Synchronize an ArcGIS item's metadata", "description": "Metadata for an ArcGIS item is updated to contain the current properties of the item only if the item already has metadata; new metadata won't be created.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.SynchronizeMetadata_conversion ( \"vegetation\" , \"NOT_CREATED\" )"}, "usage": ["By default, metadata is automatically updated when anyone who has write access to the ArcGIS item views its metadata. Metadata can also be synchronized by running this tool. The option to turn off synchronization when you view metadata doesn't affect how this tool operates.", "If someone edits an ArcGIS item's data or changes its properties and does not view the metadata, the properties recorded in the metadata are out of date with the actual properties of the item. If you were to use or publish the item's metadata in this state without first running this tool the information in the metadata would not be current.", "The ", "Export Metadata", ", ", "Export Metadata Multiple", ", ", "Validate Metadata", ", and ", "Validate Metadata Multiple", " tools all update the metadata automatically before performing other operations. The ", "Import Metadata", " and ", "Upgrade Metadata", " tools automatically update the metadata after performing other operations. ", "If your organization's metadata workflow incorporates other operations or tools that don't synchronize the metadata you may want to write a Python script that would run this tool as needed. For example, if you use a utility that publishes metadata for all items in a geodatabase to a metadata catalog nightly, ideally you would synchronize the metadata content nightly before publishing to  ensure the published information is always current.", "XML files do not support metadata synchronization. If you try to synchronize a stand-alone metadata XML file, the tool will complete successfully but the XML file will remain unchanged. ", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", ".", " When using this tool in ModelBuilder, the ", "Output Metadata", " parameter is derived from the ", "Source Metadata", " parameter.", "If you do not  have write access to the ArcGIS item or its metadata or the stand-alone metadata XML file that you are trying to modify, this tool will complete successfully, but the item's original metadata will remain unchanged."], "parameters": [{"name": "source", "isOptional": false, "description": "The item whose metadata will be synchronized. ", "dataType": "Data Element; Layer"}, {"name": "synctype", "isOptional": false, "description": "The type of synchronization that will take place. ALWAYS \u2014 Properties of the source item are always added to or updated in its metadata. Metadata will be created if it doesn't already exist. This is the deault. ACCESSED \u2014 Properties of the source item are added to or updated in its metadata when it is accessed. Metadata will be created if it doesn't already exist. CREATED \u2014 Metadata will be created and properties of the source item will be added to it if the item doesn't already have metadata. NOT_CREATED \u2014 Properties of the source item are added to or updated in existing metadata. OVERWRITE \u2014 The same as \"ALWAYS\" except all information that can be recorded automatically in the metadata will be recorded. Any properties typed in by a person will be replaced with the item's actual properties.", "dataType": "String"}]},
{"syntax": "MDPublisher_conversion (source, publisher, {url}, {service}, {user}, {password})", "name": "Metadata Publisher (Conversion)", "description": "Publishes metadata to a metadata catalog such as an ArcIMS Metadata Service. The  Metadata Publisher  tool retrieves the source item's metadata, then a copy of this metadata document is passed to the  Publisher  specified in the tool. If the source item is a stand-alone XML file, a copy of the file itself will be passed to the Publisher. The Publisher uses information from the tool parameters and from the metadata document to create a request to publish the document to the specified metadata catalog.", "example": {"title": "Publish an ArcGIS item's metadata to an ArcIMS Metadata Service", "description": "An ArcGIS item's metadata is published to an ArcIMS Metadata Service. This sample won't complete successfully unless a valid ArcIMS Metadata Service is identified and a valid User Name and Password with sufficient privileges are provided. ", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" #set local variables dir = arcpy.GetInstallInfo ( \"desktop\" )[ \"InstallDir\" ] pub = dir + \"Metadata/Publisher/arcimsmd.xml\" arcpy.MDPublisher_conversion ( \"locations\" , pub , \"http://myserver\" , \"Metadata\" , \"myUsername\" , \"myPassword\" )"}, "usage": ["This topic describes how the current version of ArcGIS for Desktop can be used with a Metadata Service hosted by a previous version of ArcIMS.", "A publisher is provided with ", "ArcGIS for Desktop", " that supports publishing ArcGIS metadata to an ArcIMS Metadata Service. This publisher, named ", "arcimsmd.xml", ", can be found in the ", "<ArcGIS Installation Location>\\Metadata\\Publisher", " folder. ", "This publisher uses the contents of the metadata to construct an appropriate ArcXML PUBLISH_METADATA request. FGDC metadata content is checked first. If appropriate information isn't found, the publisher then checks ArcGIS metadata content. The metadata will be published even if it does not contain the information required by the ArcIMS Metadata Explorer application.", "Documents in an ArcIMS Metadata Service are uniqely identified using a ", "GUID", ". When you publish a document to an ArcIMS Metadata Service, if the document doesn't already have a GUID in the ", "<rootElement>/Esri/PublishedDocID", " element one will be generated and stored in that location if you have write access to the metadata. This identifier ensures that when the document is later updated and republished, the existing copy on the server is replaced.", "If you publish metadata using a custom model that removes private information before publishing and if the ", "Source Metadata", " item did not previously have a unique identifier, the identifier created by this tool will be saved to the model's intermediate data but not to the original item.", "To avoid this problem, you can add a unique identifier to an item's metadata before it is published using the ", "XSLT Transformation", " tool with the ", "add unique identifier.xslt", " file provided in the ", "<ArcGIS Installation Location>\\Metadata\\Stylesheets\\gpTools", " folder. This operation is safe\u2014if the metadata already has a unique identifier the existing identifier will remain untouched. Be sure to save the version of the metadata containing the new identifier with the original ArcGIS item; otherwise, the next time its metadata is published the ArcIMS Metadata Service will have two documents describing the same item with two different identifiers.", "If you publish metadata to an ArcIMS Metadata Service you must provide credentials for a user who has been given permission to publish documents to that service. If you have trouble publishing, connect to ArcIMS server in the ", "Catalog", " window with the same credentials. If you have permission to publish documents with those credentials the Metadata Service's icon will show a hand holding a pencil ", ".", "This tool will not publish metadata associated with some ArcGIS items such as map, globe, and scene documents; layer files; text files; Excel files; file types; ArcIMS services; and geoprocessing tools. Publishing these items will produce an ", "Unsupported source", " error message.", "If the metadata document does not consist of well-formed XML and the metadata is published with this tool, the tool will produce an ", "Unsupported source", " error message.", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", ".", " When using this tool in ModelBuilder, the ", "Output Metadata", " parameter is derived from the ", "Source Metadata", " parameter."], "parameters": [{"name": "source", "isOptional": false, "description": "The item whose metadata will be published, or a stand-alone XML file that will be published. ", "dataType": "Data Element; Layer"}, {"name": "publisher", "isOptional": false, "description": "An XML file that defines how the metadata will be published. ", "dataType": "File"}, {"name": "url", "isOptional": true, "description": "The URL of a Web Service that hosts the metadata catalog, if appropriate. For example, the URL of an ArcIMS Server that provides a Metadata Service. ", "dataType": "String"}, {"name": "service", "isOptional": true, "description": "The name of the service to which you want to publish, if appropriate. For example, for an ArcIMS Metadata Service this is the case-sensitive name of the Metadata Service. ", "dataType": "String"}, {"name": "user", "isOptional": true, "description": "The name used to access the metadata catalog when publishing documents, if appropriate. For example, when publishing documents to an ArcIMS Metadata Service you must log in to the service using a name that has been granted metadata_publisher privileges or a higher privilege. ", "dataType": "String"}, {"name": "password", "isOptional": true, "description": "The password you used to access the metadata catalog when publishing documents, if appropriate. For example, when publishing documents to an ArcIMS Metadata Service, this is the password required to log in with the specified User Name. ", "dataType": "String"}]},
{"syntax": "MetadataImporter_conversion (source, target)", "name": "Metadata Importer (Conversion)", "description": "Copies metadata from the source item to the target item. Metadata is retrieved from the source item and transferred to the target item without changing it. The source and target may be ArcGIS items or stand-alone metadata XML files.  The source metadata should be ArcGIS metadata. If the imported information is not stored in the ArcGIS metadata format it must be upgraded to ArcGIS metadata before it  will be automatically updated to contain the item's intrinsic properties. This tool is useful for saving changes made to your metadata with an XSLT stylesheet. For example, a model could update metadata using  XSLT Transformation  with a custom stylesheet, then use this tool to import the changes to the original ArcGIS item. ", "example": {"title": "Import an ArcGIS metadata template to an item", "description": "Imports an ArcGIS metadata template XML file containing information shared by all of a project's data. The target item's metadata won't be updated to contain its properties after. Import a template before editing the item's metadata.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.MetadataImporter_conversion ( \"c:/data/template.xml\" , \"places\" )"}, "usage": ["Metadata may include unique identifiers that help to manage documents in a metadata catalog. This tool does not remove any unique identifiers in the source metadata before importing it. This is the desired behavior if you modify the source item's metadata as described above using an XSLT stylesheet, for example, to update contact information, and need to import the result back to the original item. In this case, you want the metadata to keep the same unique identifier it had before. ", "However, if you want to import metadata created outside of ArcGIS or to copy information from one item to another, use ", "Import Metadata", " instead; it removes unique identifiers from the imported information. Many items should not share the same unique identifier.", "After the source metadata is copied to the target, no changes are made to the target item's metadata. For example, the target item's metadata will not be synchronized to contain the item's current properties.", " Text or HTML files containing metadata can't be imported using this tool. If you try to import a text file using this tool, it will look for any metadata that may exist describing the text file's data and import that. If a text file actually contains metadata content, that content must be reformatted to follow a metadata standard's XML format. Then, the metadata content contained within the XML file can be imported using this tool. ", "This tool can import one source item to one target item. To import one source item to many target items or to import different source items to different target items, open the tool in batch mode and set the tool's parameters appropriately.", "The ", "Source Metadata", " parameter has a complex data type. If you use this tool in a model, create a variable for the ", "Source Metadata", " parameter in ModelBuilder by right-clicking the tool and choosing ", "Make Variable ", ">", " From Parameter ", ">", " Source Metadata", ".", " When using this tool in ModelBuilder, the ", "Output Metadata", " parameter is derived from the ", "Target Metadata", " parameter.", "If you do not  have write access to the ArcGIS item or its metadata or the stand-alone metadata XML file that you are trying to modify, this tool will complete successfully, but the item's original metadata will remain unchanged."], "parameters": [{"name": "source", "isOptional": false, "description": "The item whose metadata will be imported or a stand-alone XML file that will be imported. If the source item is a stand-alone file, it must contain well-formed XML data. ", "dataType": "Data Element; Layer"}, {"name": "target", "isOptional": false, "description": "The item to which the metadata will be imported or a stand-alone XML file that will be replaced. ", "dataType": "Data Element; Layer"}]},
{"syntax": "ImportMetadata_conversion (Source_Metadata, Import_Type, Target_Metadata, Enable_automatic_updates)", "name": "Import Metadata (Conversion)", "description": " Imports metadata to the target item after converting the source item's metadata to ArcGIS metadata, if appropriate. The source and target may be ArcGIS items or stand-alone metadata XML files.  This tool processes the source metadata before importing it and updates the target metadata after. Any intrinsic properties of the source item that were added automatically to its metadata by ArcGIS are removed along with any unique  identifiers before converting the information to the ArcGIS metadata format, if necessary. After the imported information is saved, the target item's metadata is automatically updated with its intrinsic properties. This tool is useful for copying metadata from one item to another when you start creating its metadata; the imported metadata acts as a template. Using another metadata document as a template can save time if two items share some information such as legal restrictions or a description of the project for which they were created.", "example": {"title": "ImportMetadata example (Python window)", "description": "Imports the source data's FGDC metadata to a geodatabase item. The original metadata is converted to ArcGIS metadata. Some methods of importing data into a geodatabase handle the original item's metadata while others do not.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data/data.gdb\" arcpy.ImportMetadata_conversion ( \"c:/data/streams.shp\" , \"FROM_FGDC\" , \"streams\" )"}, "usage": ["Metadata may include unique identifiers that help to manage documents in a metadata catalog. As a precaution, this tool removes any unique identifiers in the source metadata before importing it. This is the desired behavior if your intention is to copy common information from one item to another. If many items share the same identifier it is no longer unique, and problems can arise if the metadata is later published. ", "However, if you modify a source item's metadata using the ", "XSLT Transformation", " tool, for example, to update contact information, and need to import the resulting XML file back to the source item, use ", "Metadata Importer", " instead. In this case it is not appropriate to remove the source item's unique identifier or alter its content before importing the metadata.", "After importing, the metadata is updated automatically to include the current properties of the target item. By default, the ", "Enable automatic updates", " parameter is checked. With this option, the imported information will be modified to contain the actual properties of the item. Any properties not already recorded in the metadata will be added. This ensures the metadata will remain current as the item changes. For example, if the imported metadata included a title describing another item, that title will be replaced with the target item's name. You can edit the title later so it better describes the target item.", "If you choose not to enable automatic updates, properties of the target item that aren't recorded in the metadata will still be added. However, the imported information will never be updated by ArcGIS. For example, if the imported metadata describes the item as having a specific number of features and this number changes later, this change won't be recorded in the metadata when automatic updates occur. ", "By default, metadata is updated automatically when you view it.", "If you import items into a geodatabase using the ArcSDE administrative commands, the original item's metadata won't be imported. Import the original item's metadata into the new item with this tool.", "This tool can import one source item to one target item. To import one source item to many target items or to import different source items to different target items, open the tool in batch mode and set the tool's parameters appropriately.", " Text or HTML files containing metadata can't be imported using this tool. If you try to import a text file using this tool, it will look for any metadata that may exist describing the text file's data and import that. If a text file actually contains metadata content, that content must be reformatted to follow a metadata standard's XML format. Then, the metadata content contained within the XML file can be imported using this tool. ", " When using this tool in ModelBuilder, the ", "Output Metadata", " parameter is derived from the ", "Target Metadata", " parameter.", "If you do not  have write access to the ArcGIS item or its metadata or the stand-alone metadata XML file that you are trying to modify, this tool will complete successfully, but the item's original metadata will remain unchanged."], "parameters": [{"name": "Source_Metadata", "isOptional": false, "description": "The item whose metadata will be imported or a stand-alone XML file that will be imported. If the source item is a stand-alone file, it must contain well-formed XML data. ", "dataType": "Data Element; Layer"}, {"name": "Import_Type", "isOptional": false, "description": "The format of the metadata that will be imported. By default, the FROM_ISO_19139 conversion will be performed. FROM_ARCGIS \u2014 The source metadata is ArcGIS metadata. The metadata won't be converted. FROM_ESRIISO \u2014 The source metadata contains ESRI-ISO-formatted metadata; that is, it was created using the ISO metadata editor provided with ArcGIS Desktop 9.3.1 and earlier releases. The source metadata will be converted to ArcGIS metadata when you run the tool. FROM_FGDC \u2014 The source metadata is stored in the FGDC CSDGM metadata standard's XML format. The source metadata will be converted to ArcGIS metadata when you run the tool. FROM_ISO_19139 \u2014 The source metadata is formatted according to the ISO 19139 metadata standard. The source metadata will be converted to ArcGIS metadata when you run the tool. ", "dataType": "String"}, {"name": "Target_Metadata", "isOptional": false, "description": "The item to which the metadata will be imported or a stand-alone XML file that will be replaced. ", "dataType": "Data Element; Layer"}, {"name": "Enable_automatic_updates", "isOptional": false, "description": " ENABLE \u2014 Information in the imported metadata describing the item's properties will be modified to contain the actual item properties. For example, if the imported metadata includes the number of features contained by a feature class, this number will be updated in the item's metadata by the metadata synchronization process after the features have been edited in ArcGIS. Also, additional properties that were not present in the imported metadata and that can be synchronized for the item will be added. This is the default. DISABLE \u2014 Imported information won't be modified. For example, the number of features contained by a feature class won't be updated in the item's metadata by the metadata synchronization process after the features have been edited in ArcGIS; the metadata will always contain the old, out-of-date number. Additional properties of the item that were not present in the imported metadata and that can be synchronized for the item will be added. ", "dataType": "Boolean"}]},
{"syntax": "ExportMetadataMultiple_conversion (Source_Metadata, Translator, Output_Folder)", "name": "Export Metadata Multiple (Conversion)", "description": "Exports metadata for many ArcGIS items to a designated folder. This tool is a model that uses  Export Metadata  to export metadata for many ArcGIS items. ", "example": {"title": "Export metadata for many ArcGIS items", "description": "Updates and exports metadata for several ArcGIS items to XML files that are formatted correctly for the ISO 19139 metadata standard. The output XML files are stored in the specified folder.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" #set local variables sources = \"vegetation;vegtypes;vegreport.doc;vegmap.mxd;veglayer.lyr\" dir = arcpy.GetInstallInfo ( \"desktop\" )[ \"InstallDir\" ] translator = dir + \"Metadata/Translator/ESRI_ISO2ISO19139.xml\" arcpy.ExportMetadataMultiple_conversion ( sources , translator , \"c:/data/export\" )"}, "usage": ["The output XML files are named by appending ", "_export.xml", " to the item's name and are stored in the specified folder."], "parameters": [{"name": "Source_Metadata", "isOptional": false, "description": "The set of items whose metadata will be converted, or the set of stand-alone XML files that will be converted. ", "dataType": "Data Element"}, {"name": "Translator", "isOptional": false, "description": "An XML file that defines the conversion that will be performed. The translator files provided with ArcGIS for Desktop can be found in the <ArcGIS Installation Location>\\Metadata\\Translator folder. The following translators are provided: ARCGIS2FGDC.xml \u2014 Translates content stored in the ArcGIS metadata format to the FGDC CSDGM XML format. This translator is used by default when you export metadata from the Description tab using the FGDC CSDGM Metadata style. Metadata is converted using an XSLT transformation and won't produce a log file. ARCGIS2ISO19139.xml \u2014 Translates content stored in the ArcGIS metadata format to the ISO 19139 XML format. This translator is used by default when you export metadata from the Description tab using any of the ISO-based metadata styles. It is the preferred translator for exporting metadata to the ISO 19139 XML format. Metadata is converted using an XSLT transformation and won't produce a log file. ESRI_ISO2ISO19139.xml \u2014 Translates content stored in either the ArcGIS metadata format or the ESRI-ISO metadata format to the ISO 19139 XML format. This translator is provided for backwards compatibility to support existing models and Python scripts. It has some known limitations with exporting metadata to the ISO 19139 XML format. Use the ARCGIS2ISO19139.xml translator instead. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ESRI_ISO.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ArcGIS metadata format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . This translator is used when you import FGDC-formatted metadata by running the Import Metadata tool with the FROM_FGDC type and when you upgrade metadata by running the Upgrade Metadata tool with the FGDC_TO_ARCGIS type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. FGDC2ISO19139.xml \u2014 Translates content stored in the FGDC CSDGM XML format to the ISO 19139 XML format; that is, it translates metadata content that is visible under the FGDC Metadata (read-only) heading in the Description tab . Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine. ISO19139_2ESRI_ISO.xml \u2014 Translates content stored in the ISO 19139 XML format to the ArcGIS metadata format. This translator is used when you import ISO 19139-formatted metadata by running the Import Metadata tool with the FROM_ISO_19139 type. Metadata is converted using the Esri Metadata Translator tool's translation engine and produces a log file containing messages produced by the translation engine.", "dataType": "File"}, {"name": "Output_Folder", "isOutputFile": true, "isOptional": false, "description": "An existing folder where the output XML files containing the converted metadata will be stored. ", "dataType": "Folder"}]},
{"syntax": "Clip_arc (in_cover, clip_cover, out_cover, {feature_type}, {fuzzy_tolerance})", "name": "Clip (Coverage)", "description": "Uses the outside polygon boundary of the clip coverage to cookie-cut features and attributes from the input coverage. \r\n Learn more about how Clip works \r\n", "example": {"title": "Clip example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a new coverage that contains the clipped out area of a larger coverage. ", "code": "# Name: Clip_Example.py # Description: Clips a subset out of a polygon coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"tongass1\" clipCover = \"tong_basin4\" outCover = \"C:/output/basin4\" featureType = \"POLY\" # Execute Clip arcpy.Clip_arc ( inCover , clipCover , outCover , featureType , \"\" )"}, "usage": ["A fuzzy tolerance value of zero will not be accepted by CLIP.", "The Clip Coverage must have polygon topology.", "When the input coverage contains linear data belonging to different planar graphs, the data will be maintained in the output coverage. For example, with coincident or colinear arcs, such as arcs representing utility cables at different levels or a road following a stream, the coincident and colinear line segments will be preserved. However, additional vertices may be inserted. In the case of intersecting arcs, such as a road passing over a stream, nodes will not be inserted at the apparent intersection.", "Boundaries of interior polygons in the Clip Coverage are not used in CLIP. Any Clip Coverage polygon whose internal number is greater than one is considered inside the Clipping window.", "The User-ID for each feature will be the same in output coverage as it is in input coverage.", "When clipping polygons, new label points for polygons are only generated when necessary. Each old polygon keeps its original label point position if the label is within the clipping boundary.", "Annotation", " is saved if its lower left starting point falls within the clipping polygon.", "New nodes created on the clipping boundary have their attributes set to zero.", "Route systems are maintained for LINE, NET, LINK, and RAW options but ignored on the POLY option. Route systems are duplicated for arcs split into multiple pieces and eliminated for eliminated arcs. CLIP maintains all route system subclasses.", "Route systems are duplicated for arcs split into multiple pieces and eliminated for removed arcs. The RAT<subclass>-ID item is the unique route identifier and is used to remove duplicates. If you need to maintain all the user-defined attributes, make sure all the route IDs are unique before running CLIP; otherwise, some user-defined attributes may be lost.", "When all the regions are removed, the region subclasses are maintained as empty subclasses.", "Region subclasses of the input coverage are maintained with the POLY and NET options. Regions in the input coverage are clipped in the output coverage by the extent of the Clip Coverage.", "The output coverage tics are copied from the input coverage.", "Annotation features from the input coverage are clipped and saved in the output coverage.", "If an NAT exists in the input coverage, it will be updated in the output coverage. New nodes will have their attributes set to zero.", "The output coverage inherits these data model contents from the input coverage:", "The coordinate precision of each output coverage is determined by the current processing rule as set by the ", "Derived Precision", " environment setting. If the processing rule has not been established during the current session, the output coverages will be in the same precision as the input coverage.", "Projection files will be compared for similarity using the level of comparison specified with the ", "Compare Projections", " environment setting."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing features to be clipped. ", "dataType": "Coverage"}, {"name": "clip_cover", "isOptional": false, "description": "The coverage whose outer polygon defines the clipping region. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be created. The output coverage cannot already exist. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "The feature class to be clipped: POLY \u2014 Polygons and region subclasses are clipped and a PAT is saved. Label points for remaining polygons are moved only if their original location falls outside the clipping boundary. Route systems are ignored. LINE \u2014 Arcs are clipped, and an AAT is saved. Route systems are maintained. POINT \u2014 Points are clipped, and a PAT is saved. NET \u2014 Polygons and arcs are clipped, and their PAT and AAT are saved. Route systems and regions are maintained and clipped. LINK \u2014 Arcs and points are clipped, and their AAT and PAT are saved. Route systems are maintained. RAW \u2014 Points, arcs, and annotation in a coverage with or without topology (no attribute files) are clipped. Route systems are maintained, but regions, PAT, and AAT are not saved. ", "dataType": "String"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in the output coverage. By default, the minimum fuzzy tolerance value from the input coverage and erase coverage is used. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}]},
{"syntax": "LasDatasetToRaster_conversion (in_las_dataset, out_raster, {value_field}, {interpolation_type}, {data_type}, {sampling_type}, {sampling_value}, {z_factor})", "name": "LAS Dataset To Raster (Conversion)", "description": "Creates  a raster using elevation, intensity,  or RGB values stored in the lidar files ( *.las ) referenced by the LAS dataset.", "example": {"title": "LasDatasetToRaster example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.LasDatasetToRaster_3d ( 'baltimore.lasd' , 'baltimore.tif' , 'INTENSITY' , 'TRIANGULATION LINEAR WINDOW_SIZE 10' , 'FLOAT' , 'CELLSIZE' , 10 , 3.28 )"}, "usage": ["When a LAS dataset is specified as input, all the data points in the LAS files it references will be processed.", "The LAS dataset layer can be used to filter LAS points by class code or return values. The layer can be created by using the ", "Make LAS Dataset Layer", " tool, or by loading the LAS dataset in ArcMap or ArcScene and specifying the desired class codes and return values through the layer properties dialog box.", "When exporting a large raster, consider specifying the ", "Output Data Type", " as an integer to save on disk space if the accuracy requirements of your z-values are such that they can be represented by integer data.", "Including a study area boundary as a clip constraint in the definition of the input LAS dataset is highly recommended. One reason is to prevent interpolation from occurring outside the real data extent of the survey. Secondly, there can be a severe performance penalty when using natural neighbor options if the data area is not properly defined. ", "When using BINNING only clip, erase, and replace constraints are honored. Breaklines and anchor points are not. The triangulation option honors all types of constraints but takes longer to execute."], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The location and name of the output raster. When storing a raster dataset in a geodatabase or in a folder such as an Esri Grid, no file extension should be added to the name of the raster dataset. A file extension can be provided to define the raster's format when storing it in a folder: If the raster is stored as a TIFF file or in a geodatabase, its raster compression type and quality can be specified using geoprocessing environment settings. .bil\u2014Esri BIL .bip\u2014Esri BIP .bsq\u2014Esri BSQ .dat\u2014ENVI DAT .img\u2014ERDAS IMAGINE .png\u2014PNG .tif\u2014TIFF", "dataType": "Raster Dataset"}, {"name": "value_field", "isOptional": true, "description": "Specifies the lidar information that will be used in generating the raster output. ELEVATION \u2014 Elevation from the lidar files will be used to create the raster. This is the default. INTENSITY \u2014 Intensity information from the lidar files will be used to create the raster. RGB \u2014 Imagery derived from the RGB values embedded with the lidar points will be used to create the raster.", "dataType": "String"}, {"name": "interpolation_type", "isOptional": false, "description": "The interpolation method used to produce the raster. BINNING \u2014Cell values are obtained using the points that fall in the extent of the cell, with the exception of cells that do not contain points in their extent. The following options are available for this technique: TRIANGULATION \u2014Cell values are obtained by interpolating measurements from a triangulated representation of the LAS dataset. The following options are available for this technique: Cell Assignment Type \u2014Method used to define the value for any cell that contains points within its extent. AVERAGE \u2014Assigns the average value of all points in the cell. This is the default. MINIMUM \u2014Assigns the minimum value found in the points within the cell. MINIMUM \u2014Assigns the maximum value found in the points within the cell. IDW \u2014Uses Inverse Distance Weighted interpolation to determine the cell value. NEAREST \u2014Uses Nearest Neighbor assignment to determine the cell value. Void Fill Method \u2014The interpolation method used to define values for cells that do not have points within their extent. NONE \u2014NoData is assigned to the cell. SIMPLE \u2014Averages the values from data cells immediately surrounding a NoData cell to eliminate small voids. LINEAR \u2014Triangulates across void areas and uses linear interpolation on the triangulated value to determine the cell value. This is the default. NATURAL_NEIGHBOR \u2014Uses natural neighbor interpolation to determine the cell value. Interpolation Method \u2014The interpolation method that defines cell values: LINEAR \u2014Uses linear interpolation against the triangulated LAS dataset surface to determine cell value. NATURAL_NEIGHBOR \u2014Uses natural neighbor interpolation to determine cell value. Point Thinning Type \u2014Determines if LAS data points are thinned: NONE \u2014LAS points will not be thinned. This is the default. WINDOW_SIZE \u2014LAS points will be thinned by identifying the point that satisfies the selection criteria within the area defined by the window size. Point Selection Method \u2014Selection method used for thinning LAS data points when using WINDOW_SIZE thinning: MAXIMUM \u2014The point with the highest value in each window size is maintained. This is the default. MINIMUM \u2014The point with the lowest value in each window size is maintained. CLOSEST_TO_MEAN \u2014The point whose value is closest to the average of all point values in the window size is maintained. Resolution \u2014A numeric value that defines the area of the window size used for thinning points.", "dataType": "String"}, {"name": "data_type", "isOptional": true, "description": "The data type of the output raster can be defined by the following keywords: FLOAT \u2014 Output raster will use 32-bit floating point, which supports values ranging from -3.402823466e+38 to 3.402823466e+38. This is the default. INT \u2014 Output raster will use an appropriate integer bit depth. This option will round Z-values to the nearest whole number and write an integer to each raster cell value.", "dataType": "String"}, {"name": "sampling_type", "isOptional": true, "description": " Specifies the method used for interpreting the Sampling Value to define the resolution of the output raster. OBSERVATIONS \u2014 Defines the number of cells that divide the lengthiest side of the LAS dataset extent. CELLSIZE \u2014 Defines the cell size of the output raster. This is the default.", "dataType": "String"}, {"name": "sampling_value", "isOptional": true, "description": " Specifies the value used in conjunction with the Sampling Type to define the resolution of the output raster. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}]},
{"syntax": "GPXToFeatures_conversion (Input_GPX_File, Output_Feature_class)", "name": "GPX To Features (Conversion)", "description": "\r\nConverts GPX files into features.", "example": {"title": "GPXToFeatures Example 1 (Python Window)", "description": "The following Python snippet converts a GPX file into features from the Python window.", "code": "import arcpy arcpy.GPXtoFeatures_conversion ( 'c: \\\\ GPX_Files \\\\ Hike.gpx' , 'c: \\\\ gisData \\\\ Hike.shp' )"}, "usage": ["This tool converts the point information inside a GPX file into features. The output features will include the geometry (including elevation or Z-value) as well as attribute fields for Name, Description, Type, DateTimeS (string type), Elevation, and DateTime (date type - if possible).   Shapefiles do not support Date-Time fields, they only support Date fields. Output shapefiles will only have a DateTime field of type string created. All other output format types will attempt to create a real DateTime field as long as the date format complies to the XML Time standard. Most GPX files follow the XML Time standard.", "GPX files collect points in two ways: waypoints and tracks. Waypoints are generally single, unrelated points while tracks make up a route or collection of related points with a start and end point. The type of point collected is specified in the output feature class by the code WPT (waypoint) or TRKPT (track point) within the Type attribute field. Waypoints can have a name and description for each individual point. Tracks have a name and description associated with the track itself, not for each individual point.", " You can use the ", "Points To Line", " tool to create polylines for each track. ", " The Python code below shows how this workflow is accomplished using a script. ", " ", "Output will be generated in the WGS84 coordinate system. The output features can be reprojected to another coordinate system, if desired, using the ", "Project", " tool.", "Both the 1.0 and 1.1 ", "Topografix GPX", " schemas are supported. Files that do not conform to one of these schemas will not translate."], "parameters": [{"name": "Input_GPX_File", "isInputFile": true, "isOptional": false, "description": "The GPX file to convert. ", "dataType": "File"}, {"name": "Output_Feature_class", "isOutputFile": true, "isOptional": false, "description": " The feature class to create. ", "dataType": "Feature Class"}]},
{"syntax": "KMLToLayer_conversion (in_kml_file, output_folder, {output_data}, {include_groundoverlay})", "name": "KML To Layer (Conversion)", "description": "\r\nConverts a KML or KMZ file into feature classes  and a layer file.  The layer file  maintains the symbology found within the original KML or KMZ file. Learn more about KML support in ArcGIS", "example": {"title": "KMLToLayer example 1 (Python window)", "description": "Converts a KMZ file into an FGDB from the Python window.", "code": "import arcpy arcpy.KMLToLayer_conversion ( r'C:\\kmls\\earthquakes.kml' , r'C:\\gisdata\\fromkmls' , 'earthquake_09' )"}, "usage": ["This tool creates a file geodatabase containing a feature class within a feature dataset.  The  feature class name will be named ", "point", ", ", "line", ", ", "polygon", ", or ", "multipatches", ", dependent on the original features of the KML file. At the same folder level as the file geodatabase will be a layer (", ".lyr", ") file which can be added to ArcMap to draw the features. This layer file draws features based on their schema of point, line, or polygon, while maintaining the original KML symbology. ", "Each feature class created will have  attributes which maintain information about the original KML file. The original folder structure, name, and pop-up information, as well as fields that help define how the features sit on a surface, all make up the attributes of each feature.", "Rasters, or ground overlays, will be converted into a raster catalog inside the file geodatabase. The source raster in its native format is available in a folder of ", "GroundOverlays", " at the same level as the output file geodatabase. ", "Converting overlays using the ", "KML To Layer", " tool may take a long time, depending on source data. All available rasters and overlays inside the KML will be converted. All of the imagery will be converted if a KML references a service that provides imagery. Highly detailed imagery may take a long time to convert due its file size.", "Output will be generated in the WGS84 coordinate system. The output features can be reprojected to another coordinate system, if desired, using the ", "Project", " tool.", "\r\nInput up to KMZ version 2.2 of the  ", "OGC KML", " standard is mostly  supported. Point locations that use the address tag (by way of geocoding) are not supported. A valid latitude and longitude location is required inside the source KML."], "parameters": [{"name": "in_kml_file", "isInputFile": true, "isOptional": false, "description": " The KML or KMZ file to translate. ", "dataType": "File"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The destination folder for the file geodatabase and layer ( .lyr ) file. ", "dataType": "Folder"}, {"name": "output_data", "isOutputFile": true, "isOptional": true, "description": " The name of the output file geodatabase and layer file ( .lyr ). Defaults to the name of the input KML file. ", "dataType": "String"}, {"name": "include_groundoverlay", "isOptional": true, "description": "Include ground overlay (raster, air photos, and so on). Use caution if the KMZ points to a service that serves raster imagery. The tool will attempt to translate the raster imagery at all available scales. This process could be lengthy and possibly overwhelm the service. GROUNDOVERLAY \u2014 Ground overlays are included in the output. NO_GROUNDOVERLAY \u2014 Ground overlays are not included in the output. This is the default.", "dataType": "Boolean"}]},
{"syntax": "MapToKML_conversion (in_map_document, data_frame, out_kmz_file, {map_output_scale}, {is_composite}, {is_vector_to_raster}, {extent_to_export}, {image_size}, {dpi_of_client}, {ignore_zvalue})", "name": "Map To KML (Conversion)", "description": "This tool converts a map document into a KML file containing a translation of Esri geometries and symbology. This file is compressed using ZIP compression, will have a  .kmz  extension, and can be read by any KML client including ArcGIS Explorer, ArcGlobe, and Google Earth. Learn more about KML support in ArcGIS", "example": {"title": "MapToKML example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.MapToKML_3d ( \"city.mxd\" , \"Layers\" , \"city.kmz\" , \"1\" )"}, "usage": ["You can control the appearance of KML in two ways.  ", "Learn more about creating KML in ArcGIS for Desktop", "You can reduce the size of the output KMZ document if your map has scale-dependent layer display properties and you choose an appropriate map output scale.", "All KML/KMZ files are created in the WGS84 coordinate system. You need to be sure your layers will properly project if they aren't already in WGS84. You can use the ", "Project", " tool to reproject your data prior to KML conversion if your projection requires a transformation.", "To output a single raster image draped over topography, use the ", "Return single composite image", " option.", "To output every layer as a separate raster image, use the ", "Convert Vector to Raster", " option."], "parameters": [{"name": "in_map_document", "isInputFile": true, "isOptional": false, "description": "The map document to convert to KML. ", "dataType": "ArcMap Document"}, {"name": "data_frame", "isOptional": false, "description": "The data frame of the map document to convert to KML. ", "dataType": "String"}, {"name": "out_kmz_file", "isOutputFile": true, "isOptional": false, "description": "The KML file to write. This file is compressed and will have a .kmz extension. It can be read by any KML client including ArcGIS Explorer, ArcGlobe, and Google Earth. ", "dataType": "File"}, {"name": "map_output_scale", "isOptional": true, "description": "The scale at which to export each layer in the map. This parameter is important with any scale dependency, such as layer visibility or scale-dependent rendering. If the layer is not visible at the export scale, it will not be included in the created KML file. Any value, such as 1, can be used if there are no scale dependencies. If exporting a layer that is to be displayed as 3D vectors and the is_composite parameter is set to NO_COMPOSITE, you can set this parameter to any value, as long as your features do not have any scale-dependent rendering. Only numeric characters should be entered; for example, enter 20000 as the scale, not 1:20000 or 20,000. ", "dataType": "Double"}, {"name": "is_composite", "isOptional": true, "description": " COMPOSITE \u2014 Directs the output KML file to contain only a single image that composites all the features in this map into a single raster image. The raster is draped over the terrain as a KML GroundOverlay. Select this option to reduce the size of the output KMZ file. When you check this box, individual features and layers in the KML are not selectable. Also, the next parameter, is_vector_to_raster , is ignored. NO_COMPOSITE \u2014 Layers are returned separately in the KML. Whether the layers are returned all as rasters or as a mix of vectors and rasters is determined by the next parameter, is_vector_to_raster .", "dataType": "Boolean"}, {"name": "is_vector_to_raster", "isOptional": true, "description": " VECTOR_TO_RASTER \u2014 Converts each vector layer in the map into a separate raster image in the KML output. Normal raster layers are also added to the KML output. Each output KML raster layer is selectable, and its transparency can be adjusted in certain KML clients. VECTOR_TO_VECTOR \u2014 Preserves vector layers in the map as KML vectors.", "dataType": "Boolean"}, {"name": "extent_to_export", "isOptional": true, "description": "The geographic extent of the area to be exported. The extent rectangle bounds should be specified as a space-delimited string of WGS84 geographic coordinates in the form left lower right upper (xmin, ymin, xmax, ymax). ", "dataType": "Extent"}, {"name": "image_size", "isOptional": true, "description": "Size of returned image in pixels. Defines the vertical and horizontal resolution of any rasters in the output KML document. ", "dataType": "Long"}, {"name": "dpi_of_client", "isOptional": true, "description": "Defines the device resolution for any rasters in the output KML document. Typical screen resolution is 96 dpi. If the data inside your map supports a high resolution and your KML requires it, consider increasing the value. ", "dataType": "Long"}, {"name": "ignore_zvalue", "isOptional": true, "description": " ABSOLUTE \u2014 Use the Z-values of features when creating KML. The features will be drawn inside KML clients relative to sea level. CLAMPED_TO_GROUND \u2014 Override Z-values in your features and create KML with the features clamped to the ground. The features will be draped over the terrain. This setting is used for features that do not have Z-values. This is the default.", "dataType": "Boolean"}]},
{"syntax": "LayerToKML_conversion (layer, out_kmz_file, {layer_output_scale}, {is_composite}, {boundary_box_extent}, {image_size}, {dpi_of_client}, {ignore_zvalue})", "name": "Layer To KML (Conversion)", "description": "This tool converts a feature or raster layer  into a KML file containing a translation of Esri geometries and symbology. This file is compressed using ZIP compression, has a  .kmz  extension, and can be read by any KML client including ArcGIS Explorer, ArcGlobe, and Google Earth. Learn more about KML support in ArcGIS", "example": {"title": "LayerToKML example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.LayerToKML_conversion ( \"bldg.lyr\" , \"bldg.kmz\" )"}, "usage": ["You can control the appearance of KML in two ways.  ", "Learn more about creating KML in ArcGIS for Desktop", "You can reduce the size of the output KMZ document if your layer has scale-dependent  display properties and you choose an appropriate map output scale.", "All KML/KMZ files are created in the WGS84 coordinate system. You need to be sure your layers will properly project if they aren't already in WGS84. You can use the ", "Project", " tool to reproject your data prior to KML conversion if your projection requires a transformation.", "To output a single raster image draped over topography, use the ", "Return single composite image", " option."], "parameters": [{"name": "layer", "isOptional": false, "description": "The feature or raster layer or layer file ( .lyr ) to be converted to KML. ", "dataType": "Layer"}, {"name": "out_kmz_file", "isOutputFile": true, "isOptional": false, "description": "The KML file to write. This file is compressed and has a .kmz extension. It can be read by any KML client including ArcGIS Explorer, ArcGlobe, and Google Earth. ", "dataType": "File"}, {"name": "layer_output_scale", "isOptional": true, "description": "The scale at which to export the layer. This parameter is used with any scale dependency, such as layer visibility or scale-dependent rendering. If the layer is not visible at the export scale, it will not be included in the created KML file. Any value, such as 1, can be used if there are no scale dependencies. If exporting a layer that is to be displayed as 3D vectors and the is_composite parameter is set to NO_COMPOSITE you can set this parameter to any value, as long as your features do not have any scale-dependent rendering. Only numeric characters should be entered; for example, enter 20000 as the scale, not 1:20000 or 20,000. ", "dataType": "Double"}, {"name": "is_composite", "isOptional": true, "description": " COMPOSITE \u2014 The output KML file will be a single composite image representing the raster or vector features in the source layer. The raster is draped over the terrain as a KML GroundOverlay. Select this option to reduce the size of the output KMZ file. When you check this box, individual features and layers in the KML are not selectable. NO_COMPOSITE \u2014 If your layer has vector features, they are preserved as KML vectors. (If your layer is a raster, you can choose either option for this parameter without any visual difference.)", "dataType": "Boolean"}, {"name": "boundary_box_extent", "isOptional": true, "description": " The geographic extent of the area to be exported. The extent rectangle bounds should be specified as a space-delimited string of WGS84 geographic coordinates in the form left lower right upper (xmin, ymin, xmax, ymax). ", "dataType": "Extent"}, {"name": "image_size", "isOptional": true, "description": "Defines the vertical and horizontal resolution of any rasters in the output KML document. ", "dataType": "Long"}, {"name": "dpi_of_client", "isOptional": true, "description": "Defines the device resolution for any rasters in the output KML document. ", "dataType": "Long"}, {"name": "ignore_zvalue", "isOptional": true, "description": " ABSOLUTE \u2014 Use the Z-values of features when creating KML. The features will be drawn inside KML clients relative to sea level. CLAMPED_TO_GROUND \u2014 Override Z-values in your features and create KML with the features clamped to the ground. The features will be draped over the terrain. This setting is used for features that do not have Z-values. This is the default.", "dataType": "Boolean"}]},
{"syntax": "MultipatchToCollada_conversion (in_features, output_folder, {prepend_source}, {field_name})", "name": "Multipatch To Collada (Conversion)", "description": "\r\n Converts one or more multipatch features into a collection of COLLADA files and referenced texture image files in an output folder. The inputs can be a layer or a feature class.\r\n", "example": {"title": "MultipatchToCollada example 1 (Python window) ", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.MultipatchToCollada_conversion ( \"Sample.gdb/Buildings\" , \"C:/COLLADA\" , \"PREPEND_SOURCE_NAME\" , \"BldName\" )"}, "usage": ["\r\n COLLADA files are an XML representation of a 3D object that can reference additional image files that act as textures draped onto the 3D geometry. This means that exporting a multipatch feature to COLLADA can result in the creation of several files\u2014a ", ".dae", " file containing the XML representation of the 3D object and one or more image files (for example, a ", ".jpg", " or ", ".png", " file) that contain the textures.\r\n", " This tool creates one COLLADA representation for each multipatch feature that it exports. The tool uses a field value from each feature\u2014by default, this is the Object ID\u2014to define the output file names. This allows easier identification of which feature was exported to which COLLADA file and also provides the methodology for defining unique names when exporting multiple features to the same directory. Texture files are stored in the same directory as the COLLADA file.", " This tool automatically overwrites any existing COLLADA files with the same file name.  When this happens, a warning message is given stating which files were overwritten with a new file during the export process. A GP message is also generated for any features that fail to export\u2014for example, if the output location is read-only, or the disk is full.", " To ensure a new COLLADA file is created for all the exported multipatch features, set the destination directory to an empty or new folder and choose a file name field that is unique for each feature. Exporting two features with the same attribute value will result in the second exported feature overwriting the COLLADA file of the first.", " When iteratively updating a multipatch feature by exporting it to COLLADA and making changes outside of ArcGIS, export the feature to the same location each time. This will keep a single file on disk for that feature, representing the most up-to-date state of the 3D object.", " If the exported multipatch is in a projected coordinate system, such as a building stored in a UTM zone, then a KML file containing the coordinates as WGS84 will also be created in the output folder. Note that this process will not use a datum transformation, which may result in positional discrepancies when viewing the KML. ", " When converting multipatches from a layer, the Multipatch to COLLADA tool will automatically embed any colors defined in the layer's renderer. For example, if the layer is rendering features based on a use-type attribute\u2014such as red for commercial, blue for residential, and so on\u2014then these colors will be included in the output COLLADA files. The displayed color is applied to both textured and untextured multipatch features, with the former requiring an update to the feature's underlying texture files. You can use a single display color of ", "white", " to export textured multipatches with unaltered images."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The multipatch features to be exported. ", "dataType": "Feature Layer"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The destination folder where the output COLLADA files and texture image files will be placed. ", "dataType": "Folder"}, {"name": "prepend_source", "isOptional": true, "description": " Prepend the file names of the output COLLADA files with the name of the source feature layer. PREPEND_SOURCE_NAME \u2014 Prepend the file names. PREPEND_NONE \u2014 Do not prepend the file names. This is the default.", "dataType": "Boolean"}, {"name": "field_name", "isOptional": true, "description": " The feature attribute to use as the output COLLADA file name for each exported feature. If no field is specified, the feature's Object ID is used. ", "dataType": "Field"}]},
{"syntax": "ImportFromE00_conversion (Input_interchange_file, Output_folder, Output_name)", "name": "Import from E00 (Conversion)", "description": "Imports an  ArcInfo Workstation  interchange file ( .e00 ).   An interchange file is used to transport coverages, INFO tables, text files such as AML macros, and other ArcInfo files. For coverages, grids, and tins, it contains all  information, including appropriate INFO table information. Interchange files are designated with the  .e00  file suffix. This is the  ArcView GIS  version of the utility for importing  .e00  files. ", "example": {"title": "ImportFromE00 example (Python window)", "description": "The following Python window script demonstrates how to use the ImportFromE00 tool in immediate mode to import a coverage.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.ImportFromE00_conversion ( \"citylim.e00\" , \"C:/output\" , \"citylim\" )"}, "usage": [" This tool does not enforce ", "ArcInfo Workstation", " dataset naming limitations. If your ", ".e00", " file contains a coverage, grid, or tin, you should avoid using an output parameter that contains spaces, or has a name longer than 13 characters. INFO table names should be 32 characters or shorter. Other coverage name limitations are listed in ", "Knowlege Base article 21052", ".", "This tool is only available in ", "ArcGIS for Desktop", ".  It is not available in ", "ArcGIS for Server", ".", "When importing INFO tables, you do not need to use the ! naming convention when setting the output parameter. Simply set the parameter to include the path to the target workspace and the name of the table you desire. The table will be created in that workspace's INFO database. For example, instead of using ", "D:/workspace/INFO!sometable", ", use ", "D:/workspace/sometable", ". ", "This tool does not honor the geoprocessing overwrite output setting.  You must  delete any expected output before importing a ", ".e00", " file.", "If you have an ArcInfo license and have installed ", "ArcInfo Workstation", ", you can also use the ", "Import_From_Interchange_File", " tool, which has more advanced capabilities."], "parameters": [{"name": "Input_interchange_file", "isInputFile": true, "isOptional": false, "description": " ArcInfo Workstation interchange file to convert. The name of this file cannot contain spaces. ", "dataType": "File"}, {"name": "Output_folder", "isOutputFile": true, "isOptional": false, "description": " The location in which the output will be created. ", "dataType": "Folder"}, {"name": "Output_name", "isOutputFile": true, "isOptional": false, "description": " The name of the output. This string cannot contain any spaces. If this output already exists, the tool will not overwrite it, even if the geoprocessing overwrite output setting is set to true. ", "dataType": "String"}]},
{"syntax": "FeatureClassToShapefile_conversion (Input_Features, Output_Folder)", "name": "Feature Class To Shapefile (Conversion)", "description": "Copies the features from one or more feature classes or layers to a folder of shapefiles.", "example": {"title": "FeatureClassToShapefile example (Python window)", "description": "The following Python window script demonstrates how to use the FeatureClassToShapefile function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/airport.gdb\" arcpy.FeatureClassToShapefile_conversion ([ \"county\" , \"parcels\" , \"schools\" ], \"C:/output\" )"}, "usage": ["Shapefiles have many limitations compared to feature classes in a geodatabase.   For example, shapefile attributes\r\ncannot store null values, they round up numbers, they have poor\r\nsupport for Unicode character strings, they do not allow field\r\nnames longer than 10 characters, and they cannot store both a date\r\nand time in a field. Additionally,\r\nthey do not support capabilities found in geodatabases such as\r\ndomains and subtypes.\r\n", "Learn more about shapefile limitations", "The name of the output shapefile will be the name of the input feature class. For example, if the input is ", "C:\\base.gdb\\rivers", ", the output shapefile will be named ", "rivers.shp", ". To explicitly control the output shapefile name and for some additional conversion options, see the ", "Feature Class To Feature Class", " tool.", "If the output shapefile already exists in the ", "Output Folder", ", a number will be appended to the end to make the shapefile name unique (for example, ", "rivers_1.shp", ")."], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": "The list of input feature classes or feature layers that will be converted and added to the output folder. ", "dataType": "Feature Layer"}, {"name": "Output_Folder", "isOutputFile": true, "isOptional": false, "description": "The folder where the shapefiles will be written. ", "dataType": "Folder"}]},
{"syntax": "RasterToOtherFormat_conversion (Input_Rasters, Output_Workspace, {Raster_Format})", "name": "Raster To Other Format (Conversion)", "description": "Converts one or more raster dataset formats supported by ArcGIS to a BIL, BIP, BMP, BSQ, ENVI DAT, GIF, GRID, ERDAS IMAGINE, JPEG, JPEG 2000, PNG TIFF, or to a geodatabase raster dataset format.", "example": {"title": "RasterToOtherFormats example 1 (Python window)", "description": "This sample converts several input rasters of different formats and outputs them as rasters all of the same format.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data/PrjWorkspace/RasGP\" arcpy.RasterToOtherFormat_conversion ( \"test.tif;test2.tif;test3.tif\" , \"OtherFormat\" , \"BIL\" )"}, "usage": ["You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset."], "parameters": [{"name": "Input_Rasters", "isInputFile": true, "isOptional": false, "description": "The input raster datasets that you wish to convert. ", "dataType": "Raster Dataset"}, {"name": "Output_Workspace", "isOutputFile": true, "isOptional": false, "description": "The location where converted raster datasets will be stored. ", "dataType": "Workspace ;Raster Catalog"}, {"name": "Raster_Format", "isOptional": true, "description": "The format of the output raster dataset. BIL \u2014 Esri Band Interleaved by Line file. BIP \u2014 Esri Band Interleaved by Pixel file. BMP \u2014 Bitmap graphic raster dataset format. BSQ \u2014 Esri Band Sequential file. DAT \u2014 ENVI DAT file. GIF \u2014 Graphic Interchange Format for raster datasets. GRID \u2014 Esri Grid raster dataset format. IMAGINE Image \u2014 ERDAS IMAGINE raster data format. JP2000 \u2014 JPEG 2000 raster dataset format. JPEG \u2014 Joint Photographic Experts Group raster dataset format. PNG \u2014 Portable Network Graphic raster dataset format. TIFF \u2014 Tag Image File Format for raster datasets. ", "dataType": "String"}]},
{"syntax": "PolylineToRaster_conversion (in_features, value_field, out_rasterdataset, {cell_assignment}, {priority_field}, {cellsize})", "name": "Polyline to Raster (Conversion)", "description": "Converts polyline features to a raster dataset. \r\n Learn more about how the Polyline to Raster tool works \r\n", "example": {"title": "PolylineToRaster example 1 (Python window)", "description": "Converts polyline features to a raster dataset.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.PolylineToRaster_conversion ( \"roads.shp\" , \"CLASS\" , \"c:/output/roads.img\" , \"MAXIMUM_COMBINED_LENGTH\" , \"LENGTH\" , 30 )"}, "usage": ["Any feature class (geodatabase, shapefile or coverage) containing polyline features can be converted to a raster dataset.", "The input field type determines the type of output raster.  If the field is integer, the output raster will be integer; if it is floating point, the output will be floating point.", "If the input field contains string values, the output raster will contain an integer value field and a string field.", "This tool is a complement to the ", "Raster to Polyline", " tool, which convert a raster to a polyline feature class.", "This tool provides greater control over the assignment of cell values when more than one feature is present in an output cell than the ", "Feature to Raster", " tool."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The polyline input feature dataset to be converted to a raster. ", "dataType": "Feature Layer"}, {"name": "value_field", "isOptional": false, "description": " The field used to assign values to the output raster. It can be any field of the input feature dataset's attribute table. ", "dataType": "Field"}, {"name": "out_rasterdataset", "isOutputFile": true, "isOptional": false, "description": " The output raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for an Esri Grid raster format. ", "dataType": "Raster Dataset"}, {"name": "cell_assignment", "isOptional": true, "description": "The method to determine how the cell will be assigned a value when more than one feature falls within a cell. MAXIMUM_LENGTH \u2014 The feature with the longest length that covers the cell will determine the value to assign to the cell. MAXIMUM_COMBINED_LENGTH \u2014 If there is more than one feature in a cell with the same value, the lengths of these features will be combined. The combined feature with the longest length within the cell will determine the value to assign to the cell. ", "dataType": "String"}, {"name": "priority_field", "isOptional": true, "description": "This field is used when a feature should take preference over another feature with the same attribute. ", "dataType": "Field"}, {"name": "cellsize", "isOptional": true, "description": " The cell size for the output raster dataset. The default cell size is the shortest of the width or height of the extent of the input feature dataset, in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}]},
{"syntax": "PolygonToRaster_conversion (in_features, value_field, out_rasterdataset, {cell_assignment}, {priority_field}, {cellsize})", "name": "Polygon to Raster (Conversion)", "description": "Converts polygon features to a raster dataset. \r\n Learn how the Polygon to Raster tool works \r\n", "example": {"title": "PolygonToRaster example 1 (Python window)", "description": "Converts polygon features to a raster dataset.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.PolygonToRaster_conversion ( \"ca_counties.shp\" , \"NAME\" , \"c:/output/ca_counties.img\" , \"MAXIMUM_AREA\" , \"MALES\" , 0.25 )"}, "usage": ["Any feature class (geodatabase, shapefile or coverage) containing polygon features can be converted to a raster dataset.", "The input field type determines the type of output raster.  If the field is integer, the output raster will be integer; if it is floating point, the output will be floating point.", "If the input field contains string values, the output raster will contain an integer value field and a string field.", "However, if the field is of type floating point and the values are expressed as integers, then the output raster will be integer.", "This tool is a complement to the ", "Raster to Polygon", " tool, which convert a raster to a polygon feature class.", "This tool provides greater control over the assignment of cell values when more than one feature is present in an output cell than the ", "Feature to Raster", " tool.", "If \"bleeding\" or stripes occur in the output raster then use the ", "Check Geometry", " and ", "Repair Geometry", " tools to correct the input feature data.", "When converting overlapping polygons, you may want the polygons with the smallest area to be assigned to a cell. An easy way to do this is to add an additional field to the input feature class and calculate it to the inverse area of the polygons, then use this field as the ", "Priority Field", " when running this tool."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The polygon input feature dataset to be converted to a raster. ", "dataType": "Feature Layer"}, {"name": "value_field", "isOptional": false, "description": " The field used to assign values to the output raster. It can be any field of the input feature dataset's attribute table. ", "dataType": "Field"}, {"name": "out_rasterdataset", "isOutputFile": true, "isOptional": false, "description": " The output raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for an Esri Grid raster format. ", "dataType": "Raster Dataset"}, {"name": "cell_assignment", "isOptional": true, "description": "The method to determine how the cell will be assigned a value when more than one feature falls within a cell. CELL_CENTER \u2014 The polygon which overlaps the center of the cell yields the attribute to assign to the cell. MAXIMUM_AREA \u2014 The single feature with the largest area within the cell yields the attribute to assign to the cell. MAXIMUM_COMBINED_AREA \u2014 If there is more than one feature in a cell with the same value, the areas of these features will be combined. The combined feature with the largest area within the cell will determine the value to assign to the cell.", "dataType": "String"}, {"name": "priority_field", "isOptional": true, "description": "This field is used to determine which feature should take preference over another feature with the same value in the Value field. ", "dataType": "Field"}, {"name": "cellsize", "isOptional": true, "description": " The cell size for the output raster dataset. The default cell size is the shortest of the width or height of the extent of the input feature dataset, in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}]},
{"syntax": "PointToRaster_conversion (in_features, value_field, out_rasterdataset, {cell_assignment}, {priority_field}, {cellsize})", "name": "Point to Raster (Conversion)", "description": "Converts point features to a raster dataset. \r\n Learn how the Point to Raster tool works \r\n", "example": {"title": "PointToRaster example 1 (Python window)", "description": "Converts point features to a raster dataset.\r\n", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.PointToRaster_conversion ( \"ca_ozone_pts.shp\" , \"ELEVATION\" , \"c:/output/ca_elev\" , \"MAXIMUM\" , \"\" , 2000 )"}, "usage": ["Any feature class (geodatabase, shapefile or coverage) containing point or multipoint features can be converted to a raster dataset.", "Multipoints are treated as a set of individual points.", "The input field type determines the type of output raster.  If the field is integer, the output raster will be integer; if it is floating point, the output will be floating point.", "If the input field contains string values, the output raster will contain an integer value field and a string field.", "This tool is a complement to the ", "Raster to Point", " tool, which convert a raster to a point feature class.", "This tool provides greater control over the assignment of cell values when more than one feature is present in an output cell than the ", "Feature to Raster", " tool.", "The ", "Priority field", " is only used with the MOST_FREQUENT option for ", "Cell assignment type", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The point or multipoint input feature dataset to be converted to a raster. ", "dataType": "Feature Layer"}, {"name": "value_field", "isOptional": false, "description": " The field used to assign values to the output raster. It can be any field of the input feature dataset's attribute table. If the Shape field of a point or multipoint dataset contains z or m values, then either of these can be used. ", "dataType": "Field"}, {"name": "out_rasterdataset", "isOutputFile": true, "isOptional": false, "description": " The output raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for an Esri Grid raster format. ", "dataType": "Raster Dataset"}, {"name": "cell_assignment", "isOptional": true, "description": "The method to determine how the cell will be assigned a value when more than one feature falls within a cell. MOST_FREQUENT \u2014 If there is more than one feature within the cell, the one with the most common attribute, in <field>, is assigned to the cell. If they have the same number of common attributes, the one with the lowest FID is used. SUM \u2014 The sum of the attributes of all the points within the cell (not valid for string data). MEAN \u2014 The mean of the attributes of all the points within the cell (not valid for string data). STANDARD_DEVIATION \u2014 The standard deviation of attributes of all the points within the cell. If there are less than two points in the cell, the cellis assigned NoData (not valid for string data). MAXIMUM \u2014 The maximum value of the attributes of the points within the cell (not valid for string data). MINIMUM \u2014 The minimum value of the attributes of the points within the cell (not valid for string data). RANGE \u2014 The range of the attributes of the points within the cell (not valid for string data). COUNT \u2014 The number of points within the cell.", "dataType": "String"}, {"name": "priority_field", "isOptional": true, "description": "This field is used when a feature should take preference over another feature with the same attribute. Priority field is only used with the MOST_FREQUENT cell assignment type option. ", "dataType": "Field"}, {"name": "cellsize", "isOptional": true, "description": " The cell size for the output raster dataset. The default cell size is the shortest of the width or height of the extent of the input feature dataset, in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}]},
{"syntax": "FloatToRaster_conversion (in_float_file, out_raster)", "name": "Float to Raster (Conversion)", "description": "Converts a file of binary floating-point values representing raster data to a raster dataset.", "example": {"title": "FloatToRaster example 1 (Python window)", "description": "Converts a file of binary floating-point values representing raster data to a raster dataset.\r\n", "code": "import arcpy arcpy.FloatToRaster_conversion ( \"c:/data/elevation.flt\" , \"c:/output/elev\" )"}, "usage": ["The input file is an IEEE floating-point format, 32-bit signed binary file.", "Two inputs are required: the binary floating-point file with a ", ".flt", " extension (", "<in_float_file>.flt", ") and an ASCII header file with a ", ".hdr", " extension (", "<in_float_file>.hdr", "). You only specify the ", ".flt", " file; however, there needs to be an existing ", ".hdr", " file in the same directory with the same file name.", "The ASCII file consists of header information containing a set of keywords.", "There are two variations  of the structure of the ASCII file.  One identifies the origin by the coordinates of the lower left corner of the lower left cell, or as the center of the lower left cell.", "The format of the file in general  is:", "The definitions of the keywords are as follows:", "NCOLS", " and ", "NROWS", " are the number of columns and rows in the raster defined by the binary file.", "XLLCORNER", " and ", "YLLCORNER", " are the coordinates of the lower left corner of the lower left cell.", "You can also use ", "XLLCENTER", " and ", "YLLCENTER", " to specify the origin by the coordinates of the center of the lower left cell.", "CELLSIZE", " is the cell size of the raster.", "NODATA_VALUE", " is the value that is to represent NoData cells.", "BYTEORDER", " represents how multibyte binary numbers are stored on the system on which the binary file was generated. On Intel-based systems, the byte order is ", "LSBFIRST", " (also known as Big Endian). On most other architectures (all UNIX systems  except Alpha, and older Macintosh  with Motorola CPUs), the byte order is ", "MSBFIRST", " (also known as Little Endian).", "The ", "NODATA_VALUE", " is the value in the input file that determines which cells should be assigned the value of NoData in the output raster. NoData is normally reserved for those cells whose true value is unknown.", "In a floating-point binary file, the values are written as binary 32-bit signed floating-point numbers. The first record of the file corresponds to the first row of the raster. Going from left to right, the first 32 bits are the first cell, the next 32 bits are the second cell, and so on, to the end of the record (row). This is repeated for the second record (the second row of the raster) and all the way until the last record (the bottom row of the raster).", "This tool  supports both the lower left corner and the center of the lower left cell for determining the origin. ", "Raster to Float", " only writes the origin as the lower left corner of the lower left cell.", "Once the output raster has been created, use the ", "Define Projection", " tool to give it the appropriate coordinate system."], "parameters": [{"name": "in_float_file", "isInputFile": true, "isOptional": false, "description": "The input floating-point binary file. The file must have a .flt extension. There must be a header file in association with the floating-point binary file, with a .hdr extension. ", "dataType": "File"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": " The output raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for an Esri Grid raster format. ", "dataType": "Raster Dataset"}]},
{"syntax": "FeatureToRaster_conversion (in_features, field, out_raster, {cell_size})", "name": "Feature to Raster (Conversion)", "description": "Converts features to a raster dataset.", "example": {"title": "FeatureToRaster example 1 (Python window)", "description": "Converts features to a raster dataset.\r\n", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.FeatureToRaster_conversion ( \"roads.shp\" , \"CLASS\" , \"c:/output/roadsgrid\" , 25 )"}, "usage": ["Any feature class (geodatabase, shapefile or coverage) containing point, line, or polygon features can be converted to a raster dataset.", "The input field type determines the type of output raster.  If the field is integer, the output raster will be integer; if it is floating point, the output will be floating point.", "This tool always uses the cell center to decide the value of a raster pixel. If more control over how different types of input feature geometries are to be converted, please refer to the respective specific conversion tools: ", "Point to Raster", ", ", "Polyline to Raster", ", and ", "Polygon to Raster", ".", "This tool is a complement to the ", "Raster to Point", ", ", "Raster to Polyline", ", and ", "Raster to Polygon", " tools, which convert a raster to different types of feature dataset geometries.", "When selecting the input feature data,  the default field will be the first valid field available.  If no other valid fields exist, the ObjectID field (for example, OID or FID) will be the default."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature dataset to be converted to a raster dataset. ", "dataType": "Feature Layer"}, {"name": "field", "isOptional": false, "description": " The field used to assign values to the output raster. It can be any field of the input feature dataset's attribute table. If the Shape field of a point or multipoint dataset contains z or m values, then either of these can be used. ", "dataType": "Field"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": " The output raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for an Esri Grid raster format. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": " The cell size for the output raster dataset. The default cell size is the shortest of the width or height of the extent of the input feature dataset, in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}]},
{"syntax": "DEMToRaster_conversion (in_dem_file, out_raster, {data_type}, {z_factor})", "name": "DEM to Raster (Conversion)", "description": "Converts a digital elevation model (DEM) in a United States Geological Survey (USGS) format to a raster dataset. \r\n Learn about how DEM to Raster tool works \r\n", "example": {"title": "DEMToRaster example 1 (Python window)", "description": "Converts a USGS DEM to a raster dataset.", "code": "import arcpy arcpy.DEMToRaster_conversion ( \"c:/data/fixed.dem\" , \"c:/output/fixeddem.tif\" , \"FLOAT\" , 1 )"}, "usage": ["The majority of DEM files are integer. If a floating-point type DEM is converted with the output data type set to INTEGER, the values will be truncated; however, it is not easy to identify if a particular DEM file happens to be floating point. To prevent inadvertent loss of floating-point data when it exists, the default output data type of the raster will be floating point. The only cost is for integer DEMs, where the resulting raster will occupy more disk space than it needs. This can be rectified by subsequently running the ", "Int", " tool on the raster. If the input dataset is known to be an integer type, then the INTEGER option can be selected instead of the default.", "DEM to Raster", " applies the spatial resolution value stored in the DEM. In USGS DEMs, this information is stored in Data Element 14 in Logical Record Type A. The spatial resolution value is used to scale all input DEM elevation values.", "The output raster will have square cells if the specified format is Esri Grid. If the input DEM has a different sample point spacing in the x and y directions, it will be resampled by bilinear interpolation during the conversion process to a cell size equal to the smaller of the point spacings of the DEM in the x or y.", "For output to a Grid raster, the projection and units information contained in the DEM header record is transferred to a map projection file in the output grid directory. If the output raster is a different format, the projection information will be transferred to the .aux file."], "parameters": [{"name": "in_dem_file", "isInputFile": true, "isOptional": false, "description": "The input USGS DEM file. The DEM must be standard USGS 7.5 minute, 1 degree, or any other file in the USGS DEM format. The DEM may be in either fixed or variable record-length format. ", "dataType": "File"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": " The output raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for an Esri Grid raster format. ", "dataType": "Raster Dataset"}, {"name": "data_type", "isOptional": true, "description": "Data type of the output raster dataset. INTEGER \u2014 An integer raster dataset will be created. FLOAT \u2014 A floating-point raster dataset will be created. This is the default. ", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure; the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}]},
{"syntax": "ASCIIToRaster_conversion (in_ascii_file, out_raster, {data_type})", "name": "ASCII to Raster (Conversion)", "description": "Converts an ASCII file representing raster data to a raster dataset.", "example": {"title": "ASCIIToRaster example 1 (Python window) ", "description": "Converts an ASCII file representing raster data to a raster dataset.", "code": "import arcpy arcpy.ASCIIToRaster_conversion ( \"c:/data/elevation.asc\" , \"c:/output/elevation\" , \"INTEGER\" )"}, "usage": ["The input file is an ASCII-formatted text file.", "The structure of the ASCII file consists of header information containing a set of keywords, followed by cell values in row-major order.", "There are two variations  of the structure of the ASCII file.  One identifies the origin by the coordinates of the lower left corner of the lower left cell, or as the center of the lower left cell.", "The format of the file in general  is:", "The definitions of the keywords are as follows:", "NCOLS", " and ", "NROWS", " are the number of columns and rows in the raster defined by the ASCII file.", "XLLCORNER", " and ", "YLLCORNER", " are the coordinates of the lower left corner of the lower left cell.", "You can also use ", "XLLCENTER", " and ", "YLLCENTER", " to specify the origin by the coordinates of the center of the lower left cell.", "CELLSIZE", " is the cell size of the raster.", "NODATA_VALUE", " is the value that is to represent NoData cells.", "Cell values should be delimited by spaces. No carriage returns are necessary at the end of each row in the ASCII file. The number of columns in the header is used to determine when a new row begins.", "An example of an ASCII raster file is:", "The ", "NODATA_VALUE", " is the value in the ASCII file that will be assigned to  NoData cells in the output raster. This value is normally reserved for those cells whose true value is unknown. When the output raster is created, a system-generated NoData value will be used in place of the ", "NODATA_VALUE", ".  The ", "NoData in raster datasets", " topic  contains information on how to change the NoData value once the raster is created.", "The number of cell values contained in the file must be equal to the number of rows times the number of columns, or an error will be returned.", "The output data type can be either float or integer.", "Once the output raster has been created, use the ", "Define Projection", " tool to give it the appropriate coordinate system."], "parameters": [{"name": "in_ascii_file", "isInputFile": true, "isOptional": false, "description": "The input ASCII file to be converted. ", "dataType": "File"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": " The output raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for an Esri Grid raster format. ", "dataType": "Raster Dataset"}, {"name": "data_type", "isOptional": true, "description": "The data type of the output raster dataset. INTEGER \u2014 An integer raster dataset will be created. FLOAT \u2014 A floating-point raster dataset will be created. ", "dataType": "String"}]},
{"syntax": "TableToTable_conversion (in_rows, out_path, out_name, {where_clause}, {field_mapping}, {config_keyword})", "name": "Table to Table (Conversion)", "description": "Converts an input table to a dBASE or geodatabase table.", "example": {"title": "TableToTable Example (Python Window)", "description": "The following Python window script demonstrates how to use the TableToTable tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.TableToTable_conversion ( \"vegtable.dbf\" , \"C:/output/output.gdb\" , \"vegtable\" )"}, "usage": ["This tool supports the following table formats as input: ", "For file input (", ".csv", " or ", ".txt", "), the first row of the input file is used as the field names on the output table.  Field names cannot contain spaces or special characters (such as ", "$", " or ", "*", "), and you will  receive an error if the first row of the input file contains spaces or special characters.", "Learn more about table formats supported in ArcGIS", "This tool can convert input tables to dBASE (", ".dbf", "), geodatabase (personal, file, or SDE), or INFO tables. ", "This tool can be used to export an ArcGIS table to a dBASE table (", ".dbf", ") that can be read and edited in Microsoft Excel.", "The ", "Field Map", " parameter controls how the input fields in the ", "Input Features", " will be written to the ", "Output Features", ".", "When converting geodatabase data that has subtypes or ", "domains", " to a dBASE table, both the subtype and domain codes and descriptions can be included in the output. Use the ", "Transfer field domain descriptions", "  geoprocessing environment to control this behavior. By default, only domain and subtype codes will be included in the output, not descriptions.", "Conversion to dBASE (", ".dbf", ") table with subtype and domain descriptions may take more time (slower performance) than without descriptions. If you do not require the subtype and domain descriptions in your dBASE (", ".dbf", ") table output, it is recommended you use the unchecked (", "False", " or ", "NOT_TRANSFER_DOMAINS", " in scripting) default behavior of the ", "Transfer field domain descriptions", " environment to achieve best performance."], "parameters": [{"name": "in_rows", "isInputFile": true, "isOptional": false, "description": "The input table to be converted to a new table. ", "dataType": "Table View; Raster Layer"}, {"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The destination where the output table will be written. ", "dataType": "Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the output table. If the Output Location is a folder, convert the Input Rows to a dBASE table by specifying a name with the extension .dbf , or convert the Input Rows to a INFO table by specifying a name with no extension. If the Output Location is a geodatabase, convert the Input Rows to a geodatabase table by specifying a name with no extension. ", "dataType": "String"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of records. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, coverages, or dBASE or INFO tables, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}, {"name": "field_mapping", "isOptional": true, "description": "The fields and field contents chosen from the input table. You can add, rename, or delete output fields as well as set properties such as data type and merge rule. Learn more about choosing and setting the output fields . ", "dataType": "Field Mappings"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the default storage parameters (configurations) for geodatabases in a relational database management system (RDBMS). This setting is applicable only when using SDE geodatabase tables. ArcSDE configuration keywords are set by the database administrator. ", "dataType": "String"}]},
{"syntax": "TableToGeodatabase_conversion (input_table, output_geodatabase)", "name": "Table To Geodatabase (Conversion)", "description": "Converts one or more tables to geodatabase tables in an output personal, file, or SDE geodatabase. The inputs can be dBASE, INFO, VPF, OLE DB tables, or geodatabase tables, or table views.", "example": {"title": "TableToGeodatabase Example (Python Window)", "description": "The following Python window script demonstrates how to use the TableToGeodatabase tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.TableToGeodatabase_conversion ([ \"accident.dbf\" , \"vegtable.dbf\" ], \"C:/output/output.gdb\" )"}, "usage": ["The name of the output geodatabase tables will be based on the name of the input table. To control the output name and for additional conversion options use the ", "Table to Table", " tool.", "The ", "Copy Rows", " and ", "Table To Table", " tools can also be used to convert a table to a geodatabase table.", "If the name of the output table already exists in the output geodatabase, a number will be appended to the end of the name to make it unique (for example, OutputTable_1)."], "parameters": [{"name": "input_table", "isInputFile": true, "isOptional": false, "description": "The list of tables to be converted to geodatabase tables. Input tables can be INFO, dBASE, OLE DB, or geodatabase tables, or table views. ", "dataType": "Table View"}, {"name": "output_geodatabase", "isOutputFile": true, "isOptional": false, "description": "The destination geodatabase where the output geodatabase tables will be placed. ", "dataType": "Workspace"}]},
{"syntax": "RasterToGeodatabase_conversion (Input_Rasters, Output_Geodatabase, {Configuration_Keyword})", "name": "Raster To Geodatabase (Conversion)", "description": "Loads multiple raster datasets into a geodatabase or raster catalog. If this tool is used to load raster datasets into a raster catalog, then you need to run the  Calculate Default Spatial Grid Index  tool after the loading is completed.", "example": {"title": "RasterToGeodatabase example (Python window)", "description": "This is a Python sample for RasterToGeodatabase.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.RasterToGeodatabase_conversion ( \"test.tif;test2.tif;test3.tif\" , \"c:/data/ToGDB.gdb/catalog\" )"}, "usage": ["The output is the location of the geodatabase where you will store the raster.", "All raster datasets loaded into an unmanaged raster catalog must be a file on disk. Geodatabase raster datasets can only be loaded into raster catalogs that are managed.", "When converting the raster dataset to a personal geodatabase (", ".mdb", "), the raster dataset is stored on the file system in a hidden folder. This prevents you from going over the 2 GB limit that is imposed on a personal geodatabase; the actual raster dataset is saved as an ERDAS IMAGINE file.", "If this tool is used to load raster datasets into a raster catalog, then the ", "Calculate Default Spatial Grid Index tool", " will need to be run after the loading is completed. ", "Learn more about geodatabase items\u2014Spatial index grid size", "When converting the raster dataset to an ArcSDE geodatabase, the raster dataset is stored on the ArcSDE server as a raster SDE format. A configuration keyword can be specified (if configuration keywords are specified by the ArcSDE administrator).", "The ", "Cellsize", " and ", "Mask", " parameters are ignored by this tool."], "parameters": [{"name": "Input_Rasters", "isInputFile": true, "isOptional": false, "description": "Input raster dataset(s). ", "dataType": "Raster Dataset"}, {"name": "Output_Geodatabase", "isOutputFile": true, "isOptional": false, "description": "Either the path and name of a geodatabase or the path and name of a raster catalog. ", "dataType": "Workspace ; Raster Catalog"}, {"name": "Configuration_Keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}]},
{"syntax": "ImportCAD_conversion (Input_Files, out_personal_gdb, {spatial_reference}, {Explode_Complex})", "name": "Import From CAD (Conversion)", "description": "Converts a collection of CAD files into feature classes and data tables using a predefined and highly normalized schema for the purpose of further translation into GIS data. The output is stored in an interim staging geodatabase. The geodatabase can then be used, or further postprocessing can be performed.", "example": {"title": null, "description": null, "code": "# To create Points from the CAD block inserts. # Create the Geoprocessor object import arcgisscripting gp = arcgisscripting.create () # Script arguments... blocks = sys.argv [ 1 ] if blocks == '#' : blocks = \"C:/Test_data/CAD/MetroGAS/res00051_pc22j_ImportCAD1.mdb/blocks\" # provide a default value if unspecified res00051_pc22j_dwg = sys.argv [ 2 ] if res00051_pc22j_dwg == '#' : res00051_pc22j_dwg = \"C:/Test_data/CAD/MetroGAS/res00051_pc22j.dwg\" # provide a default value if unspecified Select_Set_Name = sys.argv [ 3 ] if Select_Set_Name == '#' : Select_Set_Name = \"[SetName] = 'COD_90_50M'\" # Local variables... Pivoted_AttrTags_by_OwnerID = \"C:/Test_data/CAD/MetroGAS/res00051_pc22j_ImportCAD1.mdb/Attrib_PivotTable\" Attrib_Table = \"C:/Test_data/CAD/MetroGAS/res00051_pc22j_ImportCAD1.mdb/Attrib\" Entities_Table = \"C:/Test_data/CAD/MetroGAS/res00051_pc22j_ImportCAD1.mdb/Entity\" joined = \"joined\" Output_Staging_Geodatabase = \"C:/Test_data/CAD/MetroGAS/res00051_pc22j_ImportCAD1.mdb\" points_new2 = \"points_new\" Point = \"C:/Test_data/CAD/MetroGAS/res00051_pc22j_ImportCAD1.mdb/CADStaging/Point\" points_new = \"points_new\" points_new3 = \"points_new\" try : # Process: Import from CAD ... gp.ImportCAD_conversion ( \"'C:/Test_data/CAD/MetroGAS/res00051_pc22j.dwg'\" , Output_Staging_Geodatabase , \"\" , \"Do_Not_Explode_Complex\" ) # Process: Select Data... gp.SelectData_management ( Output_Staging_Geodatabase , \"CADStaging/Point\" , ) # Process: Make Layer... gp.MakeFeatureLayer_management ( Point , points_new , \"\" , \"\" , \"EntID EntID VISIBLE\" ) # Process: Select Data (Entities Table)... gp.SelectData_management ( Output_Staging_Geodatabase , \"Entity\" , ) # Process: Add Join... gp.AddJoin_management ( points_new , \"EntID\" , Entities_Table , \"EntID\" , \"OUTER\" , ) # Process: Select Data (Attrib Table)... gp.SelectData_management ( Output_Staging_Geodatabase , \"Attrib\" , ) # Process: Make Table View... gp.MakeTableView_management ( Attrib_Table , joined , Select_Set_Name , \"\" , \"EntID EntID VISIBLE;OwnerID OwnerID VISIBLE;AttrFlag AttrFlag VISIBLE;MSSet MSSet VISIBLE;AtrTag AtrTag VISIBLE;AtrHndl AtrHndl VISIBLE;AttrType AttrType VISIBLE;AttrStr AttrStr VISIBLE;AttrLong AttrLong VISIBLE;AttrDbl AttrDbl VISIBLE\" ) # Process: Pivot Table... gp.PivotTable_management ( joined , \"'Attrib.OwnerID';'OwnerID'\" , \"AttrTag\" , \"AttrStr\" , Pivoted_AttrTags_by_OwnerID ) # Process: Add Join2... gp.AddJoin_management ( points_new2 , \"Point.EntID\" , Pivoted_AttrTags_by_OwnerID , \"OwnerID\" , \"INNER\" , ) # Process: Copy Features... gp.CopyFeatures_management ( points_new3 , blocks , \"\" , \"0\" , \"0\" , \"0\" ) except : # If an error occurred while running a tool print the messages print gp.GetMessages ()"}, "usage": ["This tool will output a ", "File based Geodatabase", " (.GDB) by default, but will also output an ", "Personal Geodatabase", " (.MDB) if the user defines the proper extension.", "A fixed set of feature classes will be generated in the specified output feature dataset. These feature classes contain the geometry for the lines, areas, points, and document extent, and optionally, point feature classes can be generated for each unique block or cell name.", "This tool creates a new geodatabase and will not append to an existing one.", "Postprocessing will be required if you need to perform any joins of the data or relationships. The output is data from the CAD inputs and no relationships exist. Its flattened structure allows it to be put back together however you want.", "The Input files parameter will allow the addition of different kinds of CAD data (DGN, DWG, DXF) in one operation.", "CAD property tables are created in the output workspace. These normalized tables can be linked together using key fields, such as the EntID that is also found in the feature classes created by the Import CAD tool.", "If a table in the output feature dataset or workspace has the same name as one created by ImportCAD, the table will be deleted and re-created.", "The Import from CAD tool does not support the creation of annotation feature classes.", "The CAD property tables can be accessed from the output workspace in a model or script using the ", "Select Data", " tool. Likewise, the feature classes generated from the CAD data can be accessed using the Select Data tool on the output feature dataset.", "CAD text and attribute entities are converted to point features.", "Import from CAD cannot output to an ArcSDE feature class; it will create a new personal geodatabase.", "If you are going to copy the DocPath field in the CadDoc feature class or the TextMemo field in the TxtProp table from the staging geodatabase to a ArcSDE geodatabase, there are some limitations you need to be aware of. The DocPath field in the CadDoc feature class and the TextMemo field in the TxtProp table each have a memo field that is too long to fit in an ArcSDE geodatabase. For information about limitations of your ArcSDE geodatabases, see your system administrator. Portions of the information in these fields are duplicated in other fields. They have a shorter field length, so they can be moved to an ArcSDE geodatabase. The DocName field contains portions of the DocPath field, and the TextValue field contains portions of the TextMemo field.", "This function is generally used in the process of translating CAD data to an existing geodatabase using a series of other geoprocessing functions in a script or ModelBuilder model and as such, the resulting data from this function is typically temporary.", "Normalized tables are generated in the specified output workspace. These tables contain the tabular attributes of the layer properties, text properties, entity properties, block or cell attributes, and extended entity data or MSLink values.", "If a projection file exists for the input CAD file it will automatically populate the spatial reference parameter with the projections information. If multiple CAD files are used as inputs the spatial reference will be taken from the first CAD file which has a projection file, in the input list.", "When generating a default spatial reference for the staging geodatabase, you should always use the extent default model of the CAD file as the foundation. This means that if there are entities outside the extents of the default CAD field extents, they could be rejected. Be sure the model with the greatest extent is used as the default model in a DGN drawing. Only MicroStation version 8 supports the use of multiple models in a single design file.", "If a DGN file has multiple models, be sure the first model has the largest domain. Import from CAD calculates the domain for the entire DGN file from the first model. If this is not the case, be sure to expand the domain in your first model to be large enough so all will fit.", "If you use a PRJ file for the spatial reference parameter, be sure to set your x,y,z domain. If you do not set a domain, this could cause some features not to be imported if they fall outside the domain calculated on the fly.", "If you need to convert CAD annotation to geodatabase annotation, there are specific tools for this, such as ", "Import CAD Annotation", "."], "parameters": [{"name": "Input_Files", "isInputFile": true, "isOptional": false, "description": "The collection of CAD files to convert to geodatabase features. ", "dataType": "CAD Drawing Dataset"}, {"name": "out_personal_gdb", "isOutputFile": true, "isOptional": false, "description": "The new output geodatabase where all the input CAD features are placed along with the accompanying tables. If you need joins of the tables or features, then postprocessing is required. ", "dataType": "Workspace"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference to be applied to the Output Staging Geodatabase. ", "dataType": "Spatial Reference"}, {"name": "Explode_Complex", "isOptional": true, "description": "Specifies if block inserts in DWG or DXF and/or cells in DGN will create a feature for each constituent element in the complex object or just a single point feature at its defined insertion point. DO_NOT_EXPLODE_COMPLEX \u2014 The tool processes each complex object as a single point feature. EXPLODE_COMPLEX \u2014 The tool expands the geometry of complex objects into their subentities, which are then added to the appropriate feature classes in the output feature dataset. ", "dataType": "Boolean"}]},
{"syntax": "ImportCoverageAnnotation_conversion (input_features, output_featureclass, reference_scale, {use_levels}, {match_symbols_from_first_input}, {require_symbol_from_table}, {feature_linked}, {linked_feature_class}, {create_annotation_when_feature_added}, {update_annotation_when_feature_modified})", "name": "Import Coverage Annotation (Conversion)", "description": "Converts a collection of coverage annotation features to geodatabase annotation. You can convert each level to individual annotation classes or merge them into a single class. Also, if you choose map layers as input, the level and font overrides will be honored.", "example": {"title": "ImportCoverageAnnotation Example (Python Window)", "description": "The following Python Window script demonstrates how to use the ImportCoverageAnnotation tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.ImportCoverageAnnotation_conversion ( \"roads/annotation\" , \"Ontario.gdb/roads_anno\" , 10000 , \"CLASSES_FROM_LEVELS\" , \"NO_MATCH\" , \"NO_SYMBOL_REQUIRED\" , \"STANDARD\" , \"\" , \"AUTO_CREATE\" , \"AUTO_UPDATE\" )"}, "usage": ["The conversion requires an ", "exclusive lock", " so it may not be opened by another application. Output written to ArcSDE geodatabases will not be versioned.", "You can link annotation features to features in a point, line, or polygon feature class. If converting annotation to ArcSDE, the link feature class must not be registered as versioned.", "You can convert each coverage annotation level to individual annotation classes or merge them into a single class.", "If you select coverage annotation features and/or use a definition query, only those features that are selected and visible will be converted.", "You can create a selection set of coverage features and create a new layer from the selection. If you use that new layer as input to the conversion, only those features in the layer will be converted.", "If you override the font symbol or color for coverage annotation features, those settings will be honored during the conversion. If you want to convert more than one annotation feature class using the symbol overrides that you applied to one layer, make sure that layer is the first input, and check the option to Match symbols from first input.", "Choose a reference scale that is roughly equal to the scale at which the annotation will normally be displayed. You can think of the reference scale as \"freezing\" the symbol and text sizes to a particular map scale. When viewing the map, the symbol and text sizes change depending on the viewing scale. Symbols and text will appear larger as you zoom in (the current map scale is larger than the reference scale) and smaller when you zoom out (the current map scale is smaller than the reference scale).", "Annotation that is feature-linked is associated with a specific feature in another feature class in the geodatabase. If checked, when you create the output annotation feature class, a relationship class will be automatically generated as well. This relationship class defines the relationship between the annotation and the features and enables you to define and customize this relationship. It will not establish the links between features and annotation. That will have to be done within the ArcMap Editor."], "parameters": [{"name": "input_features", "isInputFile": true, "isOptional": false, "description": "The coverage annotation features that you want to convert to geodatabase annotation. If you choose a coverage annotation layer in ArcMap, the following properties of that layer will be honored during the conversion: Visible drawing levels. Only those levels that are turned on for drawing will be converted. Substitution of font and color properties for the text symbol. Selection. Only the selected features will be converted. Definition query. Only visible features that match the definition query will be converted.", "dataType": "Feature Layer"}, {"name": "output_featureclass", "isOutputFile": true, "isOptional": false, "description": "Browse into an existing geodatabase and type in the name of the new annotation feature class to create. ", "dataType": "Feature Class"}, {"name": "reference_scale", "isOptional": false, "description": "Enter the scale to use as a reference for the annotation. This sets the scale to which all symbol and text sizes in the annotation will be based. ", "dataType": "Double"}, {"name": "use_levels", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specify whether all coverage annotation drawing levels will be converted to annotation classes within the feature class. CLASSES_FROM_LEVELS \u2014 Each coverage annotation drawing level will be converted to an annotation class, within the Output Feature Class. This is the default. ONE_CLASS_ONLY \u2014 All coverage annotation drawing levels will be converted to a single annotation class within the Output Feature Class. ", "dataType": "Boolean"}, {"name": "match_symbols_from_first_input", "isOptional": true, "description": "If you are converting coverage annotation from more than one coverage or annotation subclass and need to substitute the font properties for a symbol and apply them to all the input features, you can use this option. MATCH_FIRST_INPUT \u2014 Match the symbols from the first input layer and have them apply to all layers. NO_MATCH \u2014 Each drawing file retains its own font properties. This is the default. ", "dataType": "Boolean"}, {"name": "require_symbol_from_table", "isOptional": true, "description": "Specify whether the output annotation features must reference a symbol stored in the symbol collection for the feature class. NO_SYMBOL_REQUIRED \u2014 Any type of annotation (including graphics) may be stored in the annotation feature class. This is the default. REQUIRE_SYMBOL \u2014 The annotation must reference one of the predefined symbols in the collection; the symbol cannot be stored inline. ", "dataType": "Boolean"}, {"name": "feature_linked", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Choose whether the output annotation feature class will be linked to the features in another feature class. FEATURE_LINKED \u2014 The output annotation feature class will be linked to the features in another feature class. STANDARD \u2014 The output annotation feature class will not be linked to the features in another feature class. This is the default. ", "dataType": "Boolean"}, {"name": "linked_feature_class", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. The feature class to which you are linking annotation features. This option is only available if you choose FEATURE_LINKED for the previous parameter. ", "dataType": "Feature Layer"}, {"name": "create_annotation_when_feature_added", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specify whether new annotation will be generated when you add new features to the feature class to which this annotation feature class is linked. This option is only available if you choose FEATURE_LINKED for the Feature-linked parameter and specify a Linked Feature Class. AUTO_CREATE \u2014 When editing in ArcMap, a new piece of annotation will be automatically generated when you add new features to the feature class to which this annotation feature class is linked. This is the default. NO_AUTO_CREATE \u2014 When editing in ArcMap, a new piece of annotation will not be automatically generated when you add new features to the feature class to which this annotation feature class is linked. ", "dataType": "Boolean"}, {"name": "update_annotation_when_feature_modified", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specify whether the ArcMap Editor will automatically update the placement of annotation when you edit features in the feature class to which this annotation feature class is linked. This option is only available if you choose FEATURE_LINKED for the Feature-linked parameter and specify a Linked Feature Class. AUTO_UPDATE \u2014 The annotation will be repositioned according to the modified feature shape. This is the default. NO_AUTO_UPDATE \u2014 The annotation will remain in its original position. ", "dataType": "Boolean"}]},
{"syntax": "ImportCADAnnotation_conversion (input_features, output_featureclass, reference_scale, {use_levels}, {match_symbols_from_first_input}, {require_symbol_from_table}, {feature_linked}, {linked_feature_class}, {create_annotation_when_feature_added}, {update_annotation_when_feature_modified})", "name": "Import CAD Annotation (Conversion)", "description": "Converts a collection of CAD annotation features to geodatabase annotation. You can convert each level to individual annotation classes or merge them into a single class. Also, if you choose map layers as input, the level and font overrides will be honored.", "example": {"title": "ImportCADAnnotation Example (Python Window)", "description": "The following Python Window script demonstrates how to use the ImportCADAnnotation tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:\\data\" arcpy.ImportCADAnnotation_conversion ( \"roads.dxf/annotation\" , \"roads.gdb/roadsanno\" , 1200 , \"CLASSES_FROM_LEVELS\" , \"NO_MATCH\" , \"NO_SYMBOL_REQUIRED\" , \"STANDARD\" , \"\" , \"AUTO_CREATE\" , \"NO_AUTO_UPDATE\" )"}, "usage": ["The conversion requires an ", "exclusive lock", " so it may not be opened by another application. Output written to ArcSDE geodatabases will not be versioned.", "You can convert each CAD drawing layer to individual annotation classes or merge them into a single class. Also, if you choose map layers as input, the drawing layers and font overrides will be honored.", "Choose a reference scale that is roughly equal to the scale at which the annotation will normally be displayed. You can think of the reference scale as \"freezing\" the symbol and text sizes to a particular map scale. When viewing the map, the symbol and text sizes change depending on the viewing scale. Symbols and text will appear larger as you zoom in (the current map scale is larger than the reference scale) and smaller when you zoom out (the current map scale is smaller than the reference scale).", "If you select CAD annotation features and/or use a definition query, only those features that are selected and visible will be converted.", "If you check the option to require symbols to be selected from the symbol table, newly created or updated annotation features must reference one of the predefined symbols in the collection; the symbol cannot be stored inline. When unchecked, you may store any type of annotation (including graphics) in the annotation feature class.", "Annotation that is feature-linked is associated with a specific feature in another feature class in the geodatabase. If checked, when you create the output annotation feature class, a relationship class will be automatically generated as well. This relationship class defines the relationship between the annotation and the features and enables you to define and customize this relationship. It will not establish the links between features and annotation. That will have to be done within the ArcMap Editor."], "parameters": [{"name": "input_features", "isInputFile": true, "isOptional": false, "description": "The CAD annotation features that you want to convert to geodatabase annotation. If you choose a CAD annotation layer in ArcMap, the following properties of that layer will be honored during the conversion: Visible drawing layers. Only those layers that are turned on for drawing will be converted. Substitution of font and color properties for the text symbol. Selection. Only the selected features will be converted. Definition Query. Only visible features that match the definition query will be converted.", "dataType": "Feature Layer"}, {"name": "output_featureclass", "isOutputFile": true, "isOptional": false, "description": "The geodatabase annotation feature class to which you want to convert CAD annotation. ", "dataType": "Feature Class"}, {"name": "reference_scale", "isOptional": false, "description": "Enter the scale to use as a reference for the annotation. This sets the scale to which all symbol and text sizes in the annotation will be made relative. ", "dataType": "Double"}, {"name": "use_levels", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specify whether all CAD drawing layers or levels will be converted to annotation classes within the feature class. CLASSES_FROM_LEVELS \u2014 Each CAD drawing level or layer will be converted to an annotation class within the Output Feature Class. This is the default. ONE_CLASS_ONLY \u2014 All CAD drawing levels or layers will be converted to a single annotation class within the Output Feature Class. ", "dataType": "Boolean"}, {"name": "match_symbols_from_first_input", "isOptional": true, "description": "If you are converting CAD annotation from more than one drawing file and need to substitute the font properties for a symbol and apply that to all the input features, you can use this option. MATCH_FIRST_INPUT \u2014 Match the symbols from the first input layer and have them apply to all layers. NO_MATCH \u2014 Each drawing file retains its own font properties. This is the default. ", "dataType": "Boolean"}, {"name": "require_symbol_from_table", "isOptional": true, "description": "Specify if the output annotation features will reference a symbol stored in the symbol collection for the feature class. NO_SYMBOL_REQUIRED \u2014 The output annotation features will not reference a symbol stored in the symbol collection for the feature class. This is the default. REQUIRE_SYMBOL \u2014 The output annotation features will reference a symbol stored in the symbol collection for the feature class. ", "dataType": "Boolean"}, {"name": "feature_linked", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Choose whether the output annotation feature class will be linked to the features in another feature class. The feature-linked option will not be available with an ArcGIS for Desktop Basic license. FEATURE_LINKED \u2014 The output annotation feature class will be linked to the features in another feature class. STANDARD \u2014 The output annotation feature class will not be linked to the features in another feature class. This is the default. ", "dataType": "Boolean"}, {"name": "linked_feature_class", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. The feature class to which you are linking annotation features. The feature class must be a point, line, or polygon feature class. If converting annotation into ArcSDE, the link feature class must not be registered as versioned. This option is only available if you choose FEATURE_LINKED for the previous parameter. ", "dataType": "Feature Layer"}, {"name": "create_annotation_when_feature_added", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specify whether new annotation will be generated when you add new features to the feature class to which this annotation feature class is linked. This option is only available if you choose FEATURE_LINKED for the Feature-linked parameter and specify a Linked Feature Class. AUTO_CREATE \u2014 The ArcMap Editor will automatically generate a new piece of annotation when you add new features to the feature class to which this annotation feature class is linked. This is the default. NO_AUTO_CREATE \u2014 The ArcMap Editor will not automatically generate a new piece of annotation when you add new features to the feature class to which this annotation feature class is linked. ", "dataType": "Boolean"}, {"name": "update_annotation_when_feature_modified", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specify whether to automatically update the placement of annotation when you edit features in the feature class to which this annotation feature class is linked. This option is only available if you choose FEATURE_LINKED for the Feature-linked parameter and specify a Linked Feature Class. AUTO_UPDATE \u2014 The annotation will be repositioned according to the modified feature shape. This is the default. NO_AUTO_UPDATE \u2014 The annotation will remain in its original position. ", "dataType": "Boolean"}]},
{"syntax": "FeatureClassToGeodatabase_conversion (Input_Features, Output_Geodatabase)", "name": "Feature Class To Geodatabase (Conversion)", "description": "Converts one or more feature classes or feature layers to geodatabase feature classes.", "example": {"title": "FeatureClassToGeodatabase example (Python window)", "description": "The following Python window script demonstrates how to use the FeatureClassToGeodatabase function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.FeatureClassToGeodatabase_conversion ([ \"climate.shp\" , \"majorrds.shp\" ], \"C:/output/output.gdb\" )"}, "usage": ["The inputs can include shapefiles, coverage feature classes, VPF feature classes, or geodatabase feature classes. The inputs can also be feature layers (a layer in the ArcMap or ArcScene table of contents or a feature layer created by the ", "Make Feature Layer", " tool).", "If the input is a layer with selected features, only those selected features will be written to the new output feature class.", "The name of the output feature classes will be based on the name of the input feature class name. For example, if the input is ", "C:\\base\\streams.shp", ", the output feature class will be named streams.", "If the name already exists in the output geodatabase, a number will be appended to the end to make it unique, for example, \"_1\".", "This tool does not support annotation."], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": "One or more feature classes or feature layers to be imported into an ArcSDE, file, or personal geodatabase. ", "dataType": "Feature Layer"}, {"name": "Output_Geodatabase", "isOutputFile": true, "isOptional": false, "description": "The output or destination geodatabase. This can be a file or personal geodatabase, or an ArcSDE geodatabase. ", "dataType": "Feature Dataset; Workspace"}]},
{"syntax": "FeatureClassToFeatureClass_conversion (in_features, out_path, out_name, {where_clause}, {field_mapping}, {config_keyword})", "name": "Feature Class To Feature Class (Conversion)", "description": "Converts a shapefile, coverage feature class, or geodatabase feature class to a shapefile or geodatabase feature class.", "example": {"title": "FeatureClassToFeatureClass example (Python window)", "description": "The following Python window script demonstrates how to use the FeatureClassToFeatureClass tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/GreenvalleyDB.mdb/Public Buildings\" arcpy.FeatureClassToFeatureClass_conversion ( \"buildings_point\" , \"C:/output/output.gdb\" , \"buildings_point\" )"}, "usage": ["The ", "Field Map", " parameter controls how the input fields in the ", "Input Features", " will be written to the ", "Output Features", ".", "The ", "Copy Features", " tool can also be used to convert a shapefile, coverage feature class, or geodatabase (file, personal, or ArcSDE) feature class to a shapefile or geodatabase (file, personal, or ArcSDE) feature class.", "An SQL expression can be used to select a subset of features. For further details on the syntax for the Expression parameter, see ", "Building an SQL Expression", " or ", "SQL Reference", ".", "When converting geodatabase data that has subtypes or ", "domains", " to a shapefile, both the subtype and domain codes and descriptions can be included in the output. Use the ", "Transfer field domain descriptions", "  geoprocessing environment to control this behavior. By default, only domain and subtype codes will be included in the output, not descriptions.", "Conversion to shapefiles  with subtype and domain descriptions may take more time (slower performance) than without descriptions. If you do not require the subtype and domain descriptions in your shapefile output, it is recommended you use the unchecked (", "False", " or ", "NOT_TRANSFER_DOMAINS", " in scripting) default behavior of the ", "Transfer field domain descriptions", " environment to achieve best performance."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature class or feature layer that will be converted. ", "dataType": "Feature Layer"}, {"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The location in which the output feature class will be created. This can be either a geodatabase or a folder. If the output location is a folder, the output will be a shapefile. ", "dataType": "Workspace;Feature Dataset"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the output feature class. ", "dataType": "String"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of features. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, or coverages, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}, {"name": "field_mapping", "isOptional": true, "description": "The fields and field contents chosen from the input feature class. You can add, rename, or delete output fields as well as set properties such as data type and merge rule. Learn more about choosing and setting the output fields . ", "dataType": "Field Mappings"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for geodatabases in file and ArcSDE geodatabases. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords for ArcSDE Enterprise Edition are set up by your database administrator. ", "dataType": "String"}]},
{"syntax": "RegionPoly_arc (in_cover, out_cover, in_subclass, {out_table})", "name": "Region To Polygon Coverage (Coverage)", "description": "Converts a region subclass to a polygon coverage and creates an INFO table containing overlapping region information. \r\n Learn more about how Region To Polygon Coverage works \r\n", "example": {"title": "RegionPoly example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a polygon coverage from a region subclass.\r\n", "code": "# Name: RegionPoly_Example.py # Description: Creates a polygon coverage from a region subclass # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"hydro\" outCover = \"C:/output/genhydro\" inSubclass = \"general\" outTable = \"C:/output/genhydrotab\" # Execute RegionPoly arcpy.RegionPoly_arc ( inCover , outCover , inSubclass , outTable )"}, "usage": ["All items in the region subclass polygon attribute table (PAT) are maintained in the Output Coverage PAT.", "The Output Coverage PAT contains only the attributes of the first region associated with each polygon. Values of zero indicate void areas in which the subclass does not exist.", "The attributes of the second to the Nth regions associated with each polygon are stored in the Output Table.", "If only one region is associated with each polygon (a planar region subclass), then the Output Table does not need to be specified. However, an Output Table must be specified when using non-planar region subclasses.", "The polygon User-IDs of the Output Coverage will be altered.", "Use the Output Table to relate the Output Coverage polygons to the Input Coverage region subclass."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing the region subclass to convert to polygons. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The polygon coverage to be created from the Input Subclass. The coverage may not already exist. ", "dataType": "Coverage"}, {"name": "in_subclass", "isInputFile": true, "isOptional": false, "description": "The region subclass of the Input Coverage that will be converted to a polygon coverage. ", "dataType": "String"}, {"name": "out_table", "isOutputFile": true, "isOptional": true, "description": "The output INFO table that will contain information for regions associated with each polygon. ", "dataType": "INFO Table"}]},
{"syntax": "PolyRegion_arc (in_cover, out_cover, out_subclass)", "name": "Polygon Coverage To Region (Coverage)", "description": "Converts polygons to regions in a one-to-one mapping in a region subclass. Each polygon in the Input Coverage becomes a region in the Output Subclass. Attributes in the polygon attribute table (PAT) are copied to the corresponding region PATsubclass. The Output Coverage can be the same as the Input Coverage; if so, the Output Subclass is then created in the Input Coverage. \r\n Learn more about how Polygon Coverage To Region works \r\n", "example": {"title": "PolyRegion example (stand-alone script)", "description": " The following stand-alone script demonstrates how to create a region coverage from a polygon coverage.", "code": "# Name: PolyRegion_Example.py # Description: Creates a region coverage from a polygon coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"citylim\" outCover = \"C:/output/cityregions\" outSubclass = \"city\" # Execute PolyRegion arcpy.PolyRegion_arc ( inCover , outCover , outSubclass )"}, "usage": ["This tool can be used on an Input Coverage that does not have arc topology; however, the Input Coverage must have polygon topology.", "The tool builds region topology for the Output Subclass. Topology in the Input Coverage is maintained in the Output Coverage.", "When the Output Coverage is not the same as the Input Coverage, a new coverage will be created, and the Input Coverage is copied to the Output Coverage.", "The Output Coverage cannot refer to an existing coverage unless it's the Input Coverage.", "Polygon attributes are copied to the region subclass.", "Region subclass names may be from 1 to 13 characters and cannot start with a numeric character."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The polygon coverage to be converted to a region subclass. Each polygon of the <in_cover> is made into a region. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage that will contain the new subclass. ", "dataType": "Coverage"}, {"name": "out_subclass", "isOutputFile": true, "isOptional": false, "description": "The name of the region subclass that will be created. ", "dataType": "String"}]},
{"syntax": "ArcRoute_arc (in_cover, out_route_system, {in_route_item}, {out_route_item}, {measure_item}, {coordinate_priority}, {use_blanks})", "name": "Line Coverage To Route (Coverage)", "description": "Creates a route system by creating whole arc sections for each arc in the input coverage and can also be used to append arcs to an existing route system.", "example": {"title": "ArcRoute example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a route system for a line coverage that contains road segments.", "code": "# Name: ArcRoute_Example.py # Description: Adds a route system to a streams coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"stream\" outRouteSystem = \"nstreams\" inRouteItem = \"STREAM_NAME\" coordinatePriority = \"LL\" useBlanks = \"NO_BLANK\" # Execute ArcRoute arcpy.ArcRoute_arc ( inCover , outRouteSystem , inRouteItem , \"\" , \"\" , coordinatePriority , useBlanks )"}, "usage": ["Creates a route system from lines or appends lines to a route system. It groups lines that are topologically connected and have unique values for the Input item to create the route system. The unique values of the Input item are always written to the Output item in the route attribute table (RAT); these values help identify routes once they have been created.", "When appending routes to an existing route system, the ", "Output Route Item", " must be the name of an existing item on the route attribute table of the route system. The tool will append a section to an existing route for every input arc having an Input item equal to an ", "Output Route Item", " in the route attribute table, provided the input arcs are topologically connected to the route being appended. The Measure Item on the original part of the route being appended is updated based on the measures assigned to the new sections and the specified Starting node. For those groups of arcs having values for the ", "Input Route Item", " not found in the ", "Output Route Item", ", a new route is created.", "Line Coverage To Route", " groups arcs into routes based on both the unique values in the ", "Input Route Item", " and the topological connectivity of the arcs. Line Coverage To Route cannot be used to group topologically disjointed sets of arcs into the same route based on the Input Route Item.", "If the ", "Input Route Item", " is a floating point, then it is truncated to integer for building the routes in the new route-system.", "Line Coverage To Route", " will merge two or more routes if they have the same value for the ", "Output Route Item", " and become connected due to arcs being appended.", "Line Coverage To Route", " will not append a section to an existing route if the existing route contains overlapping sections. The input arcs are assigned to a new route instead.", "The unique values in the ", "Input Route Item", " are always written to the ", "Output Route Item", " in the route attribute table. Use the values in the ", "Output Route Item", " to help you identify routes once they have been created.", "The", " Input Cover", " must have an arc attribute table, and node numbers must exist and be up-to-date. Use ", "Build", " with the LINE option to create or update an arc attribute table. Use ", "Renumber Node", " to update node numbers.", "When using the ", "Create Route From Null Values", ", note that the BLANK option is considered to be zero (0.0) or null where the Input Route Item is numeric, and null where the Input Route Item is character. The ", "Create Route From Null Values", " BLANK option is not considered if the ", "Input Route Item", " is not specified."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage from which the routes are to be created. ", "dataType": "Coverage"}, {"name": "out_route_system", "isOutputFile": true, "isOptional": false, "description": "The name of the route system to be created or appended. ", "dataType": "String"}, {"name": "in_route_item", "isInputFile": true, "isOptional": true, "description": "The name of an item in the arc attribute table used to group arcs into separate routes. A new route is created in the route attribute table for each unique value in this item. The default is to create a route for each topologically connected set of arcs. ", "dataType": "INFO Item"}, {"name": "out_route_item", "isOutputFile": true, "isOptional": true, "description": "The name of the new item in the route attribute table that will contain the unique values in the input route item. When appending routes to an existing route system, it is an existing item in the route attribute table used to append routes. The default item is Input Route Item. ", "dataType": "String"}, {"name": "measure_item", "isOptional": true, "description": "An item in the arc attribute table of Input Coverage whose value is accumulated to produce the measure values. The default item is LENGTH. ", "dataType": "INFO Item"}, {"name": "coordinate_priority", "isOptional": true, "description": "Determines coordinate priority when choosing a start node for the route. UL \u2014 Upper left. This is the default. UR \u2014 Upper right. LL \u2014 Lower left. LR \u2014 Lower right.", "dataType": "String"}, {"name": "use_blanks", "isOptional": true, "description": "Specifies whether arcs having a null or 0 value for the input route item will be used to create a route. BLANK \u2014 Arcs having a null or 0 value for the Input Route Item will be used to create routes. This is the default. NOBLANK \u2014 Arcs having a null or 0 value for the Input Route Item will not be used to create routes. ", "dataType": "Boolean"}]},
{"syntax": "RegionClass_arc (in_cover, {out_cover}, out_subclass, {in_region_item}, {out_region_item}, {selection_file}, {method})", "name": "Line Coverage To Region (Coverage)", "description": "Converts arcs to preliminary regions in a new or existing coverage or appends preliminary regions to an existing region subclass. Arcs are grouped into preliminary regions based on the unique value of the Line item and must form closed loops. The unique values are saved in the output subclass Region item. \r\n Learn more about how Line Coverage To Region works \r\n", "example": {"title": "RegionClass example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a region coverage from a line coverage.\r\n", "code": "# Name: RegionClass_Example.py # Description: Creates regions in a line coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"tong_azone\" outSubclass = \"districts\" method = \"SINGLERING\" # Execute RegionClass arcpy.RegionClass_arc ( inCover , \"\" , outSubclass , \"\" , \"\" , \"\" , method ) arcpy.Build_arc ( inCover , \"POLY\" )"}, "usage": ["The ", "Input Coverage", " must have an AAT to specify the Input Region Item.", "To create fully built regions from the preliminary regions, use ", "Clean", " (or ", "Build", ") with the POLY option on the Output Coverage.", "The arcs in each group, which are determined by the unique value of the Input Region Item, must form closed loops. When the Input Region Item is not specified, each arc in the Input Coverage becomes a preliminary region and should form a closed loop.", "Depending on the Input Region Item, arcs may be reused in any grouping with repeated use of this tool. In this way, duplicate or overlapping preliminary regions may be appended to the same subclass or to different subclasses.", "The Output Region Item may be the same as the Input Region Item.", "If the selection file has no arcs selected, the command will consider all arcs as selected. If the selection file has everything selected, everything is also selected for Line Coverage To Region.", "If a selection file is not specified, all arcs are selected and available for grouping into regions. However, arcs in the Input Coverage that are already part of one or more fully structured regions are not available for appending to the subclass since they may not form closed rings when grouped.", "If the Output Region Item already exists in the PATsubclass, the specified Output Region Item must have the same item definition as the existing one.", "Region subclass names may be from 1 to 13 characters and cannot start with a numeric character.", "When preliminary regions are appended to a subclass, the polygon topology is removed from the Output Coverage (or the Input Coverage) because it needs to be rebuilt.", "This tool does not compute the area of the preliminary regions but sets the area values in the PATsubclass to zero and computes the perimeter. ", "Building", " with the POLY option will calculate the area values."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing the arcs from which the preliminary regions are created. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": true, "description": "The coverage that will contain the preliminary regions. If the output coverage is not specified, the preliminary regions are created in the input coverage. ", "dataType": "Coverage"}, {"name": "out_subclass", "isOutputFile": true, "isOptional": false, "description": "The name of the region subclass to be created or appended. ", "dataType": "String"}, {"name": "in_region_item", "isInputFile": true, "isOptional": true, "description": "Item in the AAT of the input coverage whose values are used to group arcs into preliminary regions. The item is appended to the region PATsubclass. If not specified, each group of arcs becomes a preliminary region. ", "dataType": "INFO Item"}, {"name": "out_region_item", "isOutputFile": true, "isOptional": true, "description": "Output name for the input region Item to be used in the region PATsubclass instead of the Input Region Item name. ", "dataType": "String"}, {"name": "selection_file", "isOptional": true, "description": "The name of a selection file that can be used to specify a subset of the arcs to be grouped into preliminary regions. ", "dataType": "File"}, {"name": "method", "isOptional": true, "description": "Determines whether regions will be created from multiple rings of arcs or single rings of arcs. MULTIRING \u2014 Creates regions from multiple rings of arcs whose values for the input region item are identical. SINGLERING \u2014 Each ring of arcs becomes a region. ", "dataType": "String"}]},
{"syntax": "Append_arc (in_covers, out_cover, {append_method}, {feature_classes}, {number_method})", "name": "Append (Coverage)", "description": "Combines an unlimited number of coverages into a single  coverage . Append checks for the existence of the coverage, verifies that the list of feature attribute table items matches the items in previously entered coverages (unless the FEATURES_ONLY option is used), and calculates Tic-ID and feature User-ID offsets according to the specified offset option. \r\n Learn more about how Append works \r\n", "example": {"title": "Buffer example (stand-alone script)", "description": "The following stand-alone script demonstrates how to append several coverage into one output coverage.\r\n", "code": "# Name: Append_Example.py # Description: Appends several coverages into one output # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCovers = [ \"tongzone1\" , \"tongzone2\" , \"tongzone3\" , \"tongzone4\" ] outCover = \"C:/output/tongass\" appendMethod = \"FEATURES_ONLY\" featureClasses = \"LINE\" numberMethod = \"TICS_ONLY\" # Execute Append arcpy.Append_arc ( inCovers , outCover , appendMethod , featureClasses , numberMethod )"}, "usage": ["All Input Coverages to be appended must contain the ", "feature class", " or set of feature classes and feature attribute tables to be appended. For example, if the NET feature type option is used, all coverages should have line and polygon features, and corresponding AATs and PATs.", "The TOL file with ", "single precision", " coverages and the PAR file with ", "double precision", " coverages will not be transformed or retained in the Output Coverage files.", "The ", "fuzzy tolerance", " of the Output Coverage will default to 0.002 if the width of the BND is between 1 and 100; otherwise, the tolerance is 1/10,000 of the width or height of the BND, whichever is greater.", "The item definitions of the feature attribute tables must be the same and in the same order for all appended coverages (unless the FEATURES_ONLY option is used).", "Using the ROUTE.subclass option is the same as using the ROUTE.subclass and SECTION.subclass options together; that is, both the routes and sections of the specified subclass are appended. If the User-ID of any route is not unique across the coverages being appended, that route will be merged with other routes of the same User-ID. The Output Coverage may, therefore, contain less routes than the total number of routes in the Input Coverages. Append never modifies route measures.", "All polygons in the Input Coverages are appended with the REGION.subclass option, not just those pertaining to the regions being appended. Region subclasses are not maintained with the POLY option; you must also specify REGION.subclass. After combining region subclasses with Append, the new topological relationships between features must be calculated using ", "Build", " or ", "Clean", ". Another way to combine region subclasses is to use ", "Union", ". Union calculates topology when combining region subclasses.", "Annotation", " for each coverage is appended to the Output Coverage. As with all other feature classes, all TATs within a subclass must have identical item definitions.", "The ", "projection", " file (PRJ) will be copied to the Output Coverage. The first coverage entered will provide the projection information for the Output Coverage.", "The coordinate precision of the Output Coverage is determined by the current processing rule as set by the ", "Derived Precision", " environment setting. If the processing rule has not been established, then the processing rule will be HIGHEST. This means that Append will create an Output Coverage in the highest precision of the Input Coverages.", "Projection files will be compared for similarity using the level of comparison specified with the ", "Compare Projections", " environment setting.", "The Feature Type parameter's Add Value button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run or its derived data does not exist, the Feature Type parameter may not be populated with values. The Add Value button allows you to add expected value(s) so you can complete the Append dialog and continue to build your model."], "parameters": [{"name": "in_covers", "isInputFile": true, "isOptional": false, "description": "The input coverages to be appended. There is no limit to the number of coverages that can be entered. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage to be created. The output coverage cannot already exist. ", "dataType": "Coverage"}, {"name": "append_method", "isOptional": true, "description": "Determines whether only coordinates will be appended (FEATURES_ONLY) or if features will also be appended. FEATURES_ONLY \u2014 Location information for all feature classes that are appended. Feature attribute tables are not appended. FEATURES_ATTRIBUTES \u2014 The features in the specified coverage define the set of features to be appended. ", "dataType": "String"}, {"name": "feature_classes", "isOptional": true, "description": "The feature class of the input coverage or coverages. The Add Value button, which is used only in ModelBuilder, allows you to add expected value(s) so you can complete the dialog and continue to build your model. POLY \u2014 Polygon feature coordinates and attributes are appended, including label points. LINE \u2014 Arc feature coordinates and attributes are appended. POINT \u2014 Point feature coordinates and attributes are appended. NODE \u2014 Arc and node feature coordinates and their attributes are appended. NET \u2014 Arc and polygon feature coordinates and their attributes are appended. LINK \u2014 Arc and point feature coordinates and their attributes are appended. ANNO.subclass \u2014 Annotation features and attributes of the subclass are appended. SECTION.subclass \u2014 Section feature coordinates and attributes of the subclass are appended. ROUTE.subclass \u2014 Route and section feature coordinates and attributes of the subclass are appended. REGION.subclass \u2014 Region feature coordinates and attributes of the subclass are appended. Polygon feature coordinates and attributes are also appended. ", "dataType": "String"}, {"name": "number_method", "isOptional": true, "description": "Specifies how tics and coverage features will be numbered in the Output Coverage. IDs can be offset to ensure unique ID values for Output Coverage features. The ID offset is equal to 1 plus the highest ID value in the previously appended coverages. Offsets can be calculated for the following types of IDs: NO \u2014 Neither Tic-IDs nor feature User-IDs will be modified. This is the default option. TICS_ONLY \u2014 ID offsets will be calculated for tics. FEATURES_ONLY \u2014 User-ID offsets will be calculated for the feature class(es) specified by the feature classes argument. Tic-IDs are not modified. FEATURES_TICS \u2014 ID offsets will be calculated for both tics and features. ", "dataType": "String"}]},
{"syntax": "VPFImport_arc (input_vpf, output, {tile_name}, {control_file}, {standard_vpf})", "name": "Import From VPF (Coverage)", "description": "Converts a VPF table to an INFO table, or converts either an untiled VPF coverage or VPF tile to an ArcGIS coverage.", "example": {"title": "VPFImport example (stand-alone script)", "description": "The following stand-alone script demonstrates how to import a coverage from VPF format.\r\n", "code": "# Name: VPFImport_Example.py # Description: Import coverages from a VPF tile # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inputVpf = \"vpfLibrary/lib_000:hydro\" output = \"C:/output/coast1\" tileName = \"E/J/B/D\" standardVpf = \"NO_EXTRA\" # Execute VPFImport arcpy.VPFImport_arc ( inputVpf , output , tileName , \"\" , standardVpf )"}, "usage": ["Full VPF pathnames must be specified in the ", "Input VPF Coverage or Table", " parameter.", "Pathnames are specified as ", "vpfDatabase\\vpfLibrary\\vpfCoverage", " or ", "vpfDatabase\\vpfLibrary\\vpfCoverage\\vpfTile", " when using the Coverage option.", "Pathnames are specified as ", "vpfDatabase\\vpfLibrary\\vpfTable", " when using the Table option.", "If the VPF coverage was created using the Export To VPF tool with the option to convert all tables selected, then the Output Coverage will be identical to the Input VPF Coverage.", "VPF coverages with multiple linear feature classes will be translated into coverages with route subclasses. Multiple area feature classes will be converted to region subclasses. Text, connected node, and point feature classes will be translated into INFO tables that can be related to the appropriate coverage Feature Attribute Tables. Complex feature classes will also be converted to INFO tables.", "\r\n", "Learn about Conversion control file information", "\r\n"], "parameters": [{"name": "input_vpf", "isInputFile": true, "isOptional": false, "description": "The name of the VPF table, untiled coverage, or tile to be converted. The full pathname must be specified. ", "dataType": "VPF Coverage; VPF Table"}, {"name": "output", "isOptional": false, "description": "The output table or coverage to be created. ", "dataType": "Data Element"}, {"name": "tile_name", "isOptional": true, "description": "The input VPF tile, if one exists. ", "dataType": "String"}, {"name": "control_file", "isOptional": true, "description": "A file that can be used to ignore specific VPF feature classes or three-dimensional coordinates during translation. The name of this file is defined by the user. ", "dataType": "File"}, {"name": "standard_vpf", "isOptional": true, "description": "Specifies whether nonstandard VPF tables will be converted. NO_EXTRA \u2014 This option prevents VPFImport from importing extra tables created using VPFExport . This is the default. EXTRA \u2014 This option only needs to be used if the data being translated was converted to VPF using VPFExport. ", "dataType": "Boolean"}]},
{"syntax": "SDTSImport_arc (in_transfer_prefix, output, {out_point_cover}, {layer_name}, {data_dictionary}, {convert_void})", "name": "Import From SDTS (Coverage)", "description": "Creates ArcGIS coverages or grids from a Spatial Data Transfer Standard (SDTS) Topological Vector Profile (TVP) or Point Profile Transfer.", "example": {"title": "SDTSImport example (stand-alone script)", "description": "The following stand-alone script demonstrates how to import a coverage from SDTS format.\r\n", "code": "# Name: SDTSImport_Example.py # Description: Imports a coverage from SDTS format # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = r\"C:\\data\" # Set local variables inTransferPrefix = \"UTHY\" output = \"C:/output/uthydro\" dataDictionary = \"DROP_DD\" # Execute SDTSImport arcpy.SDTSImport_arc ( inTransferPrefix , output , \"\" , \"\" , dataDictionary , \"\" )"}, "usage": ["SDTS is a large standard composed of smaller, more limited subsets that are federally approved as part of the SDTS FIPS 173 standard. These subsets are called profiles. The Topological Vector Profile (designed specifically for planar vector data with topology), raster, and point profiles are the only profiles supported by ", "Import From SDTS", ". The type of profile being converted is automatically determined by the tool.", "Import From SDTS can read the U.S. Bureau of the Census TIGER, U.S. Geological Survey (USGS) DLG-3 vector data, National Geodetic Survey geodetic control point data, and USGS DEM raster data in SDTS format.", "A set of files containing global information about more than one transfer can be external from the transfer but must be under a directory called masterdd located at the same directory level as the transfer directory. If the command is executed from the same directory as the transfer files, specify a path to the prefix so the masterdd is used correctly. For example, ..tvpdata\\tr01 would be used as the input prefix if the master data dictionary is at the same level as the tvpdata directory.", "Polygon and line topology is generated for TVP data. Point topology is generated for Point Profile data.", "Complex spatial object modules in a TVP transfer are converted to either regions or route/sections depending on the spatial object type.", "Aggregated spatial object modules or data layers are converted separately by executing Import From SDTS for each layer or raster object record number in the transfer.", "The raster profile provides a flexible way to encode raster data. In a raster transfer, there should be one RSDF module, one LDEF module, and one or more cell modules. Each record in the RSDF module denotes one raster object. Each raster object can have multiple layers. Each layer will be encoded as one record in the LDEF module. The actual grid data is stored in the cell module which is referenced by the layer record. A typical USGS DEM dataset will have one RSDF record, one LDEF record, and one cell file. A typical ERDAS image dataset will have one RSDF record, multiple LDEF records, and one or more cell files.", "Related attribute tables are written to the output dataset.", "The relate environment is stored in Output Dataset.REL and/or Output Point Cover.REL.", "An additional cross reference table, Output Dataset.XREF, will be written to store both the from and to table information for all related attribute tables.", "Attribute tables will be named with either Output Dataset or Output Point Cover prefix and either APxx or ASxx extensions. Region and Route subclasses will be named with FFxx extensions.", "Use the AIDF module when converting an SDTS dataset that was created by the ", "Export to SDTS", " tool. The module contains information on items and redefined items in INFO.", "Information in the CATS module is saved as follows:", "If the COMT subfield contains valuable information, the NAME and COMT subfields are saved in Output Dataset.CATS as items NAME and COMT."], "parameters": [{"name": "in_transfer_prefix", "isInputFile": true, "isOptional": false, "description": "A four-character prefix common to all files in the SDTS transfer. The prefix may include a pathname to a directory. If no directory pathname is given, the files in the transfer will be read from the current workspace. ", "dataType": "String"}, {"name": "output", "isOptional": false, "description": "The coverage or grid to be created. ", "dataType": "Data Element"}, {"name": "out_point_cover", "isOutputFile": true, "isOptional": true, "description": "The name of an optional point coverage to be created when the Topological Vector Profile is converted. This option is ignored if the SDTS dataset is not the Point Profile Transfer. ", "dataType": "Coverage"}, {"name": "layer_name", "isOptional": true, "description": "The name of an aggregated spatial object that represents a single data layer in a transfer. There can be multiple layers in a single transfer. By default, the first layer encountered is the only one that will be converted. ", "dataType": "String"}, {"name": "data_dictionary", "isOptional": true, "description": "Option to retain or drop the data dictionary. DD \u2014 Retain the data dictionary. DROP_DD \u2014 Discard the data dictionary.", "dataType": "Boolean"}, {"name": "convert_void", "isOptional": true, "description": "Used to convert or preserve void and fill values in the raster transfer. In the raster profile, NULL values are defined in two general categories: (Undefined, not relevant) or (Relevant but unknown or missing). CONVERT \u2014 Convert void and fill values in the raster transfer. PRESERVE \u2014 Preserve void and fill values in the raster transfer.", "dataType": "Boolean"}]},
{"syntax": "S57Arc_arc (in_s57_file, out_workspace, {clean})", "name": "Import From S57 (Coverage)", "description": "Converts data from S-57 format to one or more coverages.", "example": {"title": "S57Arc example (stand-alone script)", "description": "The following stand-alone script demonstrates how to import a coverage from S-57 format.\r\n\r\n", "code": "# Name: S57Arc_Example.py # Description: Imports from S-57 format to coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inS57File = \"US5MI82M.000\" outWorkspace = \"C:/output/holland_harb\" clean = \"NO_CLEAN\" # Execute S57Arc arcpy.S57Arc_arc ( inS57File , outWorkspace , clean )"}, "usage": ["S-57 is a data standard developed by the International Hydrographic Organization (IHO) to be used for the exchange of digital hydrographic data.", "Each S-57 exchange dataset contains one catalog file and one or more base cells. Import From S57 reads the catalog file, converts it to an INFO file, then converts each base cell file to one or two coverages. One of these coverages will contain all the isolated nodes (for instance, spatial point objects); the other coverage will contain all the spatial and feature objects plus the data descriptive information.", "The Import From S57 importer creates either one or two coverages per base cell file, depending on the types of objects contained within the file.", "Import From S57 creates an INFO file to capture all the objects being converted for each base cell file. The resulting file is called coverage", ".object", " and has the following items: NAME has a content of Record Identifier, LNAM has a content of Feature object long name, CLASS contains feature class, and PRIM contains the Object's spatial primitive (VI, VC, VE, VF, FP, FN, FL, FA, and CF). This INFO file will have a record for each spatial and feature object being converted. The NAME and PRIM columns apply to both spatial and feature objects;however, the LNAM and CLASS columns only apply to feature objects. You can use this table to quickly identify what sort of objects exist in the base cell file.", "This INFO file is an enhancement to the importer and is not required by the exporter (", "Export To S57", ").", "\r\n", "Learn about Clean and how it can affect your output coverage", "\r\n"], "parameters": [{"name": "in_s57_file", "isInputFile": true, "isOptional": false, "description": "The data catalog filename or base cell filename in the S-57 exchange set. If a catalog filename is specified, all base cell files listed in the catalog file will be converted. If a base cell filename is specified, only that base cell file will be converted. ", "dataType": "File"}, {"name": "out_workspace", "isOutputFile": true, "isOptional": false, "description": "The workspace where all output coverages will be written. ", "dataType": "Folder"}, {"name": "clean", "isOptional": true, "description": "Specifies whether to run the Clean command. CLEAN \u2014 Cleans the newly created coverages. This is the default. NO_CLEAN \u2014 Does not clean the newly created coverages. ", "dataType": "Boolean"}]},
{"syntax": "Import_arc (feature_type, interchange_file, out_dataset)", "name": "Import From Interchange File (Coverage)", "description": "Converts an  ArcInfo Workstation  export interchange file. An  ArcInfo Workstation  interchange file can be used to transport coverages; INFO tables; text files, such as AML macros; and other  ArcInfo Workstation  files between various machine types. An interchange file contains all coverage information and appropriate INFO table information in a fixed-length ASCII format. There are many ways to use Import From Interchange. One way is to transport a coverage and its associated INFO tables. Each coverage file and its INFO tables are read from the interchange file into an output coverage. This is done by using the keyword COVER for the first argument. Another way is to transfer an INFO table. In this case, any INFO path name/user name can be used to specify the name of the output INFO table. This option is invoked by using the keyword INFO for the first argument. A third way is to use Import with the TEXT option to transfer key files, AML macros, and other text files. If multiple volumes are provided by  Export To Interchange , the ASCII interchange file name for Import from Interchange must be in the format <interchange_file>.E00 through <interchange_file>.Enn, where nn is the last volume ID. Even if only one volume is produced, this file name must have the .E00 extension.", "example": {"title": "Import example (stand-alone script)", "description": "The following stand-alone script demonstrates how to import a coverage from an ", "code": "# Name: Import_Example.py # Description: Imports from E00 format to a coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables featureType = \"COVER\" interchangeFile = \"citylim.e00\" outDataset = \"C:/output/citylimit\" # Execute Import arcpy.Import_arc ( featureType , interchangeFile , outDataset )"}, "usage": ["The AUTO option determines what is contained in an import file, then imports it correctly.", "Knowledge Base article 21052", "\r\nincludes a list of file naming limitations for coverages.", "In ModelBuilder, you should specify the feature type of the data being imported. The AUTO option does not set the properties of the output variable in the model, because the data type is unknown until the process is executed. For example, if the .e00 file contains a coverage, set the feature type to COVER so the output variable may be connected to any other process that requires a coverage as input.", "IMPORT reads any export file that has been fully or partially compressed as well as decompressed. IMPORT automatically recognizes whether the export file is compressed.", "For the COVER option, all INFO data files saved in the interchange file, whose names contain the coverage name prior to the last period in the INFO data file name, are written to the workspace INFO database for the output coverage.", "Any INFO path name/user name can be specified as the output data file when the INFO option is used. However, the specified INFO directory must exist prior to using IMPORT.", "The STACK option will work in one of two ways. If the interchange file was made by the EXPORT command with the STACK option, then only the stack directory will be re-created. None of the grids associated with the stack will be re-created. If the interchange file was made by the EXPORT command with the STACKALL option, then both the stack directory and the grids associated with that stack are re-created. Only the grids that do not presently exist in the workspace will be re-created.", "ArcCatalog", " doesn't show  .e00 files by default, but it is easy to configure ArcCatalog to show them. From the ", "Customize", " pull-down menu, choose the ", "ArcCatalog Options", " command, click the ", "File Types", " tab, then click the ", "New Type", " button. In the dialog that appears, type ", "e00", " into the ", "File extension", " field (be sure to use zeros, not the letter ", "O", "), type a description such as Export File into the ", "Description of type", " field, click ", "Change Icon", " and choose an icon, then click ", "OK", ". Click ", "OK", " on the ", "ArcCatalog Options", " dialog box. ", "ArcCatalog", " will refresh and show you .e00 files."], "parameters": [{"name": "feature_type", "isOptional": false, "description": "The type of file to be imported. Auto is the default option. AUTO \u2014 COVER \u2014 FONT \u2014 GRID \u2014 INFO \u2014 LINESET \u2014 PLOT \u2014 MAP \u2014 MARKERSET \u2014 SHADESET \u2014 STACK \u2014 TEXT \u2014 TEXTSET \u2014 TIN \u2014 ", "dataType": "String"}, {"name": "interchange_file", "isOptional": false, "description": "The prefix name of the ArcInfo Workstation interchange file to be converted. A volume ID of .e00, .e01, and so on, will always be appended to the given interchange_file to specify the file or files to be imported. ", "dataType": "File"}, {"name": "out_dataset", "isOutputFile": true, "isOptional": false, "description": "The name of the output dataset. ", "dataType": "Data Element"}]},
{"syntax": "DLGArc_arc (in_dlg_file, out_cover, {out_point_cover}, {area_calculation}, {x_shift}, {y_shift}, {category})", "name": "Import From DLG (Coverage)", "description": "Converts a Standard or Optional formatted Digital Line Graph (DLG) file to a coverage.", "example": {"title": "DLGArc example (stand-alone script)", "description": "The following stand-alone script demonstrates how to import a coverage from DLG format.\r\n", "code": "# Name: DLGArc_Example.py # Description: Imports from DLG format to a coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inDlgFile = \"509954.PL.opt.dlg\" outCover = \"C:/output/954netcov\" outPointCover = \"C:/output/954pointcov\" # Execute DLGArc arcpy.DLGArc_arc ( inDlgFile , outCover , outPointCover , \"\" , \"\" , \"\" , \"\" )"}, "usage": ["Topology data contained in the DLG file is ignored. You can use the ", "Build", " tool after running Import From DLG, creating topology on the newly created coverage. Sometimes the coverage will have arc intersections and will need to be cleaned using the ", "Clean", " tool.", "The feature attribute table NAT is automatically created and does not require an additional build for nodes.", "The ", "Output Coverage", " may require editing before polygons or lines can be built and feature attribute tables created. For example, checks should be made on the output coverage to ensure that label points occur within their polygons, arcs match at nodes, polygons close, arcs do not cross, and so on.", "Feature numbers in the DLG area (A) records become label point User-IDs for polygons; feature numbers in line (L) records become User-IDs for arcs in the output coverage. The same User-IDs are written to the INFO table output coverage.ACODE and output coverage.PCODE.", "The CODE files, output coverage.ACODE and output coverage.PCODE, hold line attribute codes and polygon attribute codes.", "The DLG feature numbers for node (N) records become User-IDs for nodes in output coverage. There is no corresponding CODE file for node attributes. The User-IDs and attribute codes for node (N) records are stored directly in the output coverage.NAT feature attribute table.", "For the ", "Output Point Coverage", ", the DLG feature numbers in degenerate (zero-length) line (L) records become User-IDs for points in the Output Point Coverage. An Output Point Coverage.XCODE is created to hold degenerate line (L) attribute codes.", "CODE files for a particular feature class are only created if feature attributes are found in the DLG.", "The fields in the CODE files consist of a Cover-ID, major and Minor fields. The Cover-ID is each feature's unique identifier. MAJOR1 is the first major feature code in the DLG; MINOR1 is the first minor code. There are as many major/minor pairs in the table as the maximum number of pairs for that feature class in the DLG. If no value appears for a particular code, it will be set to -99999.", "Import From DLG writes the projection parameters stored in the DLG file to a projection definition file (PRJ) stored in the output coverage subdirectory.", "DLG header records may contain parameters of a transformation that are used to convert the internal file coordinates to the ground coordinate system. Import From DLG will automatically apply any transformation parameters found in the header records to the feature coordinates and control points (in addition to any specified shift).", "The header records in the DLG file provide information used to determine the tic locations of the output coverages.", "If the fields used to store control point information are blank, Import From DLG will generate tics for the output coverages.", "DLG files usually provide four control points representing the four corners of the map area in the same map coordinate system used to store geographic features. Any transformation or x,y shift applied to the feature coordinates will apply to the control points as well.", "A DLG category is equivalent to a layer.", "Usually, only one category is included in a DLG file; however, some DLGs contain more than one category.", "Categories are listed in DLG header records. Use of the tool's DLG Category parameter enables you to create a set of coverages for other layers or categories contained in the DLG.", "The coordinate precision of the output coverage is determined by the current creation rule. If the precision setting has not yet been established during the current session, then the creation rule will be Single. This means the Import From DLG tool will create an output coverage in single precision. (Coordinate values in DLG files have a format definition of F12.2.)", "Join Info Tables", " can be used to merge each coverage feature attribute table with the DLG code files\u2014INFO table PCODE to Output Coverage.PAT, ACODE with Output Coverage.AAT, and XCODE with Output Point Coverage.PAT."], "parameters": [{"name": "in_dlg_file", "isInputFile": true, "isOptional": false, "description": "The DLG file to be converted to a coverage. ", "dataType": "File"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The name of the coverage to be created from the DLG data. If the input DLG file contains data produced by the United States Geological Survey (USGS), the output coverage will usually contain line, polygon, and node features created from nondegenerate line (L) records, area (A) records, and node (N) records. An Output Coverage.NAT for node features will be created. This is the only feature attribute table that is created by Import From DLG. An INFO table named Output Coverage.ACODE will be created that contains the major/minor pair values for all line (L) records. Similarly, an INFO table named Output Coverage.PCODE will store the major/minor codes for area (A) records. ", "dataType": "Coverage"}, {"name": "out_point_cover", "isOutputFile": true, "isOptional": true, "description": "The name of an Optional point coverage to be created from degenerate line (L) records in the DLG. In a DLG, points are stored as zero-length arcs (that is, degenerate lines). These are optionally converted to points by specifying an output point coverage. An INFO table named Output Point Coverage.XCODE will be created, which contains all major and minor code pair values for degenerate line (L) records. No coverage will be created from degenerate line records unless an Output Point Coverage is specified. ", "dataType": "Coverage"}, {"name": "area_calculation", "isOptional": true, "description": "Determines how the area (A) records are written to the output coverage. NOFIRST \u2014 The first area record is ignored. Labels are written for all other area records and the corresponding major/minor codes are written to the Output Coverage.PCODE file. This is the default option. ALL \u2014 All area records are converted to output coverage label points and the major and minor codes for all area (A) records in the Output Coverage.PCODE file. Usually the label point for the first area record is in the outside polygon. ATTRIBUTED \u2014 The label location in the DLG file is used only for those area records that carry major/minor codes. For some DLG categories, only area records with attributes have valid label locations. The first area (A) record is not included. ", "dataType": "String"}, {"name": "x_shift", "isOptional": true, "description": "A constant value to be added to all x coordinates during DLG conversion. The default X Shift value is zero. ", "dataType": "Double"}, {"name": "y_shift", "isOptional": true, "description": "A constant value to be added to all y coordinates during DLG conversion. The default Y Shift value is zero. ", "dataType": "Double"}, {"name": "category", "isOptional": true, "description": "The optional name of a specific DLG category to be converted from the DLG file. Only the specified category will be converted if one is given. Otherwise, only the first category in the DLG will be written to the output coverages: output coverage, point coverage. ", "dataType": "String"}]},
{"syntax": "Generate_arc (in_file, out_cover, feature_type)", "name": "Generate (Coverage)", "description": "Generates a coverage from coordinates stored in a file. \r\n Learn more about how Generate works \r\n", "example": {"title": "Generate example (stand-alone script)", "description": "The following stand-alone script demonstrates how to generate a coverage from a file of coordinates.\r\n\r\n", "code": "# Name: Generate_Example.py # Description: Generates a coverage from a file of coordinates. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inFile = \"wells.gen\" outCover = \"C:/output/wells\" featureType = \"POINTS\" # Execute Generate arcpy.Generate_arc ( inFile , outCover , featureType )"}, "usage": ["This tool creates new coordinate features but does not create topology or attributes for these features. Other tools, such as ", "Build", " or ", "Clean", ", can be used to create feature topology.", "The coordinate precision of the output coverage is determined by the precision setting. To convert a double-precision file to a double-precision coverage, the precision must be set to Double.", "While processing duplicate tics with the same ID number, the last tic read with the same ID number is the one whose coordinates are stored in the coverage tic file.", "When polygons are generated, each polygon will be closed automatically from the last vertex entered to the first vertex if the two vertices are not the same.", "Coordinates in the input file can be in x,y,z format, but Generate ignores the z-values if they exist.", "The geoprocessing tool takes in a text file that has a User-ID number for each line, followed by the series of xy coordinate pairs that define the line.", "This tool is not interactive, as the GENERATE command in ", "ArcInfo Workstation", " was.", "Use the ", "Ungenerate", " tool to create the file to be used as the input file."], "parameters": [{"name": "in_file", "isInputFile": true, "isOptional": false, "description": "The file containing feature coordinates that will be used to generate a coverage. ", "dataType": "File"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be generated. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": false, "description": "The type of features to create: ANNOTATIONS \u2014 Adds annotations to the coverage. CIRCLES \u2014 Generates circles, each with a specified center and radius. CURVES \u2014 Generates curves using the specified grain value as the distance between vertices on each curve. FISHNET \u2014 Creates a fishnet of rectangular cells. LINES \u2014 Adds arcs to the coverage. LINKS \u2014 Adds links to the coverage. POINTS \u2014 Adds label points to the coverage. POLYGONS \u2014 Adds polygons and label points to the coverage. TICS \u2014 Adds tics to the coverage. ", "dataType": "String"}]},
{"syntax": "TigerArc_arc (in_tiger_file_prefix, out_cover, {out_point_cover}, {out_landmark_cover}, {tiger_version})", "name": "Basic Tiger Conversion (Coverage)", "description": "Converts a set of U.S. Bureau of Census TIGER/Line files into one or more coverages. \r\n Learn more about how Basic Tiger Conversion works. \r\n", "example": {"title": "TigerArc example (stand-alone script)", "description": "The following stand-alone script demonstrates how to import a set of TIGER/Line files to coverage format. ", "code": "# Name: TigerArc_Example.py # Description: Imports a set of TIGER/Line files into three coverages # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inTigerFilePrefix = \"TGR12043.RT\" outCover = \"C:/output/tgr12043line\" outPointCover = \"C:/output/tgr12043pnt\" outLandmarkCover = \"C:/output/tgr12043land\" # Execute TigerArc arcpy.TigerArc_arc ( inTigerFilePrefix , outCover , outPointCover , outLandmarkCover )"}, "usage": ["Creates up to three output coverages. Both the out_point_cover and out_landmark_cover coverages are created only when the dataset contains area point and landmark features:", "At least one output coverage must be specified.", "Converts all versions released after April 1989. The minimum input required by this  tool is Record Types 1 and 2.", "This  tool does not support Record Types F and G, released with the 1992 School District version. These are temporary record types, not found in earlier or subsequent versions.", "Only those files in the workspace directory specified by the input TIGER file prefix will be used in the conversion. If the file prefix does not include a path, files in the current workspace directory will be used. Files that are not required for your particular application can be renamed or deleted. Files on CD-ROM will have to be copied to disk before they can be renamed or deleted.", "The output coverages for this  tool will always be in double precision. TIGER/Line files often contain tiny line segments that would be lost if converted to single precision."], "parameters": [{"name": "in_tiger_file_prefix", "isInputFile": true, "isOptional": false, "description": "The filename prefix, common to all files in the set of TIGER/Line files being converted. The prefix may include a directory path. ", "dataType": "String"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The name of the output coverage to be created containing the basic line features and attribute data from the set of TIGER/Line files. ", "dataType": "Coverage"}, {"name": "out_point_cover", "isOutputFile": true, "isOptional": true, "description": "The name of the output coverage that contains point features that represent polygon label points for polygons in out_cover. ", "dataType": "Coverage"}, {"name": "out_landmark_cover", "isOutputFile": true, "isOptional": true, "description": "The name of the output point coverage containing landmark features. ", "dataType": "Coverage"}, {"name": "tiger_version", "isOptional": true, "description": "The input TIGER/Line files version. 1995 \u2014 1997 \u2014 1998 \u2014 1999 \u2014 2000 \u2014 2002 \u2014 2003 \u2014 20041 \u2014 20042 \u2014 20051 \u2014 20052 \u2014 ", "dataType": "String"}]},
{"syntax": "TigerTool_arc (in_tiger_file_prefix, out_cover_prefix, {join_attributes}, {projection}, {zone_number}, {tiger_version}, {restart})", "name": "Advanced Tiger Conversion (Coverage)", "description": "Converts a set of U.S. Census Bureau TIGER/Line files to a set of coverages.", "example": {"title": "TigerTool example (stand-alone script)", "description": "The following stand-alone script demonstrates how to import a set of TIGER/Line files to a set of coverages. ", "code": "# Name: TigerTool_Example.py # Description: Imports coverages from TIGER/Line files # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inTigerFilePrefix = \"tgr23005.bw\" outCoverPrefix = \"C:/output/bla23005\" joinAttributes = \"NO_JOIN\" tigerVersion = 1995 # Execute TigerTool arcpy.TigerTool_arc ( inTigerFilePrefix , outCoverPrefix , joinAttributes , \"\" , \"\" , tigerVersion , \"\" )"}, "usage": ["Advanced Tiger Conversion performs the following conversion and processing tasks:", "Once the Advanced Tiger Conversion converts your data, it begins to process the output coverages. It merges the polygon points with line features by combining features from the output line and area coverage with the output area point coverage, then builds line and polygon topology for the merged coverage.", "TIGER data is normally topologically correct but can contain intersection errors. An intersection error occurs when two line features overlap without intersecting at a node, usually because the shape record that defines the shape of the line feature is missing. The ensuing two-point arc may cross over features that the shaped arc would not.", "Polygon topology must be present to fully process TIGER data. If an intersection error is detected when topology is being built, polygon topology will not be created, and the tool will issue a message and stop processing.", "The Join attributes option will join the basic line, area, and landmark point features to their feature attribute tables. This makes display and query easier and is recommended for simple applications.", "TIGER data contains very large attribute tables that can sometimes slow down the application's processes.", "You can specify either UTM or State Plane coordinate systems for your output coverages. If this option is not used, the projection for each coverage will be defined as Geographic. The tool will define the datum using the version number read from the TIGER/Line file. Datasets earlier than 1995 are in NAD27, and those released in 1995 or later are in NAD83 with the exception of Hawaii, which is only in NAD27.", "Converts all versions released after April 1989.", "Starting in 2007, the U.S. Census Bureau switched to a shapefile based format. Use the ", "Feature_Class_To_Coverage", " tool on these.", "This tool does not support Record Types F and G released with the 1992 School District version. These are temporary record types, not found in earlier or subsequent versions.", "The output coverages created in the TIGER file conversion will always be in double precision. TIGER/Line files often contain tiny line segments that would be lost if converted to single precision.", "Advanced Tiger Conversion creates up to three output coverages. An ouput point coverage and an output landmark coverage are only created when the dataset contains area point and landmark features."], "parameters": [{"name": "in_tiger_file_prefix", "isInputFile": true, "isOptional": false, "description": "The filename prefix, common to all files in the set of TIGER/Line files being converted. The prefix may include a directory pathname. ", "dataType": "String"}, {"name": "out_cover_prefix", "isOutputFile": true, "isOptional": false, "description": "The prefix of the output coverages to be created from the TIGER/Line files. ", "dataType": "String"}, {"name": "join_attributes", "isOptional": true, "description": "Determines if the basic line, area, and landmark point features will be joined to their feature attribute tables. JOIN \u2014 The output features will be joined to their feature attribute tables. NO_JOIN \u2014 The output features will not be joined to their feature attribute tables. ", "dataType": "Boolean"}, {"name": "projection", "isOptional": true, "description": "The spatial reference of the output coverages. UTM \u2014 TIGER files will be projected to the Universal Transverse Mercator (UTM) coordinate system. STATE \u2014 TIGER files will be projected to the State Plane coordinate system. ", "dataType": "String"}, {"name": "zone_number", "isOptional": true, "description": "The zone number of the specified coordinate system. ", "dataType": "Long"}, {"name": "tiger_version", "isOptional": true, "description": "The input TIGER/Line files version. 1995 \u2014 1997 \u2014 1998 \u2014 1999 \u2014 2000 \u2014 2002 \u2014 2003 \u2014 20041 \u2014 20042 \u2014 20051 \u2014 20052 \u2014 ", "dataType": "String"}, {"name": "restart", "isOptional": true, "description": "Determines whether processing will continue if the TIGER data contains intersection errors. RESTART \u2014 Processing will continue if the TIGER data contains intersection errors. NO_RESTART \u2014 Processing will stop if the TIGER data contains intersection errors. ", "dataType": "Boolean"}]},
{"syntax": "Ungenerate_arc (in_cover, out_generate_file, feature_type, {duplicate_nodes}, {format})", "name": "Ungenerate (Coverage)", "description": "Creates a text file of x,y coordinates from the input coverage.", "example": {"title": "Ungenerate example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a text file from a coverage.\r\n\r\n", "code": "# Name: Ungenerate_Example.py # Description: Creates a text file from a coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"tong_azone\" outGenerateFile = \"C:/output/tong_azone.gen\" featureType = \"POLY\" format = \"FIXED\" # Execute Ungenerate arcpy.Ungenerate_arc ( inCover , outGenerateFile , featureType , \"\" , format )"}, "usage": ["Ungenerate provides a useful mechanism to create simple coordinate files from coverages. This allows you to easily transfer coverages to other mapping systems or view and update individual coordinates using your computer's text editor.", "The output text file is in a format that is readable by the ", "Generate", " tool. ", "User-IDs are written to the Output Generate File for lines, points, polygons, regions, and tics.", "All internal calculations are done in double precision regardless of the precision of the input or output dataset. Single-precision output is accurate to approximately seven significant digits. Ignore values beyond seven significant digits for ungenerated files in single precision. For accuracy to approximately 15 digits, use double precision.", "The coordinates created by Ungenerate are in the same coordinate precision as the Input Coverage. Single-precision coordinates are generated for single-precision coverages, and double-precision coordinates for double-precision coverages.", "For files created with the Region.<subclass>, further processing with the ", "Line Coverage To Region", " tool and the ", "Build", " or ", "Clean", " tool is necessary to fully restore regions.", "When using the Poly option, polygons that are not properly closed will not be written to the Output file. If there are two label points in one polygon, only one will be written. If no label point exists, then zero is written for the ID number of that polygon feature, and the x,y coordinate for the centroid will be written.", "Ungenerate writes a flag for island polygons. The island polygon will have an ID number of -99999 and a set of vertices defining the polygon. This is useful for converting polygon coverages to other systems that require special handling of island polygons.", "The Poly option treats all line segments that are wholly contained within a polygon as island polygons and tags them with an ID number of -99999. Line segments that fall inside the polygon but touch the border of the polygon are also considered islands inside the polygon. However, line segments that fall outside a polygon are considered part of the universe polygon and are ignored.", "All vertices that define each component arc of a polygon feature will be written. To weed out vertices of very dense coverages, use the ", "Simplify Line", " tool prior to using the Generate tool. The arcs for each polygon are written starting at the lower left of the polygon and continuing in a clockwise direction. Arcs that are shared by more than one polygon will be duplicated in the output file. Nodes and arc orientation are not preserved.", "When using the ANNO option, the annotations within a specified class will be written to a file in the following format. All the annotation shape points up to 500 will be written.", "It is important to note that the above format for Ungenerate annotation is not compatible with the format expected by the Generate command. Its main use is to provide a simple way to translate annotation to a different format."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage from which feature coordinates will be written. ", "dataType": "Coverage"}, {"name": "out_generate_file", "isOutputFile": true, "isOptional": false, "description": "The text file to which the x,y coordinates will be written. ", "dataType": "File"}, {"name": "feature_type", "isOptional": false, "description": "The type of features that will be used in the generation of the output file. LINE \u2014 Coordinates for arcs will be written. POINT \u2014 Coordinates for label points will be written. POLY \u2014 Coordinates for arcs and labels that make up polygon features will be written. TIC \u2014 Coordinates for tics will be written. LINK \u2014 Coordinates that define links will be written. REGION.subclass \u2014 Coordinates for the regions in the specified subclass will be written. ANNO.subclass \u2014 Coordinates for the annotation in the specified subclass will be written. ", "dataType": "String"}, {"name": "duplicate_nodes", "isOptional": true, "description": "Determines whether duplicate node coordinates will be retained or dropped in the Output Generate File. This applies only to the POLY option. NODES \u2014 Specifies that duplicate node coordinates will be written to the Output Generate File. This applies only to the POLY option. This is the default. NONODES \u2014 Specifies that duplicate node coordinates will be dropped from the Output Generate File. This applies only to the POLY option. ", "dataType": "Boolean"}, {"name": "format", "isOptional": true, "description": "Selects either exponential or fixed representation of floating point numbers in the Output Generate File. EXPONENTIAL \u2014 The coordinates will be written to the Output Generate File in exponential notation. This is the default. This option retains all significant digits and is recommended to preserve precision. FIXED \u2014 The coordinates will be written using approximately seven significant digits for single-precision coverages and approximately 15 significant digits for double-precision coverages. ", "dataType": "String"}]},
{"syntax": "VPFExport_arc (in_cover, out_file, {tile_name}, {control_file}, {standard_table}, {index_table})", "name": "Export To VPF (Coverage)", "description": "Converts a coverage into either a Vector Product Format (VPF) Coverage or VPF Tile. \r\n Learn more about the Vector Product Format \r\n", "example": {"title": "VPFExport example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a VPF coverage using coverage.", "code": "# Name: VPFExport_Example.py # Description: Creates a VPF coverage from an ArcInfo coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"caligrat\" outFile = \"C:/output/caligrat\" standardTable = \"NO_EXTRA\" indexTable = \"NO_FIT\" # Execute VPFExport arcpy.VPFExport_arc ( inCover , outFile , \"\" , \"\" , standardTable , indexTable )"}, "usage": ["The coverage must not have a mask file. Use the ", "Clean", " tool to remove mask files.", "The coverage must have a projection defined or it will not be converted. The coverage must not have a mask file. Only Clean coverages will export.", "A full VPF pathname must be specified with Output VPF Coverage or Table.", "Pathnames are specified as vpfDatabase\\vpfLibrary\\vpfCoverage when the ", "Input Coverage or Table", " type is set to Coverage.", "Pathnames are specified as vpfDatabase\\vpfLibrary\\vpfTable when the ", "Input Coverage or Table", " type is set to Table.", "If the vpfDatabase or the vpfLibrary directories do not exist; they will be created.", "The Output VPF tile will be appended to vvpfDatabase\\vpfLibrary\\vpfCoverage\\ specified by the Output VPF Coverage or Table and is only valid when the Input Coverage or Table type is set to Coverage.", "Projections must match between the library and the coverage being created.", "The VPF standard specifies only coverages in geographic coordinates. Using units of Decimal Degrees, on the WGS 1984 datum, you cannot clean a coverage that has units in Decimal Degrees. You should build the coverage in this case, or understand how cleaning will affect your coverage.", " In the default translation of a coverage to a VPF coverage, arcs become edges, polygons become faces, and nodes remain nodes. Feature attribute tables become feature tables with a 1:1 relationship with primitive tables. One exception is the annotation TAT, which becomes one primitive table plus as many feature tables as there are annotation subclasses in the coverage. Route and Section tables become unconnected \"extra\" tables. ", " Other coverage tables are translated to VPF as extra tables, meaning that the tables are not needed to make up VPF coverages but should be maintained for the VPF to Coverage tool to translate all of the coverage information back. If there are no VPF tiles, the extra tables are located at the coverage level; otherwise, they are located at the tile level. Extra tables that may be generated by the tool are TIC, SEC, RAT, LNK, TRN, ADD, and ADDRESS.LST. The naming convention is X_TIC, X_SEC, X_RAT, and so on.", "\r\n", "Learn about export conversion control files", "\r\n"], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The input coverage that will be converted to VPF format. ", "dataType": "Coverage; INFO Table"}, {"name": "out_file", "isOutputFile": true, "isOptional": false, "description": "The name of the VPF coverage or table to be created. The full pathname must be specified. ", "dataType": "Data Element"}, {"name": "tile_name", "isOptional": true, "description": "The name of the VPF tile to be created. ", "dataType": "String"}, {"name": "control_file", "isOptional": true, "description": "A file that can be used to drop, add, change, or ignore items and other information during translation. The name of this file is defined by the user. Polycov.ccf, poly_cov_con, and conversionfile are all acceptable names. An input coverage defines feature translations for specified feature classes as well as specifies feature classes to be ignored. It can also be used to determine which values are to be filled in the database and library header files at creation. ", "dataType": "File"}, {"name": "standard_table", "isOptional": true, "description": "Specifies whether nonstandard ArcInfo Workstation tables will be converted. EXTRA \u2014 Translates all ArcInfo Workstation files to VPF. This option only needs to be used if the data being translated to VPF will be converted back using the Import From VPF tool. This is the default option. NO_EXTRA \u2014 Prevents VPFEXPORT from creating extra tables when creating a VPF coverage. This option should only be used if the exported coverage will not be imported back using the Import From VPF tool. VPFEXPORT considers files such as TIC and LAB to be extra files. These files are not necessary to create a VPF coverage. ", "dataType": "Boolean"}, {"name": "index_table", "isOptional": true, "description": "Specifies whether to create a feature index table (FIT). NO_FIT \u2014 Do not create a feature index table. FIT \u2014 Create a feature index table. ", "dataType": "Boolean"}]},
{"syntax": "SDTSExport_arc (SDTS_type, in_dataset, out_transfer_prefix, {in_point_cover}, {out_DD_transfer}, {Conv_Ctrl_File})", "name": "Export To SDTS (Coverage)", "description": "Creates a Spatial Data Transfer Standard (SDTS). Topological Vector Profile (TVP). or Point Profile Transfer from an ArcGIS coverage or grid.", "example": {"title": "SDTSExport example (stand-alone script)", "description": "The following stand-alone script demonstrates how to export a polygon coverage to SDTS format.", "code": "# Name: SDTSExport_Example.py # Description: Exports a polygon coverage to SDTS format # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables SDTSType = \"TVP\" inDataset = \"stand\" outTransferPrefix = \"C:/output/jrdl\" convCtrlFile = \"pvalues.ccf\" # Execute SDTSExport arcpy.SDTSExport_arc ( SDTSType , inDataset , outTransferPrefix , \"\" , \"\" , convCtrlFile )"}, "usage": ["SDTS is a large standard composed of smaller, more limited subsets that are Federally approved as part of the SDTS FIPS 173 standard. These subsets are called profiles. The Topological Vector Profile (designed specifically for planar vector data with topology), Raster, and Point profiles are the only profiles supported by SDTSEXPORT.", "The following conditions must be met when creating a TVP transfer:", "Annotation will be ignored when creating a TVP transfer.", "SDTS Export control file format"], "parameters": [{"name": "SDTS_type", "isOptional": false, "description": "The type of SDTS profile that will be created: TVP \u2014 Topological Vector Profile, designed specifically for planar vector data with coverage topology. POINT \u2014 Point profile, designed for high-precision point datasets. RASTER \u2014 Raster profile. Grids and lattices are supported. ", "dataType": "String"}, {"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The input coverage or grid. ", "dataType": "Coverage;Raster Dataset"}, {"name": "out_transfer_prefix", "isOutputFile": true, "isOptional": false, "description": "A four-character prefix used to name each file in the transfer. The prefix may include a pathname to a directory. By default, the files in the transfer will be written to the current workspace. ", "dataType": "String"}, {"name": "in_point_cover", "isInputFile": true, "isOptional": true, "description": "The name of the Point Coverage to be converted when the transfer type is TVP. This option will be ignored if the transfer type is set to POINT. ", "dataType": "Coverage"}, {"name": "out_DD_transfer", "isOutputFile": true, "isOptional": true, "description": "A four-character prefix for the Master Data Dictionary. A directory named MASTERDD will be created at the same directory level as the Out Transfer directory. This option is used for creating a single master data dictionary for coverages or grids that share a common data dictionary. ", "dataType": "String"}, {"name": "Conv_Ctrl_File", "isOptional": true, "description": "A file that can be used to add information during translation. The name of this file is defined by the user. ", "dataType": "File"}]},
{"syntax": "ArcS57_arc (in_workspace, log_file, {out_workspace})", "name": "Export To S57 (Coverage)", "description": "Converts coverages to S-57 object files.", "example": {"title": "ArcS57 example (stand-alone script)", "description": "The following stand-alone script demonstrates how to export a coverage to S-57 format.", "code": "# Name: ArcS57_Example.py # Description: Export a coverage to S-57 format # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env import os # Set environment settings env.workspace = \"C:/data\" # Set local variables input = \"tongass1\" logFile = \"ArcS57_log.txt\" outWorkspace = \"C:/output/tongass_s57\" # Execute ArcS57 arcpy.ArcS57_arc ( input , logFile , outWorkspace )"}, "usage": ["S-57 is a data standard developed by the International Hydrographic Organization (IHO) to be used for the exchange of digital hydrographic data.", "The ", "Output Log File", " will be created in the process and then holds the report of the export.", "The ", "Input Workspace", " in which the S-57 files are located must contain coverages or the directory cannot be selected."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The workspace containing the coverage to be converted (exported). ", "dataType": "Folder"}, {"name": "log_file", "isOptional": false, "description": "Contains the report of the export process. ", "dataType": "File"}, {"name": "out_workspace", "isOutputFile": true, "isOptional": true, "description": "The folder that will contain the S-57 object files. ", "dataType": "Folder"}]},
{"syntax": "Export_arc (feature_type, in_dataset, interchange_file, {compression_type}, {max_lines})", "name": "Export To Interchange File (Coverage)", "description": "Converts a coverage to an interchange file for transfer to another platform. \r\n Learn more about how Export to Interchange File works \r\n", "example": {"title": "Export example (stand-alone script)", "description": "The following stand-alone script demonstrates how to export a coverage to an interchange file.", "code": "# Name: Export_Example.py # Description: Exports a coverage to ArcInfo interchange format # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables featureType = \"COVER\" inDataset = \"stand\" interchangeFile = \"C:/output/stand.e00\" compressionType = \"NONE\" #Execute Export arcpy.Export_arc ( featureType , inDataset , interchangeFile , compressionType , \"\" )"}, "usage": ["When exporting a coverage, all associated INFO tables are written to the interchange file. For example, if the coverage name specified for input data is Forest, an INFO table named Forest.LABEL would be saved in the interchange file. A table named Forest1.LABEL, however, would not be saved in the interchange file.", "Export files created with the ", "Compression type", " parameter set to FULL can be significantly smaller than export files created with the PARTIAL or NONE options.", "Exporting a grid can result in an export file much larger than the original grid, even when FULL compression is used. This is because each grid cell must be represented in the export file in ASCII format, which is less efficient than the grid's binary format. Also, integer grids are stored in a compressed format, which cannot be maintained in the export file.", "The best solution is not to export large grids. To create a single file for transfer, a utility, such as PKZIP (or \"tar\" on UNIX systems), can be used to place the workspace containing the grid into a single file. Versions of PKZIP and tar are available on both UNIX and PC systems. Warning: Do not PKZIP or tar just the grid, because you'll lose the INFO tables. If you don't want to transfer all the grids in your workspace, create a temporary workspace and copy the grids to be transferred into it.", "Coverages to be exported must not contain edit masks. In other words, do not export an uncleaned cover. Run the ", "Build", " or ", "Clean", " tool to remove edit masks (msk.adf file in the coverage directory). ", "Export has a limit of -999999999 on negative User-IDs.", "Export files for coverages with nondefault text alignment will have data loss if imported to a version prior to 8.0.1.", "When exporting text files, the FULL and PARTIAL compression options support line lengths up to 300 characters. Longer line lengths will get truncated. The NONE option supports up to 80 characters per line."], "parameters": [{"name": "feature_type", "isOptional": false, "description": "The data type to be exported. COVER \u2014 a coverage, associated INFO files, and any index files. FONT \u2014 an IGL font file. GRID \u2014 an integer or floating-point grid. INFO \u2014 an INFO file. LINESET \u2014 a lineset file. MAP \u2014 a map composition created with the ARCPLOT map composer. MARKERSET \u2014 a markerset file. PLOT \u2014 a plotfile or graphics file. SHADESET \u2014 a shadeset file. STACK \u2014 a stack. STACKALL \u2014 a stack and all of the grids associated with that stack. TEXT \u2014 any ASCII text file. TEXTSET \u2014 a textset file. TIN \u2014 a tin.", "dataType": "String"}, {"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The dataset or file to export. ", "dataType": "Data Element"}, {"name": "interchange_file", "isOptional": false, "description": "The prefix name of the interchange file or files to be created by Export. A volume ID of E00 will be appended to the file name of the first interchange file, E01 to the second file, and so on. Each subsequent file is created when the {max_lines} for each file is reached. ", "dataType": "File"}, {"name": "compression_type", "isOptional": true, "description": "Specifies how numbers and blanks will be compressed in the export file. There are three options: NONE \u2014 No compression is performed. This option can also produce an export file that can be listed on your terminal or line printer. This is the default and the preferred method for creating export files. PARTIAL \u2014 Compresses blanks but does not compress numbers. FULL \u2014 Compresses both blanks and numbers using ASCII compression characters. This option requires the least amount of storage space (on tape or disk). ", "dataType": "String"}, {"name": "max_lines", "isOptional": true, "description": "Maximum number of lines for each volume (for example, disk file) of an Export To Interchange File file. A volume has the extension .E00 through .E99. Only one export file is created if this is not specified. ", "dataType": "Long"}]},
{"syntax": "ArcDLG_arc (in_cover, out_dlg_file, {in_point_cover}, {in_projection_file}, {x_shift}, {y_shift}, {in_header_file}, {transform})", "name": "Export To DLG (Coverage)", "description": "Creates a Digital Line Graph from a coverage. The DLG is output in DLG-3 Optional (as opposed to Standard) format. \r\n Learn more about how Export To DLG works. \r\n", "example": {"title": "ArcDLG example (stand-alone script)", "description": "The following stand-alone script demonstrates how to export a polygon and a point coverage to DLG format. ", "code": "# Name: ArcDLG_Example.py # Description: Exports two coverages to DLG format # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"stand\" outDlgFile = \"C:/output/stand.dlg\" inPointCover = \"tong_basin4\" inProjectionFile = \"stand.prj\" xShift = - 500000 yShift = - 6000000 inHeaderFile = \"standheader.txt\" # Execute ArcDLG arcpy.ArcDLG_arc ( inCover , outDlgFile , inPointCover , inProjectionFile , xShift , yShift , inHeaderFile , \"\" )"}, "usage": ["Before creating a DLG file using Export To DLG, each node should be sequentially numbered using the ", "Renumber Nodes tool", ". This will ensure that all arc, node, and polygon feature internal numbers are sequential.", "There are two distribution formats for a DLG file: Standard and Optional. This tool writes a DLG in the Optional format only.", "The ", "DLG To Coverage tool", " reads and converts Standard or Optional DLG files into a coverage.", "To create a DLG file that contains only point features, you should specify an input coverage that contains one rectangular polygon defining the area covered by the points. Use the name of the point coverage for the Point Coverage option.", "Only one DLG category or layer can be saved in a DLG file created by Export To DLG. A DLG category is equivalent to a layer in a map.", "The internal feature numbers (cover#) are written as the output DLG feature numbers.", "Coverage topology is saved in the DLG file using conventions that are similar to the way topology is stored in a coverage (for example, polygons are defined in clockwise loops, islands as counterclockwise loops; each feature has a unique identification number;and negative numbers for lines indicate reverse directions)."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage to be converted to DLG format. It may contain polygon, line, and node features. ", "dataType": "Coverage"}, {"name": "out_dlg_file", "isOutputFile": true, "isOptional": false, "description": "The output DLG-3 Optional format file to be created. ", "dataType": "File"}, {"name": "in_point_cover", "isInputFile": true, "isOptional": true, "description": "A coverage containing point features to be written as zero length, degenerate Line (L) records in the output DLG. ", "dataType": "Coverage"}, {"name": "in_projection_file", "isInputFile": true, "isOptional": true, "description": "A text file containing input projection parameters to be saved in the DLG header. ", "dataType": "File"}, {"name": "x_shift", "isOptional": true, "description": "A constant value to be added to all coverage x-coordinates during the conversion to DLG. X Shift overrides any x-shift parameters found in either projection file or input cover projection definition file. If a value for X Shift is not specified, the default is zero. ", "dataType": "Double"}, {"name": "y_shift", "isOptional": true, "description": "A constant value to be added to all y-coordinates during conversion to DLG. Y Shift overrides any y-shift parameters found in either the projection file or input cover projection definition file. If a value for Y Shift is not specified, the default is zero. ", "dataType": "Double"}, {"name": "in_header_file", "isInputFile": true, "isOptional": true, "description": "The file containing information to be written into the header of the DLG file. ", "dataType": "File"}, {"name": "transform", "isOptional": true, "description": "This operation controls whether a coordinate transformation is performed. Usually, coordinates are transformed to preserve accuracy when written to the DLG. TRANSFORM \u2014 Transforms coordinates in the DLG file NOTRANSFORM \u2014 Does not transform coordinates in the DLG file", "dataType": "Boolean"}]},
{"syntax": "Thiessen_arc (in_cover, out_cover, {proximal_tolerance})", "name": "Thiessen (Coverage)", "description": "Converts input coverage points to an output coverage of Thiessen proximal polygons. \r\n Learn more about how Thiessen works \r\n", "example": {"title": "Thiessen example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a thiessen proximal polygon around each well in a point coverage.", "code": "# Name: Thiessen_Example.py # Description: Creates a thiessen area around each well in a point coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"wells\" outCover = \"C:/output/wellareas\" # Execute Thiessen arcpy.Thiessen_arc ( inCover , outCover , \"\" )"}, "usage": ["Thiessen polygons have the unique property that each polygon contains only one input point, and any location within a polygon is closer to its associated point than to the point of any other polygon.", "To ignore close points, specify a proximal tolerance. For point coverages with an automation scale between 1:10,000 and 1:100,000, try a tolerance between 1.668 ft (0.508 m) and 16.620 ft (5.080 m).", "The Output Coverage inherits these data model contents from the Input Coverage: items from the point attribute table, tics, and the projection file.", "Thiessen polygons can be used to apportion a point coverage into regions known as Thiessen or Voronoi polygons. Each region contains only one Input Coverage point. Each region has the unique property that any location within a region is closer to the region's point than to the point of any other region.", "All items in the Input Coverage point attribute table (PAT) are copied to their associated polygons in the Output Coverage PAT.", "The Output Coverage polygon label points are located at the same positions as the Input Coverage points.", "If the Input Coverage has a PRJ file, it is copied to the Output Coverage."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage that must have a point feature attribute table created by BUILD with the POINT option. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The polygon coverage in which the Thiessen proximal polygons will be produced. ", "dataType": "Coverage"}, {"name": "proximal_tolerance", "isOptional": true, "description": "Tolerance used to eliminate Input Coverage points that fall within the specified distance of other Input Coverage points. The default Proximal Tolerance is the machine precision of the computer. ", "dataType": "Double"}]},
{"syntax": "PointNode_arc (point_cover, node_cover, {search_radius})", "name": "Point Node (Coverage)", "description": "Performs a spatial JOINITEM on the point coverage and the node coverage. It transfers the attributes from a point feature class to a node feature class. Each point feature in the point coverage is matched to the corresponding node feature in the node coverage. If any point is within the search radius of a node, the attributes are copied.", "example": {"title": "PointNode example (stand-alone script)", "description": "The following stand-alone script demonstrates how to use the Point Node tool.\r\n\r\n", "code": "# Name: PointNode_Example.py # Description: Performs PointNode on two coverages. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables pointCover = \"wells142\" nodeCover = \"county\" searchRadius = 3600 # Execute PointNode arcpy.PointNode_arc ( pointCover , nodeCover , searchRadius )"}, "usage": ["If any point is within the ", "search radius", " of a node, the attributes of that point are copied. If more than one point matches a node, then the nearest point is selected. If two or more points are the same distance from the node, then one is selected randomly. If two or more nodes are within the search radius of a single node, then the node nearest the point is matched.", "The coverage-ID number for each matching point is stored as the node-ID number in the NAT. If there are no matches to a node, then the node-ID is equal to the internal node number.", "If the node coverage does not have an NAT, then the NAT is first built with the ", "Build", " command using the NODE feature type.", "Additional items from the PAT file are added to the NAT file. If an existing NAT file has additional attributes, items from the PAT will be appended after them. Item values with no matching points will be blank for character items and zero for numeric. Existing items and item values in the NAT file will be maintained.", "The point cover must have a point attribute table for this command to work.", "The node cover can be the same as the point cover, in which case, the attributes of the PAT are transferred to the NAT within the point coverage.", "If an item in the PAT exists in the NAT, the item values in the NAT are updated with values from the PAT."], "parameters": [{"name": "point_cover", "isOptional": false, "description": "The coverage containing point features to be transferred. ", "dataType": "Coverage"}, {"name": "node_cover", "isOptional": false, "description": "An existing cover whose node attribute values will be updated or created if the NAT does not exist. ", "dataType": "Coverage"}, {"name": "search_radius", "isOptional": true, "description": "The maximum distance apart that points and nodes can be for the attributes of the point class to be transferred and recorded to the node coverage as a node class. The radius is given in coverage units. The default search radius is the width or height of the node coverage's BND divided by 100, whichever is larger. ", "dataType": "Double"}]},
{"syntax": "PointDistance_arc (from_cover, to_cover, out_info_table, {search_radius})", "name": "Point Distance (Coverage)", "description": "Computes the point-to-point distance between each point in a coverage to all points in the same or another coverage within a specified search radius.", "example": {"title": "PointDistance example (stand-alone script)", "description": "The following stand-alone script demonstrates how to calculate the distances between points in two coverages.", "code": "# Name: PointDistance_Example.py # Description: Calculates the distances between the points in two coverages # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables fromCover = \"wells\" toCover = \"trees\" outInfoTable = \"C:/output/distance\" # Execute PointDistance arcpy.PointDistance_arc ( fromCover , toCover , outInfoTable , \"\" )"}, "usage": ["The definition of the distance item in the output INFO file will be the same as the highest precision of the two point coverage inputs.", "The search radius is the maximum distance in coverage units a feature can be from the current point for consideration. The diagonal width of the from coverage\u2019s BND will be used as the default if no {search_radius} is specified.", "Distance is set to zero when no match is found within the ", "search radius", " for a particular point. If no matching points are found, the tool gives a warning, and no output info table is created.", "Point Distance will not work if either coverage contains more than 130,000 points.", "The output INFO table can become very large when both coverages contain many points. Use a smaller search radius to limit the number of combinations.", "If Point Distance is used to calculate the distance between other points in a single coverage, the cover# will have an A and a B appended to the item name.", "Projection files will be compared for similarity using the level of comparison specified with the Project Compare environment setting. For more information, see ", "Compare Projections", ".", "The results are recorded in an output table containing items for the internal numbers and distance. The input with the highest precision for distance is the one used for the output INFO distance field."], "parameters": [{"name": "from_cover", "isOptional": false, "description": "The point coverage for which distances to another coverage's points are to be computed. ", "dataType": "Coverage"}, {"name": "to_cover", "isOptional": false, "description": "The point coverage from which point distances are to be measured. Distances between all points in the same coverage can be calculated by specifying the same coverage name for both <from_cover> and <to_cover> arguments. ", "dataType": "Coverage"}, {"name": "out_info_table", "isOutputFile": true, "isOptional": false, "description": "The INFO data table created by Point Distance, which holds the distance measurements. The number of records created in <output Info table:> depends on the search radius used, but it can be as large as the number of points in the <from cover> times the number of points in the <to cover:>. ", "dataType": "INFO Table"}, {"name": "search_radius", "isOptional": true, "description": "The maximum distance in coverage units a feature can be from the current point for consideration as the closest feature. The default value is the diagonal width of the from coverage\u2019s BND. ", "dataType": "Double"}]},
{"syntax": "Near_arc (in_cover, near_cover, out_cover, {feature_type}, {search_radius}, {location})", "name": "Near (Coverage)", "description": "Computes the distance from each point in a coverage to the nearest arc, point, or node in another coverage.", "example": {"title": "Near example (stand-alone script)", "description": "The following stand-alone script demonstrates how to calculate the distances between wells in one coverage and roads in another.\r\n\r\n", "code": "# Name: Near_Example.py # Description: Computes the distances between wells and roads # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"wells142\" nearCover = \"road\" outCover = \"C:/output/nearwells\" featureType = \"LINE\" searchRadius = 12000 location = \"LOCATION\" # #Execute Near arcpy.Near_arc ( inCover , nearCover , outCover , featureType , searchRadius , location )"}, "usage": ["The search radius is the maximum distance in coverage units a feature can be from the current point for consideration as the closest feature. The default is the width or height of the near coverage BND divided by 100, whichever is larger.", "The results are recorded in the Output coverage point attribute table (PAT). Items for DISTANCE and the internal number of the closest feature are added or updated; items for x and y coordinates are added when Record x,y coordinates of nearest feature are checked. All item values are set to zero if no feature is found within the search radius.", "DISTANCE values are recalculated if this item already exists in the input coverage. If the DISTANCE item is added, it will be in the same precision as the coverage.", "Items output by NEAR include:", "These items are added to the input coverage PAT. If an item named DISTANCE already exists, the values will be recalculated. If the DISTANCE item is added, it will be in the same precision as the coverage. The values for both items will be zero if no match is found within the search_radius for a particular input coverage point.", "The calculated distance from point to arc will be from the point to the nearest location along the arc. The calculated distance from point to node will be between the nearest node locations on the arcs.", "The distance and the internal number of the closest feature are saved as new items in the input coverage's feature attribute table.", "NEAR skips duplicate node numbers to increase operation time.", "The coordinate precision of input coverage is not affected by NEAR.", "NEAR is useful for assigning point attributes to nearest arcs, or vice versa. This operation is helpful in assigning address ranges to arcs; associating point attributes to nodes, such as DIME node numbers; or finding the nearest available line in a network (for example, determining which sewer line a property might connect with).", "Common uses for Near include:"], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing points from which distances are calculated to each closest arc, point, or node in the <near_cover:>. ", "dataType": "Coverage"}, {"name": "near_cover", "isOptional": false, "description": "The line or point coverage whose features are used to calculate distances from each input cover point. This must be different from the input cover. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be created. The <input_cover> is copied to the <output_cover:>, then NEAR is performed on the <output_cover:>. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "The type of feature that will be searched from points to find the nearest feature and calculate the distance between them. LINE \u2014 A point-to-arc distance will be determined. New items for distance and the internal number of the closest arc in the <near_cover:> will be added to the <input_cover> PAT. This is the default option. POINT \u2014 A point-to-point distance will be determined. New items for distance and the internal number of the closest point in the <near_cover:> will be added to the <input_cover> PAT. NODE \u2014 A point-to-node distance will be determined. New items for distance and the internal node number of the closest node in the <near_cover:> will be added to the <input_cover> PAT. ", "dataType": "String"}, {"name": "search_radius", "isOptional": true, "description": "The maximum distance in coverage units between input cover features and near cover features for which distance and near cover internal number will be determined. If no near cover feature is within the search radius of a given input cover point or line, both the internal number and distance output by NEAR will be zero. The default search radius is the width or height of the near coverage BND divided by 100, whichever is larger. This default search radius is used whenever the search radius argument is set to zero or skipped. ", "dataType": "Double"}, {"name": "location", "isOptional": true, "description": "Determines whether the x,y coordinates of the \"nearest point\" of the closest arc, point, or node are to be saved as well as the cover# and distance. The new items are X-COORD and Y-COORD. NO_LOCATION \u2014 The x,y coordinates of the nearest point are not saved. This is the default. LOCATION \u2014 The x,y coordinates of the nearest point, as well as the cover# and distance, will be saved. ", "dataType": "Boolean"}]},
{"syntax": "Buffer_arc (in_cover, out_cover, {feature_type}, {buffer_item}, {buffer_table}, {buffer_distance}, {fuzzy_tolerance}, {buffer_shape}, {buffer_side})", "name": "Buffer (Coverage)", "description": "Creates buffer polygons around specified input coverage features. \r\n Learn more about how Buffer works \r\n", "example": {"title": "Buffer example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a buffer around a road coverage. ", "code": "# Name: Buffer_Example.py # Description: Creates a buffer around a road coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"road\" outCover = \"C:/output/roadbuf\" featureType = \"LINE\" bufferDistance = 200 bufferShape = \"ROUND\" bufferSide = \"FULL\" # Execute Buffer arcpy.Buffer_arc ( inCover , outCover , featureType , \"\" , \"\" , bufferDistance , \"\" , bufferShape , bufferSide )"}, "usage": ["Negative and positive distances can be used for buffer distance with the POLY option. It is possible to shrink some polygons and grow others in the same coverage when the buffer item contains positive and negative numbers.", "The ROUND, FLAT, FULL, LEFT, and RIGHT options apply only to line data.", "Small sliver polygons may be created by the Buffer tool. They can cause problems when coded as nonbuffer areas inside buffer zones. Remove them with ", "Eliminate", ".", "Features will not be buffered if their buffer distance is zero. If you do not want to buffer a feature in the input coverage, give it a buffer distance value of zero in a buffer item or of DIST in a buffer table.", "When performing a buffer on a coverage that contains island polygons, all the polygons with a buffer distance greater than zero get buffered. However, since the island buffer is entirely within its surrounding polygon buffer, it is dissolved into the surrounding polygon during Buffer's dissolve phase and the island buffers will not be visible.", "Nodes can be buffered on coverages with or without an NAT. If there is no NAT, the nodes in the AAT must be numbered (FNODE# and TNODE# must be greater than zero). Run ", "Renumber Nodes", " if the node numbers are zero.", "The LEFT and RIGHT options generate buffers at the left or right of a line. Caution must be exercised in interpreting the left or right of a line. A single line segment defines two half planes, cutting a rectangular buffer into two clearly defined left and right sections. When connecting line segments, points that are on the left side of one segment may be on the right side of another connecting segment. This ambiguity can be prevented by a concept of not crossing over the line.", "The LEFT and RIGHT options use the topological left and right of the line, so line topology must be present for the use of these options. To access the Line buffer styles, use ", "Build", " on the input coverage with LINE and NODE.", "The BUFFER function works in Euclidean space and uses a two-dimensional algorithm. A buffer will be the same width no matter what the coordinate system is. It will not reflect the curvature or the shape of the earth. For the best results, generate the buffer in a map projection that minimizes distortion in the area of interest.", "For buffers around lines, the fuzzy tolerance for the output coverage may not be the same as the fuzzy tolerance for the input coverage. The output tolerance may be reset as a function of the buffer distance. This may happen for larger buffer distances and reflects the progressively courser approximation of a circle with a fixed number of straight line segments. If this effect is to be avoided, a buffer can be grown in a number of steps. First, run Buffer with the LINE option using a small buffer distance, then run a number of buffers on this using the POLY option until you achieve the desired resultant buffer distance. Fuzzy tolerance changes only for buffers around lines; it does not change for buffers around polygons.", "The coordinate precision of the output coverage is determined by the current processing rule as set by the ", "Precision for Derived Coverages", " environment setting. If the processing rule has not been established during the current session, then the processing rule will be HIGHEST. This means that Buffer will create an output coverage in the same precision as the input coverage.", "On both ", "single-precision", " and ", "double-precision", " coverages, Buffer calculates a minimum tolerance based on the mathematical precision of the coverage (based on the width of the BND and the number of decimal places). If the calculated minimum tolerance is greater than the entered fuzzy tolerance, the calculated minimum tolerance is used.", " ", "Learn more about how the default fuzzy tolerance is calculated", "The projection file (PRJ) will be copied to the output coverage."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing features to be buffered. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The polygon buffer coverage to be created. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "The feature class to be buffered: LINE \u2014 Arcs will be buffered. This is the default. POLY \u2014 Polygons will be buffered. POINT \u2014 Points will be buffered. NODE \u2014 Nodes will be buffered. ", "dataType": "String"}, {"name": "buffer_item", "isOptional": true, "description": "An item in the feature attribute table of input coverage whose value is used as the feature's buffer distance. If a buffer table is used, the buffer item functions as a lookup item in the buffer table. ", "dataType": "String"}, {"name": "buffer_table", "isOptional": true, "description": "An INFO lookup table that lists a buffer distance for each buffer item. A buffer table can be specified only if the buffer item is specified. The buffer table contains at least two items: Buffer Item\u2014Defined the same as buffer item in the input coverage feature attribute table. The buffer table must be sorted on this item in ascending order. DIST\u2014The buffer distance for each buffer item value. DIST must be defined as a numeric item (that is, N, I, F, or B). A lookup table will categorize item values.", "dataType": "INFO Table"}, {"name": "buffer_distance", "isOptional": true, "description": "The distance used to create buffer zones around input coverage features when buffer item and buffer table are not specified. The default buffer distance is 0.125 coverage units. This default buffer distance will be applied whenever a value for this parameter is not specified. The smallest buffer distance that can be computed is 0.00000005 coverage units. Specifying a buffer distance below this threshold will result in an empty output coverage. For polygon features, if a negative buffer distance is used, buffers will be generated on the insides of polygons. ", "dataType": "Double"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in the out_cover. By default, the minimum fuzzy tolerance value from the in_cover is used. ", "dataType": "Double"}, {"name": "buffer_shape", "isOptional": true, "description": "For lines, the shape of the buffer at the line endpoints. ROUND \u2014 Will make an end in the shape of a halfcircle. FLAT \u2014 Will construct rectangular line endings with the middle of the short side of the rectangle coincident with the endpoint of the line. ", "dataType": "String"}, {"name": "buffer_side", "isOptional": true, "description": "For lines, the topological side on which the buffer may be generated. FULL \u2014 On all sides. This is the default. LEFT \u2014 Half buffer on the topological left side of a line. RIGHT \u2014 Half buffer on the topological right side of a line. ", "dataType": "String"}]},
{"syntax": "Union_arc (in_cover, union_cover, out_cover, {fuzzy_tolerance}, {join_attributes})", "name": "Union (Coverage)", "description": "Computes the geometric intersection of two polygon coverages. All polygons from both coverages will be split at their intersections and preserved in the output coverage. \r\n Learn more about how Union works \r\n", "example": {"title": "Union example (stand-alone script)", "description": "The following stand-alone script demonstrates how to union two coverages.", "code": "# Name: Union_Example.py # Description: Unions two coverages # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"tong_azone\" unionCover = \"tong_flood2\" outCover = \"C:/output/studyarea\" joinAttributes = \"NO_JOIN\" # Execute Union arcpy.Union_arc ( inCover , unionCover , outCover , \"\" , joinAttributes )"}, "usage": ["The input coverage and the union coverage must have polygon topology.", "Region subclasses in either the input or union coverage are maintained in the output coverage. Subclasses with identical names and attribute schemas are appended.", "Label points are generated in each output coverage polygon. The new polygon User-IDs are set equal to the polygon internal number minus one.", "The input coverage, union coverage, and output coverage must have different names, even when in different workspaces.", "Existing input coverage annotation is copied to the output coverage by UNION.", "Route systems in the input coverage will be maintained in the output coverage. However, UNION on routes and sections themselves is not permitted.", "Region subclasses from both input coverage and union coverage are maintained. If the same subclass exists in both coverages, the subclass contents are appended. If the same subclass exists in both coverages but the item definitions are different, a message is given and the subclass is not propagated.", "The coordinate precision of the output coverage is determined by the ", "Precision for Derived Coverages", " environment.", "Projection files will be compared for similarity using the level of comparison specified in the ", "Compare Projections", " environment.", "On single-precision coverages, UNION calculates a minimum tolerance based on the mathematical precision of the coverage (based on the width of the BND and the number of decimal places). If the calculated minimum tolerance is greater than the fuzzy tolerance entered, the calculated minimum tolerance is used.", "The output coverage inherits the items from the point attribute table, tics, and the projection file data model contents from the input coverage.", "When the input coverage contains linear data belonging to different planar graphs, the data will be maintained in the output coverage. For example, with coincident or colinear arcs, such as arcs representing utility cables at different levels or a road following a stream, the coincident and colinear line segments will be preserved. However, additional vertices may be inserted. In the case of intersecting arcs, such as a road passing over a stream, nodes will not be inserted at the apparent intersection."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage whose polygons will be combined with the union coverage. ", "dataType": "Coverage"}, {"name": "union_cover", "isOptional": false, "description": "The union coverage whose polygons will be combined with the input coverage. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage that will be created containing the results of the operation. ", "dataType": "Coverage"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in the output coverage. By default, the minimum fuzzy tolerance value from the input and union coverages is used. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}, {"name": "join_attributes", "isOptional": true, "description": "Specifies whether all items in both the input and the union coverage will be joined to the output coverage feature attribute table. JOIN \u2014 All items from both coverages will appear in the output coverage feature attribute table. If duplicate item names are encountered, the item in the input coverage will be maintained, and the one in the join file will be dropped. This is the default option and is used unless NO_JOIN is specified. NO_JOIN \u2014 Only the feature's internal number (cover#) from the input coverage and the union coverage are joined in the output coverage feature attribute table. This option is useful in reducing the size of the output coverage feature attribute table. The cover# field can then be used in the Add Join tool to link the features in the resulting coverage back to the features in the input or union coverage. ", "dataType": "Boolean"}]},
{"syntax": "Intersect_arc (in_cover, intersect_cover, out_cover, {feature_type}, {fuzzy_tolerance}, {join_attributes})", "name": "Intersect (Coverage)", "description": "Computes the geometric intersection of two coverages. Only those features in the area common to both coverages will be preserved in the output coverage. \r\n Learn more about how Intersect works \r\n", "example": {"title": "Intersect example (stand-alone script)", "description": "The following stand-alone script demonstrates how to intersect two coverages.", "code": "# Name: Intersect_Example.py # Description: Intersects two coverages # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"stream\" intersectCover = \"citylim\" outCover = \"C:/output/citystreams\" featureType = \"LINE\" joinAttributes = \"NO_JOIN\" # Execute Intersect arcpy.Intersect_arc ( inCover , intersectCover , outCover , featureType , \"\" , joinAttributes )"}, "usage": ["The intersect coverage must have polygon topology.", "The input coverage, intersect coverage, and output coverage must have different names, even when in different workspaces.", "Label points are generated in each output coverage polygon when the POLY option is used. The new polygon User-IDs are set equal to the polygon internal number minus one.", "Route systems in the input coverage will be maintained in the output coverage when using the LINE option. However, INTERSECT on routes and sections themselves is not permitted.", "Region subclasses from both input coverage and intersect coverage are maintained with the POLY option. Subclasses with identical names and attribute schemas are appended. Output regions are clipped by the extent of the output coverage.", "Annotation is copied from the input coverage and saved in the output coverage.", "The coordinate precision of the output coverage is determined by the ", "Precision for Derived Coverages", " environment.", "Projection files will be compared for similarity using the level of comparison specified in the ", "Compare Projections", " environment.", "The output coverage inherits the items from the point attribute table, tics, and the projection file data model contents from the input coverage.", "When the input coverage contains linear data belonging to different planar graphs, the data will be maintained in the output coverage. For example, with coincident or colinear arcs, such as arcs representing utility cables at different levels or a road following a stream, the coincident and colinear line segments will be preserved. However, additional vertices may be inserted. In the case of intersecting arcs, such as a road passing over a stream, nodes will not be inserted at the apparent intersection."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage whose polygon, line, or point features will be intersected with the intersect coverage. ", "dataType": "Coverage"}, {"name": "intersect_cover", "isOptional": false, "description": "The intersect coverage. This coverage must contain polygon features. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage that will be created to contain the results. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "The input coverage feature class to be overlaid and preserved in the output coverage. POLY \u2014 The input coverage's polygon feature class will be used as input. This is the default option. LINE \u2014 The input coverage's line (arc) feature class will be used as input. POINT \u2014 The input coverage's point feature class will be used as input. ", "dataType": "String"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in the output coverage. By default, the minimum fuzzy tolerance value from the input and erase coverages is used. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}, {"name": "join_attributes", "isOptional": true, "description": "Specifies whether all items in both the input coverage feature attribute and identity coverage will be joined to the output coverage feature attribute table. JOIN \u2014 All feature attribute items from both coverages will appear in the output coverage feature attribute table. If a duplicate item is encountered, the item in the input coverage will be maintained and the one in the join file will be dropped. This is the default option. NO_JOIN \u2014 Only the feature's internal number (cover#) from the input coverage and the intersect coverage are joined in the output coverage feature attribute table. This option is useful in reducing the size of the output coverage feature attribute table. The Add Join tool can then be used to get the attributes to the output coverage features. ", "dataType": "Boolean"}]},
{"syntax": "Identity_arc (in_cover, identity_cover, out_cover, {feature_type}, {fuzzy_tolerance}, {join_attributes})", "name": "Identity (Coverage)", "description": "Computes the geometric intersection of two coverages. All features of the input coverage, as well as those features of the identity coverage that overlap the input coverage, are preserved in the output coverage. \r\n Learn more about how Identity works \r\n", "example": {"title": "Identity example (stand-alone script)", "description": "The following stand-alone script demonstrates how to split roads where they pass through city boundaries.", "code": "# Name: Identity_Example.py # Description: Splits roads where they pass through city boundaries. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"major_rds\" identityCover = \"citylim\" outCover = \"C:/output/major_roads\" featureType = \"LINE\" # Execute Identity arcpy.Identity_arc ( inCover , identityCover , outCover , featureType , \"\" , \"\" )"}, "usage": ["The identity coverage must have polygon topology.", "The input coverage, identity coverage, and output coverage must have different names, even when in different workspaces.", "Route systems in the input coverage are maintained in the output coverage. However, IDENTITY on routes and sections themselves are not permitted.", "Region subclasses from both input coverage and identity coverage are maintained with the POLY option. Subclasses with identical names and attribute schemas are appended. Output regions are clipped by the extent of the output coverage.", "Label points are generated in each output coverage polygon when the POLY option is used. The new polygon User-IDs are set equal to the polygon internal number minus one. When the LINE option is used, User-IDs of the input coverage are maintained.", "Annotation is copied from input coverage and saved in output coverage.", "The coordinate precision of the output coverage is determined by the ", "Precision for Derived Coverages", " environment.", "Projection files will be compared for similarity using the level of comparison specified in the ", "Compare Projections", " environment.", "The output coverage inherits the items from the point attribute table, tics, and the projection file data model contents from the input coverage.", "When the input coverage contains linear data belonging to different planar graphs, the data will be maintained in the output coverage. For example, with coincident or colinear arcs, such as arcs representing utility cables at different levels or a road following a stream, the coincident and colinear line segments will be preserved. However, additional vertices may be inserted. In the case of intersecting arcs, such as a road passing over a stream, nodes will not be inserted at the apparent intersection."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage that will be overlaid with the identity coverage. ", "dataType": "Coverage"}, {"name": "identity_cover", "isOptional": false, "description": "The coverage that will be identitied with the input coverage. Must have polygon features. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be created. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "The feature class from the input coverage that will be used. POLY \u2014 Poly on poly overlay. This is the default option. LINE \u2014 Line on poly overlay. POINT \u2014 Point on poly overlay. ", "dataType": "String"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in the output coverage. By default, the minimum fuzzy tolerance value from the input and erase coverages is used. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}, {"name": "join_attributes", "isOptional": true, "description": "Specifies whether all items in both the input coverage feature attribute and identity coverage will be joined to the output coverage feature attribute table. JOIN \u2014 All feature attribute items from both coverages will appear in the output coverage feature attribute table. If a duplicate item is encountered, the item in the input coverage will be maintained and the one in the join file will be dropped. This is the default option. NO_JOIN \u2014 Only the feature's internal number (cover#) from the input coverage and the intersect coverage are joined in the output coverage feature attribute table. This option is useful in reducing the size of the output coverage feature attribute table. The Add Join tool can then be used to get the attributes to the output coverage features. ", "dataType": "Boolean"}]},
{"syntax": "Erase_arc (in_cover, erase_cover, out_cover, {feature_type}, {fuzzy_tolerance})", "name": "Erase (Coverage)", "description": "Creates a new output coverage by overlaying the polygons of the erase coverage with the features of the input coverage. Only those portions of the input coverage features falling outside the erase polygon outer boundaries are copied to the output coverage. Learn more about how Erase works", "example": {"title": "Erase example (stand-alone script)", "description": "The following stand-alone script demonstrates how to erase an area from a coverage. ", "code": "# Name: Erase_Example.py # Description: Creates an empty area inside a polygon coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"tongass1\" eraseCover = \"tong_azone\" outCover = \"C:/output/tong_nozone\" featureType = \"POLY\" # Execute Erase arcpy.Erase_arc ( inCover , eraseCover , outCover , featureType , \"\" )"}, "usage": ["The output coverage cannot already exist.", "Annotation is erased if its lower left starting point falls within the erasing polygon.", "New nodes have their attributes set to zero.", "Input coverage polygons that are coincident with erase coverage polygons are removed.", "The erase coverage must have polygon topology.", "User-IDs for all features are the same in the output coverage as they are in the input coverage.", "For the POLY and NET options, polygon topology is rebuilt in the output coverage. Whenever possible, input polygon label points are preserved in the output. Each old polygon keeps its original label point position if it falls outside an erase coverage polygon.", "Boundaries of interior polygons in the erase coverage are not used in ERASE. Any erase coverage polygon whose internal number is greater than one is considered inside the erasing window; an internal polygon number of one is considered outside. Only those input features (or portions of them) that are outside the erasing region are stored in the output coverage.", "The outside boundaries of the erase coverage define the area of Input coverage features to be removed. Any erase coverage polygon with an internal number greater than one is considered inside the erasing window; an internal polygon number of one is considered outside.", "If the erase coverage polygon happens to fall completely within an input polygon, then no polygons are erased. An extra polygon is inserted in the output coverage, as defined by the outline polygon of the erase coverage, and given a label point with a User-ID = 0. This is for the POLY option. If the LINE option is used, then nothing extra is added. The output coverage remains identical to the input coverage.", "Route systems will be rebuilt for LINE, NET, LINK, and RAW options but ignored on the POLY option. Route systems are duplicated for arcs split into multiple pieces and eliminated for eliminated arcs. ERASE maintains all route system subclasses.", "Region subclasses in the input coverage are maintained with the POLY option. Regions in the input coverage are erased in the output coverage by the extent of the erase coverage.", "Region subclasses in the erase coverage are not inherited.", "If a node attribute table (NAT) exists before ERASE, then it remains afterward for those nodes that survive the ERASE. New nodes have their attributes set to zero.", "The output coverage tics are copied from the input coverage.", "The projection file (PRJ) is copied to the output coverage.", "The coordinate precision of the output coverage is determined by the current processing rule as set by the ", "Precision for Derived Coverages", " environment setting. If the processing rule is not established, the output coverage is the same precision as the input coverage.", "When the input coverage contains linear data belonging to different planar graphs, the data will be maintained in the output coverage. For example, with coincident or colinear arcs, such as arcs representing utility cables at different levels or a road following a stream, the coincident and colinear line segments will be preserved. However, additional vertices may be inserted. In the case of intersecting arcs, such as a road passing over a stream, nodes will not be inserted at the apparent intersection.", "Learn more about how the default fuzzy tolerance is calculated"], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing features to be erased. ", "dataType": "Coverage"}, {"name": "erase_cover", "isOptional": false, "description": "The coverage whose outer polygon defines the erasing region. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be created. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "The set of features to be erased: POLY \u2014 Polygons are erased, and the polygon attribute table (PAT) is updated. This is the default. LINE \u2014 Arcs are erased, and the arc attribute table (AAT) is updated. POINT \u2014 Points are erased, and the point attribute table (PAT) is updated. NET \u2014 Polygons and arcs are erased, and their PAT and AAT are updated. LINK \u2014 Arcs and points are erased, and their AAT and PAT are updated. RAW \u2014 Arcs, data points, and annotation in a coverage that do not have topology (no attribute files) are erased. Route systems are maintained, but regions PAT and AAT are not saved. ", "dataType": "String"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in the output coverage. By default, the minimum fuzzy tolerance value from the input coverage and erase coverage is used. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}]},
{"syntax": "Split_arc (in_cover, split_cover, split_item, {path}, {feature_type}, {fuzzy_tolerance})", "name": "Split (Coverage)", "description": "Clips portions of the input coverage into multiple coverages. Each new output coverage contains only those portions of the input coverage features overlapped by the split coverage polygons. The unique values in the Split Item are used to name the output coverages. The number of output coverages is determined by the number of unique values in the Split Item. \r\n Learn more about how Split works \r\n", "example": {"title": "Split example (stand-alone script)", "description": "The following stand-alone script demonstrates how to split one coverage into multiple output coverages.\r\n\r\n", "code": "# Name: Split_Example.py # Description: Splits one coverage into multiple output coverages. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"county\" splitCover = \"citylim\" splitItem = \"citycode\" path = \"C:/output/city\" featureType = \"LINE\" # Execute split arcpy.Split_arc ( inCover , splitCover , splitItem , path , featureType , \"\" )"}, "usage": ["Values in the Split Item must be unique.", "The Split Item data type must be character.", "The output coverages will be named for Split Item values; therefore, they must start with a valid character.", "Split Item values must be 13 characters or less.", "The split coverage must have polygon topology.", "Topology is maintained in the output coverages.", "The number of output coverages is dictated by the Split Item of the split coverage. The maximum number of output coverages equals the total number of unique values in the Split Item.", "The Output Workspace in which each output coverage is created must contain an INFO directory before running Split. Split will stop if it cannot find the INFO directory.", "Annotation is copied when its lower left starting point falls within a new output coverage.", "Polygon topology is rebuilt for each output coverage. Each polygon User-ID is set equal to the old input coverage polygon User-ID. New label point positions are only generated when the original input coverage label point position is clipped by the split coverage.", "Empty output coverages can be created by Split for split coverage areas in which no input coverage features are located.", "If the input coverage contains a node feature class with attributes, then each of the output coverages will contain nodes that fall within the appropriate split coverage. New NAT records will be created as required by the Split operation.", "Attributes for new nodes are set to zero.", "Region subclasses in the input coverage are maintained.", "Empty region subclasses are created if the regions that make up the subclass are not in the output coverage.", "Region subclasses are maintained as empty subclasses when all the regions are removed.", "Route systems will be maintained. Route systems will be duplicated for arcs split into multiple pieces. Each resultant coverage will have a complete set of route system subclasses containing only those route systems attached to arcs within that coverage (empty route system files will be created if necessary).", "Route systems are copied when their arcs are split into multiple pieces and dropped for arcs that are removed.", "Routes and sections on the arcs will be carried over and maintained. Route systems will be split at their intersections. Split maintains all route system subclasses.", "Split treats route systems as if it were performing repeated ", "Clips", ". Each output coverage from Split contains the route systems within that particular split polygon and the parts of those that pass through that polygon. Routes retain their User-IDs across split coverages so that ", "Append", " can reassemble them. Split sections will have new section measures interpolated at their split ends.", "Four new tic locations are generated for each output coverage at the corners of the split coverage BND. No existing tics are retained.", "The feature attribute table for each output coverage contains the same items as the input coverage feature attribute table.", "Split will distribute all annotation subclasses present in the input coverage to all of the output coverages based on the split coverage polygon in which the lower left starting point falls for each annotation string. If a particular subclass has no actual annotation within a particular output coverage, it is still created.", "The input coverages projection file (PRJ) will be copied to each output coverage.", "The coordinate precision of each output coverage is determined by the current processing rule as set by the ", "Derived Precision", " environment setting. If the processing rule has not been established during the current session, then the output coverages will be in the same precision as the input coverage.", "When the input coverage contains linear data belonging to different planar graphs, the data will be maintained in the output coverage. For example, with coincident or colinear arcs, such as arcs representing utility cables at different levels or a road following a stream, the coincident and colinear line segments will be preserved. However, additional vertices may be inserted. In the case of intersecting arcs, such as a road passing over a stream, nodes will not be inserted at the apparent intersection."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage to be split. ", "dataType": "Coverage"}, {"name": "split_cover", "isOptional": false, "description": "The coverage used to split the input coverage. ", "dataType": "Coverage"}, {"name": "split_item", "isOptional": false, "description": "The item in the split coverage that will be used to split the input coverage. The unique values in the Split Item are used to name the output coverages. The number of output coverages is determined by the number of unique values in the Split Item. ", "dataType": "INFO Item"}, {"name": "path", "isOptional": true, "description": "The workspace in which the output coverage will be maintained. ", "dataType": "Folder"}, {"name": "feature_type", "isOptional": true, "description": "The feature classes to be split: POLY \u2014 Polygons will be split. This is the default. LINE \u2014 Arcs will be split. POINT \u2014 Points will be split. NET \u2014 Polygons and lines will be split. LINK \u2014 Points and lines will be split. RAW \u2014 Arcs, data points, and annotation in a coverage that does not have topology (no attribute files) will be split. Attributes are ignored. ", "dataType": "String"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in each output coverage. By default, the minimum fuzzy tolerance value from the input coverage and split coverage is used. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}]},
{"syntax": "Reselect_arc (in_cover, out_cover, info_express, {in_feature_type}, {selection_file}, {out_feature_type})", "name": "Select (Coverage)", "description": "Extracts selected features from an input coverage and stores them in the output coverage.  Features are selected for extraction based on logical expressions or by applying the criteria contained in a  selection file . Any item, including redefined items, in the specified feature attribute table of the Input coverage can be used.", "example": {"title": "Reselect example (stand-alone script)", "description": "The following stand-alone script demonstrates how to select features from a coverage.", "code": "# Name: Reselect_Example.py # Description: Selects streams of interest from a larger stream coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"stream\" outCover = \"C:/output/studystreams\" infoExpress = [ \"RESELECT stream_name CN 'AQUEDUCT'\" , \"NSELECT\" , \"RESELECT stream_order > 3\" , \"ASELECT length > 10000\" ] inFeatureType = \"LINE\" # Execute Reselect arcpy.Reselect_arc ( inCover , outCover , infoExpress , inFeatureType , \"\" , \"\" )"}, "usage": ["When using the same input coverage and output coverage for feature classes Anno, Section, Route, or Region, the output feature class subclass name must be different from the input feature class subclass name.", "Use of indexed items  can speed up the logical selection process. You can use the ", "Index Item", " tool to create an attribute index.", "The same subclass names may be specified when the ", "Output Coverage", " is not the same as the ", "Input Coverage", ".", "\r\nLearn more about ", "How to Build an INFO Query", "\r\n"], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The input coverage containing the features that will be selected. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage containing the selected features. ", "dataType": "Coverage"}, {"name": "info_express", "isOptional": false, "description": "Write a query that contains one or more logical expressions to select features from the input coverage. Each expression takes one of the following forms: RESELECT <expression>\u2014Reduces the selected set of records with a selection expression to those that meet its criteria. If no selection expression follows, the selected set will be empty. ASELECT <expression>\u2014Adds unselected records that meet the selection expression criteria to the currently selected set. If no selection expression follows, the selected set will contain all features. NSELECT\u2014Reverses the current selection to the unselected set.", "dataType": "INFO Expression"}, {"name": "in_feature_type", "isInputFile": true, "isOptional": true, "description": "The feature class to select: Poly \u2014 Polygons are reselected using PAT item values. Line \u2014 Arcs are reselected using AAT item values. Point \u2014 Points are reselected using PAT item values. Anno.<subclass> \u2014 Annotation from the specified subclass is reselected using TAT subclass item values. Route.<subclass> \u2014 Routes from the specified subclass are reselected using RAT subclass item values. Section.<subclass> \u2014 Sections from the specified subclass are reselected using SEC subclass item values. Region.<subclass> \u2014 Regions from the specified subclass are reselected using the region PAT subclass item values. ", "dataType": "String"}, {"name": "selection_file", "isOptional": true, "description": "A preexisting file that identifies the features to select. ", "dataType": "File"}, {"name": "out_feature_type", "isOutputFile": true, "isOptional": true, "description": "The feature class in the output coverage. This must be the same as that of the input feature class, with this exception: When the input feature class is an Anno, Route, Section, or Region subclass and the output coverage is the same as the Input coverage, the output feature class must have a different subclass name. ", "dataType": "String"}]},
{"syntax": "Update_arc (in_cover, update_cover, out_cover, {feature_type}, {fuzzy_tolerance}, {keep_border})", "name": "Update (Coverage)", "description": "Replaces the input coverage areas with the update coverage polygons using a cut and paste operation. \r\n Learn more about how Update works \r\n", "example": {"title": "Update example (stand-alone script)", "description": "The following stand-alone script demonstrates how to use the Update tool.", "code": "# Name: Update_Example.py # Description: Updates a coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"c3zone4\" updateCover = \"newsub\" outCover = \"C:/output/newc3zone4\" featureType = \"POLY\" # Execute Update arcpy.Update_arc ( inCover , updateCover , outCover , featureType , \"\" , \"\" )"}, "usage": ["Before using UPDATE, the following conditions must be met:", "The input coverage and the update coverage must have polygon topology.", "The item definitions for items past the User-ID in the input coverage and update coverage feature attribute tables must match exactly, including redefined items. The input coverage and update coverage items are joined to the output coverage attribute table using the old polygon internal number as the relate item.", "New label point positions are only generated for the output coverage polygons when necessary. The User-ID for each polygon is equal to its old input coverage User-ID (the update coverage User-ID for updated polygons). Thus, you should attempt to make the User-ID values in the input coverage different from User-ID values in the update coverage to avoid having duplicate User-IDs in the output coverage.", "UPDATE maintains all route system subclasses. If a Section (SEC) subclass is present on either coverage, then it must be present on both, and it must have identical item definitions. Route systems attached to update arcs are retained. Route systems attached to input arcs are retained if they survive. UPDATE manages route systems like an ", "Erase", ", followed by an ", "Append", ", then a ", "Clean", ".", "If DROP_BORDER is used, polygon boundaries along the outer edge of the update coverage are dropped. Even though the outer boundaries of some update polygons are dropped, the item values for the update polygons that overlap input coverage polygons will be assigned to the polygons in the output coverage. The DROP_BORDER option is not recommended for region coverages because some output regions may not be maintained.", "Annotation features are updated by identifying and deleting the set of input coverage annotation whose lower left starting points fall within an update coverage polygon. Update coverage annotation is then appended for all annotation subclasses present in the output coverage. Both coverages must have identical sets of annotation subclasses with the exception of blank annotation classes, which can be either present or absent. Where subclasses have attributes, they must have identical attribute definitions.", "If the update coverage contains nodes on its border that are coincident with nodes in the input coverage, the update coverage nodes are maintained. The only exception is if the input coverage node contains data in the NAT and the update coverage node does not have an NAT.", "Region subclasses are updated with both the POLY and NET options. All regions are maintained with the KEEP_BORDER option only. The DROP_BORDER option does not maintain all regions and, therefore, is not recommended when updating region coverages.", "The input coverage and update coverage region subclasses may be the same or different. When the subclasses are the same, they must have the same item definitions.", "The coordinate precision of the output coverage is determined by the ", "Precision for Derived Coverages", " environment.", "Projection files will be compared for similarity using the level of comparison specified in the ", "Compare Projections", " environment.", "The output coverage inherits the items from the point attribute table, tics, and the projection file data model contents from the input coverage.", "When the input coverage contains linear data belonging to different planar graphs, the data will be maintained in the output coverage. For example, with coincident or colinear arcs, such as arcs representing utility cables at different levels or a road following a stream, the coincident and colinear line segments will be preserved. However, additional vertices may be inserted. In the case of intersecting arcs, such as a road passing over a stream, nodes will not be inserted at the apparent intersection."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing polygons to be updated. ", "dataType": "Coverage"}, {"name": "update_cover", "isOptional": false, "description": "The coverage whose polygons will replace input coverage areas. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be created. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "The set of feature classes to be updated. POLY \u2014 Polygons and PAT values are updated. This is the default option. NET \u2014 Polygons and arcs, as well as PAT and AAT values, are updated. ", "dataType": "String"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in the output coverage. By default, the minimum fuzzy tolerance value from the input and erase coverages is used. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}, {"name": "keep_border", "isOptional": true, "description": "Specifies whether the outside border of the update coverage will be kept after it is inserted into the input coverage. KEEP_BORDER \u2014 The outside border of the update coverage will be kept in the output coverage. This is the default. DROP_BORDER \u2014 The outside border of the update coverage is dropped after the update coverage is inserted into the input coverage. Item values of the update polygons take precedence over input coverage item values in the output coverage. ", "dataType": "Boolean"}]},
{"syntax": "Create_arc (out_cover, {template_cover})", "name": "Create Coverage (Coverage)", "description": "Creates a new  coverage . The coverage can be initialized with the  TIC , BND, and PRJ files copied from an existing coverage or grid.", "example": {"title": "Create example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create an empty coverage. ", "code": "# Name: Create_Example.py # Description: Creates an empty coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy import os # Set local variables outCover = \"c:/output/emptycov\" # The output workspace must exist at least as an empty directory. #    Use os.makedirs if the parent directory may not exist. try : os.makedirs ( os.path.dirname ( outCover )) except : pass # Execute Create arcpy.Create_arc ( outCover )"}, "usage": ["The coordinate precision of the Output Coverage is determined by the ", "Precision for derived coverages environment", ", regardless of whether the Template Coverage is specified.", "To establish the location of the TICs in the Output Coverage, specify a Template Coverage or edit the Output Coverage manually. You can then use the Output Coverage as the destination (output) coverage of the ", "Transform", " tool."], "parameters": [{"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The name of the coverage that will be created. ", "dataType": "Coverage"}, {"name": "template_cover", "isOptional": true, "description": "An existing coverage or grid whose TIC file, boundary information (BND file), and projection information (PRJ file) will be copied to the Output Coverage. ", "dataType": "Coverage"}]},
{"syntax": "VPFTile_arc (VPF_library, {sig_digits}, {VPF_standard}, {spec_cover})", "name": "VPF Tile Topology (Coverage)", "description": "Creates cross-tile topology for all tiled coverages in a Vector Product Format (VPF) database library, or topology for an individual tile in a VPF library. This tool is used for postprocessing a VPF coverage or library from  Export to VPF  output. For efficiency purposes, it's recommended that you build cross-tile topology only after you have finished converting all coverages in your VPF library. \r\n Learn more about how VPF Tile Topology works \r\n", "example": {"title": "VPFTile example (stand-alone script)", "description": "The following stand-alone script demonstrates how to export two coverages to VPF and then create cross-tile topology for the VPF covereages.", "code": "# Name: VPFTile_Example.py # Description: Exports two coverages to VPF format then builds tile topology # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover1 = \"coastb\" inCover2 = \"coastc\" outFile1 = \"C:/output/vpfdb/wlib/coast2\" outFile2 = \"C:/output/vpfdb/wlib/coast3\" vpfLibrary = \"C:/output/vpfdb\" vpfStandard = 96 specCover = \"ALL\" # Execute VPFExport arcpy.VPFExport_arc ( inCover1 , outFile1 ) arcpy.VPFExport_arc ( inCover2 , outFile2 ) # Execute VPFTile arcpy.VPFTile_arc ( vpfLibrary , \"\" , vpfStandard , specCover )"}, "usage": ["Military Standard MIL-STD-2407 (June 28, 1996) refines the definition of cross-tile topology. The VPF Tile Topology command has been updated to meet the new specification. You may use the optional parameter to choose the 93 or 96 VPF Standard.", "VPF Tile Topology works on all tiled coverages of a VPF library or a single coverage within that library. The last optional parameter, VPF_cover, allows you to select a particular coverage in which cross-tile topology should be populated. It is more efficient, however, to implement VPF Tile Topology after all coverages for a library have been converted from the VPF format.", "VPF Tile Topology using the 1993 VPF Standard does not handle the situation where more than two edges coincide along a tile boundary. It is impossible for VPF Tile Topology to know how to assign left-face and right-face values to each edge.", "The 1996 VPF Standard requires that cross-tile topology be implemented on the connected nodes. For cross-tile to work correctly, the \"first_edge\" column in the connected node tables must be a Triplet type.", "Since VPF Tile Topology makes modifications to the primitive file in your VPF database, it is important to understand the requirements of the VPF data before the cross-tile process can occur correctly. The most important things to remember about creating 1996 cross-tile topology are the following:"], "parameters": [{"name": "VPF_library", "isOptional": false, "description": "Location of the VPF database library for which cross-tile topology is to be created. ", "dataType": "Folder"}, {"name": "sig_digits", "isOptional": true, "description": "The number of digits the software will use when trying to match node coordinates at tile boundaries. The larger the number, the smaller the search tolerance. The default value is 4. ", "dataType": "Long"}, {"name": "VPF_standard", "isOptional": true, "description": "The VPF Standard to be used. The VPF Standard has two ways of defining cross-tile topology. 93 \u2014 Creates cross-tile topology based on the September 30, 1993, version of the VPF Standard (MIL-STD-2407). 93 is the default option. 96 \u2014 Creates cross-tile topology based on the June 28, 1996, version of the VPF Standard (MIL-STD-2407). ", "dataType": "Long"}, {"name": "spec_cover", "isOptional": true, "description": "Specifies whether to process all the coverages in the VPF library or only the specified coverage. ALL \u2014 Process all the coverages in the VPF library VPF_cover \u2014 Process only the specified coverage ", "dataType": "String"}]},
{"syntax": "CreateLabels_arc (in_cover, {id_base})", "name": "Create Labels (Coverage)", "description": "Creates label points for polygons that have no labels and assigns each a User-ID. \r\n Learn more about how Create Labels works \r\n", "example": {"title": "CreateLabels example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create labels in a polygon coverage. ", "code": "# Name: CreateLabels_Example.py # Description: Creates labels in a polygon coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"texas\" idBase = 0 # Execute CreateLabels arcpy.CreateLabels_arc ( inCover , idBase )"}, "usage": ["After Create Labels, the polygon User-IDs stored in the Input Coverage PAT are not equal to the new label point User-IDs generated by Create Labels. You must use ", "Build", " or ", "Update IDs", " to make them equal.", "The ID Base value provides the starting point for creating the label point User-IDs. The specified value will be the User-ID for the first polygon encountered that has no label point. User-IDs are then incremented by one for each subsequent polygon having no label point. Specifying a value of zero will create new labels for all polygons, where each User-ID will equal the polygon's internal number minus one.", "The Input Coverage must contain ", "polygon topology", ".", "If the specified ID Base value is negative, all new User-IDs will start with the specified value and be decremented by one for each new label point. Negative ID values will not be generated.", "If a ", "coverage", " contains polygons and only some of the polygons have label points, Create Labels will only generate labels in those polygons for which no labels exist if you specify an ID Base.", "The coordinate precision of the coverage is not changed by Create Labels."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage to which label points are to be added. ", "dataType": "Coverage"}, {"name": "id_base", "isOptional": true, "description": "The minimum User-ID value to be assigned to new label points. The specified value will be the User-ID for the first polygon encountered that has no label point. User-IDs are then incremented by one for each subsequent polygon having no label point. Specifying an ID Base of zero will create new labels for all polygons, where each User-ID will equal the polygon's internal number minus one. This is the default value. ", "dataType": "Long"}]},
{"syntax": "Clean_arc (in_cover, {out_cover}, {dangle_length}, {fuzzy_tolerance}, {feature_type})", "name": "Clean (Coverage)", "description": "Generates a  coverage  with correct polygon or arc\u2013node topology. To do this, Clean edits and corrects geometric coordinate errors, assembles arcs into polygons, and creates feature attribute information for each polygon or arc (that is, creates a PAT or AAT). \r\n Learn more about how Clean works \r\n", "example": {"title": "Clean example (stand-alone script)", "description": "The following stand-alone script demonstrates how to clean a coverage for polygon topology. ", "code": "# Name: Clean_Example.py # Description: Cleans a coverage for polygon topology. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"zones\" outCover = \"C:/output/zones\" fuzzyTolerance = 0.25 featureType = \"POLY\" #Execute Clean arcpy.Clean_arc ( inCover , outCover , \"\" , fuzzyTolerance , featureType )"}, "usage": ["While processing, Clean:", "If the Input Coverage has either PAT or AAT feature attribute tables, they are automatically updated in the Output Coverage for the POLY option. Only the AAT will be updated when using the LINE option. The internal number of each Input Coverage feature is used to relate attribute information from the Input Coverage feature attribute table to the Output Coverage to ensure that the attributes are properly joined to the output feature attribute tables. Feature User-IDs do not have to be unique to ensure that each input feature keeps its attributes in the Output Coverage.", "Do not run Clean on a ", "geographic", " coverage. Geographic coverages have units in decimal degrees, decimal seconds, radians, and so on. These units are designed to measure angles; they do not measure distances. They represent a spherical coordinate system and should not be confused with a 2D rectilinear coordinate system upon which commands like Clean, ", "Buffer", ", ", "Union", ", and other overlay processes are designed to work. You must first ", "Project", " your geographic coverage to a suitable ", "projection", " to convert your angles of latitude and longitude using angular units to a 2D rectilinear Cartesian coordinate system using rectilinear units of Feet, Meters, Kilometers, and so on.", "Clean requires free disk space approximately 13 times the size of your input coverage to create temporary scratch files. These scratch files are created in your current workspace by default. You can override the default and have the scratch files created in another directory by setting the ARCTMPDIR variable. You must have write access to this directory.", "\r\n", "Learn more about temporary files created by geoprocessing tools", "\r\n", "Polygon label points are not required by the Clean command but must be used if you want nonzero User-IDs for polygons. Polygons that contain no label points are assigned a User-ID of zero in Clean.", "Before Clean , place only one label point in each polygon. If a polygon contains more than one label point, one of the label points in the polygon is arbitrarily chosen to assign the User-ID. Even though one label point is chosen for User-ID assignment, other label points are not removed by Clean.", "Clean with the POLY option creates one additional polygon called the background or universe polygon. It is always given polygon internal number 1, and its area is the total sum of the areas of all other polygons in the coverage. It is shown as a negative AREA in the PAT.", "The ", "Dangle Length", " and ", "Fuzzy Tolerance", " for the Output Coverage are set and verified by Clean.", "The ", "fuzzy tolerance", " is used by the Clean tool. This is the distance the Clean tool is allowed to move features, to eliminate duplicate nodes, create nodes at intersections of lines, and eliminate duplicate features. The fuzzy tolerance is measured in coverage units. When using the Clean tool it is critical that an appropriate fuzzy tolerance be assigned, so that necessary features are not eliminated by mistake.", "The POLY and LINE options with Clean will update the SEC and RAT with adjustments for splitting and arc renumbering. However, any routes or sections attached to arcs merged away by Fuzzy Tolerance are deleted.", "Route systems in the Input Coverage will be maintained in the Output Coverage as long as each route has a unique User-ID.", "Clean cannot be executed on a region coverage that contains unclosed regions. Clean will stop execution upon detecting the first unclosed region.", "Clean with the LINE option does not maintain any region subclass information (topology or attribute). The LINE option deletes the .PAT and the .PATsubclass files.", "Clean with the POLY option maintains region topology. If preliminary region topology exists, Clean will create region topology by creating a .PAT and a .PATsubclass.", "Clean copies Input Coverage annotation to the Output Coverage.", "Links are not maintained after Clean has been executed.", "Clean will update the NAT if it exists in the Input Coverage. New nodes will have their attributes set to zero.", "Clean will update any SECs if they exist in the Input Coverage. Sections on arcs that are split are also split, and the measures on these arcs are updated.", "Do not use Clean on a COGO coverage. Any arcs that are split by Clean will not have their COGO attributes updated. For example, an arc with a DISTANCE item value of 100 that is split by Clean will give the two new arcs DISTANCE values of 100. Use the ", "Build", " tool instead and resolve any overlapping arcs in the COGO editing environment.", "If you copy a ", "double precision", " coverage to a ", "single precision", " coverage, you must Clean the Output Coverage to rebuild polygon topology.", "On single precision coverages, Clean calculates a minimum tolerance based on the mathematical precision of the coverage (the width of the BND and the number of decimal places). If the calculated minimum tolerance is greater than the Fuzzy Tolerance entered on the command line, the calculated minimum tolerance is used.", "The coordinate precision of the Output Coverage is determined by the current processing rule as set by the Precision for ", "Derived Coverages", " environment setting. If the processing rule has not been established during the current session, then the processing rule will be HIGHEST. This means that Clean will create an Output Coverage in the same precision as the Input Coverage.", "The projection file (PRJ) will be copied to the Output Coverage."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage to be cleaned. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": true, "description": "The coverage created by Clean. If the Input Coverage and the Output Coverage have the same name, the Input Coverage will be replaced. By default, the Input Coverage is replaced. ", "dataType": "Coverage"}, {"name": "dangle_length", "isOptional": true, "description": "The minimum length allowed for dangling arcs in the Output Coverage. A dangling arc is an arc that has the same polygon internal number on its left and right sides and ends at a dangling node. Dangling arcs are removed for both the POLY and LINE options. If the Dangle Length is not provided, the dangle length is read from the coverage TOL file if the TOL file exists;otherwise, dangle length is set to zero (the default). ", "dataType": "Double"}, {"name": "fuzzy_tolerance", "isOptional": true, "description": "The minimum distance between coordinates in each out_cover. Learn more about how the default fuzzy tolerance is calculated ", "dataType": "Double"}, {"name": "feature_type", "isOptional": true, "description": "Specifies whether to create polygon topology and a PAT or arc\u2013node topology and an AAT. POLY is the default option. If POLY is used on a coverage that has an existing AAT, Clean will also automatically rebuild the AAT. POLY \u2014 Polygon topology and a PAT are created. If POLY is used on a coverage that has an existing AAT, Clean will also automatically rebuild the AAT. POLY is the default option. LINE \u2014 Arc\u2013node topology and an AAT are created. ", "dataType": "String"}]},
{"syntax": "Build_arc (in_cover, feature_type, {anno_subclass})", "name": "Build (Coverage)", "description": "Creates or updates feature attribute tables and polygon topology. Build is also used to synchronize polygon User-IDs with label point User-IDs. \r\n Learn more about how Build works \r\n", "example": {"title": "Build example (stand-alone script)", "description": "The following stand-alone script demonstrates how to build line topology for a road coverage. ", "code": "# Name: Build_Example.py # Description: Builds line topology for a road coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"road\" featureType = \"LINE\" # Execute Build arcpy.Build_arc ( inCover , featureType )"}, "usage": ["Build and ", "Clean", " are similar commands as they are both used to define coverage topology. The basic difference is that Clean can detect and create intersections but Build cannot. However, since Build does not use a fuzzy tolerance, the coordinates will not be adjusted while topology is being built.", "If a coverage feature attribute table exists, the additional items in the feature attribute table will be updated using the old internal number of each of the features specified as the relate item.", "User-defined items in existing feature attribute tables are always maintained.", "Build uses the ", "Renumber Nodes", " routine whenever the NAT is updated. However, unlike a regular Renumber Nodes operation, the coincident nodes are not dissolved; they are preserved. Nodes are renumbered to provide a continuous sequence of node numbers beginning with one.", "If an NAT exists, it will be updated when a build with the POLY or NODE option is used.", "Links are not maintained after ", "Build", " has been executed.", "When using Build with the POLY option, polygons must have label points to retain their attributes. If there are no attributes, label points are not required to generate a PAT. Polygons containing no label points will be assigned a User-ID of zero.", "Build does not create polygon labels.", "If you want to assign specific User-IDs, place only one label point in each polygon before running Build. If a polygon contains more than one label point, one is chosen arbitrarily to assign the User-ID.", "If you want the User-IDs to be automatically assigned, use the ", "Create Labels", " tool, then add polygon attributes.", "Do not Build a point coverage with the POLY (default) option. User-defined point attributes may be lost.", "Do not Build a polygon coverage with the POINT option. User-defined polygon attributes may be lost.", "Build with the POLY option creates one additional polygon called the background polygon. It is always given polygon internal number 1, and its area is the total sum of the areas of all other polygons in the coverage. It is shown as a negative AREA in the PAT.", "Build with the POLY option maintains and updates the SEC and RAT files of any existing route system in the coverage.", "Once the annotation attribute table (TAT) is created, it never needs to be updated with Build. The TAT created by using Build allows you to store attributes and set up relates to other features.", "Polygon Build will update the SEC with correct ARCLINK# values.", "Internal FNODE# and TNODE# are assigned to each node after a line coverage is built using NODE. The arc end points are sorted by their y coordinates from top to bottom, and equal y by x scanning from left to right. The internal node IDs are assigned in that order.", "Several coverage tools generate connecting arcs having the same Arc-ID, separated by node numbers that are equal to zero (for example, ", "Generate", "). When Build is run, it will unsplit these arcs, converting the nodes to vertices. If you want to maintain these arcs as separate arcs, run ", "Renumber Nodes", " before performing ", "Build.", "Build requires free disk space several times the size of your Input Coverage to create temporary scratch files. These scratch files are created in your current workspace by default. You may override the default and have the scratch files created in another directory by setting the ARCTMPDIR variable. You must have write access to this directory.", "Build cannot be executed on a region coverage that contains unclosed regions.", "Build will stop execution upon detecting the first unclosed region.", "When creating regions from a line coverage, each arc must form a closed loop that defines a region. Build with the LINE option must be issued before region topology is created with the ", "Line Coverage To Region", " tool.", "Build with the POLY option adds topology to preliminary regions by creating a .PAT and .PATsubclass. The region subclass must be created first with a previously executed tool (for example, ", "Line Coverage To Region", ").", "You may add new arcs that define a region (also closed loops) to a coverage that already has a region subclass. A previously executed tool (for example, Line Coverage To Region) must be used to group the new arcs to form the new subclass for regions. The IDs of the new arcs must be numbered greater than any of the existing arcs. Build them with a new subclass name and new arcs will create a new region subclass.", "Build should not be executed on a coverage if it contains coordinate errors. Errors that could cause problems for Build with the POLY option include intersecting arcs (where no node is defined at the intersection), unclosed polygons, or unmatched nodes and polygon slivers."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage to be built. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": false, "description": "The feature class to be built. POINT \u2014 Creates a PAT for label points. This is the default option. LINE \u2014 Creates an AAT for arcs. POLY \u2014 Creates a PAT and defines polygon topology. NODE \u2014 Creates an NAT for nodes. ANNO \u2014 Creates a TAT for the Annotation Subclass. ", "dataType": "String"}, {"name": "anno_subclass", "isOptional": true, "description": "The name of the Annotation Subclass to be built. ", "dataType": "String"}]},
{"syntax": "Tolerance_arc (in_cover, {tolerance_type}, {tolerance_value})", "name": "Tolerance (Coverage)", "description": "Sets a coverage's tolerances. \r\n Learn more about how Tolerance works \r\n", "example": {"title": "Tolerance stand-alone script", "description": "The following stand-alone script demonstrates how to use the Tolerance tool.  \r\nThe script uses Describe to check the tolerances on all the coverages in a workspace. \r\nIf any don't match a predetermined standard, it uses the Tolerance tool to update them.", "code": "# Name: Tolerance_Example.py # Description: Checks/updates tolerances on all coverages in a workspace. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # set the tolerance standards fuzzyValue = 1.0 dangleValue = 0.0 tic_matchValue = 0.0 editValue = 100.0 nodesnapValue = 10.0 weedValue = 10.0 grainValue = 10.0 snapValue = 10.0 coverageList = arcpy.ListDatasets ( \"*\" , \"coverage\" ) for cov in coverageList : desc = arcpy.Describe ( cov ) if desc.tolerances.fuzzy <> fuzzyValue : arcpy.Tolerance_arc ( cov , \"fuzzy\" , fuzzyValue ) if desc.tolerances.dangle <> dangleValue : arcpy.Tolerance_arc ( cov , \"dangle\" , dangleValue ) if desc.tolerances.ticmatch <> tic_matchValue : arcpy.Tolerance_arc ( cov , \"tic_match\" , tic_matchValue ) if desc.tolerances.edit <> editValue : arcpy.Tolerance_arc ( cov , \"edit\" , editValue ) if desc.tolerances.nodesnap <> nodesnapValue : arcpy.Tolerance_arc ( cov , \"nodesnap\" , nodesnapValue ) if desc.tolerances.weed <> weedValue : arcpy.Tolerance_arc ( cov , \"weed\" , weedValue ) if desc.tolerances.grain <> grainValue : arcpy.Tolerance_arc ( cov , \"grain\" , grainValue ) if desc.tolerances.snap <> snapValue : arcpy.Tolerance_arc ( cov , \"snap\" , snapValue )"}, "usage": ["A ", "Tolerance Value", " of zero will not be accepted for the following options: FUZZY, EDIT, NODESNAP, WEED, GRAIN, and SNAP.", "If no ", "Tolerance Type", " is specified, the default type is FUZZY.", "To see which tolerances have been set and which are Verified, open the Coverage Properties page and go to the Tolerances tab. To do this, right-click the coverage name in the ", "Catalog", " window or ArcCatalog and click Properties.", "Only one tolerance is set for each execution of this tool.", "Unverified tolerances cannot be verified with this tool. However, if you are using the tool to change existing tolerances to a smaller value, verified tolerances will remain verified."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage for which tolerances will be set. ", "dataType": "Coverage"}, {"name": "tolerance_type", "isOptional": true, "description": "The type of tolerance to be set. FUZZY \u2014 Sets the Input Coverage's fuzzy tolerance to the value specified in the Tolerance Value. This is the default option. DANGLE \u2014 Sets the Input Coverage's dangle length to the value specified in the Tolerance Value. TIC_MATCH \u2014 Sets the tic match tolerance to the value specified in the Tolerance Value. EDIT \u2014 Sets the Input Coverage's edit distance to the value specified in the Tolerance Value. NODESNAP \u2014 Sets the Input Coverage's node snap distance to the value specified in the Tolerance Value. WEED \u2014 Sets the weed tolerance to the value specified in the Tolerance Value. GRAIN \u2014 Sets the grain tolerance to the value specified in the Tolerance Value. SNAP \u2014 Sets the Input Coverage's general snapping distance to the value specified in the Tolerance Value. ", "dataType": "String"}, {"name": "tolerance_value", "isOptional": true, "description": "The value to be set for the selected option's tolerance. A Tolerance Value of zero will not be accepted for the following options: FUZZY, EDIT, NODESNAP, WEED, GRAIN, and SNAP. ", "dataType": "Double"}]},
{"syntax": "IDEdit_arc (in_cover, feature_type)", "name": "Update IDs (Coverage)", "description": "Updates User-IDs in a coverage after they have been modified in a feature attribute table. The feature attribute table is used to determine the correct User-ID for each feature in the coverage. This value is recorded in all places where the feature User-ID is stored. \r\n Learn more about how Update IDs works \r\n", "example": {"title": "IDEdit example (stand-alone script)", "description": "The following stand-alone script demonstrates how to update User-IDs in a coverage. ", "code": "# Name: IDEdit_Example.py # Description: Adds polygon labels, then updates the User-IDs for a coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"texas\" featureType = \"POLY\" #Execute IDEdit arcpy.IDEdit_arc ( inCover , featureType )"}, "usage": ["Tools such as ", "Add Item", " and ", "Calculate Field", " can be used to add or modify User-IDs in a coverage's feature attribute table before Update IDs is used.", "If the ", "Create Labels", " tool has been used to create new label points for coverage polygons, the polygon User-IDs stored in the coverage PAT are not equal to the new label point User-IDs. Create Labels stores the new label points and their User-IDs in the LAB file. Update IDs may be used to change the label point User-IDs to be equal to the User-IDs stored in the PAT.", "To change the User-IDs stored in the PAT to be equal to the label point User-IDs, use the ", "Build", " tool.", "There is no NODE option because the Update IDs tool is not necessary after updating User-IDs in the NAT file."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage for which User-IDs have been modified. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": false, "description": "Specifies the class of features for which User-IDs are to be updated. POLY \u2014 Polygon User-IDs will be updated. This is the default option. LINE \u2014 Arc User-IDs will be updated. POINT \u2014 Point User-IDs will be updated. ANNO.subclass \u2014 Annotation User-IDs will be updated. ", "dataType": "String"}]},
{"syntax": "Renode_arc (in_cover, {from_item}, {to_item})", "name": "Renumber Nodes (Coverage)", "description": "Updates  arc\u2013node topology  by renumbering nodes for the input coverage arcs and identifies arcs that share the same node locations. The tool renumbers the internal node numbers for each arc, assigns the same node number for arcs that share a common node location, and updates the FNODE# and TNODE# items in the arc attribute table (AAT) when it exists. \r\n Learn more about how Renumber Nodes works \r\n", "example": {"title": "Renode example (stand-alone script)", "description": "The following stand-alone script demonstrates how to renumber nodes in a coverage.", "code": "# Name: Renode_Example.py # Description: Renumbers the nodes in a coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"major_rds_raw\" # Execute Renode arcpy.Renode_arc ( inCover )"}, "usage": ["If you specify one elevation item, you must specify the other. These items must be numeric.", "Using ", "From Node Elevation Item", " and ", "To Node Elevation Item", " can create more nodes than are actually present. To delete unwanted nodes, run the tool again without specifying these items.", "If the input coverage has a node attribute table (NAT), Renumber Nodes does the same thing as ", "Build", " with the NODE option.", "All nodes in the input coverage are sequentially renumbered starting with 1.", "All feature attribute tables as well as ", "polygon topology", " and arc\u2013node topology are maintained by Renumber Nodes.", "Renumber Nodes updates the arc\u2013node topology by renumbering the ", "From Node Elevation Item", " and ", "To Node Elevation Item", " of each arc and identifying arcs that share the same node locations. ", "Renumber Nodes", " renumbers the internal node numbers for each arc, assigns the same node number for arcs that share a common node location, and updates the FNODE# and TNODE# items in the AAT when it exists.", "If you want to run ", "Renumber Nodes", " on a coverage in the AAT, both of these items must be present and specified for the Renumber Nodes process to work while building nodes.", "Renumber Nodes", " dissolves coincident and duplicate nodes into single nodes. Use Renumber Nodes with caution. For example, while modeling freeway ramps, you may run into situations having coincident arcs in different planes with multiple nodes sharing the same x,y coordinates. Running ", "Build", " with the LINE option will preserve this topology. However, running Renumber Nodes will collapse the coincident nodes into single nodes and destroy the topology.", "Build", " with POLY will eliminate overpass/underpass situations. Coincident nodes at different elevations will dissolve.", "The term \"elevation\" as referenced here is relative. It doesn't need to be the real world or three dimensional elevation. Rather, it signifies the elevation of the ends of an arc relative to another arc that it may meet or cross. Its primary use is in building the arc-node topology to reflect transportation network situations, such as an overpass/underpass.", "You can model an underpass/overpass situation by giving one set of node elevation values to arcs AB, BC, while arcs XY, YZ could share a different set of node elevation values. Running ", "Renumber Nodes", " with the elevation arguments will actually retain two distinct nodes at the cross point. As in real life, this will prevent a path between nodes A and Z. Many transportation data vendors make such information available in their data products.", "The \"elevation\" values for all nodes where there is no conflict could be set to zero. Only in cases where there are multiple nodes sharing the same x,y coordinates will the elevation values be looked up to determine whether they are in the same plane and dissolved or in different planes and retained.", "The coordinate precision of the coverage is unaffected by Renumber Nodes.", "Overlay commands, such as ", "Clip", " and ", "Dissolve", " will honor these extra nodes and preserve topology."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage whose nodes will be renumbered. ", "dataType": "Coverage"}, {"name": "from_item", "isOptional": true, "description": "The INFO item signifying the elevation of the from_node of each arc. ", "dataType": "String"}, {"name": "to_item", "isOptional": true, "description": "The INFO item signifying the elevation of the to_node of each arc. ", "dataType": "String"}]},
{"syntax": "AddXY_arc (in_cover, {feature_type})", "name": "Add XY Coordinates (Coverage)", "description": "Adds the items X-COORD and Y-COORD for labels or points to the input coverage PAT, or for nodes to the input coverage NAT, and calculates their values. The tool determines the  feature classes  of the input coverage and lists those available to which x,y coordinates can be added. The tool is most commonly used to gain access to a coverage's geometry to perform queries and analysis or to extract points or nodes based on their x,y location.", "example": {"title": "AddXY example (stand-alone script)", "description": "The following stand-alone script demonstrates how to add X-COORD and Y-COORD items to a coverage's PAT. The items are populated with the coordinate values from the coverage's point features.", "code": "# Name: AddXY_Example.py # Description: Adds X and Y coordinates to a coverage's point attribute table # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"wells\" featureType = \"POINT\" #Execute AddXY arcpy.AddXY_arc ( inCover , featureType )"}, "usage": ["If the items X-COORD and Y-COORD already exist, they will be overwritten.", "If the input coverage is ", "single precision", ", the items will be defined with Item width 4, Display width 12, Item type F, and Decimal places 3. If the input coverage is ", "double precision", ", the items will be defined with Item width 8, Display width 18, Item type F, and Decimal places 5. Once defined, this definition will not change even if the ", "derived precision", " is changed.", "If the point or node locations are moved after using Add XY Coordinates, the X-COORD and Y-COORD values will not represent the new locations. To update their values to the new location, rerun the tool. The values for X-COORD and Y-COORD are not modified by other tools, such as ", "Project", " and ", "Transform", ".", "If your input coverage is in a ", "geographic coordinate system", ", the X-COORD and Y-COORD represents the longitude and latitude, respectively."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing points or polygon labels whose x,y coordinates will become attributes in the PAT, or in the coverage containing nodes, to the NAT. ", "dataType": "Coverage"}, {"name": "feature_type", "isOptional": true, "description": "Type of coverage feature whose x,y coordinates will become feature attributes. POINT \u2014 Point feature coordinates to be added to the PAT including label points. This is the default option. NODE \u2014 Node feature coordinates to be added to the NAT. ", "dataType": "String"}]},
{"syntax": "Transform_arc (in_cover, out_cover, {transform_type})", "name": "Transform (Coverage)", "description": "Moves all features in a coverage based on a set of from and to control points. \r\n Learn more about how Transform works \r\n", "example": {"title": null, "description": "The following stand-alone script demonstrates how to use the Transform tool on a digitized coverage.\r\nFirst it creates an empy output coverage based on a template coverage.  The template has tics and a defined projection. The empty output coverage is named roads.\r\nThen it transforms the digitized road features into the empty roads coverage.", "code": "# Name: Transform_Example.py # Description: Transforms digitized road features into a new output coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"roads_dig\" outCover = \"C:/output/roads\" transformType = \"AFFINE\" # Create the empty output coverage arcpy.Create_arc ( outCover , \"citylim\" ) # Execute Transform arcpy.Transform_arc ( inCover , outCover , transformType ) # Print the RMS error print arcpy.GetMessage ( 4 )"}, "usage": ["The", " Output Coverage", " must already exist with a ", "tic", " file containing x,y coordinates for at least two in the desired location and units. Except for the tics, any existing features in the ", " Output Coverage", " will be replaced by features from the ", "Input Coverage", ".", "This tool scales, skews, rotates, and shifts all coordinates in a coverage, but it does not perform \"rubber sheeting\".", "The input coverage tic file and the output coverage tic file must contain at least two that have the same Tic-IDs and represent corresponding locations in both coverages. The two coverages do not have to have identical tics; only those tics whose IDs are common to both coverages will be used in the transformation.", "The tolerances of the input coverage will be transformed and retained in the output coverage.", "At least one more than the required minimum is needed to generate an RMS error with coordinate transformation.", "Since the default $ANGLE value is 0, label points that have not been rotated will remain unrotated in the output. However, if the $ANGLE value of some labels are set, the transform angle will be added to the label angles.", "All annotation subclasses present in the input coverage will be transformed to the output coverage. The position and size of input coverage annotation are transformed in the output coverage.", "Existing section tables will be copied to the output coverage. The relation between the sections and the arcs will be maintained by the transform process.", "Region information will be maintained.", "The coordinate precision of the output coverage may not be the same as the precision of the input coverage. Precision is determined by the value of the applicable precision environment at the time the coverage was created. This can be done with the ", "Create Coverage", " tool.", "Feature attributes (that is, LENGTH, AREA, and PERIMETER) from the input coverage are updated after they are transformed.", "The AFFINE transformation scales, skews, rotates, and translates all coordinates in the coverage using the same equation. Based on a minimum of three control points, this transformation can scale the x coordinates differently than the y coordinates. An AFFINE transformation can perform plane reflections as well.", "The SIMILARITY transformation scales, rotates, and translates the coverage coordinates using an equation whose parameters are generated from the coordinates. If you use the SIMILARITY option, you can use two or more tics to calculate the equation parameters. This transformation cannot apply differential scaling or skewing to the x and y coordinates, but may be useful to edgematch scanner output where a scanned map has been cut into several sections to fit on the scanner.", "Use the PROJECTIVE transformation option only if your Input Coverage was digitized directly off aerial photographs. The accuracy of the transformation will depend on the surface terrain being photographed, the angle between the camera and the ground, and the elevation from which the photograph was taken. The best results are obtained when photographing from higher elevations and located directly above a portion of the surface that is relatively flat. PROJECTIVE needs at least four control points to calculate the transformation.", "The least squares solution is used to determine the transformation. To detect errors in the tic locations or Tic-IDs that could produce an incorrect transformation, it is recommended that you provide at least one more than the minimum number of tics required."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage whose coordinates are to be transformed. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The name of an existing coverage containing destination tics . The features from the input coverage will be transformed into this coverage. ", "dataType": "Coverage"}, {"name": "transform_type", "isOptional": true, "description": "The type of coordinate transformation to be performed: AFFINE \u2014 Performs an affine transformation. At least three tics are required to define this transformation. If only two tics are matched, a similarity transformation will be applied. The AFFINE equations use six parameters. PROJECTIVE \u2014 Performs a projective transformation. This requires a minimum of four tics to define the transformation. The projective transformation is only used to transform coordinates digitized directly off high-altitude aerial photography or aerial photographs of relatively flat terrain, assuming there is no systematic distortion in the air photos. PROJECTIVE uses eight parameters. SIMILARITY \u2014 Performs a similarity transformation. At least two tics are necessary for this transformation. This transformation is also known as a Helmert, orthogonal, two-dimensional linear conformal, or four-parameter transformation. ", "dataType": "String"}]},
{"syntax": "Project_arc (in_cover, out_cover, projection_file)", "name": "Project (Coverage)", "description": "Changes the  coordinate system  of your  coverage  including its  datum  or  spheroid . \r\n Learn more about how Project works \r\n", "example": {"title": "Project example (stand-alone script)", "description": "The following stand-alone script demonstrates how to project a coverage from a geographic coordinate system into a projected coordinate system.", "code": "# Name: Project_Example.py # Description: Projects a global coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"worldgrat\" outCover = \"C:/output/sinusoidcov\" projectionFile = \"geo_to_sinusoidal.prj\" # Execute Project arcpy.Project_arc ( inCover , outCover , projectionFile )"}, "usage": ["This tool can convert a dataset from a spherical coordinate system with angular units (such as Geographic) to a planar coordinate system with linear units. Most Coverage tools, among them ", "Build", " and ", "Clean", ", assume you have a planar, two-dimensional ", "dataset", ". So if your dataset is in a ", "geographic coordinate system", " in ", "decimal degrees", " (DD, angular units), the Project tool projects your dataset to any suitable ", "projected coordinate system", " in linear units (meters or feet).", "A coverage can maintain an explicit definition of the coordinate system in which it is stored. This can be created using the ", "Define Projection", " tool. If not defined, the ", "projection", " will be listed as unknown.", "Output projection information can be specified using a Project File or from an empty output coverage. The Project File must contain both input and output projection definitions. Use of a Project File will override any projection information stored in the data's PRJ file.", "Clarke 1866 is the default spheroid if it is not inherent to the projection (such as NEWZEALAND_GRID).", "Do not name an output file the same as the Project File, even if the Project File has a ", ".prj", " extension.", "When projecting a coverage, the Output Coverage can be an existing, empty coverage. The coordinates of the Input Coverage will be projected into the coordinate system defined by the PRJ file of the Output Coverage.", "Depending on the input and output projection definitions, an arc in the input coverage may need to be clipped into more than one segment while the output coverage is being generated. This will occur whenever an arc encounters the horizon line or crosses the line of longitude opposite the central meridian.", "Whenever a vertex is encountered that cannot be projected, the previous vertex will be interpreted as the end of an arc, and the partially projected arc will be written to the output. It is possible for an arc to be split into several arcs if subsequent vertices are encountered that can be projected. In this case, the output retains the original IDs so attributes can be relinked. Examine this illustration; arcs 2 and 3 will be clipped by the horizon during projection of the line. The output coverage will contain one arc 2 but two arc 3s. In cases such as these, Project will generate arcs having duplicate User-IDs.", "If regions exist in the input coverage, regions in the output coverage will be preliminary regions. When the Build tool is used to re-create the polygon topology, region topology will also be re-created.", "\r\n", "List of supported map projections", "\r\n", " Learn about", "To find tables of predefined ", "geographic coordinate system", ", ", "projected coordinate system", ", and geographic (datum) transformations, see ", "An overview of map projections", "."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage whose coordinates are to be converted. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage whose coordinates have been converted to the new coordinate system. The output coverage may exist, but must be empty. ", "dataType": "Coverage"}, {"name": "projection_file", "isOptional": false, "description": "The name of a text file defining the input and output projection parameters. ", "dataType": "File"}]},
{"syntax": "DefineProjection_arc (in_cover, projection_file)", "name": "Define Projection (Coverage)", "description": "Records the  coordinate system  information of the Input Coverage including any associated  projection  parameters, such as  datum  and  spheroid . It creates or modifies the Input Coverage's projection definition file (PRJ) that stores the projection parameters. \r\n Learn more about how Define Projection works \r\n", "example": {"title": "DefineProjection example (stand-alone script)", "description": "The following stand-alone script demonstrates how to define the projection on a global coverage. The coverage is stored in geographic coordinates. ", "code": "# Name: DefineProjection_Example.py # Description: Defines the projection of a global coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"worldgrat\" projectionFile = \"newgeo.prj\" # Execute DefineProjection arcpy.DefineProjection_arc ( inCover , projectionFile )"}, "usage": [" Can be used if the input dataset or feature class does not have a projection defined. If the input dataset or feature class already has a projection defined, a warning will be raised but the tool will execute successfully.", "To actually project the ", "dataset", " or transform the datum or spheroid, you need to use the ", "Project", " tool, which requires that your dataset have a PRJ file.", "Define Projection will not change the coordinates of the output dataset. To project a dataset from one projection to another, you must use Project.", "To find tables of predefined ", "geographic coordinate system", ", ", "projected coordinate system", ", and geographic (datum) transformations, see ", "An overview of map projections", "."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage for which the projection information is being defined. ", "dataType": "Coverage"}, {"name": "projection_file", "isOptional": false, "description": "The name of a text file defining the input projection parameters. ", "dataType": "File"}]},
{"syntax": "JoinItem_arc (in_info_table, join_info_table, out_info_table, relate_item, {start_item}, {relate_type})", "name": "Join Info Tables (Coverage)", "description": "Joins the item definitions and values of two tables based on a shared item. Joining involves appending items ( fields ) of one table to those of another through an  attribute  or item common to both tables. A join is usually used to attach more attributes to the attribute table of a geographic layer.  A record in the Join Info Table is matched to each record of the Input Info Table when the Relate Item and Start Item values are equal. The item values from the two records are copied to the output table. \r\n Learn more about how Join Info Tables works \r\n", "example": {"title": "JoinItem example (stand-alone script)", "description": "The following stand-alone script demonstrates how to join two INFO tables.", "code": "# Name: JoinItem_Example.py # Description: Joins two INFO tables # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inInfoTable = \"maritime1/us5tx51m_p/point\" joinInfoTable = \"maritime1/us5tx51m.lights_ncode\" outInfoTable = \"C:/output/lightpoints\" relateItem = \"RCID\" # Execute JoinItem arcpy.JoinItem_arc ( inInfoTable , joinInfoTable , outInfoTable , relateItem , \"\" , \"\" )"}, "usage": ["To maintain the integrity of a feature INFO table, do not insert items before the Input INFO Table-ID (when the Output INFO Table equals the Input INFO Table).", "It is recommended that you use a Relate Item of the same definition in the two tables to be joined.", "If the same item name is encountered in both tables, the item from the Input INFO Table is maintained and the Join INFO Table item is excluded.", "To avoid loss of information and redundant data storage, ensure that items in the two tables match one-to-one. If this one-to-one correspondence does not exist, one of the following will occur:", "When specifying the Start Item, do not insert the items to be joined before the Input Info Table-ID in any feature INFO table.", "If the Input Info Table and Join Info Table have identical fields, then Join Info Tables will return a message \"No Non-duplicate items in join.file\". It means there were no new items to join.", "The speed of execution will depend on the organization of the files being joined. In general, LINK is the fastest matching operation, then LINEAR with an indexed Relate Item, then ORDERED. Although the fastest option, LINK cannot be applied to most cases.", "The LINEAR option is optimized if the Relate Item has been indexed.", "The Input Info Table and Join Info Table can contain redefined items. A redefined item can be used as the Relate Item in most cases. When a redefined item contains items of different types and is numeric, its use as the Relate Item is not recommended. Such a redefined item can result in illogical or untranslatable numbers.", "If the Join Info Table contains more than one record for each Relate Item value, the first record encountered by Join Info Tables will be used. This only applies to the LINEAR option.", "The alternate item names of the Relate Item need not be the same for Join Info Tables to work.", "You can match an integer item to a numeric item as long as the values are identical. For example, an integer item with a value of 123 will match a numeric item with a value of 123.00 but fail to match a value of 123.01 or 122.99.", "You can match a numeric item to another numeric item having a different number of decimal places as long as the values match. For example, a numeric item with two decimal places with a value of 123.45 will match a numeric item with four decimal places carrying a value of 123.4500. However, it will fail to match a value of 123.4501 or 123.4499. The number of decimal places in the Relate Item of the Output INFO Table will be the same as the number of decimal places in the Relate Item of the Input INFO Table.", "To maintain the input table, name the output table differently."], "parameters": [{"name": "in_info_table", "isInputFile": true, "isOptional": false, "description": "The INFO data file to which items and their values are to be added. ", "dataType": "INFO Table"}, {"name": "join_info_table", "isOptional": false, "description": "The INFO data file that contains the items and values to be added. ", "dataType": "INFO Table"}, {"name": "out_info_table", "isOutputFile": true, "isOptional": false, "description": "The INFO data file produced by Join Info Tables. If Output Info Table already exists, it will be replaced. ", "dataType": "INFO Table"}, {"name": "relate_item", "isOptional": false, "description": "An item contained in the Input Info Table that is used as an index to records in the Join Info Table. This can be a redefined item. ", "dataType": "String"}, {"name": "start_item", "isOptional": true, "description": "The item in the Input Info Table list after which the Join Info Table items will be inserted. The default Start Item is the last item in the Input Info Table. ", "dataType": "String"}, {"name": "relate_type", "isOptional": true, "description": "How Join Info Table records are matched to Input Info Table records. LINEAR \u2014 The values written to the Output Info Table are merged from the Input Info Table and Join Info Table records with matching Relate Item values. The Relate Item must exist for both files. Both files can be sorted in any order. This is the default option. ORDERED \u2014 The Join Info Table must be sorted on Relate Item. Both the Join Info Table and Input Info Table must contain the Relate Item. LINK \u2014 Only the Input Info Table must contain Relate Item. The Input Info Table can be sorted in any order. The Relate Item value in each record of the Input Info Table indicates the record number in the Join Info Table that is to be merged. This method is used when relating an INFO file to another file on the basis of its internal record number. ", "dataType": "String"}]},
{"syntax": "DropItem_arc (in_info_table, out_info_table, drop_item)", "name": "Drop Item (Coverage)", "description": "Deletes one or more items from an INFO table.", "example": {"title": "DropItem example (stand-alone script)", "description": "The following stand-alone script demonstrates how to drop an item from a coverage's polygon attribute table.\r\n", "code": "# Name: DropItem_Example.py # Description: Drops an item from a coverage's PAT to make an INFO table # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inInfoTable = \"stand/polygon\" outInfoTable = \"C:/output/standrel\" dropItem = \"VALUE-PER-METER\" # Execute DropItem arcpy.DropItem_arc ( inInfoTable , outInfoTable , dropItem )"}, "usage": ["The ", "Output Info Table", " can be the same name as the ", "Input Info Table", ". However, if the ", "Output Info Table", " already exists, it will be replaced.", "Do not drop items before the User-ID in feature attribute tables. Redefined items will be dropped if their item definition relates to an item that was dropped.", "Up to 100 items can be dropped with Drop Item.", "The ", "Items to Drop", " parameter's ", "Add Item", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the Items to Drop parameter may not be populated with item names. The Add Item button allows you to add expected fields so you can complete the Drop Index dialog box and continue to build your model."], "parameters": [{"name": "in_info_table", "isInputFile": true, "isOptional": false, "description": "The INFO table containing the items to be dropped. ", "dataType": "INFO Table"}, {"name": "out_info_table", "isOutputFile": true, "isOptional": false, "description": "The INFO table to be created. ", "dataType": "INFO Table"}, {"name": "drop_item", "isOptional": false, "description": "The item or items to be dropped from the input table. ", "dataType": "INFO Item"}]},
{"syntax": "AddItem_arc (in_info_table, out_info_table, item_name, item_width, output_width, item_type, {decimal_places}, {start_item})", "name": "Add Item (Coverage)", "description": "Adds a new blank or zero item to a new or existing INFO table.", "example": {"title": " AddItem example (stand-alone script)", "description": "The following stand-alone script demonstrates how to add a binary item to a coverage's point attribute table. It uses the same table as input and output so that no new table gets created. ", "code": "# Name: AddItem_Example.py # Description: Adds an item to a coverage's polygon attribute table # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inInfoTable = \"tra_airport/polygon\" outInfoTable = inInfoTable itemName = \"sites\" itemWidth = 4 outputWidth = 5 itemType = \"BINARY\" # Execute AddItem arcpy.AddItem_arc ( inInfoTable , outInfoTable , itemName , itemWidth , outputWidth , itemType , \"\" , \"\" )"}, "usage": ["This tool creates a new INFO table that is a copy of the Input Info Table with a new item containing blanks or zeroes. Data in other items are copied from the Input Info Table to the Output Info Table. When the Input Info Table and the Output Info Table have the same name, the item is added to the Input Info Table;otherwise, the Output Info Table is created as a new internal INFO table, and the Input Info Table is not altered.", "Do not insert items before the cover-ID in a feature attribute table.", "Do not insert items before the COUNT item in a grid VAT.", "If Item Type defines a character, blanks are inserted for each record. If Item Type defines a numeric item, then zeroes are inserted for each record.", "Adding items to coverage tic files is not recommended. Negative results may occur depending on the operations performed on the coverage. ", "Clean", ", ", "Copy", ", and other tools will maintain only the original tic items.", "A similar tool, ", "Join Info Tables", ", adds items by merging two INFO tables. ", "Add Item", " is different from ", "Join Info Tables", " in that it adds only one item at a time, and the new item values are always zero or blank. ", "Join Info Tables", " may be preferable when many items are to be added from an existing INFO table."], "parameters": [{"name": "in_info_table", "isInputFile": true, "isOptional": false, "description": "The INFO table to which the item is to be added. ", "dataType": "INFO Table"}, {"name": "out_info_table", "isOutputFile": true, "isOptional": false, "description": "The INFO table to be created. ", "dataType": "INFO Table"}, {"name": "item_name", "isOptional": false, "description": "The new item to be added to the INFO table. ", "dataType": "String"}, {"name": "item_width", "isOptional": false, "description": "The INFO width of the added item. Supported widths: BINARY\u2014Either 2 or 4 bytes CHARACTER\u20141 to 320 characters DATE\u2014Always 8 bytes; stored as mm/dd/yy FLOATING\u2014Either 4 bytes (single precision) or 8 bytes (double precision) NUMERIC\u20141 to 16 digits INTEGER\u20141 to 16 digits", "dataType": "Long"}, {"name": "output_width", "isOutputFile": true, "isOptional": false, "description": "The output width of the added item. This is the number of characters used to display an item value. For example, in a 2-byte integer (item type BINARY), values can be as high as 32767, which requires five characters for display. Dates can be displayed using eight (mm/dd/yy) or ten (mm/dd/yyyy) characters. For international date displays, months and days can be switched (for example, dd/mm/yy). ", "dataType": "Long"}, {"name": "item_type", "isOptional": false, "description": "The INFO item type of the added item. BINARY \u2014 Binary integer; requires less storage than integer CHARACTER \u2014 Text DATE \u2014 Date; stores day, month, and year FLOATING \u2014 Floating-point binary number, either single or double precision NUMERIC \u2014 Decimal number stored as one byte per digit INTEGER \u2014 Integer number stored as one byte per digit ", "dataType": "String"}, {"name": "decimal_places", "isOptional": true, "description": "The number of decimal places for the added item. This needs to be specified for INFO item types NUMERIC and FLOATING. ", "dataType": "Long"}, {"name": "start_item", "isOptional": true, "description": "The item in the in_info_table after which the new item is to be added. The default start_item is the last item in the in_info_table. ", "dataType": "INFO Item"}]},
{"syntax": "IndexItem_arc (in_info_table, index_item)", "name": "Index Item (Coverage)", "description": "Creates an attribute index to increase access speed to the specified item during query operations.", "example": {"title": "IndexItem example (stand-alone script)", "description": "The following stand-alone script demonstrates how to index an item in a coverage, and then select features using the indexed item.", "code": "# Name: IndexItem_Example.py # Description: Indexes an item then uses that item to select features. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inInfoTable = \"stream/arc\" indexItem = \"STRM_ORD\" inCover = \"stream\" outCover = \"C:/output/lowerstreams\" infoExpress = [ \"RESELECT STRM_ORD > 3\" ] # Execute IndexItem arcpy.IndexItem_arc ( inInfoTable , indexItem ) arcpy.Reselect_arc ( inCover , outCover , infoExpress )"}, "usage": ["Indexed items speed up selection operations of large INFO files.", "Item indexes are preserved when the coverage or INFO table is copied to a new location.", "Item indexes are preserved when exporting and importing a coverage to an interchange (.e00) file using the ", "Export To Interchange File", " and ", "Import From Interchange File", " tools.", "Items must be indexed to support native mode reselects against external database management system (DBMS) tables.", "An index file becomes outdated if the following commands are used on the INFO table: ", "Calculate Field", ", ", "Update", ", ", "Build", ", or ", "Clean", ". Attempting to use the outdated index will cause the following message to be generated: \"Index file is obsolete.\" If you receive this message, rebuild the index file and continue.", "The following commands are affected by indexed items: ", "Eliminate", ", ", "Select", ", and ", "Join Info Tables", "."], "parameters": [{"name": "in_info_table", "isInputFile": true, "isOptional": false, "description": "The name of the INFO table containing the item to be indexed ", "dataType": "INFO Table"}, {"name": "index_item", "isOptional": false, "description": "The name of the item to be indexed ", "dataType": "INFO Item"}]},
{"syntax": "DropIndex_arc (in_info_table, {index_item})", "name": "Drop Index (Coverage)", "description": "Drops an attribute index from the specified item and INFO table.", "example": {"title": "DropIndex example (stand-alone script)", "description": "The following stand-alone script demonstrates how to drop the index from an item in a coverage's polygon attribute table. ", "code": "# Name: DropIndex_Example.py # Description: Drops the index from an item in a polygon featureclass # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inInfoTable = \"tongass1/polygon\" indexItem = \"IN-FISH\" # Execute DropIndex arcpy.DropIndex_arc ( inInfoTable , indexItem )"}, "usage": ["Many operations in ArcGIS will not maintain item indexes. Anytime the coverage or feature attribute table is updated, item indexes are deleted.", "If there are no indexes on a coverage, the dialog box will not show any fields on which to drop an index.", "Item indexes are preserved when INFO tables are duplicated with the ", "Copy", " tool or ", "Copy Features", " tool.", "Item indexes are stored in the workspace INFO directory. They are named with an ARC prefix (for example, ARC0001R.003). This file is an index for the third item in the INFO table whose internal name is ARC0001.DAT.", "Indexed items speed up ArcGIS selection and relate operations. The ", "Index Item", " tool is used to create an item index.", "The Indexed Item parameter's ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the Indexed Item parameter may not be populated with field names. The Add Field button allows you to add expected fields so you can complete the Drop Index dialog and continue to build your model."], "parameters": [{"name": "in_info_table", "isInputFile": true, "isOptional": false, "description": "The name of the INFO table containing the item whose index is to be deleted. ", "dataType": "INFO Table"}, {"name": "index_item", "isOptional": false, "description": "Selects the item indexes from the input INFO table to be removed. If no Index Item is given, all item indexes for the file will be dropped. The Add Item button, which is used only in ModelBuilder, allows you to add expected items so you can complete the dialog and continue to build your model. ", "dataType": "INFO Item"}]},
{"syntax": "SimplifyLineOrPolygon_arc (in_cover, out_cover, simplification_tolerance, {simplification_operator}, {ErrorCheck})", "name": "Simplify Line Or Polygon (Coverage)", "description": "Simplifies a line or a polygon boundary by removing small fluctuations or extraneous bends from it while preserving its essential shape. \r\n Learn more about how Simplify Line Or Polygon works \r\n", "example": {"title": "SimplifyLineOrPolygon example (stand-alone script)", "description": "The following stand-alone script demonstrates how to simplify a lake coverage.", "code": "# Name: SimplifyLineOrPolygon_Example.py # Description: Simplifies a lake coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"lakeshore\" outCover = \"C:/output/cartolake\" simplificationTolerance = 110 simplificationOperator = \"BEND_SIMPLIFY\" # Execute SimplifyLineOrPolygon arcpy.SimplifyLineOrPolygon_arc ( inCover , outCover , simplificationTolerance , simplificationOperator , \"\" )"}, "usage": ["If the input coverage already contains intersecting lines, or if you want a quick result and don't care about topological errors in the output coverage, use the default option, which is not to Check for topological errors. Any topological errors introduced by the process will not be checked and corrected. If the input coverage contains intersecting lines, and you choose to Check for topological errors, it will fail at the input data validation, and the program will terminate with a message: \"Intersecting lines are found in in_cover. The program is terminated\".", "If the input coverage contains no intersecting lines, check the Check for topological errors option to find and avoid errors generated by the simplification process. If any topological errors are found, the involved arcs will be regeneralized using a reduced tolerance. The result will be checked for topological errors again. The process iterates until no more errors are found. With this option, the program will run much longer than with the default option.", "Unless there is no input_coverage.AAT, the output_coverage.AAT will contain a new item, TOLFLAG, which stores the tolerance in decimal numbers used for each arc. Tolerances smaller than the Simplification Tolerance indicate arcs that are undersimplified to avoid topological errors.", "If polygon topology exists for the input coverage, it is not preserved in the output coverage. The coverage PAT, however, is preserved. Build must be used to re-create polygon topology. If labels exit, they will be preserved. If a label falls outside the polygon boundary as the boundary is simplified, it will be moved just inside the nearest line segment of the polygon boundary.", "If regions exist in the Input Coverage, regions in the Output Coverage will be preliminary regions. When ", "Build", " is used to re-create the polygon topology, region topology will also be re-created.", "The coordinate precision of the output coverage is determined by the current processing rule set by the ", "derived precision", " environment setting. If the processing rule has not been established, the output coverage will be the same precision as the input coverage."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing arcs or polygons to be simplified. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be created. The output coverage name must be different from the input coverage name. ", "dataType": "Coverage"}, {"name": "simplification_tolerance", "isOptional": false, "description": "Sets the tolerance in coverage units. A tolerance must be specified and must be greater than zero. ", "dataType": "Double"}, {"name": "simplification_operator", "isOptional": true, "description": "Specifies the simplification operator. POINT_REMOVE \u2014 Uses the Douglas-Peucker algorithm for line simplification with enhancements. This operator is the default. BEND_SIMPLIFY \u2014 Detects and removes extraneous bends from the original line. ", "dataType": "String"}, {"name": "ErrorCheck", "isOptional": true, "description": "Specifies whether to check for topological errors, including line-crossing, line-overlapping, zero-length lines, collapsed polygons, and holes falling outside of polygons. NO_ERROR_CHECK \u2014 Specifies to not check for topological errors. This is the default. ERROR_CHECK \u2014 Specifies to check for topological errors. ", "dataType": "Boolean"}]},
{"syntax": "SimplifyBuilding_arc (in_cover, out_cover, simplification_tolerance, {minimum_area}, {selection_file}, {CheckConflict})", "name": "Simplify Building (Coverage)", "description": "Simplifies the boundary or footprint of building polygons while maintaining their essential shape and size. \r\n Learn more about how Simplify Building (Coverage Tools) works \r\n", "example": {"title": "Buffer example (stand-alone script)", "description": "The following stand-alone script demonstrates how to simplify a building coverage.", "code": "# Name: SimplifyBuilding_Example.py # Description: Simplifies a building coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"campus\" outCover = \"C:/output/cartocampus\" simplificationTolerance = 6 minimumArea = 55 checkConflict = \"CHECK_CONFLICT\" # Execute SimplifyBuilding arcpy.SimplifyBuilding_arc ( inCover , outCover , simplificationTolerance , minimumArea , \"\" , checkConflict ) arcpy.Clean_arc ( outCover )"}, "usage": ["The input coverage must have a polygon topology.", "Due to the possibility of creating overlapping boundaries, preliminary regions are used as the resulting features. To create fully built regions from the preliminary regions, use ", "Clean", " with the POLY option on the out_cover.", "This tool can run for quite a long time if the in_cover is large. To make it run faster, use the Arc command INDEX (see ", "ArcInfo Workstation", " Help for command reference) on the input coverage to create a spatial index.", "The following info tables will be created: output_coverage.bnd, output_coverage.tic, output_coverage.aat, and output_coverage.patbldgsim.", "The out_cover.aat will store the new arcs and not carry any information from the input_coverage.aat.", "All route and region attributes and topology from the in_cover will be lost.", "All attributes in the input_coverage.pat, except AREA and PERIMETER, will be copied to the output_coverage.patbldgsim. The tool will not compute the AREA values of the preliminary regions, but set the AREA values in the output_coverage.PATBLDGSIM to zero and compute the PERIMETER. The ", "Clean", " tool with the POLY option will calculate the AREA values.", "The output_coverage.patbldgsim will also contain two new items: BDS-STATUS and BDS-GROUP. The item BDS-STATUS records the following simplification status:", "The item BDS-GROUP stores a unique positive value for each group of connected buildings. A single building will receive a BDS-GROUP value of 0. A single building with a hole will receive a unique negative value for both outer and inner boundaries.", "If a selection file is not specified or if it contains no polygons, all polygons in the input coverage are selected for simplification. If the selection file does not contain the polygon feature class or if it does not match the input coverage (that is, the selection file was not derived from the input coverage), the program will stop.", "If more than 500 adjacent polygons in a group are detected, the tool will stop and a message will be returned: \"More than 500 adjacent polygons detected. The process has stopped.\""], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The input coverage containing building polygons. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage containing simplified buildings as preliminary regions with a subclass BLDGSIM. The output coverage name must be different from the input coverage name. ", "dataType": "Coverage"}, {"name": "simplification_tolerance", "isOptional": false, "description": "Sets the simplification tolerance in coverage units. A tolerance must be specified and must be greater than zero. ", "dataType": "Double"}, {"name": "minimum_area", "isOptional": true, "description": "Sets the minimum area to be retained in coverage units. The default is the square of the simplification tolerance. Enter 0 to include all buildings. ", "dataType": "Double"}, {"name": "selection_file", "isOptional": true, "description": "A special file created using the ArcPlot command WRITESELECT (see ArcInfo Workstation Help for command reference). It identifies coverage features selected in ArcPlot. This option allows you to simplify selected buildings in the input coverage. ", "dataType": "File"}, {"name": "CheckConflict", "isOptional": true, "description": "Specifies whether or not to check for potential conflicts, that is, overlapping or touching, among buildings. NOT_CHECK \u2014 Specifies not to check for potential conflicts; the resulting buildings may overlap. CHECK_CONFLICT \u2014 Specifies to check for potential conflicts so that some of the conflicts can be avoided and flagged. ", "dataType": "Boolean"}]},
{"syntax": "FindConflicts_arc (in_cover, out_cover, conflict_distance)", "name": "Find Conflicts (Coverage)", "description": "Finds where simplified buildings overlap or are too close to each other, based on a specified distance.", "example": {"title": "FindConflicts example (stand-alone script)", "description": "The following stand-alone script demonstrates how to use the FindConflicts tool.\r\n", "code": "# Name: SimplifyBuilding_Example.py # Description: Simplifies a building coverage and finds conflicts in the output # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables for SimplifyBuilding inSimplifyCover = \"campus\" outSimplifyCover = \"C:/output/tempcampus\" simplificationTolerance = 6 minimumArea = 55 # Set local variables for FindConflicts inCover = outSimplifyCover outCover = \"C:/output/cartocampus\" conflictDistance = 5.5 # Execute SimplifyBuilding and Clean arcpy.SimplifyBuilding_arc ( inSimplifyCover , outSimplifyCover , simplificationTolerance , minimumArea , \"\" , \"\" ) arcpy.Clean_arc ( outSimplifyCover ) # Execute FindConflicts arcpy.FindConflicts_arc ( inCover , outCover , conflictDistance )"}, "usage": ["Finding conflicts among simplified buildings is a part of the postprocessing of the ", "Simplify Building", " tool. Therefore, the input coverage must have buildings as regions created by the Simplify Building tool followed by the ", "Clean", " tool with the POLY option.", "This tool will help you locate where buildings are within the specified distance. A buffer will be created around each building or group of connected buildings. Overlapping buffers indicate a conflict. An item, FREQUENCY, will be added to the out_cover.PAT, carrying the number of buffers that share each polygon. A FREQUENCY value of 1 means no conflict; a value of 2 or more, according to how many buffers overlap, indicates a conflict area. Buildings connected in one group are not considered conflicting with each other. Only the outer boundary of such a group will be checked with neighboring buildings or groups of buildings.", "The output coverage is created only if conflicts are identified. Since the input buildings are regions, the buffers in the output coverage are also regions with a subclass BUF. You can select and view the conflict areas (the polygons with a FREQUENCY value of 2 or more) and make necessary edits."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The input coverage containing buildings as regions, with the subclass BLDGSIM and the item BDS-GROUP, obtained by the Simplify Building tool followed by the Clean tool with the POLY option. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage containing overlapping region buffers, with a subclass BUF, that show spatial conflicts among buildings. This coverage will only be created when conflicts are found. The <out_cover> name must be different from the <in_cover> name. ", "dataType": "Coverage"}, {"name": "conflict_distance", "isOptional": false, "description": "Sets the conflict distance in coverage units. Buildings within this distance are considered in spatial conflict. The distance must be greater than 0. ", "dataType": "Double"}]},
{"syntax": "Eliminate_arc (in_cover, out_cover, info_express, {polygon_boundary}, {feature_type}, {selection_file}, {polygon_option})", "name": "Eliminate (Coverage)", "description": "Merges the selected polygons with neighboring polygons if they have the largest shared border or the largest area. Eliminate  is often used to remove sliver polygons created during polygon overlay or buffering. With the LINE option,  Eliminate  merges selected arcs separated by pseudo nodes into single arcs. \r\n Learn more about how Eliminate works \r\n", "example": {"title": "Eliminate example (stand-alone script)", "description": "The following stand-alone script demonstrates how to remove sliver polygons from a coverage.\r\n", "code": "# Name: Eliminate_Example.py # Description: Removes sliver polygons from an input coverage # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"txlnd\" outCover = \"C:/output/texout\" infoExpress = \"reselect AREA LE 68000\" polygonBoundary = \"KEEP_EDGE\" featureType = \"POLY\" polygonOption = \"AREA\" # Execute Eliminate arcpy.Eliminate_arc ( inCover , outCover , infoExpress , polygonBoundary , featureType , \"\" , polygonOption )"}, "usage": ["Only the selected set of polygons or lines will be eliminated. Polygons that border the background polygon will not be eliminated when KEEP_EDGE is specified.", "For the POLY option, an arc with a negative User-ID will never be eliminated, even if it's the longest arc in a selected polygon. When this happens, the next longest arc is eliminated unless it's along the coverage boundary when the Keep polygon boundary option is selected (KEEP_EDGE).", "It's possible to eliminate a sliver polygon existing as a neighbor to the background polygon, by not selecting the Keep polygon boundary option (NO_KEEP_EDGE) and still maintain the outer polygon boundary. ", "Build", " the coverage with the LINE option, then ", "Reselect", " on the cover.AAT using the following logical expression:", "For the feature type POLY option, route systems attached to eliminated arcs will be removed, and ARCLINK# will be renumbered. For the feature type LINE option, route systems belonging to eliminated arcs (whether actually removed or not) will be deleted. Those belonging to arcs that increased in length by absorbing eliminated arcs will have their F-POS and T-POS adjusted so they will occupy the same set of locations as previously.", "For the LINE option, all arcs must be split where they intersect other arcs. Eliminate will abort execution if it encounters overlapping arcs. ", "Clean", " with LINE can be used to split arcs.", "If the Input Coverage has an NAT (node attribute table), it will be maintained in the Output Coverage.", "Use of indexed items can speed up logical feature selection in Eliminate. See Index Item for details.", "Input Coverage annotation is copied to the Output Coverage.", "The projection file (PRJ) will be copied to the Output Coverage.", "If the Input Coverage has regions, they are maintained in the Output Coverage with the appropriate polygons eliminated from the region subclasses.", "\r\n"], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage whose selected polygons or arcs will be merged into neighboring features. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The new coverage with all the selected sliver polygons merged into larger features. There should be a smaller number of polygons than the Input Coverage contains. ", "dataType": "Coverage"}, {"name": "info_express", "isOptional": false, "description": "An INFO query containing one or more logical expressions to select features from the input coverage. Reselect\u2014Reduces the selected set of records with a selection expression to those that meet its criteria. If no selection expression follows, the selected set will be empty. Aselect\u2014Adds unselected records that meet the selection expression criteria to the currently selected set. If no selection expression follows, the selected set will contain all features. Nselect\u2014Reverses the current selection to the unselected set.", "dataType": "INFO Expression"}, {"name": "polygon_boundary", "isOptional": true, "description": "Ensures that polygons along the coverage boundary are not altered. NO_KEEP_EDGE \u2014 Allows the elimination of outer polygon boundaries. This is the default. KEEP_EDGE \u2014 Is only used with the POLYGON option. Any polygon which is a neighbor of the background polygon will not be eliminated when KEEP_EDGE is specified. ", "dataType": "Boolean"}, {"name": "feature_type", "isOptional": true, "description": "The feature class(es) to be eliminated in the Output Coverage. This parameter is only used for polygon coverages. POLY \u2014 Polygon features will be eliminated; an AAT will not be created for the Output Coverage. LINE \u2014 Line features will be eliminated; a PAT will not be created for the Output Coverage. ", "dataType": "String"}, {"name": "selection_file", "isOptional": true, "description": "A Selection File is a preexisting file that identifies which features will be used. ", "dataType": "File"}, {"name": "polygon_option", "isOptional": true, "description": "Specifies which method will be used for eliminating polygons. This parameter is only used for polygon coverages. BORDER \u2014 Merges a selected polygon with a neighboring unselected polygon by dropping an Arc. The neighboring polygon is the one with the longest shared border. This is the default and the way Eliminate worked with the POLY option in all pre-6.1.1 releases. AREA \u2014 Merges a selected polygon with a neighboring unselected polygon by dropping an Arc. The neighboring polygon is the one with the largest area. ", "dataType": "Boolean"}]},
{"syntax": "Dissolve_arc (in_cover, out_cover, dissolve_item, {feature_type})", "name": "Dissolve (Coverage)", "description": "Creates a new coverage by merging adjacent polygons, lines, or regions that have the same value for a specified item.", "example": {"title": "Dissolve example (stand-alone script)", "description": "The following stand-alone script demonstrates how to dissolve polygons into larger sections.\r\n", "code": "# Name: Dissolve_Example.py # Description: Dissolves polygons into larger sections # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"tra_airport\" outCover = \"C:/output/airport_sect\" dissolveItem = \"section\" featureType = \"POLY\" # Execute Dissolve arcpy.Dissolve_arc ( inCover , outCover , dissolveItem , featureType )"}, "usage": ["This tool is used to create a simplified coverage from one that is more complex. Although the input coverage may contain information concerning many feature attributes, the output coverage contains information only about the dissolve item.", "The merging of polygons with this tool is the counterpart of intersecting polygons in overlays. ", "Dissolve", " will remove the boundaries.", "The output coverage cannot already exist.", "Dissolve", " maintains linear data belonging to different planar graphs in the same coverage. These may include arcs representing utility cables at different levels or a road passing over a stream. If there are arcs that appear to intersect, but do not, nodes will not be inserted at the apparent intersection. Coincident and colinear line segments are preserved; additional vertices may be inserted. Two colinear arcs, one representing a road that follows the second, a stream, are maintained as colinear.", "With the POLY option, ", "Dissolve", " will remove dangling arcs and pseudo nodes. The output coverage PAT with the POLY option or the output coverage AAT with the LINE option will only contain the dissolve item but no additional attributes. If #ALL is used as the dissolve item, input coverage item definitions and data are preserved in the output coverage but User-IDs will be altered.", "If a pseudo node has attributes that are not zero or blank, arcs will not be joined for polygon features.", "Route systems are copied to the output coverage. If the NET option is specified, sections attached to any deleted arcs will be removed and ARCLINK# will be renumbered. If the LINE option is specified, sections residing on merged arcs will have their F-POS and T-POS values adjusted so they occupy the same set of locations as previously.", "This tool maintains all section subclasses.", "Region subclasses are maintained with the POLY and NET options. The regions may change shape when polygons or arcs are dissolved.", "With the NET option, ", "Dissolve", " will not remove dangling arcs or pseudo nodes. The output coverage AAT will have the same items as the input coverage; only the records for the dissolved arcs will be removed.", "With the NET option, arcs will be joined if all item values for one arc are identical to all item values of the other arc on an item-by-item basis.", "The NAT will be maintained for any nodes whose arcs were dissolved.", "The polygons output by ", "Dissolve", " with POLY will contain both topology and attributes. The items in the PAT will be AREA, PERIMETER, COVER#, COVER-ID, and the dissolve item.", "If all polygon attributes are coded with the same value, then ", "Dissolve", " with POLY using the #ALL option will leave you with a null set. If you want to retain the bounding or outer polygon, be sure to code the universe polygon with a different value than the rest of the set.", "The dissolve item may be a redefined item. If a redefined item is used, its begin column must be the begin column of a normal item, and its end column must be the end column of a normal item. The redefined item may span several normal items. For either a normal or a redefined item, all redefined items that are wholly contained within its range will be transferred to the output feature attribute table.", "The lines output by ", "Dissolve", " with LINE will contain attributes. The items in the AAT will be FNODE#, TNODE#, LPOLY#, RPOLY#, LENGTH, COVER#, COVER-ID, and the dissolve item. With the LINE option, redefined items are maintained. Dissolve is allowed on equation items, producing an item called EQUATION.", "The regions output by ", "Dissolve", " with REGION.subclass will contain topology and attributes. The items in the region PAT will be AREA, PERIMETER, SUBCLASS#, SUBCLASS-ID, and the dissolve item. If polygon and arc attributes exist in the input coverage, they will be maintained in the output coverage.", "The coordinate precision of the output coverage is determined by the current processing environment settings. The default precision setting for derived coverages is HIGHEST; therefore, Dissolve will create an output coverage in the same precision as the input coverage.", "The projection file (PRJ) will be copied to the output coverage.", "The maximum number of arcs connected to a node that ", "Dissolve", " can handle in a coverage is 100.", "Dissolve", " eliminates nodes or arcs between adjacent lines or polygons containing equal values for the dissolve item.", "With the POLY or NET option, if merged polygons contain label points, one of the points is preserved in the output coverage. If the POLY option has been specified, dangling arcs of any length are removed; pseudo nodes are also removed, unless they are the only node in a polygon (for example, an island or donut).", "With the LINE option, ", "Dissolve", " eliminates nodes between adjacent arcs containing equal values for the dissolve item. The new cover-ID of the merged arc will be the lowest cover-ID of the combined arcs. If a resulting arc reaches the 500-vertex-per-arc size limit, a pseudo node is added and a new arc is begun.", "With the NET option, arcs are joined as units, and before two arcs are joined, the total number of vertices is compared to the 500-vertex limit. If greater than 500, the join is not done."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing features to be dissolved. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The coverage to be created. The output coverage cannot already exist. ", "dataType": "Coverage"}, {"name": "dissolve_item", "isOptional": false, "description": "The item in the in_cover feature attribute table that is used to dissolve features. Dissolve_item\u2014An item name will be used to perform the dissolve. The item may be a redefined item. #ALL\u2014All items past the cover-ID in the PAT, AAT, or region subclass PAT will be used as a single dissolve item. If there are no items past the cover-ID, then the cover-ID will be used.", "dataType": "String"}, {"name": "feature_type", "isOptional": true, "description": "The feature classes to be preserved in the output coverage: POLY \u2014 Polygons will be dissolved; an AAT will not be created for the output coverage. This is the default option. LINE \u2014 Nodes will be dissolved; a PAT will not be created for the output coverage. NET \u2014 Polygons will be dissolved, and both a PAT and AAT will be created for the output coverage. REGION.subclass \u2014 Region subclass will be dissolved, and all existing attributes of the input coverage will be maintained in the output coverage. ", "dataType": "String"}]},
{"syntax": "CollapseDualLinestoCenterline_arc (in_cover, out_cover, maximum_width, {minimum_width})", "name": "Collapse Dual Lines To Centerline (Coverage)", "description": "Derives centerlines (single lines) from dual-line features, such as road casings, based on specified width tolerances. \r\n Learn more about how Collapse Dual Lines To Centerline works \r\n", "example": {"title": " CollapseDualLinesToCenterline example (stand-alone script)", "description": "The following stand-alone script demonstrates how to create a line coverage of street centerlines from a line coverage of street casings.", "code": "# Name: CollapseDualLinesToCenterline_Example.py # Description: Creates street centerlines from a street casing coverage. # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"streets\" outCover = \"C:/output/centerlines\" maximumWidth = 50 # Execute CollapseDualLinesToCenterline arcpy.CollapseDualLinesToCenterline_arc ( inCover , outCover , maximumWidth , \"\" )"}, "usage": ["In addition to the standard items, the Output Coverage.AAT will contain the following five new items:", "The values for item_width, output_width, and item_type in the item definition for all these items are 4, 5, and B."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing near parallel dual lines, such as road casings, from which centerlines are derived. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage containing the derived centerlines. The output coverage name must be different from the input coverage name. ", "dataType": "Coverage"}, {"name": "maximum_width", "isOptional": false, "description": "Sets the maximum width in coverage units. ", "dataType": "Double"}, {"name": "minimum_width", "isOptional": true, "description": "Sets the minimum width in coverage units. The default is zero. ", "dataType": "Double"}]},
{"syntax": "AggregatePolygons_arc (in_cover, out_cover, cell_size, distance, {orthogonal_option})", "name": "Aggregate Polygons (Coverage)", "description": "Combines disjoint and adjacent polygons into new area features based on a distance. \r\n Learn more about how Aggregate Polygons works \r\n", "example": {"title": "AggregatePolygons example (stand-alone script)", "description": "The following stand-alone script demonstrates how to use the AggregatePolygons tool.\r\n", "code": "# Name: AggregatePolygons_Example.py # Description: Aggregates city limits polygons into a county boundary # Requirements: ArcInfo Workstation # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"C:/data\" # Set local variables inCover = \"citylim\" outCover = \"c:/output/countybnd\" cellSize = 10 distance = 150 orthogonalOption = \"NON_ORTHOGONAL\" # Execute AggregatePolygons arcpy.AggregatePolygons_arc ( inCover , outCover , cellSize , distance , orthogonalOption )"}, "usage": ["This tool involves GRID functions and requires the ", "ArcGIS Spatial Analyst extension", " software license.", "The ", "Input Coverage", " must have a polygon topology.", "Due to the possibility of creating overlapping boundaries, preliminary regions are used as the resulting features. To create fully built regions from the preliminary regions, use the ", "Clean", " tool with the POLY option on the Output Coverage.", "The output coverage will not contain any attributes from the input coverage but will have a one-to-many relationship table, output_coverage.RXP (an INFO file), that links the aggregated preliminary regions to their source polygons. The .RXP extension stands for regions (output) cross-referencing polygons (input). This table will contain two items: output_coverage# and input_coverage#. With this link, you can derive attributes for the output features. The link can become incorrect when using the Clean tool to obtain region topology with a large fuzzy tolerance that causes small regions to collapse and disappear; the output_coverage# numbers will be reordered and not match the .RXP table."], "parameters": [{"name": "in_cover", "isInputFile": true, "isOptional": false, "description": "The coverage containing polygons to be aggregated. ", "dataType": "Coverage"}, {"name": "out_cover", "isOutputFile": true, "isOptional": false, "description": "The output coverage containing aggregated features as preliminary regions with a subclass AREAAGG. The output coverage name must be different from the input coverage name. ", "dataType": "Coverage"}, {"name": "cell_size", "isOptional": false, "description": "Sets cell size in coverage units for grid conversion. Cell size must be greater than 0. ", "dataType": "Double"}, {"name": "distance", "isOptional": false, "description": "Sets the aggregation distance in coverage units. A distance must be equal to or greater than the cell size. ", "dataType": "Double"}, {"name": "orthogonal_option", "isOptional": true, "description": "Specifies the characteristic of the input features that will be preserved when constructing the aggregated boundaries. NON_ORTHOGONAL \u2014 Specifies natural features, such as vegetation or soil polygons, which unlikely contain orthogonal shapes. This is the default. ORTHOGONAL \u2014 Specifies building-like features for which orthogonal shapes will be preserved and constructed. ", "dataType": "Boolean"}]},
{"syntax": "CreateDomain_management (in_workspace, domain_name, domain_description, field_type, {domain_type}, {split_policy}, {merge_policy})", "name": "Create Domain (Data Management)", "description": "Creates an attribute domain in the specified workspace.", "example": {"title": "Create Domain Example (Python Window)", "description": "The following Python window script demonstrates how to use the CreateDomain function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CreateDomain_management ( \"montgomery.gdb\" , \"Materials\" , \"Valid pipe materials\" , \"TEXT\" , \"CODED\" )"}, "usage": ["Domain management involves the following steps:", "Coded value domains support only default value and duplicate split policies and default value merge policies.", "Range domains support all split and merge policies. After a Split or Merge operation, the attribute values of output features are calculated based on the numeric values of the input features and the specified split or merge policy.", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The geodatabase that will contain the new domain. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the domain that will be created. ", "dataType": "String"}, {"name": "domain_description", "isOptional": false, "description": "The description of the domain that will be created. ", "dataType": "String"}, {"name": "field_type", "isOptional": false, "description": "The type of attribute domain to create. Attribute domains are rules that describe the legal values of a field type. Specify a field type that matches the data type of the field to which the attribute domain will be assigned. SHORT \u2014 Numeric values without fractional values within a specific range; coded values. LONG \u2014 Numeric values without fractional values within a specific range. FLOAT \u2014 Numeric values with fractional values within a specific range. DOUBLE \u2014 Numeric values with fractional values within a specific range. TEXT \u2014 Names or other textual qualities. DATE \u2014 Date and/or time. ", "dataType": "String"}, {"name": "domain_type", "isOptional": true, "description": "The domain type to create: CODED \u2014 Specifies a valid set of values for an attribute. For example, a coded value domain might specify valid pipe material values: CL\u2014cast iron pipe, DL\u2014ductile iron pipe, or ACP\u2014asbestos concrete pipe. RANGE \u2014 Specifies a valid range of values for a numeric attribute. For example, if distribution water mains have a pressure between 50 and 75 psi, then a range domain would specify these minimum and maximum values. ", "dataType": "String"}, {"name": "split_policy", "isOptional": true, "description": "The split policy of the created domain. The behavior of an attribute's values when a feature that is split is controlled by its split policy. DEFAULT \u2014 The attributes of the two resulting features take on the default value of the attribute of the given feature class or subtype. DUPLICATE \u2014 The attribute of the two resulting features takes on a copy of the original object's attribute value. GEOMETRY_RATIO \u2014 The attributes of resulting features are a ratio of the original feature's value. The ratio is based on the proportion into which the original geometry is divided. If the geometry is divided equally, each new feature's attribute gets one-half the value of the original object's attribute. The geometry ratio policy only applies to range domains. ", "dataType": "String"}, {"name": "merge_policy", "isOptional": true, "description": "The merge policy of the created domain. When two features are merged into a single feature, merge policies control attribute values in the new feature. DEFAULT \u2014 The attribute of the resulting feature takes on the default value of the attribute of the given feature class or subtype. This is the only merge policy that applies to nonnumeric fields and coded value domains. SUM_VALUES \u2014 The attribute of the resulting feature takes on the sum of the values from the original feature's attribute. The sum values policy only applies to range domains. AREA_WEIGHTED \u2014 The attribute of the resulting feature is the weighted average of the attribute values of the original features. This average is based on the original feature's geometry. The area weighted policy only applies to range domains. ", "dataType": "String"}]},
{"syntax": "AssignDomainToField_management (in_table, field_name, domain_name, {subtype_code})", "name": "Assign Domain To Field (Data Management)", "description": "Sets the domain for a particular field and, optionally, for a subtype. If no subtype is specified, the domain is only assigned to the specified field.", "example": {"title": "Assign Domain to Field Example (Python Window)", "description": "The following Python window script demonstrates how to use the AssignDomainToField function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.AssignDomainToField_management ( \"montgomery.gdb/Landbase/Parcels\" , \"ZONING_S\" , \"ZoningFields\" , \"1: government\" )"}, "usage": ["Domain management involves the following steps:", "When an ", "attribute domain", " is associated with a table or feature class, an attribute validation rule is created in the database. This attribute validation rule describes and constrains the valid values of a field type.", "One attribute domain can be associated with multiple fields in the same table, feature class, or subtype as well as in multiple tables and feature classes.", "The ", "Input Table", " parameter accepts feature layers or table views.", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box.", "The ", "Subtype", " parameter ", "Add Value", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the Subtype parameter may not be populated with values. The Add Value button allows you to add expected values so you can complete the ", "Assign Domain To Field", " dialog and continue to build your model."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The name of the table or feature class containing the field that will be assigned a domain. ", "dataType": "Table View"}, {"name": "field_name", "isOptional": false, "description": "The name of the field to be assigned a domain. ", "dataType": "Field"}, {"name": "domain_name", "isOptional": false, "description": "The name of a geodatabase domain to assign to the field name. Available domains will automatically be loaded. ", "dataType": "String"}, {"name": "subtype_code", "isOptional": false, "description": "The subtype code to be assigned a domain. ", "dataType": "String"}]},
{"syntax": "AddCodedValueToDomain_management (in_workspace, domain_name, code, code_description)", "name": "Add Coded Value To Domain (Data Management)", "description": "Adds a value to a domain's coded value list.", "example": {"title": "AddCodedValueToDomain Example (Python Window)", "description": "The following Python window script demonstrates how to use the AddCodedValueToDomain function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.AddCodedValueToDomain_management ( \"montgomery.gdb\" , \"material\" , \"1\" , \"PVC\" )"}, "usage": ["Domain management involves the following steps:", "The coded value domain includes both the actual value that is stored in the database (for example, 1 for pavement) and a description of what the code value means (for example, pavement).", "A coded value domain which specifies a valid set of values for an attribute can apply to any type of attribute\u2014text, numeric, date, and so on. For example, a coded value list for a text attribute might include valid pipe material values: CL - cast iron pipe; DL - ductile iron pipe; ACP - asbestos concrete pipe, or a coded value list might include the numeric values representing valid pipe diameters: .75\u20133/4\"; 2\u20132\"; 24\u201324\"; and 30\u201330\".", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The geodatabase containing the domain to be updated. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the attribute domain that will have a value added to its coded value list. ", "dataType": "String"}, {"name": "code", "isOptional": false, "description": "The value to be added to the specified domain's coded value list. ", "dataType": "String"}, {"name": "code_description", "isOptional": false, "description": "A description of what the coded value represents. ", "dataType": "String"}]},
{"syntax": "SynchronizeChanges_management (geodatabase_1, in_replica, geodatabase_2, in_direction, conflict_policy, conflict_definition, reconcile)", "name": "Synchronize Changes (Data Management)", "description": "Synchronizes updates between two replica geodatabases in a direction specified by the user.", "example": {"title": "SynchronizeChanges Example (Python Window)", "description": "The following Python window example demonstrates how to use the SynchronizeChanges function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.SynchronizeChanges_management ( \"MySDEdata.sde\" , \"My2wayReplica\" , \"MySDEdata_child.sde\" , \"BOTH_DIRECTIONS\" , \"IN_FAVOR_OF_GDB1\" , \"BY_ATTRIBUTE\" , \"\" )"}, "usage": ["This tool is used when synchronizing replicas in connected mode. To synchronize replicas in disconnected mode see the ", "Export_Data_Change_Message", ", ", "Import Message", ", ", "Export Acknowledgement Message", " and ", "Re-Export Unacknowledged Messages", " tools. ", "Two-way, one-way, and check-out replicas can be synchronized with this tool.", "The replica geodatabases can be local geodatabases or geodata services. ", "Once synchronized, the changes (edits) will be reflected in the target geodatabase and viewable by all users."], "parameters": [{"name": "geodatabase_1", "isOptional": false, "description": "The geodatabase hosting the replica to synchronize. The geodatabase may be local or remote. ", "dataType": "Workspace ;GeoDataServer"}, {"name": "in_replica", "isInputFile": true, "isOptional": false, "description": "A valid replica with a parent contained within one input geodatabase and a child in the other input geodatabase. ", "dataType": "String"}, {"name": "geodatabase_2", "isOptional": false, "description": "The geodatabase hosting the relative replica. The geodatabase may be local or remote. ", "dataType": "Workspace; GeoDataServer"}, {"name": "in_direction", "isInputFile": true, "isOptional": false, "description": "The direction in which you want changes to be sent: from geodatabase 1 to geodatabase 2, from geodatabase 2 to geodatabase 1, or to send changes in both directions. For check-out/check-in replicas or one-way replicas there is only one appropriate direction. If the replica is two-way then any of the three choices are available. BOTH_DIRECTIONS \u2014 FROM_GEODATABASE2_TO_1 \u2014 FROM_GEODATABASE1_TO_2 \u2014", "dataType": "String"}, {"name": "conflict_policy", "isOptional": false, "description": "Specifies how conflicts are resolved when they are encountered. MANUAL \u2014 Manually resolve conflicts in the versioning reconcile environment. IN_FAVOR_OF_GDB1 \u2014 Conflicts resolve in favor of the Geodatabase 1. IN_FAVOR_OF_GDB2 \u2014 Conflicts resolve in favor of the Geodatabase 2.", "dataType": "String"}, {"name": "conflict_definition", "isOptional": false, "description": " Specifies how you would like to define conflicts: BY_OBJECT \u2014 Detects conflicts by row BY_ATTRIBUTE \u2014 Detects conflicts by column", "dataType": "String"}, {"name": "reconcile", "isOptional": false, "description": "Indicates whether to automatically reconcile once data changes are sent to the parent replica if there are no conflicts present. This option is only available for check-out/check-in replicas. DO_NOT_RECONCILE \u2014 Do not reconcile. This is the default. RECONCILE \u2014 Reconcile.", "dataType": "Boolean"}]},
{"syntax": "ReExportUnacknowledgedMessages_management (in_geodatabase, output_delta_file, in_replica, in_export_option)", "name": "Re-Export Unacknowledged Messages (Data Management)", "description": "Creates an output delta file containing unacknowledged replica updates from a one-way or two-way replica geodatabase.", "example": {"title": "ReExportUnacknowledgedMessages example 1 (Python window)", "description": "The following Python window example demonstrates how to use the ReExportUnacknowledgedMessages function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.ReExportUnacknowledgedMessages_management ( \"MySDEdata.sde\" , \"dataChanges2.gdb\" , \"MyReplica1\" , \"ALL_UNACKNOWLEDGED\" )"}, "usage": ["This tool is used when synchronizing replica while disconnected. This is done by first running the ", "Export Data Change Message", " tool which creates a delta file with changes to synchronize. The delta file is then copied to the relative replica and imported using the ", "Import Message", " tool. If a delta file gets lost and you want to resend, you can use the ", "Re-Export Unacknowledged Messages", " tool to regenerate the delta file. After the changes are imported, the relative replica can export an acknowledgment file using the ", "Export Acknowledgement Message", " tool. The acknowledgment file is copied to the replica and imported using the ", "Import Message", " tool. If an acknowledgment is not received, the next time changes are sent they will include the new changes plus the previously sent changes. ", "The output delta file can be a delta file geodatabase (", ".gdb", "), delta personal geodatabase (", ".mdb", "), or a delta XML file (", ".xml", "). When specifying the output delta file, you must include the appropriate suffix (", ".gdb", ", ", ".mdb", ", or ", ".xml", "). ", "This tool cannot be used for checkout replicas.", "To synchronize replicas in a connected mode see the ", "Synchronize Changes", " tool."], "parameters": [{"name": "in_geodatabase", "isInputFile": true, "isOptional": false, "description": "Specifies the replica geodatabase from which to reexport the unacknowledged messages. The geodatabase may be a local geodatabase or a geodata service. ", "dataType": "Workspace; GeoDataServer"}, {"name": "output_delta_file", "isOutputFile": true, "isOptional": false, "description": "Specifies the delta file in which to reexport data changes. ", "dataType": "File"}, {"name": "in_replica", "isInputFile": true, "isOptional": false, "description": "The replica from which the unacknowledgment messages will be reexported. ", "dataType": "String"}, {"name": "in_export_option", "isInputFile": true, "isOptional": false, "description": " ALL_UNACKNOWLEDGED \u2014 Reexports all changes for which there has been no acknowledgment message received. MOST_RECENT \u2014 Reexports only those changes since the last set of exported changes was sent.", "dataType": "String"}]},
{"syntax": "ImportReplicaSchema_management (in_geodatabase, in_source)", "name": "Import Replica Schema (Data Management)", "description": "Applies replica schema differences using an input replica geodatabase and XML schema file.", "example": {"title": "ImportReplicaSchema Example (Python Window)", "description": "The following Python Window script demonstrates how to use the ImportReplicaSchema function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.ImportReplicaSchema_management ( \"Countries.mdb\" , \"schemaDifferences.xml\" )"}, "usage": ["The input replica schema changes file must be XML.", " Modifying the schema of a replica to\r\nmatch the schema of a relative replica is a separate\r\nprocess from data synchronization. Three tools are provided for\r\nthis purpose: ", "Compare Replica Schema", ", ", "Import Replica Schema", ", and\r\n", "Export Replica Schema", ":"], "parameters": [{"name": "in_geodatabase", "isInputFile": true, "isOptional": false, "description": "Specifies the replica geodatabase to which the replica schema will be imported. The geodatabase may be a local geodatabase or a geodata service. ", "dataType": "Workspace; GeoDataServer"}, {"name": "in_source", "isInputFile": true, "isOptional": false, "description": "Specifies the file which contains the replica schema to import. ", "dataType": "File"}]},
{"syntax": "ImportMessage_management (in_geodatabase, source_delta_file, {output_acknowledgement_file}, {conflict_policy}, {conflict_definition}, {reconcile_with_parent_version})", "name": "Import Message (Data Management)", "description": "Imports changes from a delta file into a replica geodatabase, or imports an acknowledgment message into a replica geodatabase. ", "example": {"title": "ImportMesage Example (Python Window)", "description": "The following example demonstrates how to use the ImportMessage funcion in a Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.ImportMessage_management ( \"MySDEdata.sde\" , \"DataChanges.gdb\" , \"acknowledgement.xml\" , \"IN_FAVOR_OF_IMPORTED_CHANGES\" , \"BY_OBJECT\" )"}, "usage": ["This tool is used when synchronizing replica while disconnected. This is done by first running the ", "Export Data Change Message", " tool which creates a delta file with changes to synchronize. The delta file is then copied to the relative replica and imported using the ", "Import Message", " tool. If a delta file gets lost and you want to resend, you can use the ", "Re-Export Unacknowledged Messages", " tool to regenerate the delta file. After the changes are imported, the relative replica can export an acknowledgment file using the ", "Export Acknowledgement Message", " tool. The acknowledgment file is copied to the replica and imported using the ", "Import Message", " tool. If an acknowledgment is not received, the next time changes are sent they will include the new changes plus the previously sent changes. ", "The geodatabase may be a local geodatabase or a geodata service.", "Accepts either acknowledgment messages or data change messages. Acknowledgment files are  XML (", ".xml", "). Data change messages can be delta file geodabase (", ".gdb", "), delta personal geodatabase (", ".mdb", ") or delta XML files (", ".xml", ").", "After importing a data change message, you have the option to immediately export an acknowledgment message. The output acknowledgment file must be XML.", "To synchronize replicas in a connected mode see the ", "Synchronize Changes", " tool."], "parameters": [{"name": "in_geodatabase", "isInputFile": true, "isOptional": false, "description": "Specifies the replica geodatabase to receive the imported message. The geodatabase may be local or remote. ", "dataType": "Workspace ; GeoDataServer"}, {"name": "source_delta_file", "isOptional": false, "description": "Specifies the file from which the message will be imported. ", "dataType": "Workspace ; File"}, {"name": "output_acknowledgement_file", "isOutputFile": true, "isOptional": true, "description": "When importing data changes, this allows you to optionally export a message to acknowledge the import of a data change message. This option is ignored for anything other than a data change message. ", "dataType": "File"}, {"name": "conflict_policy", "isOptional": true, "description": "Specifies how conflicts are resolved when they are encountered while importing a data change message. MANUAL \u2014 Manually resolve conflicts in the versioning reconcile environment. IN_FAVOR_OF_DATABASE \u2014 Conflicts automatically resolve in favor of the database receiving the changes. IN_FAVOR_OF_IMPORTED_CHANGES \u2014 Conflicts automatically resolve in favor of the imported changes. ", "dataType": "String"}, {"name": "conflict_definition", "isOptional": true, "description": "Specifies how you would like to define conflicts: BY_OBJECT \u2014 Detects conflicts by row. BY_ATTRIBUTE \u2014 Detects conflicts by column.", "dataType": "String"}, {"name": "reconcile_with_parent_version", "isOptional": true, "description": "Indicates whether to automatically reconcile once data changes are sent to the parent replica if there are no conflicts present. This option is only available for check-out/check-in replicas. DO_NOT_RECONCILE \u2014 Do not reconcile. This is the default. RECONCILE \u2014 Reconcile.", "dataType": "Boolean"}]},
{"syntax": "ExportReplicaSchema_management (in_geodatabase, output_replica_schema_file, in_replica)", "name": "Export Replica Schema (Data Management)", "description": "Creates a replica schema file with the schema of an input one- or two-way replica.", "example": {"title": "ExportReplicaSchema Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.worksapce = \"C:/Data\" arcpy.ExportReplicaSchema_management ( \"Countries.mdb\" , \"replicaSchema.xml\" , \"MyReplica1\" )"}, "usage": ["The output schema file must be XML. You must specify ", ".xml", " as the file suffix.", " Modifying the schema of a replica to\r\nmatch the schema of a relative replica is a separate\r\nprocess from data synchronization. Three tools are provided for\r\nthis purpose: ", "Compare Replica Schema", ", ", "Import Replica Schema", ", and\r\n", "Export Replica Schema", ":", "This tool is used when synchronizing replica schema. Additionally, see the ", "Compare Replica Schema", " and ", "Import Replica Schema", " tools."], "parameters": [{"name": "in_geodatabase", "isInputFile": true, "isOptional": false, "description": "Specifies the replica geodatabase from which to export the replica schema. The geodatabase may be local or remote. ", "dataType": "Workspace; GeoDataServer"}, {"name": "output_replica_schema_file", "isOutputFile": true, "isOptional": false, "description": "Specifies the file in which to export schema. ", "dataType": "File"}, {"name": "in_replica", "isInputFile": true, "isOptional": false, "description": "The replica from which to export schema. ", "dataType": "String"}]},
{"syntax": "ExportDataChangeMessage_management (in_geodatabase, out_data_changes_file, in_replica, switch_to_receiver, include_unacknowledged_changes, include_new_changes)", "name": "Export Data Change Message (Data Management)", "description": "Creates an output delta file containing updates from an input replica.", "example": {"title": "ExportDataChangeMessage example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ExportDataChangeMessage function in the Python window. ", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.ExportDataChangeMessage_management ( \"MySDEdata.sde\" , \"Changes.gdb\" , \"MyReplica1\" , \"SWITCH\" , \"TRUE\" , \"TRUE\" )"}, "usage": ["The geodatabase may be a local geodatabase or a geodata service.", "The output delta file can be a delta file geodatabase (", ".gdb", "), delta personal geodatabase (", ".mdb", "), or a delta XML file (", ".xml", "). When specifying the output delta file, you must include the appropriate suffix (", ".gdb", ", ", ".mdb", ", or ", ".xml", "). ", "This tool is used when synchronizing replica while disconnected. This is done by first running the ", "Export Data Change Message", " tool which creates a delta file with changes to synchronize. The delta file is then copied to the relative replica and imported using the ", "Import Message", " tool. If a delta file gets lost and you want to resend, you can use the ", "Re-Export Unacknowledged Messages", " tool to regenerate the delta file. After the changes are imported, the relative replica can export an acknowledgment file using the ", "Export Acknowledgement Message", " tool. The acknowledgment file is copied to the replica and imported using the ", "Import Message", " tool. If an acknowledgment is not received, the next time changes are sent they will include the new changes plus the previously sent changes. ", "To synchronize replicas in a connected mode see the ", "Synchronize Changes", " tool."], "parameters": [{"name": "in_geodatabase", "isInputFile": true, "isOptional": false, "description": "Specifies the replica geodatabase from which to export the data change message. The geodatabase may be local or remote. ", "dataType": "Workspace ;GeoDataServer"}, {"name": "out_data_changes_file", "isOutputFile": true, "isOptional": false, "description": "Specifies the delta file to export to. ", "dataType": "File"}, {"name": "in_replica", "isInputFile": true, "isOptional": false, "description": "The replica containing updates to be exported. ", "dataType": "String"}, {"name": "switch_to_receiver", "isOptional": false, "description": "Indicates whether to change the role of the replica to that of a receiver. The receiver may not send replica updates until updates from the relative replica sender arrive. DO_NOT_SWITCH \u2014 Do not switch replica role. This is the default. SWITCH \u2014 Switch replica role from sender to receiver.", "dataType": "Boolean"}, {"name": "include_unacknowledged_changes", "isOptional": false, "description": "Indicates whether to include data changes that have been previously exported for which no acknowledgment message has been received. NO_UNACKNOWLEDGED \u2014 Do not include data changes that have been previously sent. UNACKNOWLEDGED \u2014 Include all of the data changes that have been previously exported for which no acknowledgment message has been sent. This is the default.", "dataType": "Boolean"}, {"name": "include_new_changes", "isOptional": false, "description": "Indicates whether to include all data changes created since the last exported data change message. NO_NEW_CHANGES \u2014 Do not include data changes created since the last exported data change message. NEW_CHANGES \u2014 Include data changes created since the last exported data change message. This is the default.", "dataType": "Boolean"}]},
{"syntax": "ExportAcknowledgementMessage_management (in_geodatabase, out_acknowledgement_file, in_replica)", "name": "Export Acknowledgement Message (Data Management)", "description": "Creates an output acknowledgement file to acknowledge the reception of previously received data change messages.", "example": {"title": "ExportAcknowledgement Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.ExportAcknowledgementMessage_management ( \"MySDEdata.sde\" , \"AcknowledgementMessage.xml\" , \"MyReplica1\" )"}, "usage": ["This tool is used when synchronizing replica while disconnected. This is done by first running the ", "Export Data Change Message", " tool which creates a delta file with changes to synchronize. The delta file is then copied to the relative replica and imported using the ", "Import Message", " tool. If a delta file gets lost and you want to resend, you can use the ", "Re-Export Unacknowledged Messages", " tool to regenerate the delta file. After the changes are imported, the relative replica can export an acknowledgment file using the ", "Export Acknowledgement Message", " tool. The acknowledgment file is copied to the replica and imported using the ", "Import Message", " tool. If an acknowledgment is not received, the next time changes are sent they will include the new changes plus the previously sent changes. ", "The geodatabase may be a local geodatabase or a geodata service.", "The output acknowledgement file must be XML.", "This tool is not applicable for check-out replicas.", "To synchronize replicas in a connected mode see the ", "Synchronize Changes", " tool."], "parameters": [{"name": "in_geodatabase", "isInputFile": true, "isOptional": false, "description": "Specifies the replica geodatabase from which to export the acknowledgement message. The geodatabase may be local or remote. ", "dataType": "Workspace ; GeoDataServer"}, {"name": "out_acknowledgement_file", "isOutputFile": true, "isOptional": false, "description": "Specifies the delta file to export to. ", "dataType": "File"}, {"name": "in_replica", "isInputFile": true, "isOptional": false, "description": "The replica from which the acknowledgement message will be exported. ", "dataType": "String"}]},
{"syntax": "CreateReplicaFromServer_management (in_geodataservice, datasets, in_type, out_geodatabase, out_name, {access_type}, {initial_data_sender}, {expand_feature_classes_and_tables}, {reuse_schema}, {get_related_data}, {geometry_features}, archiving)", "name": "Create Replica From Server (Data Management)", "description": "Creates a replica using a specified list of feature classes, layers, feature datasets, and/or tables from a remote geodatabase using a geodata service published on  ArcGIS for Server .", "example": {"title": "CreateReplicaFromServer example 1 (Python window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data/MySDEdata.sde\" arcpy.CreateReplicaFromServer_management ( \"GIS Servers\\jerome\\RoadMap.GeoDataServer\" , \"Roads\" , \"TWO_WAY_REPLICA\" , env.workspace , \"MajorRoads_replica\" , \"FULL\" , \"CHILD_DATA_SENDER\" , \"USE_DEFAULTS\" , \"DO_NOT_REUSE\" , \"GET_RELATED\" )"}, "usage": ["The source must be a geodata service representing a remote ArcSDE geodatabase. The destination may be either a local or a remote geodatabase.", "The data that you wish to replicate must be versioned, but not with the option to move edits to base. The database user who is connected must also have write permissions to the data. For two-way and both types of one-way replicas, all datasets must have a globalid column and have a high-precision spatial reference. ", "For check-out and one-way replicas, the child replica geodatabase can be an ArcSDE, file, or personal geodatabase.", "For two-way and one-way, child-to-parent replicas the child geodatabase must be ArcSDE.", "To use archiving for one-way replicas, the parent workspace must be connected to the Default version. For one-way, child-to-parent replicas the child workspace must be connected to the Default version.", "The default for feature classes is to replicate all features. The default filter for tables is Schema Only; only the schema for the table will be replicated. If you set the ", "Extent", " environment   or specify replica geometry features,  it will be applied as a spatial filter where only features intersecting the extent will be replicated. Tables will also include rows that are related to rows which are part of the replica.", "Replica Geometry Features", " can be used to define the replica geometry. You can also use the ", "Extent", " environment setting to define the replica geometry.", "The replica geometry features can be points, lines, or polygons.", "A feature layer used for the replica geometry features can contain one or more features. If there are more than one, the geometries are merged, and only data that intersects the merged geometries will be replicated.", " If filters (such as spatial, selection, or definition query) have been defined on the replica geometry features, only features that satisfy these filters will be used to define the replica geometry. See ", "Preparing data for replication", " for more information.", "The ", "Re-use Schema", " parameter options are only available for checkout replicas."], "parameters": [{"name": "in_geodataservice", "isInputFile": true, "isOptional": false, "description": "The geodata service representing the geodatabase from which the replica will be created. The geodatabase referenced by the geodata service must be an ArcSDE geodatabase. ", "dataType": "GeoDataServer"}, {"name": "datasets", "isOptional": false, "description": "The list of the feature datasets, stand-alone feature classes, tables, and stand-alone attributed relationship classes from the geodata service to replicate. ", "dataType": "String"}, {"name": "in_type", "isInputFile": true, "isOptional": false, "description": "The kind of replica to create. TWO_WAY_REPLICA \u2014 Changes can be sent between child and parent replicas in both directions. ONE_WAY_REPLICA \u2014 Changes are sent from the parent replica to the child replica only. CHECK_OUT \u2014 Data is replicated, edited, and checked back in one time. ONE_WAY_CHILD_TO_PARENT_REPLICA \u2014 Changes are sent from the child replica to the parent replica only.", "dataType": "String"}, {"name": "out_geodatabase", "isOutputFile": true, "isOptional": false, "description": "The local geodatabase or geodata service that will host the child replica. Geodata services are used to represent remote geodatabases. The geodatabase can be an ArcSDE, file, or personal geodatabase. For two-way replicas the child geodatabase must be ArcSDE. For one-way and check-out replicas the geodatabase can be personal, file, or ArcSDE. Personal or file geodatabases must already exist before running this tool. ", "dataType": "Workspace ; GeoDataServer"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name that identifies the replica. ", "dataType": "String"}, {"name": "access_type", "isOptional": true, "description": "The type of access you want: FULL \u2014 Supports complex types (topologies and geometric networks) and requires the data to be versioned. SIMPLE \u2014 The data on the child is not versioned and must be simple. This allows the replica to be interoperable. Nonsimple features in the parent (for example, features in geometric networks and topologies) are converted to simple features (for example, point, line, and polygon feature classes).", "dataType": "String"}, {"name": "initial_data_sender", "isOptional": true, "description": "Used by replication to determine which replica can send changes when in disconnected mode. If you are working in a connected mode, this parameter is inconsequential. This ensures that the relative replica doesn't send updates until the changes are first received from the initial data sender. CHILD_DATA_SENDER \u2014 PARENT_DATA_SENDER \u2014", "dataType": "String"}, {"name": "expand_feature_classes_and_tables", "isOptional": true, "description": "Specifies whether you will include expanded feature classes and tables, such as those found in geometric networks, topologies, or relationship classes. USE_DEFAULTS \u2014 Adds the expanded feature classes and tables related to the feature classes and tables in the replica. The default for feature classes is to replicate all features; the default for tables is to replicate the schema only. If a spatial filter has been defined it will be applied to feature classes. ADD_WITH_SCHEMA_ONLY \u2014 Adds only the schema for the expanded feature classes and tables. ALL_ROWS \u2014 Adds all rows for expanded feature classes and tables. DO_NOT_ADD \u2014 Doesn't add expanded feature classes and tables.", "dataType": "String"}, {"name": "reuse_schema", "isOptional": true, "description": "Indicates whether to reuse a geodatabase that contains the schema of the data you want to replicate. This reduces the amount of time required to replicate the data. This option is only available for check-out replicas. DO_NOT_REUSE \u2014 Do not reuse schema. This is the default. REUSE \u2014 Reuse schema.", "dataType": "String"}, {"name": "get_related_data", "isOptional": true, "description": "Specifies whether to replicate rows related to rows already in the replica. For example, consider a feature (f1) inside the replication filter and a related feature (f2) from another class outside the filter. Feature f2 is included in the replica if you choose to get related data. DO_NOT_GET_RELATED \u2014 Do not replicate related rows. GET_RELATED \u2014 Replicate related data. This is the default.", "dataType": "String"}, {"name": "geometry_features", "isOptional": true, "description": " The features used to define the area to replicate. ", "dataType": "Feature Layer"}, {"name": "archiving", "isOptional": false, "description": "Specifies whether to use the archive class to track changes instead of using the versioning delta tables. This is only available for one-way replicas. ARCHIVING \u2014 Uses archiving to track changes. DO_NOT_ARCHIVING \u2014 Does not use archiving to track changes. This is the default.", "dataType": "Boolean"}]},
{"syntax": "CreateReplicaFootprints_management (in_workspace, out_workspace, output_featureclass_name)", "name": "Create Replica Footprints (Data Management)", "description": "Creates a feature class that contains the geometries for all the replicas in a geodatabase. Attributes in the feature class store the information from the replica manager.", "example": {"title": "CreateReplicaFootprints Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CreateReplicaFootprints function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data/MySDEdata.sde\" arcpy.CreateReplicaFootprints_management ( env.workspace , env.workspace , \"replicaFootprints\" )"}, "usage": ["The output feature class can be any geodatabase feature class (File, Personal, or ArcSDE)."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The geodatabase containing the replicas from which you would like to create the replica footprint. The geodatabase must be a local geodatabase; it cannot be a geodata service. ", "dataType": "Workspace"}, {"name": "out_workspace", "isOutputFile": true, "isOptional": false, "description": "The output geodatabase that will hold the replica footprints feature class once it is created. The geodatabase may be local or remote. ", "dataType": "Workspace"}, {"name": "output_featureclass_name", "isOutputFile": true, "isOptional": false, "description": "The name of the replica footprints feature class to be created. ", "dataType": "String"}]},
{"syntax": "CreateReplica_management (in_data, in_type, out_geodatabase, out_name, {access_type}, {initial_data_sender}, {expand_feature_classes_and_tables}, {reuse_schema}, {get_related_data}, {geometry_features}, archiving)", "name": "Create Replica (Data Management)", "description": "Creates a replica to a personal, file, or SDE geodatabase from a specified list of feature classes, layers, datasets, and/or tables in an ArcSDE geodatabase.", "example": {"title": "CreateReplica example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the CreateReplica function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data/MySDEdata.sde\" arcpy.CreateReplica_management ( \"roads\" , \"ONE_WAY_REPLICA\" , \"C:\\Data\\MyTargetGDB.gdb\" , \"MyReplica\" , \"FULL\" , \"PARENT_DATA_SENDER\" , \"USE_DEFAULTS\" , \"DO_NOT_REUSE\" , \"TRUE\" )"}, "usage": ["All datasets must be from the same ArcSDE database.", "The data that you wish to replicate must be versioned, but not with the option to move edits to base. The database user who is connected must also have write permissions to the data. For two-way and both types of one-way replicas, all datasets must have a globalid column and have a high-precision spatial reference. ", "For check-out and one-way replicas, the child replica geodatabase can be an ArcSDE, file, or personal geodatabase.", "For two-way and one-way, child-to-parent replicas the child geodatabase must be ArcSDE.", "To use archiving for one-way replicas, the parent workspace must be connected to the Default version. For one-way, child-to-parent replicas the child workspace must be connected to the Default version.", "In ArcMap the definition query and selection properties set on a layer or table determine what data gets replicated.", "For tables the default filter is Schema Only; only the schema for the table will be replicated. To apply a filter to a table you must first create a table view setting desired filters. You can then use this as input into the ", "Create Replica", " tool. See the ", "Make Table View", " for more information. For more information on filters and replication see ", "Preparing data for replication.", "Replica Geometry Features", " can be used to define the replica geometry. You can also use the ", "Extent", " environment setting to define the replica geometry.", "The replica geometry features can be points, lines, or polygons.", "A feature layer used for the replica geometry features can contain one or more features. If there are more than one, the geometries are merged, and only data that intersects the merged geometries will be replicated.", " If filters (such as spatial, selection, or definition query) have been defined on the replica geometry features, only features that satisfy these filters will be used to define the replica geometry. See ", "Preparing data for replication", " for more information.", "The ", "Re-use Schema", " parameter options are only available for checkout replicas."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The data to be replicated. This list consists of layers and tables referencing versioned, editable data from an ArcSDE geodatabase. ", "dataType": "Layer; Table View; Dataset"}, {"name": "in_type", "isInputFile": true, "isOptional": false, "description": "The kind of replica to create. TWO_WAY_REPLICA \u2014 Changes can be sent between child and parent replicas in both directions. ONE_WAY_REPLICA \u2014 Changes are sent from the parent replica to the child replica only. CHECK_OUT \u2014 Data is replicated, edited, and checked back in one time. ONE_WAY_CHILD_TO_PARENT_REPLICA \u2014 Changes are sent from the child replica to the parent replica only.", "dataType": "String"}, {"name": "out_geodatabase", "isOutputFile": true, "isOptional": false, "description": "The local geodatabase or geodata service that will host the child replica. Geodata services are used to represent remote geodatabases. The geodatabase can be an ArcSDE, file, or personal geodatabase. For two-way replicas the child geodatabase must be ArcSDE. For one-way and check-out replicas the geodatabase can be personal, file, or ArcSDE. Personal or file geodatabases must already exist before running this tool. ", "dataType": "Workspace ; GeoDataServer"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name that identifies the replica. ", "dataType": "String"}, {"name": "access_type", "isOptional": true, "description": "The type of access you want: FULL \u2014 Supports complex types (topologies and geometric networks) and requires the data to be versioned. SIMPLE \u2014 The data on the child is not versioned and must be simple. This allows the replica to be interoperable. Nonsimple features in the parent (for example, features in geometric networks and topologies) are converted to simple features (for example, point, line, and polygon feature classes).", "dataType": "String"}, {"name": "initial_data_sender", "isOptional": true, "description": "Used by replication to determine which replica can send changes when in disconnected mode. If you are working in a connected mode, this parameter is inconsequential. This ensures that the relative replica doesn't send updates until the changes are first received from the initial data sender. CHILD_DATA_SENDER \u2014 PARENT_DATA_SENDER \u2014", "dataType": "String"}, {"name": "expand_feature_classes_and_tables", "isOptional": true, "description": "Specifies whether you will include expanded feature classes and tables, such as those found in geometric networks, topologies, or relationship classes. USE_DEFAULTS \u2014 Adds the expanded feature classes and tables related to the feature classes and tables in the replica. The default for feature classes is to replicate all features; the default for tables is to replicate the schema only. If a spatial filter has been defined it will be applied to feature classes. ADD_WITH_SCHEMA_ONLY \u2014 Adds only the schema for the expanded feature classes and tables. ALL_ROWS \u2014 Adds all rows for expanded feature classes and tables. DO_NOT_ADD \u2014 Doesn't add expanded feature classes and tables.", "dataType": "String"}, {"name": "reuse_schema", "isOptional": true, "description": "Indicates whether to reuse a geodatabase that contains the schema of the data you want to replicate. This reduces the amount of time required to replicate the data. This option is only available for check-out replicas. DO_NOT_REUSE \u2014 Do not reuse schema. This is the default. REUSE \u2014 Reuse schema.", "dataType": "String"}, {"name": "get_related_data", "isOptional": true, "description": "Specifies whether to replicate rows related to rows already in the replica. For example, consider a feature (f1) inside the replication filter and a related feature (f2) from another class outside the filter. Feature f2 is included in the replica if you choose to get related data. DO_NOT_GET_RELATED \u2014 Do not replicate related rows. GET_RELATED \u2014 Replicate related data. This is the default.", "dataType": "String"}, {"name": "geometry_features", "isOptional": true, "description": " The features used to define the area to replicate. ", "dataType": "Feature Layer"}, {"name": "archiving", "isOptional": false, "description": "Specifies whether to use the archive class to track changes instead of using the versioning delta tables. This is only available for one-way replicas. ARCHIVING \u2014 Uses archiving to track changes. DO_NOT_ARCHIVING \u2014 Does not use archiving to track changes. This is the default.", "dataType": "Boolean"}]},
{"syntax": "CompareReplicaSchema_management (in_geodatabase, in_source_file, output_replica_schema_changes_file)", "name": "Compare Replica Schema (Data Management)", "description": "Generates an XML that describes schema differences between a replica geodatabase and the relative replica geodatabase. ", "example": {"title": "CompareReplicaSchema example 1 (Python window)", "description": "The following script demonstrates how to use the CompareReplicaSchema function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.CompareReplicaSchema_management ( \"MySDEdata.sde\" , \"RelativeReplicaSchema.xml\" , \"SchemaComparison.xml\" )"}, "usage": [" Modifying the schema of a replica to\r\nmatch the schema of a relative replica is a separate\r\nprocess from data synchronization. Three tools are provided for\r\nthis purpose: ", "Compare Replica Schema", ", ", "Import Replica Schema", ", and\r\n", "Export Replica Schema", ":", "The output replica schema changes file must be XML."], "parameters": [{"name": "in_geodatabase", "isInputFile": true, "isOptional": false, "description": "Specifies the replica geodatabase to which the replica schema will be compared. The geodatabase may be a local geodatabase or a geodata service. ", "dataType": "Workspace; GeoDataServer"}, {"name": "in_source_file", "isInputFile": true, "isOptional": false, "description": "Specifies the file that contains the relative replica schema to use for the comparison. ", "dataType": "File"}, {"name": "output_replica_schema_changes_file", "isOutputFile": true, "isOptional": false, "description": "Specifies the file to contain a description of the schema differences. ", "dataType": "File"}]},
{"syntax": "AddGlobalIDs_management (in_datasets)", "name": "Add Global IDs (Data Management)", "description": "Adds global IDs to a list of geodatabase feature classes, tables, and/or feature datasets. ", "example": {"title": "AddGlobalIDs Example (Python Window)", "description": "The following Python Window script demonstrates how to use the AddGlobalIDs function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/MySDEdata.sde\" arcpy.AddGlobalIDs_managment ( \"GDB1.Heather.Roads\" )"}, "usage": ["GlobalIDs uniquely identify a feature or table row within a geodatabase and across geodatabases.", "GlobalIDs are required for two-way and one-way replication."], "parameters": [{"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": "A list of geodatabase classes, tables, and/or feature datasets to which global IDs will be added. ", "dataType": "Layer;Table View; Dataset"}]},
{"syntax": "ExporttoDelta_management (in_workspace, dest_delta_database)", "name": "Export To Delta (Data Management)", "description": "Exports changes in a check-out replica geodatabase to a delta file. A delta file contains only the changes exported from a replicated geodatabase.", "example": {"title": "ExportToDelta Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.ExporttoDelta_management ( \"housing.mdb\" , \"MyCheckoutChanges.mdb\" )"}, "usage": ["This is a ", "deprecated", " tool. Note that this tool is designed for check-out replicas only and should only be used to support applications from earlier releases. It is recommended that you use the ", "Export Data Change Message", " tool in the ", "Distributed Geodatabase toolset", " in place of this tool.", "Instead of synchronizing edits directly from the child replica geodatabase, you can export the changes only from the child replica to a delta file. Delta files are smaller than the original replica geodatabase.", "The changes in the delta file may be synchronized with the parent replica using the Check In From Delta tool.", "When synchronizing from a delta XML file, as with synchronizing directly, the replica in the parent geodatabase will be unregistered.", "Synchronizing changes from a delta file does not automatically unregister the check-out replica in the associated child replica geodatabase; this must be done manually."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "Specifies the child replica geodatabase from which to export the delta file. ", "dataType": "Workspace"}, {"name": "dest_delta_database", "isOptional": false, "description": "The output delta file. ", "dataType": "File"}]},
{"syntax": "Checkout_management (in_data, {in_type}, out_workspace, out_name, {reuse_schema}, {get_related_data})", "name": "Check Out (Data Management)", "description": "Creates a check-out replica from datasets in an ArcSDE geodatabase to an ArcSDE, file, or personal geodatabase for offline editing.", "example": {"title": "CheckOut Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data/MySDEdata.sde\" arcpy.Checkout_management ( \"roads\" , \"DATA\" , \"C:/Data/MyCheckout.gdb\" , \"mycheckout1\" , \"REUSE\" , \"RELATED\" )"}, "usage": ["This is a ", "deprecated", " tool. Note that this tool is designed for check-out replicas only and should only be used to support applications from earlier releases. It is recommended that you use the ", "Create Replica", " tool in the ", "Distributed Geodatabase toolset", " in place of this tool.", "The tool accepts layers or tables that reference data from one ArcSDE server. Either add them to the list in the dialog box or create a semicolon-delimited list at the command line or in a script.", "The layers and tables must reference versioned ArcSDE feature classes and tables for which you have permissions to edit.", "The following describes how to define the rows to check out:", "This tool applies the default behavior of the create replica command in ArcMap.", "If the Check-out to Workspace parameter value is a personal or file geodatabase that does not exist, one will be created.", "Once replicated, edits may be applied to the child replica geodatabase and synchronized with the parent geodatabase."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The data to be replicated for offline editing. This list consists of layers and tables referencing versioned, editable data from an ArcSDE geodatabase. ", "dataType": "Feature Layer; Table View"}, {"name": "in_type", "isInputFile": true, "isOptional": true, "description": "Specifies if the data and schema will be replicated or if a SCHEMA-ONLY check-out of the database should occur. The default is DATA. DATA \u2014 The data and the schema will be replicated. This is the default. SCHEMA_ONLY \u2014 Only the schema will be replicated. ", "dataType": "String"}, {"name": "out_workspace", "isOutputFile": true, "isOptional": false, "description": "The workspace where the replicated data will be edited. This can be an ArcSDE, file, or personal geodatabase. If it's a personal geodatabase that doesn't exist, it will be created. ", "dataType": "Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name that identifies the check-out replica. This is recorded and maintained in both the parent and the child database until synchronized. ", "dataType": "String"}, {"name": "reuse_schema", "isOptional": true, "description": "Choose REUSE if you are using a database with the schema (no data) as the out_workspace. REUSE will be faster, since a schema will not need to be created during replica creation. The default is NO_REUSE. NO_REUSE \u2014 Do not reuse the schema. This is the default. REUSE \u2014 Reuse the schema. ", "dataType": "Boolean"}, {"name": "get_related_data", "isOptional": true, "description": "Specifies whether to replicate related data that has established relationships with the replicated data. For example, consider a feature (f1) inside the replica area and a related feature (f2) from another class outside the replica area. Feature f2 is included in the check-out replica if you choose to get related data. RELATED \u2014 Replicate related data. This is the default. NO_RELATED \u2014 Do not replicate related data. ", "dataType": "Boolean"}]},
{"syntax": "CheckinDelta_management (in_delta_database, dest_workspace, {reconcile})", "name": "Check In From Delta (Data Management)", "description": "Imports changes from a delta file into the parent replica. A delta file contains only the changes exported from a child replica geodatabase.", "example": {"title": "CheckInFromDelta Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.CheckinDelta_management ( \"MyCheckChanges.gdb\" , \"MySDEdata.sde\" , NO_RECONCILE )"}, "usage": ["This is a ", "deprecated", " tool. Note that this tool is designed for check-out replicas only and should only be used to support applications from earlier releases. It is recommended that you use the ", "Import Message", " tool in the ", "Distributed Geodatabase", " toolset in place of this tool.", "Instead of synchronizing edits directly from the child replica geodatabase, you can export the changes only from the child replica to a delta file. Delta files are smaller than the original child replica geodatabase.", "When the synchronize from a delta file succeeds, the check-out replica in the parent geodatabase is unregistered.", "Synchronizing changes from a delta file does not automatically unregister the replica in the child replica geodatabase; this must be done manually."], "parameters": [{"name": "in_delta_database", "isInputFile": true, "isOptional": false, "description": "The delta file that contains the changes to be synchronized. ", "dataType": "File"}, {"name": "dest_workspace", "isOptional": false, "description": "The parent replica geodatabase. The ArcSDE workspace to which changes will be applied. ", "dataType": "Workspace"}, {"name": "reconcile", "isOptional": true, "description": "Indicates whether the synchronize process should reconcile the synchronization version after the edits have been synchronized. NON_RECONCILE \u2014 there is no reconcile performed. This is the default. RECONCILE \u2014 There is a reconciliation performed. ", "dataType": "Boolean"}]},
{"syntax": "Checkin_management (in_workspace, dest_workspace, {reconcile})", "name": "Check In (Data Management)", "description": "Synchronizes changes from a check-out replica in an ArcSDE, file, or personal geodatabase to the parent ArcSDE geodatabase.", "example": {"title": "CheckIn Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.gp.Checkin ( \"MyCheckout.mdb\" , \"MySDEdata.sde\" , \"RECONCILE\" )"}, "usage": ["This is a ", "deprecated", " tool. This tool is designed for check-out replicas only and should only be used to support applications from earlier releases. It is recommended that you use the ", "Synchronize Changes", " tool in the ", "Distributed Geodatabase", " toolset in place of this tool.", "You must have permission to edit the data you are synchronizing.", "Once synchronized, the changes (edits) will be reflected in the parent geodatabase and are viewable by all users."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The child replica geodatabase. The workspace that contains the child replica. This can be an ArcSDE, file, or personal geodatabase. ", "dataType": "Workspace"}, {"name": "dest_workspace", "isOptional": false, "description": "The parent replica geodatabase. The ArcSDE workspace to which changes will be applied. ", "dataType": "Workspace"}, {"name": "reconcile", "isOptional": true, "description": "Indicates whether the synchronization process should reconcile the synchronization version after the edits have been synchronized. NON_RECONCILE \u2014 There is no reconcile performed. This is the default. RECONCILE \u2014 There is a reconciliation performed. ", "dataType": "Boolean"}]},
{"syntax": "UpgradeSpatialReference_management (input_dataset, {xy_resolution}, {z_resolution}, {m_resolution})", "name": "Upgrade Spatial Reference (Data Management)", "description": "Upgrades a low precision dataset's spatial reference to high precision.  Input to this tool is a stand-alone feature class, feature dataset, or raster catalog which has a low resolution spatial reference and is stored in a current version personal or ArcSDE geodatabase. The origin and precision of the high precision spatial reference\u00a0grid\u00a0will  mesh  with the existing low precision grid. For each point of the original low precision spatial reference grid there is a point in the new high precision spatial reference grid. Coordinate values will not be affected by the upgrade.", "example": {"title": "Upgrade Spatial Reference Example (Python Window)", "description": "The following Python window script demonstrates how to use the UpgradeSpatialReference function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.UpgradeSpatialReference_management ( \"Euro_WGS\" , \"0.0000000625\" , \"0.0000025\" , \"0.0000025\" )"}, "usage": ["To determine the geodatabase version, right-click the geodatabase and select \"Properties...\". The \"Upgrade Status\" section under the General tab will show the ArcGIS release that the geodatabase corresponds to. If version number is less than 9.2, you can use the \"Upgrade Geodatabase\" function to upgrade the geodatabase to the ArcGIS version being used.", "To determine if a dataset (feature class, feature dataset, or raster catalog) is high precision, right-click the dataset and select \"Properies...\", then select the General tab. In the Geometry Properties, youll see either \"Data Storage : High Precision\" or \"Low Precision\". Datasets stored in a file geodatabase are always high precision.", "Individual feature classes within a feature dataset cannot be upgraded individually as they inherit their spatial reference from the feature dataset. In order to upgrade them, upgrade the feature dataset's spatial reference, which means the spatial reference of all the feature classes in the dataset will be upgraded.", "ArcCatalog's Copy/Paste functionality automatically upgrades the spatial reference of datasets when the output is a 9.2 (or later) geodatabase.", "When used on a feature dataset, the M Resolution will not be applied to feature classes contained within a feature dataset.", "The XY tolerance property of the upgraded spatial reference will be 2.0 * (9.1 dataset resolution)."], "parameters": [{"name": "input_dataset", "isInputFile": true, "isOptional": false, "description": "The input dataset whose spatial reference precision will be upgraded. Valid input is a feature class, feature dataset, or raster catalog with a low resolution spatial reference, stored in a 9.2 or current version personal or ArcSDE geodatabase. ", "dataType": "Feature Class; Feature Dataset; Raster Catalog"}, {"name": "xy_resolution", "isOptional": true, "description": "The value to which the dataset's XY Resolution will be changed as part of the upgrade. The maximum value is the same as the dataset's current XY Resolution. ", "dataType": "Double"}, {"name": "z_resolution", "isOptional": true, "description": "The value to which the dataset's Z Resolution will be changed as part of the upgrade. The maximum value is the same as the dataset's current Z Resolution. By default, the resolution will be improved by a factor of 4. ", "dataType": "Double"}, {"name": "m_resolution", "isOptional": true, "description": "The value to which the dataset's M Resolution will be changed as part of the upgrade. The maximum value is the same as the dataset's current M Resolution. By default, the resolution will be improved by a factor of 4. ", "dataType": "Double"}]},
{"syntax": "RegisterWithGeodatabase_management (in_dataset)", "name": "Register with Geodatabase (Data Management)", "description": " Registers feature classes, tables, and raster layers that were created outside of the geodatabase with the geodatabase in order for them to fully participate in geodatabase functionality. ", "example": {"title": "RegisterWithGeodatabase example 1 (Python window)", "description": "The following Python window script demonstrates how to use the RegisterWithGeodatabase function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"Database Connections\\Connection to gpserver.sde\" arcpy.RegisterWithGeodatabase_management ( r'TOOLBOX.REGGDB_LZ77' )"}, "usage": [" Feature classes, tables, and raster layers created outside of the geodatabase can be registered with the geodatabase using this tool. ", "Before a feature class, table, or raster feature class can fully participate in all geodatabase functionality, it must be registered with the geodatabase. If it is not registered with the geodatabase, only limited functionality will be available. "], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "Feature classes, tables, or raster feature classes created outside of the geodatabase are supported. ", "dataType": "Raster Layer; Table View"}]},
{"syntax": "MigrateStorage_management (in_datasets, config_keyword)", "name": "Migrate Storage (Data Management)", "description": "This tool is designed to change the data types used to store rasters in an ArcSDE geodatabase in Oracle, PostgreSQL, and SQL Server; geometries in geodatabases in Oracle and SQL Server; and BLOB objects in  attribute columns in geodatabases in Oracle. This is done through the migration of raster, spatial, or BLOB objects using configuration keywords specified in the DBTUNE table.", "example": {"title": "MigrateStorage example (stand-alone script)", "description": "The following stand-alone script demonstrates how to use the Migrate Storage tool to migrate the input dataset to the ST_Geometry geometry storage type. ", "code": "# Name: MigrateStorage_Example.py # Description: Migrates the input dataset to the ST_Geometry geometry stoage type  # Author: ESRI # Import arcpy module import arcpy # Local variables: inputDataset = \"Database Connections \\\\ Oracle on barbara.sde \\\\ MAP.SBMigrate\" # Process: Migrate Storage arcpy.MigrateStorage_management ( inputDataset , \"ST_GEOMETRY\" )"}, "usage": ["Add datasets you want to migrate to the ", "Input Datasets", " list and specify the ", "Configuration Keyword", " that contains the migration parameters."], "parameters": [{"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": "Datasets to be migrated ", "dataType": "Table View; Raster Layer; Feature Dataset"}, {"name": "config_keyword", "isOptional": false, "description": "DBTUNE configuration keyword containing the appropriate parameter values for the migration. ", "dataType": "String"}]},
{"syntax": "Compress_management (in_workspace)", "name": "Compress (Data Management)", "description": "Compresses an enterprise geodatabase by removing states not referenced by a  version  and redundant rows.", "example": {"title": null, "description": "This stand-alone Python script uses the Compress tool to compress the geodatabase.", "code": "import arcpy arcpy.Compress_management ( \"Database Connections\\Connection to brockville.sde\" )"}, "usage": ["To improve geodatabase performance, the geodatabase should be compressed periodically.", "A compressed geodatabase is more efficient. A geodatabase that is never compressed is more likely to develop errors.", "Once a geodatabase is compressed, deleted records cannot be recovered.", "Compression of geodatabases not only reduces space requirements but can also reduce overall retrieval times.", "When the ", "Compress", " tool is executed, the geodatabase is unavailable until compression is completed.", "Only the geodatabase administrator can perform compression."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The enterprise geodatabase to be compressed. ", "dataType": "Workspace"}]},
{"syntax": "Compact_management (in_workspace)", "name": "Compact (Data Management)", "description": "Compacts a personal or file geodatabase. Compacting rearranges how the geodatabase is stored on disk, often reducing its size and improving performance.", "example": {"title": "Compact Example (Python Window)", "description": "The following Python window script demonstrates how to use the Compact function in immediate mode.", "code": "import arcpy arcpy.Compact_management ( \"c:/landuse.gdb\" )"}, "usage": ["It is recommended to compact personal geodatabases when they become larger than 250 MB.", "If data entry, deletion, or general editing is frequently performed on a database, the database should be regularly compacted to ensure optimal performance.", "If a database is open in ArcMap for editing, it cannot be compacted. To compact the database, remove any layers with a source table or feature class in that database from the Table of Contents.", "Personal and file geodatabases are stored as binary files on a disk drive. As data is added, removed, or edited, these files become fragmented, decreasing overall database performance. The ", "Compact", " tool is used to rearrange how the database is stored on disk by defragmenting these binary files, reducing the database's size on disk and improving database performance."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The personal or file geodatabase to be compacted. ", "dataType": "Workspace"}]},
{"syntax": "ClearWorkspaceCache_management ({in_data})", "name": "Clear Workspace Cache (Data Management)", "description": "Clears any ArcSDE workspaces from the ArcSDE workspace cache.", "example": {"title": "ClearWorkspaceCache Example (Python Window)", "description": "The following Python window script demonstrates how to use the ClearWorkspaceCache function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"c:/connectionFiles/Connection to gpserver.sde\" arcpy.ClearWorkspaceCache_management ()"}, "usage": ["This tool only works with ArcSDE workspaces.", "This tool can be used to help disconnect idle ArcSDE connections in a long-running application.", "If you run the tool without specifying an Input data element, all ArcSDE workspaces in the ArcSDE workspace cache will be cleared. Specify the specific ", ".sde", " file associated with the workspace you want to clear in order to clear a specific ArcSDE workspace."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": true, "description": "The ArcSDE database connection file representing the ArcSDE workspace to be removed from the cache. Specify the path to the ArcSDE connection file that was used in running your geoprocessing tools in order to remove the specific ArcSDE workspace from the cache. Passing no input parameter will clear all ArcSDE workspaces from the cache. ", "dataType": "Data Element; Layer"}]},
{"syntax": "TINCompare_management (in_base_tin, in_test_tin, {compare_type}, {continue_compare}, {out_compare_file})", "name": "TIN Compare (Data Management)", "description": "Compares two TINs and returns the comparison results. TIN Compare can report differences with geometry, TIN node and triangle tags, and spatial reference.", "example": {"title": "TinCompare Example (Python Window)", "description": "The following Python window script demonstrates how to use the TinCompare function in immediate mode.", "code": ""}, "usage": ["The tool returns messages showing the comparison result. By default, the tool will stop executing after encountering the first miscompare. To report all differences, set the continue compare option to true.", "The ", "Output Compare File", " will contain all similarities and differences between the ", "Input Base TIN", " and the ", "Input Test TIN", ". This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. For example, this table can be queried to obtain all the ObjectID values for all the rows that are different.", "The comparison tools ", "result object", " will be 'true' when no differences are found and 'false' when differences are detected."], "parameters": [{"name": "in_base_tin", "isInputFile": true, "isOptional": false, "description": "The Input Base Tin is compared with the Input Test Tin. Input Base Tin refers to data that you have declared valid. This base data has the correct geometry, tag values (if any), and spatial reference. ", "dataType": "TIN Layer"}, {"name": "in_test_tin", "isInputFile": true, "isOptional": false, "description": "The Input Test Tin is compared against the Input Base Tin. ", "dataType": "TIN Layer"}, {"name": "compare_type", "isOptional": true, "description": "The comparison type. ALL \u2014 This is the default. PROPERTIES_ONLY \u2014 Refers to both geometry and TIN tag values, if any, that are assigned to nodes and triangles. SPATIAL_REFERENCE_ONLY \u2014 Coordinate system information. ", "dataType": "String"}, {"name": "continue_compare", "isOptional": true, "description": "Indicates whether to compare all properties after encountering the first mismatch. NO_CONTINUE_COMPARE \u2014 Stop after encountering the first mismatch. This is the default. CONTINUE_COMPARE \u2014 Compare other properties after encountering the first mismatch. ", "dataType": "Boolean"}, {"name": "out_compare_file", "isOutputFile": true, "isOptional": true, "description": "The name and path of the text file which will contain the comparison results. ", "dataType": "File"}]},
{"syntax": "TableCompare_management (in_base_table, in_test_table, sort_field, {compare_type}, {ignore_options}, {attribute_tolerances}, {omit_field}, {continue_compare}, {out_compare_file})", "name": "Table Compare (Data Management)", "description": "Compares two tables or table views and returns the comparison results. This tool can report differences and similarities with tabular values and field definitions.", "example": {"title": "TableCompare Example (Python Window)", "description": "The following Python window script demonstrates how to use the TableCompare function in immediate mode.", "code": ""}, "usage": ["The tool returns messages showing the comparison result. By default, the tool will stop executing after encountering the first miscompare. To report all differences, set the continue compare option to true.", "Multiple sort fields may be specified. Both the ", "Input Base Table", " and ", "Input Test Table", " are sorted based on the fields you specify.  The first field is sorted, then the second field, and so on, in ascending order. Sorting by a common field in both the base and test table ensures that you are comparing the same row from each input dataset.", "By default the compare type is set to ALL. This means all properties of the tables being compared will be checked, including such things as field properties and attributes. However, you may choose a different compare type to check only specific properties of the tables being compared.", "The ", "Ignore Options", " provide the flexibility to omit properties from the comparison. These properties include extension properties, ", "subtypes", ", and ", "relationship classes", ". ", "When omitting fields that are not included in the field count comparison, and the field definitions and tabular values for those fields are ignored.", "Attribute tolerances can only be specified for numeric field types.", "The ", "Output Compare File", " will contain all similarities and differences between the ", "Input Base Table", " and the ", "Input Test Table", ". This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. For example, this table can be queried to obtain all the ObjectID values for all the rows that are different.", "The comparison tools ", "result object", " will be 'true' when no differences are found and 'false' when differences are detected."], "parameters": [{"name": "in_base_table", "isInputFile": true, "isOptional": false, "description": "The Input Base Table is compared with the Input Test Table . The Input Base Table refers to tabular data that you have declared valid. This base data has the correct field definitions and attribute values. ", "dataType": "Table View; Raster Layer"}, {"name": "in_test_table", "isInputFile": true, "isOptional": false, "description": "The Input Test Table is compared against the Input Base Table . The Input Test Table refers to data that you have made changes to by editing or compiling new fields, new records, or new attribute values. ", "dataType": "Table View ; Raster Layer"}, {"name": "sort_field", "isOptional": false, "description": "The field or fields used to sort records in the Input Base Table and the Input Test Table . The records are sorted in ascending order. Sorting by a common field in both the Input Base Table and the Input Test Table ensures that you are comparing the same row from each input dataset. ", "dataType": "Value Table"}, {"name": "compare_type", "isOptional": true, "description": "The comparison type. ALL is the default. The default will compare all properties of the tables being compared. ALL \u2014 Compare all properties. This is the default. ATTRIBUTES_ONLY \u2014 Only compare the attributes and their values. SCHEMA_ONLY \u2014 Only compare the schema.", "dataType": "String"}, {"name": "ignore_options", "isOptional": true, "description": "These properties will not be compared during comparison. IGNORE_EXTENSION_PROPERTIES \u2014 Do not compare extension properties. IGNORE_SUBTYPES \u2014 Do not compare subtypes. IGNORE_RELATIONSHIPCLASSES \u2014 Do not compare Relationship classes.", "dataType": "String"}, {"name": "attribute_tolerances", "isOptional": false, "description": "The numeric value that determines the range in which attribute values are considered equal. This only applies to numeric field types. ", "dataType": "Value Table"}, {"name": "omit_field", "isOptional": false, "description": "The field or fields that will be omitted during comparison. The field definitions and the tabular values for these fields will be ignored. ", "dataType": "String"}, {"name": "continue_compare", "isOptional": true, "description": "Indicates whether to compare all properties after encountering the first mismatch. NO_CONTINUE_COMPARE \u2014 Stop after encountering the first mismatch. This is the default. CONTINUE_COMPARE \u2014 Compare other properties after encountering the first mismatch. ", "dataType": "Boolean"}, {"name": "out_compare_file", "isOutputFile": true, "isOptional": true, "description": "This file will contain all similarities and differences between the Input Base Table and the Input Test Table. This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. ", "dataType": "File"}]},
{"syntax": "RasterCompare_management (in_base_raster, in_test_raster, {compare_type}, {ignore_option}, {continue_compare}, {out_compare_file}, {parameter_tolerances}, {attribute_tolerances}, {omit_field})", "name": "Raster Compare (Data Management)", "description": "Compares the properties of two raster datasets, two raster catalogs, or two mosaic dataset and then returns the comparison result.", "example": {"title": "RasterCompare example 1 (Python window)", "description": "This is a Python sample for RasterCompare.", "code": "import arcpy RasterCompare_management ( \"C:/workspace/image1.tif\" , \"C:/workspace/image2.tif\" , \"RASTER_DATASET\" , \"'Pyramids Exist'\" , \"CONTINUE_COMPARE\" , \"C:/workspace/compare01.txt\" , \"Pixel_Value 1 Value\" , \"Count 5\" , \"OID\" )"}, "usage": ["The tool returns messages showing the comparison result.", "The parameter and attribute tolerances allow your comparisons to have a specified amount of leeway.", "For Python syntax, you will need to open the tool dialog to view a list of values for the ", "Ignore Options", " parameter.  Your ", "Compare Type", " will determine which ", "Ignore Options", " are valid."], "parameters": [{"name": "in_base_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that will be compared to the test raster. Valid inputs include a raster dataset, a raster catalog or a mosaic dataset. ", "dataType": "Raster Layer; Raster Catalog Layer; Mosaic Layer"}, {"name": "in_test_raster", "isInputFile": true, "isOptional": false, "description": "The test raster that will be compared to the input base raster. Valid inputs include a raster dataset, a raster catalog or a mosaic dataset. ", "dataType": "Raster Layer; Raster Catalog Layer; Mosaic Layer"}, {"name": "compare_type", "isOptional": true, "description": "The type of comparison. RASTER_DATASET \u2014 Compares raster dataset properties. GDB_RASTER_DATASET \u2014 Compares properties of raster datasets in a geodatabase. GDB_RASTER_CATALOG \u2014 Compares properties of raster catalogs in a geodatabase. MOSAIC_DATASET \u2014 Compares properties of mosaic datasets.", "dataType": "String"}, {"name": "ignore_option", "isOptional": false, "description": "The properties specified will not be compared during comparison. Open the tool dialog to view a list of values for the Ignore Options parameter. Your Compare Type will determine which Ignore Options are valid. ", "dataType": "String"}, {"name": "continue_compare", "isOptional": true, "description": "Indicates whether to compare all properties after encountering the first mismatch. NO_CONTINUE_COMPARE \u2014 Stop after encountering the first mismatch. This is the default. CONTINUE_COMPARE \u2014 Compare other properties after encountering the first mismatch. ", "dataType": "Boolean"}, {"name": "out_compare_file", "isOutputFile": true, "isOptional": true, "description": "The name and path of the text file which will contain the comparison results. ", "dataType": "File"}, {"name": "parameter_tolerances", "isOptional": false, "description": "The Parameter Tolerance allows you to compare your parameter values with some leeway on accuracy. This allows you to account for any slight changes that might have occurred in processing your data. For a list of Parameter types, choose the parameters for which you would like to have a tolerance. For each parameter you will need a tolerance and tolerance type. The tolerance type is either the value of the tolerance or a fraction of it. When using the fraction type, the fraction is based on the base value; therefore, the tolerance value for comparison would be the fraction times the base value. For example, if your base value is 100 and you set the fraction tolerance to 0.00001, the compare tolerance will be 100 * 0.00001 = 0.001 All \u2014 This option will apply the same tolerance to the extent, pixel value, minimum pixel value, maximum pixel value, mean pixel value, and standard deviation pixel value. Extent \u2014 The extent of the raster will have an allowable tolerance. Pixel_Value \u2014 The pixel values of the raster will have an allowable tolerance. Statistics_Minimum \u2014 The minimum pixel value of the raster will have an allowable tolerance. Statistics_Maximum \u2014 The maximum pixel value of the raster will have an allowable tolerance. Statistics_Mean \u2014 The mean pixel value of the raster will have an allowable tolerance. Statistics_Standard_Deviation \u2014 The standard deviation pixel value of the raster will have an allowable tolerance.", "dataType": "Value Table"}, {"name": "attribute_tolerances", "isOptional": false, "description": "The Attribute Tolerance allows you to compare your attribute values with some leeway on accuracy. This allows you to account for any slight changes that might have occurred in processing your data. Type the field name and tolerance value for each parameter for which you want to have a tolerance. The tolerance value is the actual value of the tolerance, not a fraction. ", "dataType": "Value Table"}, {"name": "omit_field", "isOptional": false, "description": "These are the fields you would like to Omit in the comparison results. Type in the fields to omit in the comparison. When dealing with a raster catalog scenario, you are comparing attribute columns of the raster catalogs, not any attributes within the catalog items. ", "dataType": "String"}]},
{"syntax": "FileCompare_management (in_base_file, in_test_file, {file_type}, {continue_compare}, {out_compare_file})", "name": "File Compare (Data Management)", "description": "Compares two files and returns the comparison results. File Compare can report differences between two ASCII files or two binary files.", "example": {"title": "FileCompare example (Python Window)", "description": "The following Python window script demonstrates how to use the FileCompare function in immediate mode.", "code": ""}, "usage": ["The tool returns messages showing the comparison result. By default, the tool will stop executing after encountering the first miscompare. To report all differences, set the continue compare option to true.", "This tool supports masking out of characters, words, and lines of text in an ASCII file. For example, files may be identical except they may contain text representing date and time of creation. Thus, the files would miscompare. In addition, small variations occur in the way that each platform stores or manipulates numbers. This leads to differences in numeric precision among platforms. The SunOS platform may report a value of 415.999999999, while the Windows XP platform reports 416.000000000. To handle false character comparisons, File Compare provides several masking capabilities. Before comparing new text files with existing base files, edit the base files to include these special masking symbols.", "ASCII is the default file type. If entering BINARY files, change the file type to BINARY.", "When ASCII files miscompare, it will report differences, such as the total number of characters are different and report the differences for each line.", "When BINARY files miscompare, it will report that the file sizes are different and report the differences for each byte.", "The ", "Output Compare File", " will contain all similarities and differences between the ", "Input Base File", " and the ", "Input Test File", ". This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS.", "The comparison tools ", "result object", " will be 'true' when no differences are found and 'false' when differences are detected."], "parameters": [{"name": "in_base_file", "isInputFile": true, "isOptional": false, "description": "The Input Base File is compared with the Input Test File. The Input Base File refers to afile that you have declared valid. This base file has the correct content and information. ", "dataType": "File"}, {"name": "in_test_file", "isInputFile": true, "isOptional": false, "description": "The Input Test File is compared against the Input Base File. The Input Test File refers to afile that you have made changes to by editing or compiling new information. ", "dataType": "File"}, {"name": "file_type", "isOptional": true, "description": "The type of files being compared. ASCII \u2014 Compare using ASCII characters. This is the default. BINARY \u2014 Perform a binary compare.", "dataType": "String"}, {"name": "continue_compare", "isOptional": true, "description": "Indicates whether to compare all properties after encountering the first mismatch. NO_CONTINUE_COMPARE \u2014 Stops after encountering the first mismatch. This is the default. CONTINUE_COMPARE \u2014 Compares other properties after encountering the first mismatch. ", "dataType": "Boolean"}, {"name": "out_compare_file", "isOutputFile": true, "isOptional": true, "description": "This file will contain all similarities and differences between the Input Base File and the Input Test File. This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. ", "dataType": "File"}]},
{"syntax": "FeatureCompare_management (in_base_features, in_test_features, sort_field, {compare_type}, {ignore_options}, {xy_tolerance}, {m_tolerance}, {z_tolerance}, {attribute_tolerances}, {omit_field}, {continue_compare}, {out_compare_file})", "name": "Feature Compare (Data Management)", "description": "Compares two feature classes or layers and returns the comparison results.  Feature Compare  can report differences with geometry, tabular values, spatial reference, and field definitions.", "example": {"title": "FeatureCompare tool example (Python window)", "description": "The following Python window script demonstrates how to use the FeatureCompare function in immediate mode.", "code": ""}, "usage": ["The tool returns messages showing the comparison result. By default, the tool will stop executing after encountering the first miscompare. To report all differences, set the continue compare option to true.", "Multiple sort fields may be specified. The first field is sorted, then the second field, and so on, in ascending order. Sorting by a common field in both the ", "Input Base Features", " and the ", "Input Test Features", " ensures that you are comparing the same row from each input dataset.", "By default, the compare type is set to ALL. This means all properties of the features being compared will be checked, including such things as spatial reference, field properties, attributes, and geometry. However, you may choose a different compare type to check only specific properties of the features being compared.", "The ", "Ignore Options", " provides the flexibility to omit properties such as measure attributes, z attributes, point ID attributes, and extension properties. Two feature classes may be identical, yet one has measures and z coordinates and the other does not. You can choose to ignore these properties. The IGNORE_EXTENSION_PROPERTIES option refers to additional information added to a feature class or table. For example, the features of two annotation feature classes can be identical but the feature classes may have different extension properties, such as different symbols in the symbol collection and different editing behavior.", "The default XY Tolerance is determined by the default XY Tolerance of the Input Base Features. To minimize error, the value you choose for the compare tolerance should be as small as possible. If zero is entered for the XY Tolerance, an exact match is performed.", "The default M Tolerance and the default Z Tolerance is determined by the default M Tolerance and Z Tolerance of the ", "Input Base Features", ". The units are the same as those of the ", "Input Base Features", ". If zero is entered for the M Tolerance and Z Tolerance, an exact match is performed.", "When comparing GEOMETRY_ONLY, the spatial references must match. If the spatial references are different, a miscompare will be reported. If the coordinate system is different for either input, the features will miscompare. This tool does not do projection on the fly.", "The ", "Omit Fields", " parameter is a list of fields that are not included in the field count comparison\u2014their field definitions and tabular values are ignored.", "Attribute tolerances can only be specified for numeric field types.", "The ", "Output Compare File", " will contain all similarities and differences between the ", "Input Base Features", " and the ", "Input Test Features", ". This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. For example, this table can be queried to obtain all the ObjectID values for all the rows that are different. The \"has_error\" field indicates that the record contains an error. True indicates there is a difference.", "One of the first comparisons performed is  a feature count.  If the feature count is reported as being different and the ", "Continue Compare", " parameter is True the subsequent comparison messages may not accurately reflect additional differences between the ", "Input Base Features", " and ", "Input Test Features", ".     This is due to ", "Feature Compare", " inability to figure out where features have been added or removed in the ", "Input Test Features", " and simply moves to the next row in each attribute table. At the location in the attribute table where a feature has been added or deleted the tool will simply move to the next row and begin comparing the base feature with the wrong test feature because the correct one in the ", "Input Test Data", " was deleted or a feature was added before it. ", "The comparison tools ", "result object", " will be 'true' when no differences are found and 'false' when differences are detected."], "parameters": [{"name": "in_base_features", "isInputFile": true, "isOptional": false, "description": "The Input Base Features are compared with the Input Test Features. Input Base features refers to data that you have declared valid. This base data has the correct geometry definitions, field definitions, and spatial reference. ", "dataType": "Feature Layer"}, {"name": "in_test_features", "isInputFile": true, "isOptional": false, "description": "The Input Test Features are compared against the Input Base Features. Input Test Features refers to data that you have made changes to by editing or compiling new features. ", "dataType": "Feature Layer"}, {"name": "sort_field", "isOptional": false, "description": "The field or fields used to sort records in the Input Base Table and the Input Test Table. The records are sorted in ascending order. Sorting by a common field in both the Input Base Features and the Input Test Features ensures that you are comparing the same row from each input dataset. ", "dataType": "Value Table"}, {"name": "compare_type", "isOptional": true, "description": "The comparision type. ALL is the default. The default will compare all properties of the features being compared. ALL \u2014 All properties of the feature classes will be compared. This is the default. GEOMETRY_ONLY \u2014 Only the geometries of the feature classes will be compared. ATTRIBUTES_ONLY \u2014 Only the attributes and their values will be compared. SCHEMA_ONLY \u2014 Only the schema of the feature classes will be compared. SPATIAL_REFERENCE_ONLY \u2014 Only the spatial references of the two feature classes will be compared.", "dataType": "String"}, {"name": "ignore_options", "isOptional": false, "description": "These properties will not be compared during comparison. IGNORE_M \u2014 Do not compare measure properties. IGNORE_Z \u2014 Do not compare elevation properties. IGNORE_POINTID \u2014 Do not compare point id properties. IGNORE_EXTENSION_PROPERTIES \u2014 Do not compare extension properties. IGNORE_SUBTYPES \u2014 Do not compare subtypes. IGNORE_RELATIONSHIPCLASSES \u2014 Do not compare relationship classes. IGNORE_REPRESENTATIONCLASSES \u2014 Do not compare representation classes.", "dataType": "String"}, {"name": "xy_tolerance", "isOptional": true, "description": "The distance that determines the range in which features are considered equal. To minimize error, the value you choose for the compare tolerance should be as small as possible. By default, the compare tolerance is the XY Tolerance of the input base features. ", "dataType": "Linear unit"}, {"name": "m_tolerance", "isOptional": true, "description": "The measure tolerance is the minimum distance between measures before they are considered equal. ", "dataType": "Double"}, {"name": "z_tolerance", "isOptional": true, "description": "The Z Tolerance is the minimum distance between Z coordinates before they are considered equal. ", "dataType": "Double"}, {"name": "attribute_tolerances", "isOptional": false, "description": "The numeric value that determines the range in which attribute values are considered equal. This only applies to numeric field types. ", "dataType": "Value Table"}, {"name": "omit_field", "isOptional": false, "description": "The field or fields that will be omitted during comparison. The field definitions and the tabular values for these fields will be ignored. ", "dataType": "String"}, {"name": "continue_compare", "isOptional": true, "description": "Indicates whether to compare all properties after encountering the first mismatch. NO_CONTINUE_COMPARE \u2014 Stops after encountering the first mismatch. This is the default. CONTINUE_COMPARE \u2014 Compares other properties after encountering the first mismatch. ", "dataType": "Boolean"}, {"name": "out_compare_file", "isOutputFile": true, "isOptional": true, "description": "This file will contain all similarities and differences between the Input Base Features and the Input Test Features. This file is a comma-delimited text file that can be viewed and used as a table in ArcGIS. ", "dataType": "File"}]},
{"syntax": "QuickImport_interop (Input, Output)", "name": "Quick Import (Data Interoperability)", "description": "Converts data in any format supported by the ArcGIS Data Interoperability extension into feature classes. The output is stored in a geodatabase. The geodatabase can then be used directly or further post-processing can be performed.", "example": {"title": null, "description": null, "code": "# Name: QuickImport_Ex_01.py # Requirements: None # Description: Imports Map Info file to a geodatabase # Import system modules import arcpy from arcpy import env # Check out the Data Interoperability Extension arcpy.CheckOutExtension ( \"DataInteroperability\" ) # Set local variables mif_table = \"c:/data/roads.tab\" output_gdb = \"c:/workspace/mif_output.gdb\" # Execute Quick Ixport arcpy.QuickImport_interop ( mif_table , output_gdb )"}, "usage": ["This tool is used to import nonnative data to the ArcGIS environment. The output usually requires additional processing before it can be integrated with existing data or formats.", "This tool can be used as the first step in a model or script in the ArcGIS framework.", "The feature classes that are generated depend on the input data. For instance, if you import two MapInfo MIF/MID files, two features classes will be created.", "This tool  honors the Geoprocessing overwrite setting.", "When in ModelBuilder the feature classes that are generated in the output geodatabase can be accessed using the ", "Select Data", " tool.", "This  tool creates a default output schema for the geodatabase that is created. If the output requires schema changes, then the Spatial ETL Tool should be used.", "Bezier curves", " found in the input are maintained as Bezier curves in the output feature classes."], "parameters": [{"name": "Input", "isOptional": false, "description": "The data to be imported. The syntax can take multiple forms: Additional format-specific parameters can be added after the dataset, separated by a comma. However, the syntax can be complex, so if this is required it is easiest to run the tool using its dialog and copy the Python syntax from the Results window. If the source data is a file with a well-known file extension, it can be given as-is. For instance, \" c:\\data\\roads.mif \". If the source data is not a file, or the file has an unknown extension, the format can be given as part of the argument, separated by a comma. For instance, \" MIF,c:\\data\\roads.mif \". The names for supported formats can be found in the Formats Gallery, by opening this tool in dialog mode and clicking the browse button. Wildcards can be used to read in large datasets. For instance, \" MIF,c:\\data\\roads*.* \". The * character matches any series of characters for all files in the current directory. For instance, c:\\data\\roads*.mif will match c:\\data\\roads.mif , c:\\data\\roads5.mif , and c:\\data\\roads-updated.mif . The ** characters match any subdirectories, recursively. For instance, c:\\data\\**\\*.mif will match c:\\data\\roads.mif , c:\\data\\canada\\rivers.mif , and c:\\data\\canada\\alberta\\edmonton.mif . Additional format-specific parameters can be added after the dataset, separated by a comma. However, the syntax can be complex, so if this is required it is easiest to run the tool using its dialog and copy the Python syntax from the Results window.", "dataType": "Interop Source Dataset"}, {"name": "Output", "isOptional": false, "description": "The output file or personal geodatabase. ", "dataType": "Workspace"}]},
{"syntax": "QuickExport_interop (Input, {Output})", "name": "Quick Export (Data Interoperability)", "description": "Converts one or more input feature classes or feature layers into any format supported by the ArcGIS Data Interoperability extension.", "example": {"title": null, "description": null, "code": "# Name: QuickExport_Ex_01.py # Description: Buffers any layer and exports it to GML. # Requirements: Data Interoperability Extension # Import system modules import arcpy from arcpy import env # Check out the Data Interoperability Extension arcpy.CheckOutExtension ( \"DataInteroperability\" ) # Set local variables tmp_buffered = \"c:/Project/tmp_buffered.shp\" tmp_dissolved = \"c:/Project/tmp_dissolved.shp\" output_dataset = \"GML2,c:/data/buffered.gml\" input_features = \"C:/Project/roads.shp\" # Execute Buffer arcpy.Buffer_analysis ( input_features , tmp_buffered , \"10.000000 Meters\" , \"FULL\" , \"ROUND\" , \"NONE\" , \"\" ) # Execute Dissolve arcpy.Dissolve_management ( tmp_buffered , tmp_dissolved , \"\" , \"\" ) # Execute Quick Export arcpy.QuickExport_interop ( tmp_dissolved , output_dataset )"}, "usage": ["This tool is used to either export data from ArcGIS or as the final step in a model or script where the destination data is nonnative to ArcGIS.", "This tool creates a default output schema for the format you choose to write. If the output requires schema changes then you should consider using ", "Spatial ETL tools", ".", "This tool does not honor the Geoprocessing overwrite setting.", "This tool is most often used to create non-Esri data formats, but it also can create native Esri formats. You can view all export formats in the Formats Gallery accessible from this tool.", "Bezier curves", "  are transformed into line features for output."], "parameters": [{"name": "Input", "isOptional": false, "description": "The feature layers or feature classes that will be exported from ArcGIS ", "dataType": "Feature Layer"}, {"name": "Output", "isOptional": true, "description": "The format and dataset to which the data will be exported. If the destination is a file with a well-known file extension, it can be given as-is. For instance, \" c:\\data\\roads.gml \". If the destination is not a file, or the file has an unknown extension, the format can be given as part of the argument, separated by a comma. For instance, \" MIF,c:\\data\\ \". The names for supported formats can be found in the formats gallery, by opening up this tool in dialog mode and clicking the browse button. Additional format-specific parameters can be added after the dataset, separated by a comma. However, the syntax can be complex, so if this is required it is easiest to run the tool using its dialog and copy the Python syntax from the Results window. ", "dataType": "Interop Destination Dataset"}]},
{"syntax": "UncompressFileGeodatabaseData_management (in_data, {config_keyword})", "name": "Uncompress File Geodatabase Data (Data Management)", "description": "Uncompresses all the contents in a geodatabase, all the contents in a feature\r\ndataset, or an individual stand-alone feature class or table.\r\n", "example": {"title": "UncompressFileGeodatabaseData Example (Python Window)", "description": "The following example demonstrates how to use the UncompressFileGeodatabaseData funcion in the python window.", "code": "import arcpy arcpy.env.workspace = \"C:/data/\" arcpy.UncompressFileGeodatabaseData_management ( \"london.gdb\" )"}, "usage": ["When you uncompress a geodatabase, all feature classes and tables within it uncompress. ", "When you uncompress a feature dataset, all its feature classes uncompress.", "You cannot individually compress or uncompress a feature class in a feature dataset to produce a mixed state where some feature classes are compressed and others are not. Compressed feature datasets allow you to add an uncompressed feature class through operations such as creating a new, empty feature class, copying and pasting, and importing. However, you cannot edit the uncompressed feature class if there are compressed feature classes in the same feature dataset. Once you've finished adding one or more uncompressed feature classes, you can recompress or uncompress the feature dataset so all its feature classes are either compressed or uncompressed."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The geodatabase, feature dataset, feature class, or table to uncompress. ", "dataType": "Feature Dataset; Table View or Raster Layer; Workspace"}, {"name": "config_keyword", "isOptional": true, "description": "The configuration keyword defining how the data will store once uncompressed ", "dataType": "String"}]},
{"syntax": "CompressFileGeodatabaseData_management (in_data, lossless)", "name": "Compress File Geodatabase Data (Data Management)", "description": "Compresses all the contents in a geodatabase, all the contents in a feature\r\ndataset, or an individual stand-alone feature class or table.\r\n", "example": {"title": "CompressFileGeodatabaseData example 1 (Python window) ", "description": "The following example demonstrates how to use the CompressFileGeodatabaseData function in the Python window.", "code": "import arcpy arcpy.env.workspace = \"C:/data/\" arcpy.CompressFileGeodatabaseData_management ( \"london.gdb\" , \"Lossless compression\" )"}, "usage": ["Once compressed, a feature class or table is read-only and cannot be edited. Compression is ideally suited to mature datasets that do not require further editing. However, if required, a compressed dataset can always be uncompressed to return it to its original, read-write format.", "When you compress a geodatabase, all feature classes and tables within it compress. ", "When you compress a feature dataset, all its feature classes compress.", "When you specify a geodatabase as input, this tool compresses all vector feature classes and tables in the geodatabase. It does not compress raster catalogs or raster datasets. If it encounters these in the specified geodatabase, it skips over them. You can individually compress a raster catalog or raster dataset with this tool; however, it makes little sense since the data does not reduce in size. This support is provided strictly as a means to allow ArcPublisher to package to compressed and locked file geodatabase raster catalogs and datasets.", "You cannot individually compress or uncompress a feature class in a feature dataset to produce a mixed state where some feature classes are compressed and others are not. Compressed feature datasets allow you to add an uncompressed feature class through operations such as creating a new, empty feature class, copying and pasting, and importing. However, you cannot edit the uncompressed feature class if there are compressed feature classes in the same feature dataset. Once you've finished adding one or more uncompressed feature classes, you can recompress or uncompress the feature dataset so all its feature classes are either compressed or uncompressed.", "When you display compressed feature class records in ArcCatalog or ArcMap, they may not display in the same order as they did before you compressed the data. The records display in the order in which they are compressed and stored.", "When using lossless compression, floating-point values will be preserved, but compression will not be as effective. With non-lossless compression, floating-point values will be changed, but not below the limit of adequate precision. For example, state boundaries do not usually need to be measured to nanometer precision. Non-lossless compression is up to 20 percent smaller than lossless."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The geodatabase, feature dataset, feature class, or table to compress. ", "dataType": "Feature Dataset; Geometric Network; Raster Layer; Table View; Workspace"}, {"name": "lossless", "isOptional": false, "description": "Indicates whether lossless compression will be used. This parameter is ignored for pre-10.0 file geodatabases. Lossless compression \u2014 Lossless compression will be used. This is the default. Non-lossless compression \u2014 Lossless compression will not be used.", "dataType": "Boolean"}]},
{"syntax": "TransposeTimeFields_management (Input_Feature_Class_or_Table, Fields_to_Transpose, Output_Feature_Class_or_Table, Time_Field_Name, Value_Field_Name, {Attribute_Fields})", "name": "Transpose Time Fields (Data Management)", "description": "Shifts fields from columns to rows in a table or feature class that have time as the field names.This tool is useful when your table or feature class stores time in field names (such as Pop1980, Pop1990, Pop2000, and so on), and you want to create time stamps for the feature class or table so that it can be animated through time.", "example": {"title": "TransposeTimeFields (Python window)", "description": "The following Python window script demonstrates how to use the TransposeTimeFields tool in immediate mode.", "code": "import arcpy arcpy.TransposeTimeFields_management ( \"c:/data/state_pop\" , \"'Y1980 1980';'Y1981 1981';'Y1982 1982'\" , \"c:/data/state_output\" , \"Time\" , \"Value\" , \"STATE_NAME;AVG_ANUAL_\" )"}, "usage": ["If you want the output to be a table, you need to specify the input as a table.", "The output must be a geodatabase feature class. A shapefile is not a supported format for the output feature class.", "ObjectID (or OID, FID, and so on) and Shape fields should not be set as attribute fields."], "parameters": [{"name": "Input_Feature_Class_or_Table", "isInputFile": true, "isOptional": false, "description": "The input feature class or table for which time stamps will be created. ", "dataType": "Table View"}, {"name": "Fields_to_Transpose", "isOptional": false, "description": "The columns from the input table and the corresponding time values. Multiple strings can be entered, depending on how many fields you are transposing. Each string should be formatted as \"Field_Name Time\" (without the quotation marks). Each is a pair of substrings separated by a space. For example, the following string is a valid input: \"POP1980 1980\". In this example, POP1980 is the field name of a field containing population values for 1980. 1980 is the string that will be substituted for POP1980 and populated in the time field of the output table or feature class. ", "dataType": "String"}, {"name": "Output_Feature_Class_or_Table", "isOutputFile": true, "isOptional": false, "description": "The output feature class or table. The output table can be specified as a .dbf table, an info table, or a geodatabase table. The output feature class can only be stored in a geodatabase (shapefile is not available as a format for the output). The output feature class or table will contain a time field, a value field, and any number of attribute fields specified that need to be inherited from the input table. ", "dataType": "Table"}, {"name": "Time_Field_Name", "isOptional": false, "description": "The name of the time field that will be created to store time values. The default name is \"Time\". Any valid field name can be used. ", "dataType": "String"}, {"name": "Value_Field_Name", "isOptional": false, "description": "The name of the value field that will be created to store the values from the input table. The default name is \"Value\". Any valid field name can be set, as long as it does not conflict with existing field names from the input table or feature class. ", "dataType": "String"}, {"name": "Attribute_Fields", "isOptional": false, "description": "Attribute fields from the input table to be included in the output table. ", "dataType": "Field"}]},
{"syntax": "DeleteField_management (in_table, drop_field)", "name": "Delete Field (Data Management)", "description": "This tool deletes one or more fields from a table, feature class, feature layer, or raster dataset.", "example": {"title": "DeleteField example (Python window)", "description": "The following Python window script demonstrates how to use the DeleteField tool in immediate mode:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CopyFeatures_management ( \"majorrds.shp\" , \"C:/output/majorrds_copy.shp\" ) arcpy.DeleteField_management ( \"C:/output/majorrds_copy.shp\" , [ \"STREET_NAM\" , \"LABEL\" , \"CLASS\" ])"}, "usage": ["This tool can be used with any table; ArcSDE, file, or personal geodatabase feature class; coverage; raster dataset; or shapefile.", "Fields cannot be deleted from nonnative, read-only data formats in ArcGIS, such as VPF and CAD datasets.", "The ", "Drop Field", " parameter ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run or its derived data does not exist, the ", "Drop Field", " parameter may not be populated with field names. The ", "Add Field", " button allows you to add expected fields so you can complete the Delete Field dialog box and continue to build your model."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table containing the fields to be deleted. The existing input table will be modified. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "drop_field", "isOptional": false, "description": "The fields to be dropped from the input table. Only nonrequired fields may be deleted. ", "dataType": "Field"}]},
{"syntax": "CalculateField_management (in_table, field, expression, {expression_type}, {code_block})", "name": "Calculate Field (Data Management)", "description": "Calculates the values of a field for a feature class, feature layer, or raster catalog.  \r\n View examples of using Calculate Field \r\n", "example": {"title": "CalculateField example (Python window)", "description": "The following Python window script demonstrates how to use the CalculateField function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.AddField_management ( \"vegtable.dbf\" , \"VEG_TYP2\" , \"TEXT\" , \"\" , \"\" , \"20\" ) arcpy.CalculateField_management ( \"vegtable.dbf\" , \"VEG_TYP2\" , '!VEG_TYPE!.split(\" \")[-1]' , \"PYTHON_9.3\" )"}, "usage": ["Python expressions can be created using properties from the ", "Geometry", " object (", "type", ", ", "extent", ", ", "centroid", ", ", "firstPoint", ", ", "lastPoint", ", ", "area", ", ", "length", ", ", "isMultipart", ", and ", "partCount", ").", "Python expressions can use the geometry ", "area", " and ", "length", " properties with an areal or linear unit to convert the value to a different unit of measure (e.g. ", "!shape.length@kilometers!", "). If the data is stored in a geographic coordinate system and a linear unit (for example, miles) is supplied, the length will be calculated using a geodesic algorithm.   Using areal units on geographic data will yield questionable results as decimal degrees are not consistent across the globe.", "In the tool dialog box, an expression can be entered directly into the ", "Expression", " parameter, or interactively built using the Field Calculator.", "When used with a selected set of features, such as those created from a query in ", "Make Feature Layer", " or ", "Select Layer by Attribute", ", this tool will only update the selected records.", "The calculation can only be applied to one field per operation.", "Existing field values will be overwritten. A copy of the input table should be made if you want to preserve the original values", "For Python calculations, field names must be enclosed in exclamation points (", "!fieldname!", ").", "For VB calculations, field names must be enclosed in square brackets (", "[fieldname]", ").", "To calculate strings to text or character fields, in the dialog box the string must be double-quoted (\"string\"), or in scripting, the double-quoted string must also be encapsulated in single quotes ('\"string\"').", "This tool can also be used to update character items. Expressions using a character string should be wrapped, using single quotes\u2014for example, ", "[CHARITEM] = 'NEW STRING'", ". However, if the character string has embedded single quotes, wrap the string using double quotes\u2014for example, ", "[CHARITEM] = \"TYPE'A'\"", ".", "To calculate a field to be a numeric value, enter the numeric value in the ", "Expression", " parameter; no quotes around the value are required.", "The ", "arcgis.rand()", " function is supported by this tool when a Python expression is specified. The ", "arcgis.rand()", " function has been created for ArcGIS tools and should not be confused with the Python Rand() function. The syntax for the available distributions for the ", "arcgis.rand()", " function can be found at ", "The distribution syntax for random values", ".", "The expression and code block are connected. The code block must relate back to the expression; the result of the code block should be passed into the expression.", "The ", "Code Block", " parameter allows you to create complex expressions. You can enter the code block directly on the dialog box, or as a continuous string in scripting.", "The Python math module and formatting are available for use in the ", "Code Block", " parameter.  You can import additional  modules. The math module provides number-theoretic and representation functions, power and logarithmic functions, trigonometric functions, angular conversion functions, hyberbolic functions, and mathematical constants. To learn more about the math module, see Python's help.", " Saved VB .cal files from previous versions of ArcGIS may work or require minimal modifications. If you have VBA code from past releases that use ArcObjects, you will need to modify your calculations to work.", "When calculating joined data, you cannot calculate the joined columns directly. However, you can directly calculate the columns of the origin table. To calculate the joined data, you must first add the joined tables or layers to ArcMap. You can then perform calculations on this data separately. These changes will be reflected in the joined columns."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table containing the field that will be updated with the new calculation. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "field", "isOptional": false, "description": "The field that will be updated with the new calculation. ", "dataType": "Field"}, {"name": "expression", "isOptional": false, "description": "The simple calculation expression used to create a value that will populate the selected rows. ", "dataType": "SQL Expression"}, {"name": "expression_type", "isOptional": true, "description": "Specify the type of expression that will be used. VB \u2014 The expression will be written in a standard VB format. This is the default. PYTHON \u2014 The expression will be written in a standard Python format. Use of geoprocessor methods and properties is the same as creating a 9.2 version geoprocessor. PYTHON_9.3 \u2014 The expression will be written in a standard Python format. Use of geoprocessor methods and properties is the same as creating a 9.3 version geoprocessor. ", "dataType": "String"}, {"name": "code_block", "isOptional": true, "description": "Allows for a block of code to be entered for complex expressions. ", "dataType": "String"}]},
{"syntax": "CalculateEndDate_management (Input_Table, {Unique_ID_Fields}, Start_date_field, End_date_field)", "name": "Calculate End Date (Data Management)", "description": "Populates the values for a specified end date field with values calculated using the start date field specified. This tool is useful when the intervals between start date field values are not regular and you want to animate the feature class or table through time or some other value using the Animation toolbar.", "example": {"title": "CalculateEndDate (Python window)", "description": null, "code": "import arcpy arcpy.CalculateEndDate_management ( \"C:/data/HistPop.shp\" , \"State_FIP;County_FIP\" , \"Start_Date\" , \"End_Date\" )"}, "usage": ["The table is first sorted by entity (unique value field) if this is specified, and then by time stamp. With the start date field sorted in ascending order, the end date of any row is the same as the start date of the next row.", "In order to use this tool the start date field must be able to be sorted in ascending order. To test this, open the attribute table for the feature class, right-click the field and click ", "Sort Ascending", ". If the field cannot be sorted in ascending order, the field must be reformatted before using this tool.", "The end date field value for the last row will be the same as the start date field value."], "parameters": [{"name": "Input_Table", "isInputFile": true, "isOptional": false, "description": "The feature class or table for which an end date field is calculated based on the start date field specified. ", "dataType": "Table View"}, {"name": "Unique_ID_Fields", "isOptional": false, "description": "The name of the field or fields that can be used to uniquely identify spatial entities. This field or these fields are used to first sort based on entity type if there is more than one entity. For instance, for a feature class representing population values per state over time, state name could be the unique value field (the entity). If population figures are per county, you would need to set county name and state name as the unique value fields, since some county names are the same for different states. If there is only one entity, this parameter can be ignored. ", "dataType": "Field"}, {"name": "Start_date_field", "isOptional": false, "description": "The field containing values that will be used to calculate values for the end date field. The start date field and the end date field must be of the same format. ", "dataType": "Field"}, {"name": "End_date_field", "isOptional": false, "description": "The field that will be populated with values based on the start date field specified. The start date field and the end date field must be of the same format. ", "dataType": "Field"}]},
{"syntax": "AssignDefaultToField_management (in_table, field_name, default_value, {subtype_code})", "name": "Assign Default To Field (Data Management)", "description": "This tool will create a default value for a specified field.  Whenever a new row is added to the table or feature class, the specified field will be set to this default value.", "example": {"title": "AssignDefaultToField example (Python window)", "description": "The following Python window script demonstrates how to use the AssignDefaultToField tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Montgomery.gdb/Landbase\" arcpy.CopyFeatures_management ( \"blocks\" , \"C:/output/output.gdb/blocks\" ) arcpy.AssignDefaultToField_management ( \"C:/output/output.gdb/blocks\" , \"Res\" , 1 , [ \"0: Non-Residental\" , \"1: Residental\" ])"}, "usage": ["The default value is dependent on the field type chosen in the ", "Field Name", " parameter. If you pick a field that is type LONG, the default value has to be type LONG. ", "Adding subtypes to the default value is optional. If you add a subtype, there must be a subtype field in the feature class or table. You can set the subtype field using the ", "Set Subtype Field", " tool.", "The subtypes of a feature class or table can also be managed in the ", "Catalog", " window. Subtypes can be created and modified using the Subtypes Property page on the dataset Properties dialog box."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "Input table or feature class that will have a default value added to one of its fields. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "field_name", "isOptional": false, "description": "Field that will have the default value added to it each time a new row is added to the table or feature class. ", "dataType": "Field"}, {"name": "default_value", "isOptional": false, "description": "The string for the default value to be added to each new table or feature class. The field type controls what kind of string can be added. ", "dataType": "String"}, {"name": "subtype_code", "isOptional": false, "description": "The subtypes that can participate in the default value. You specify the subtypes to be used. ", "dataType": "String"}]},
{"syntax": "AddField_management (in_table, field_name, field_type, {field_precision}, {field_scale}, {field_length}, {field_alias}, {field_is_nullable}, {field_is_required}, {field_domain})", "name": "Add Field (Data Management)", "description": "Adds a new field to a table or the table of a feature class, feature layer, raster catalog, and/or rasters with attribute tables.", "example": {"title": "AddField example (Python window)", "description": "The following Python window script demonstrates how to use the AddField tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/airport.gdb\" arcpy.AddField_management ( \"schools\" , \"ref_ID\" , \"LONG\" , 9 , \"\" , \"\" , \"refcode\" , \"NULLABLE\" , \"REQUIRED\" )"}, "usage": ["Coverages, stand-alone tables, feature classes from ArcSDE and personal or file geodatabases, layer files, raster catalogs, and shapefiles will work as valid input for this command. VPF and CAD feature data overlays will not work since they are read-only formats that are not native to ArcGIS.", "For coverages, shapefiles, and dBase tables, if field type defines a character, blanks are inserted for each record. If field type defines a numeric item, zeros are inserted for each record.", "The added field will always be displayed at the end of the table.", "The ", "Field Length", " parameter is only applicable on fields of type text or blob.", "For geodatabases, if field type defines a character or number <null> is inserted into each record if the ", "Field Is Nullable", " parameter default is accepted.", "A shapefile does not support alias for fields, so you cannot add a field alias to a shapefile.", "It is only possible to add a field that is not nullable to an empty geodatabase feature class or table. This tool cannot add a field that is not nullable when the rows already exist.", "The ", "Field Domain", " parameter can use an existing domain from a feature class in a personal, file, or SDE geodatabase.", "The precision and scale of a field describe the maximum size and precision of data that can be stored in the field. The precision describes the number of digits that can be stored in the field and the scale describes the number of decimal places for float and double fields. For example, if the field value is 54.234, then scale = 3 and precision = 5.", "Use the following guidelines for choosing the correct field type for a given precision and scale:", "When creating a new field in a geodatabase feature class or table, you can specify the field's type, but not its precision, and scale. Even if the dialog box allows you to add a value for precision or scale, it will be ignored during execution.", "The name of an existing domain must be specified for the ", "Field Domain", " parameter. Entering invalid domain names or values will not cause the tool to fail but it will be ignored and no domain will be set for the field.", "Fields set as REQUIRED are permanent and you will not be able to delete them with future processing. To allow for deletion at a later time set the field to NON_REQUIRED (the default).", "A field of type raster allows you to have a raster image as an attribute. It is stored within or alongside the geodatabase. This is helpful when a picture is the best way to describe a feature. Precision, scale, and length cannot be set for fields of type raster."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input table to which the specified field will be added. The field will be added to the existing input table and will not create a new output table. Fields can be added to feature classes of ArcSDE, file or personal geodatabases, coverages, shapefiles, raster catalogs, stand-alone tables, rasters with attribute tables, and/or layers. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "field_name", "isOptional": false, "description": "The name of the field that will be added to the Input Table. ", "dataType": "String"}, {"name": "field_type", "isOptional": false, "description": "The field type used in the creation of the new field. TEXT \u2014 Names or other textual qualities. FLOAT \u2014 Numeric values with fractional values within a specific range. DOUBLE \u2014 Numeric values with fractional values within a specific range. SHORT \u2014 Numeric values without fractional values within a specific range; coded values. LONG \u2014 Numeric values without fractional values within a specific range. DATE \u2014 Date and/or Time. BLOB \u2014 Images or other multimedia. RASTER \u2014 Raster images. GUID \u2014 GUID values", "dataType": "String"}, {"name": "field_precision", "isOptional": true, "description": "Describes the number of digits that can be stored in the field. All digits are counted no matter what side of the decimal they are on. If the input table is a personal or file geodatabase the field precision value will be ignored. ", "dataType": "Long"}, {"name": "field_scale", "isOptional": true, "description": "Sets the number of decimal places stored in a field. This parameter is only used in Float and Double data field types. If the input table is a personal or file geodatabase the field scale value will be ignored. ", "dataType": "Long"}, {"name": "field_length", "isOptional": true, "description": "The length of the field being added. This sets the maximum number of allowable characters for each record of the field. This option is only applicable on fields of type text or blob. ", "dataType": "Long"}, {"name": "field_alias", "isOptional": true, "description": "The alternate name given to the field name. This name is used to give more descriptive names to cryptic field names. The field alias parameter only applies to geodatabases and coverages. ", "dataType": "String"}, {"name": "field_is_nullable", "isOptional": true, "description": "A geographic feature where there is no associated attribute information. These are different from zero or empty fields and are only supported for fields in a geodatabase. NON_NULLABLE \u2014 The field will not allow null values. NULLABLE \u2014 The field will allow null values. This is the default. ", "dataType": "Boolean"}, {"name": "field_is_required", "isOptional": true, "description": "Specifies whether the field being created is a required field for the table; only supported for fields in a geodatabase. NON_REQUIRED \u2014 The field is not a required field. This is the default. REQUIRED \u2014 The field is a required field. Required fields are permanent and can not be deleted. ", "dataType": "Boolean"}, {"name": "field_domain", "isOptional": true, "description": "Used to constrain the values allowed in any particular attribute for a table, feature class, or subtype in a geodatabase. You must specify the name of an existing domain for it to be applied to the field. ", "dataType": "String"}]},
{"syntax": "UnsplitLine_management (in_features, out_feature_class, {dissolve_field}, {statistics_fields})", "name": "Unsplit Line (Data Management)", "description": " Merges lines that have coincident endpoints and, optionally, common attribute values.", "example": {"title": "UnsplitLine example (Python window)", "description": "The following Python window script demonstrates how to use the UnsplitLine tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Portland.gdb/Streets\" arcpy.UnsplitLine_management ( \"streets\" , \"C:/output/output.gdb/streets_unsplit\" , [ \"STREETNAME\" , \"PREFIX\" ])"}, "usage": ["The attributes of the features which become aggregated by this tool can be summarized or described using a variety of statistic types. The statistic type used to summarize attributes is added to the output feature class as a single field with the following naming standard of statistic type + underscore + input field name. For example, if the input has a field named POP that is summarized, using the SUM statistics type will result in a field named SUM_POP in the output feature class.", "The availability of physical memory may limit the amount (and complexity) of input features that can be processed and dissolved into a single output feature. This limitation could cause an error to occur, as the dissolve process may require more memory than is available. To prevent this, ", "Dissolve", " may divide and process the  input features using an adaptive tiling algorithm. To determine the features that have been tiled, run the ", "Frequency", " tool on the result of this tool, specifying the same fields used in the dissolve process for the ", "Frequency Field(s)", " parameter. Any record with a frequency value of 2 has been tiled. Tile boundaries are preserved in the output features to prevent the creation of features that are too large to be used by ArcGIS. ", "Running ", "Dissolve", " on the output of a previous dissolve run will rarely reduce the number of features in the output when the original processing divided and processed the inputs using  adaptive tiling.  The maximum size of any output feature is determined by the amount of available memory at run time; therefore, output containing tiles is an indicator that dissolving any further with the available resources would cause an out-of-memory situation or result in a feature that is unusable.   Additionally, running the ", "Dissolve", " tool a second time on output that was created this way may experience very slow performance for little to no gain and may cause an unexpected failure.", "Null values are excluded from all statistical calculations. For example, the AVERAGE of 10, 5, and NULL is 7.5 ((10+5)/2). The COUNT tool returns the number of values included in the statistical calculation, which in this case is 2.", "The ", "Dissolve Field(s)", " parameter's ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Dissolve Field(s)", " parameter may not be populated with field names. The ", "Add Field", " button allows you to add expected fields so you can complete the tool's dialog box and continue to build your model."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The line features to be aggregrated. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to be created that will contain the aggregated features. ", "dataType": "Feature Class"}, {"name": "dissolve_field", "isOptional": false, "description": "The field or fields on which to aggregate features. The Add Field button, which is used only in ModelBuilder, allows you to add expected fields so you can complete the dialog box and continue to build your model. ", "dataType": "Field"}, {"name": "statistics_fields", "isOptional": false, "description": "The fields and statistics with which to summarize attributes. Text attribute fields may be summarized using the statistics FIRST or LAST. Numeric attribute fields may be summarized using any statistic. Nulls are excluded from all statistical calculations. FIRST\u2014Finds the first record in the Input Features and uses its specified field value. LAST\u2014Finds the last record in the Input Features and uses its specified field value. SUM\u2014Adds the total value for the specified field. MEAN\u2014Calculates the average for the specified field. MIN\u2014Finds the smallest value for all records of the specified field. MAX\u2014Finds the largest value for all records of the specified field. RANGE\u2014Finds the range of values (MAX\u2013MIN) for the specified field. STD\u2014Finds the standard deviation on values in the specified field. COUNT\u2014Finds the number of values included in statistical calculations. This counts each value except null values. To determine the number of null values in a field, use the COUNT statistic on the field in question, and a COUNT statistic on a different field which does not contain nulls (for example, the OID if present), then subtract the two values.", "dataType": "Value Table"}]},
{"syntax": "SplitLine_management (in_features, out_feature_class)", "name": "Split Line At Vertices (Data Management)", "description": "Creates a feature class containing lines that are generated by splitting input lines or polygon boundaries at their vertices. ", "example": {"title": "SplitLine Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the SplitLine function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.SplitLine_management ( \"roads.shp\" , \"c:/output/output.gdb/roads_split\" )"}, "usage": ["The attributes of the input features will be maintained in the output feature class. ", "If an input line has no vertices between its start and end points, it will be copied to the output as it is; otherwise, every segment between two consecutive vertices will become a line feature in the output. Similarly, every segment between two consecutive vertices along a polygon boundary will become a line feature in the output. The output feature class can be a much larger file, depending on how many vertices the input features have.", " A parametric (true) curve line or segment will not be densified and will remain true curve as an output line feature. This does not apply to shapefile data.", " The function name of this tool in scripting is ", "SplitLine", ", not ", "SplitLineAtVertices."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features that can be line or polygon. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output line feature class. ", "dataType": "Feature Class"}]},
{"syntax": "SplitLineAtPoint_management (in_features, point_features, out_feature_class, {search_radius})", "name": "Split Line At Point (Data Management)", "description": "Splits line features based on intersection or proximity to point features.", "example": {"title": "SplitLineAtPoint Example(stand-alone script)", "description": "This example shows how to use a Python script to run SplitLineAtPoint.", "code": "#Name: SplitLineAtPoint_Example.py # Description: split line features based upon near point features; Search Distance is in linear  # unit meters # Requirements:  # Author: ESRI import arcpy from arcpy import env env.workspace = \"C:/data\" inFeatures = \"streets.shp\" pointFeatures = \"events.shp\" outFeatureclass = \"splitline_out.shp\" searchRadius = \"20 Meters\" try : arcpy.SplitLineAtPoint_management ( inFeatures , pointFeatures , outFeatureclass , searchRadius ) except Exception , e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"Line  %i \" % tb.tb_lineno print e.message"}, "usage": ["Input Features", " must be lines. ", "If ", "Search Distance", " (", "search_radius", ") is unspecified, a nearest point will be used to split the line feature. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input line features to be split. ", "dataType": "Feature Layer"}, {"name": "point_features", "isOptional": false, "description": "The input point features whose locations will be used to split the input lines. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The new feature class that will be created containing the split lines. ", "dataType": "Feature Class"}, {"name": "search_radius", "isOptional": true, "description": "Used to split lines by their proximity to point features. Points within the search distance to an input line will be used to split those lines at the nearest location to the point along the line segment. ", "dataType": "Linear Unit"}]},
{"syntax": "RepairGeometry_management (in_features, {delete_null})", "name": "Repair Geometry (Data Management)", "description": "Inspects each  feature in a feature class for geometry problems.  Upon discovery of a geometry problem, a relevant fix will be applied, and a one line description will be printed identifying the feature as well as the problem encountered.  Valid inputs are shapefiles, personal and file geodatabase feature classes. \r\n Learn more about checking and repairing geometries \r\n", "example": {"title": "Repair Geometry example (Python window)", "description": "The following Python Window script demonstrates how to use the RepairGeometry function in immediate mode.", "code": "import arcpy arcpy.RepairGeometry_management ( \"c:/data/sketchy.shp\" )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "This tool uses the same logic as the ", "Check Geometry tool", " to evaluate if a geometry has a problem.", "Below is the list of geometry problems and the corresponding fix that will be performed by the tool:", "SDE geodatabases automatically check and repair feature geometries when the features are uploaded to the database, so using the ", "Check Geometry", " and ", "Repair Geometry", " tools with SDE feature classes is unnecessary.", "After applying one of the repairs above, the tool will re-evaluate the resulting geometry, and if another problem is discovered, the relevant fix will be performed. ", "Starting with the 10.0 release, a line geometry  is no longer considered \"self-intersecting\" if it crosses itself.  There was no adverse effect to these types of geometry therefore the ", "Check Geometry", " tool will no longer report the feature as a problem, and the ", "Repair Geometry", " tool will no longer perform a \"fix\" on the feature's geometry.  Prior to the 10.0 release ", "Repair Geometry", " would add vertices at the point of intersection of \"self-intersecting\" lines.  If you do want to add vertices when line features intersect themselves use the ", "Integrate", " tool."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature class or layer that will be repaired. Valid input features are shapefiles and personal and file geodatabase feature classes. ", "dataType": "Feature Layer"}, {"name": "delete_null", "isOptional": true, "description": "Specifies what action to take on null geometries. DELETE_NULL \u2014 Features which have NULL geometry will be deleted from the input. This is the default. KEEP_NULL \u2014 Features which have NULL geometry will NOT be deleted from the input. ", "dataType": "Boolean"}]},
{"syntax": "PolygonToLine_management (in_features, out_feature_class, {neighbor_option})", "name": "Polygon To Line (Data Management)", "description": " Creates a feature class containing lines that are converted from polygon boundaries with or without considering neighboring polygons.", "example": {"title": "PolygonToLine Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the PolygonToLine function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.PolygonToLine_management ( \"Habitat_Analysis.gdb/vegtype\" , \"C:/output/Output.gdb/vegtype_lines\" , \"IGNORE_NEIGHBORS\" )"}, "usage": ["If  the checkbox  ", "Identify and store polygon neighboring information", " is checked (the ", "neighbor_option", " is set to IDENTIFY_NEIGHBORS in scripting), the polygon neighboring relationship will be analyzed. As illustrated above, the boundaries are converted to lines taking into account of crossing or shared segments;    two new fields, LEFT_FID and RIGHT_FID, will be added to the output feature class and set to the feature IDs of the input polygons to the left and right of each output line. The attributes of the input features will not be maintained in the output feature class. The following scenarios help you understand the process and output in more detail:", "If  the checkbox ", "Identify and store polygon neighboring information", " is unchecked (the ", "neighbor_option", " is set to IGNORE_NEIGHBORS in scripting), the polygon neighboring relationship will be ignored. Each input polygon boundary will be written out as an enclosed line feature.  A multipart polygon will become a multipart line  in the output. The attributes of the input features will  be maintained in the output feature class. A new field, ORIG_FID, will be added to the output and set to the input feature IDs of each line. ", "For parametric (true) curve input features, the output lines will remain true curves even if they are split. This does not apply to shapefile data.", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features that must be polygon. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output line feature class. ", "dataType": "Feature Class"}, {"name": "neighbor_option", "isOptional": true, "description": "Specifies whether or not to identify and store polygon neighboring information. IDENTIFY_NEIGHBORS \u2014 Polygon neighboring relationship will be identified and stored in the output. If different segments of a polygon share boundary with different polygons, the boundary will be split such that each uniquely shared segment will become a line with its two neighboring polygon FIDs stored in the output. This is the default. IGNORE_NEIGHBORS \u2014 Polygon neighboring relationship will be ignored; every polygon boundary will become a line feature with its original polygon feature ID stored in the output.", "dataType": "Boolean"}]},
{"syntax": "PointsToLine_management (Input_Features, Output_Feature_Class, {Line_Field}, {Sort_Field}, {Close_Line})", "name": "Points To Line (Data Management)", "description": "Creates line features from points.", "example": {"title": "PointsToLine Example (Python Window)", "description": "The following Python window script demonstrates how to use the PointsToLine function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.PointsToLine_management ( \"calibration_points.shp\" , \"C:/output/output.gdb/out_lines\" , \"ROUTE1\" , \"MEASURE\" )"}, "usage": ["Line feature will not be written to the output if they are made up of less than two vertices."], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": " The point features to be converted into lines. ", "dataType": "Feature Layer"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": " The line feature class which will be created from the input points. ", "dataType": "Feature Class"}, {"name": "Line_Field", "isOptional": true, "description": " Each feature in the output will be based on unique values in the Line Field. ", "dataType": "Field"}, {"name": "Sort_Field", "isOptional": true, "description": "By default, points used to create each output line feature will be used in the order they are found. If a different order is desired, specify a Sort Field. ", "dataType": "Field"}, {"name": "Close_Line", "isOptional": true, "description": "Specifies whether output line features should be closed. CLOSE \u2014 An extra vertex will be added to ensure that every output line feature's end point will match up with its start point. Then polygons can be generated from the line feature class using the Feature To Polygon tool. NO_CLOSE \u2014 No extra vertices will be added to close an output line feature. This is the default. ", "dataType": "Boolean"}]},
{"syntax": "MultipartToSinglepart_management (in_features, out_feature_class)", "name": "Multipart To Singlepart (Data Management)", "description": "Creates a feature class containing singlepart features generated by separating multipart input features.", "example": {"title": "MultipartToSinglepart Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the MultipartToSinglepart function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.MultipartToSinglepart_management ( \"landuse.shp\" , \"c:/output/output.gdb/landuse_singlepart\" )"}, "usage": [" The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs. ", "Each part of a multipart input feature will become an individual singlepart feature in the output feature class. Features that are already singlepart will not be affected. ", "Most of the output feature types will be the same as input (input polygons remain polygons; input lines remain lines). The one exception is if the input features are type multipoint, the output feature class will be type point.", "To reconstruct multipart features from singlepart features based on a common field value, such as ORIG_FID, use the ", "Dissolve", " tool."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features that can be any feature type. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class containing features that vary with input feature type. ", "dataType": "Feature Class"}]},
{"syntax": "MinimumBoundingGeometry_management (in_features, out_feature_class, {geometry_type}, {group_option}, {group_field}, {mbg_fields_option})", "name": "Minimum Bounding Geometry (Data Management)", "description": "Creates a feature class containing polygons which represent a specified minimum bounding geometry enclosing each input feature or each group of input features.", "example": {"title": "MinimumBoundingGeometry Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the MinimumBoundingGeometry function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.MinimumBoundingGeometry_management ( \"parks.shp\" , \"c:/output/output.gdb/parks_mbg\" , \"RECTANGLE_BY_AREA\" , \"NONE\" )"}, "usage": ["The output polygon features and their attributes will vary depending on the specified geometry type and grouping choices.", "The ", "Geometry Type", " (geometry_type) options CONVEX_HULL, CIRCLE, and ENVELOPE are only available with an ", "ArcGIS for Desktop Advanced", " license.", "The ", "Group Option", " parameter on the dialog box (the ", "group_option", " parameter in scripting) will affect the output polygons and attributes in the following ways: ", "Each geometry type can be characterized by one or more unique measurements; these measurements can optionally be added to the output as new fields as described below. The width, length, and diameter values are in feature units; the orientation angles are in decimal degrees clockwise from north. The prefix, MBG_, indicates minimum boulding geometry field.", "There are special cases of input features that would result in invalid (zero-area) output polygons. In these cases, a small value derived from the input feature XY Tolerance will be used as the width, length, or diameter to create output polygons. These polygons serve as 'place holders' for keeping track of features. If the resulting polygons appear 'invisible' in ArcMap using the default polygon outline width, change to a thicker outline line symbol to display them.  The examples of these cases include: "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features that can be point, multipoint, line, polygon, or multipatch. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output polygon feature class. ", "dataType": "Feature Class"}, {"name": "geometry_type", "isOptional": true, "description": "Specifies what type of minimum bounding geometry the output polygons will represent. The CONVEX_HULL, CIRCLE, and ENVELOPE options are only available with an ArcGIS for Desktop Advanced license. RECTANGLE_BY_AREA \u2014 The rectangle of the smallest area enclosing an input feature. This is the default. RECTANGLE_BY_WIDTH \u2014 The rectangle of the smallest width enclosing an input feature. CONVEX_HULL \u2014 The smallest convex polygon enclosing an input feature. CIRCLE \u2014 The smallest circle enclosing an input feature. ENVELOPE \u2014 The envelope of an input feature.", "dataType": "String"}, {"name": "group_option", "isOptional": true, "description": "Specifies how the input features will be grouped; each group will be enclosed with one output polygon. NONE \u2014 Input features will not be grouped. This is the default. This option is not available for point input. ALL \u2014 All input features will be treated as one group. LIST \u2014 Input features will be grouped based on their common values in the specified field or fields in the group field parameter.", "dataType": "String"}, {"name": "group_field", "isOptional": false, "description": "The field or fields in the input features that will be used to group features, when LIST is specified as Group Option. At least one group field is required for LIST option. All features that have the same value in the specified field or fields will be treated as a group. ", "dataType": "Field"}, {"name": "mbg_fields_option", "isOptional": true, "description": "Specifies whether to add the geometric attributes in the output feature class or omit them in the output feature class. NO_MBG_FIELDS \u2014 Omits any input attributes in the output feature class. This is the default. MBG_FIELDS \u2014 Adds the geometric attributes in the output feature class.", "dataType": "Boolean"}]},
{"syntax": "FeatureVerticesToPoints_management (in_features, out_feature_class, {point_location})", "name": "Feature Vertices To Points (Data Management)", "description": "Creates a feature class containing points generated from specified vertices or locations of the input features. ", "example": {"title": "FeatureVerticesToPoints Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the FeatureVerticesToPoints function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.FeatureVerticesToPoints_management ( \"parcels.shp\" , \"c:/output/output.gdb/parcels_corner\" , \"ALL\" )"}, "usage": [" The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs. ", "For multipart lines or polygons, each part will be treated as a line. Therefore, each part will have its own start, end, and mid points, as well as possible dangle point(s).", "A parametric (true) curve has only the start and end points and will not be densified.", "For the DANGLE option of the ", "Point Type", " parameter on the dialog box (the ", "point_location", " parameter in scripting), an additional field, DANGLE_LEN carrying the dangle length values in feature unit, will be added to the output feature class. For an isolated line, both endpoints are dangle points; therefore, the dangle length is the line length itself. For a dangle line that intersects another line at one of its endpoints, the dangle length is measured from the dangling endpoint to the intersection."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input features that can be line or polygon. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output point feature class. ", "dataType": "Feature Class"}, {"name": "point_location", "isOptional": true, "description": " Specifies where an output point will be created. ALL \u2014 A point will be created at each input feature vertex. This is the default. MID \u2014 A point will be created at the midpoint, not necessarily a vertex, of each input line or polygon boundary. START \u2014 A point will be created at the start point (first vertex) of each input feature. END \u2014 A point will be created at the end point (last vertex) of each input feature. BOTH_ENDS \u2014 Two points will be created, one at the start point and another at the endpoint of each input feature. DANGLE \u2014 A dangle point will be created for any start or end point of an input line, if that point is not connected to another line at any location along that line. This option does not apply to polygon input.", "dataType": "String"}]},
{"syntax": "FeatureToPolygon_management (in_features, out_feature_class, {cluster_tolerance}, {attributes}, {label_features})", "name": "Feature To Polygon (Data Management)", "description": " Creates a feature class containing polygons generated from areas enclosed by input line or polygon features.", "example": {"title": "FeatureToPolygon Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the FeatureToPolygon function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.FeatureToPolygon_management ([ \"mainroads.shp\" , \"streets.shp\" ], \"c:/output/output.gdb/streetblocks\" , \"\" , \"NO_ATTRIBUTES\" , \"\" )"}, "usage": ["Where one or more input features form a closed area, a new polygon feature will be constructed and written to the output feature class. The output attributes will vary depending on the ", "Preserve attributes", " option on the dialog box (the ", "attributes", " parameter in scripting) and the ", "Label Features", " option on the dialog box (the ", "label_features", " parameter in scripting).", "When multiple feature classes or layers are specified in the list of input features, the order of the entries in the list does not affect the output feature type, but the spatial reference of the top entry on the tool dialog box (the first entry in scripting) in the list will be used during processing and set to the output.", "Parametric (true) curves in the input features will remain true curves in the output polygons, even if they are split. This does not apply to shapefile data.", "The ", "Preserve attributes", " parameter (the ", "attributes", " parameter in scripting)  does not work and should not be used. It will not be removed for backward compatibility of scripts or models. The output attribute schema and field values for certain input combinations may be produced as described below; most of them are unintended.", "If the ", "Preserve attributes", " option on the dialog box is checked (the ", "attributes", " parameter is set to ATTRIBUTES in scripting), the output attributes schema and field values will depend on whether the label features (points) are provided in the following ways:", "If the ", "Preserve attributes", " option on the dialog box is unchecked (the ", "attributes", " parameter is set to NO_ATTRIBUTES in scripting),  the input attribute schemas will be written to the output, but the attribute values will be empty.   If you do not want any attributes on the output polygon feature class, supply a point feature class that has no attributes for the ", "Label Features", " parameter.", " Where input polygon features are broken into smaller output polygon features, the ", "Identity", " tool can be used to transfer attributes from the input polygon features to the resulting polygon features.", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input features that can be line or polygon, or both. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output polygon feature class. ", "dataType": "Feature Class"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates, and the distance a coordinate can move in X, Y, or both during spatial computation. The default XY tolerance is set to 0.001 meter or its equivalent in feature units. ", "dataType": "Linear unit"}, {"name": "attributes", "isOptional": true, "description": "Specifies whether to preserve the input attribute schema or the attribures from label features in the output feature class, or omit any input attributes in the output feature class. This parameter does not work. It will not be removed for backward compatibility of scripts or models. The output attribute schema and field values for certain input combinations may be produced as described in the usage notes; most of them are unintended. ATTRIBUTES \u2014 Preserves the input attribute schema or the attribures from label features, if provided, in the output features. This is the default. NO_ATTRIBUTES \u2014 Omits any input attributes in the output feature class.", "dataType": "Boolean"}, {"name": "label_features", "isOptional": true, "description": "The optional input point features that hold the attributes to be transferred to the output polygon features. ", "dataType": "Feature Layer"}]},
{"syntax": "FeatureToPoint_management (in_features, out_feature_class, {point_location})", "name": "Feature To Point (Data Management)", "description": " Creates a feature class containing points generated from the representative locations of input features.", "example": {"title": "FeatureToPoint Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the FeatureToPoint function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.FeatureToPoint_management ( \"parcels.shp\" , \"c:/data/output/parcels_center.shp\" , \"CENTROID\" )"}, "usage": [" The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs. ", "If the ", "Inside", " option on the dialog box is ", "unchecked", " (the ", "point_location", " parameter is set to CENTROID), the location of the output point will be determined as follows:", "If the ", "Inside", " option on the dialog box is ", "checked", " (the ", "point_location", " parameter is set to INSIDE), the location of the representative point of an input feature will be contained by the input feature and determined as follows:"], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input features that can be multipoint, line, polygon, or annotation. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output point feature class. ", "dataType": "Feature Class"}, {"name": "point_location", "isOptional": true, "description": "Specifies whether to use representative centers of input features or locations contained by input features as the output point locations. CENTROID \u2014 Uses the representative center of an input feature as its output point location. This is the default. This point location may not always be contained by the input feature. INSIDE \u2014 Uses a location contained by an input feature as its output point location.", "dataType": "Boolean"}]},
{"syntax": "FeatureToLine_management (in_features, out_feature_class, {cluster_tolerance}, {attributes})", "name": "Feature To Line (Data Management)", "description": "Creates a feature class containing lines generated by converting polygon boundaries to lines, or splitting line, polygon, or both features at their intersections.", "example": {"title": "FeatureToLine Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the FeatureToLine function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.FeatureToLine_management ([ \"majorrds.shp\" , \"habitat_analysis.gdb/futrds\" ], \"c:/output/output.gdb/allroads\" , \"0.001 Meters\" , \"ATTRIBUTES\" )"}, "usage": ["Input attributes can optionally be maintained in the output feature class, determined by the ", "Preserve attributes", " option on the dialog box (the ", "attributes", " parameter in scripting).", "When multiple feature classes or layers are specified in the list of input features, the order of the entries in the list does not affect the output feature type, but the spatial reference of the top entry on the tool dialog box (the first entry in scripting) in the list will be used during processing and set to the output.", "Where input lines or polygon boundaries touch, cross, or overlap each other at locations other than their start and end vertices, they will be split at those intersections; each of the split lines will become an output line feature. If an input line or polygon boundary is not intersected by another feature, its entire shape will still be written out as a line feature.", "For multipart input features, the output lines will be singlepart.", "For input features that are parametric (true) curves, the output lines will remain true curves even if they are split. This does not apply to shapefile data.", "If the ", "Preserve attributes", " option on the dialog box is checked (the ", "attributes", " parameter is set to ATTRIBUTES in scripting), the attributes from all input entries will be maintained in the output in the order they appear in the input list. A new field, FID_xxx, where xxx is the source feature class name of a particular input entry, will be added to the output for each input entry and set to the source feature IDs. The output lines are associated with their attributes in the following ways:", "If the ", "Preserve attributes", " option on the dialog box is unchecked (the ", "attributes", " parameter is set to NO_ATTRIBUTES in scripting), none of the input attributes will be maintained in the output feature class; a single line feature will be written to the output for each set of coincident lines or polygon boundaries.", "When input features contain adjacent polygons, to get the shared boundary line with left and right polygon feature IDs as attributes in the output, use the ", "Polygon_To_Line", " tool instead.", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input features that can be line or polygon, or both. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output line feature class. ", "dataType": "Feature Class"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The minimum distance separating all feature coordinates, and the distance a coordinate can move in X, Y, or both during spatial computation. The default XY tolerance is set to 0.001 meter or its equivalent in feature units. ", "dataType": "Linear unit"}, {"name": "attributes", "isOptional": true, "description": "Specifies whether to preserve or omit the input attributes in the output feature class. ATTRIBUTES \u2014 Preserves the input attributes in the output features. This is the default. NO_ATTRIBUTES \u2014 Omits the input attributes in the output features.", "dataType": "Boolean"}]},
{"syntax": "FeatureEnvelopeToPolygon_management (in_features, out_feature_class, {single_envelope})", "name": "Feature Envelope To Polygon (Data Management)", "description": "Creates a feature class containing polygons, each of which represents the  envelope  of an input feature.", "example": {"title": "FeatureEnvelopeToPolygon Example 1 (Python window)", "description": "The following Python window script demonstrates how to use the FeatureEnvelopeToPolygon function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.FeatureEnvelopeToPolygon_management ( \"urban_analysis.gdb/parks\" , \"c:/output/output.gdb/parks_extent\" , \"SINGLEPART\" )"}, "usage": [" The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs. ", "Since the envelope of a perfectly horizontal line (parallel to the x-axis) has a zero height and the envelope of a perfectly vertical line (parallel to the y-axis) has a zero width, the resulting polygon from either line would have a zero area; such invalid polygons will be omitted in the output. The same applies to a part in a multipart line feature."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input features that can be multipoint, line, polygon, or annotation. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output polygon feature class. ", "dataType": "Feature Class"}, {"name": "single_envelope", "isOptional": true, "description": " Specifies whether to use one envelope for each entire multipart feature or one envelope per part of a multipart feature. This parameter will affect the results of multipart input features only. SINGLEPART \u2014 Uses one envelope containing an entire multipart feature; therefore, the resulting polygon will be singlepart. This is the default. MULTIPART \u2014 Uses one envelope for each part of a multipart feature; the resulting polygon of the multipart feature will remain multipart.", "dataType": "Boolean"}]},
{"syntax": "Dice_management (in_features, out_feature_class, vertex_limit)", "name": "Dice (Data Management)", "description": " Subdivides a feature into smaller features based on a specified vertex limit. This tool is intended as a way to subdivide extremely large features that cause issues with drawing, analysis, editing, and/or performance but are difficult to split up with standard editing and geoprocessing tools. This tool should not be used in any cases other than those where tools are failing to complete successfully due to the size of features.", "example": {"title": "Dice example (Python window)", "description": "The following Python window script demonstrates how to use the Dice function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/gdb/thailand.gdb\" arcpy.Dice_management ( 'thailandBoundary' , 'thai_Dice_1mill' , 1000000 )"}, "usage": ["The input can be a multipoint, line, or polygon feature layer or feature class.", " No default vertex limit is provided. The number of vertices in a single feature that may cause issues due to being excessively large is dependent on your hardware configuration.", "Features that do not exceed the vertex limit will be written to the output feature class as is.", "Attribute values from the input feature classes will be carried across to the output feature class unless the input is a layer or layers created by the ", "Make Feature Layer", " or ", "Make Table View", " tool and the field's Use Ratio Policy is checked. If a feature in an overlay operation is split, the attributes of resulting features are a ratio of the original feature's value. The ratio is based on the ratio in which the original geometry is divided. If the geometry is divided equally, each new feature's attribute gets one-half of the value of the original object's attribute. Use Ratio Policy only applies to numeric field types. Geoprocessing tools do not honor geodatabase feature class or table field split policies.", " The splitting of polygons may create new vertices.", "Polygon components (think of this as the outer boundary of a part, and all the holes and other parts that it contains) will be grouped together in the output.", "Lines are only diced at a vertex.", "For line and polygon feature classes the number of vertices specified in the ", "Vertex Limit", " parameter is not always the vertex count you get in the output after a feature has been diced. ", "This tool does not use the Output Coordinate System environment. This means there will be no projecting of features prior to processing. You always end up with the same coordinate system as the input. Any projecting should be done after the ", "Dice", " tool has been run. This is done because projecting the problem feature may cause a system failure if it exceeds system resources."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or feature layer. The geometry type must be multipoint, line, or polygon. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class of diced features. ", "dataType": "Feature Class"}, {"name": "vertex_limit", "isOptional": false, "description": "Features with geometries that exceed this vertex limit will be subdivided before being written to the output feature class. ", "dataType": "Long"}]},
{"syntax": "DeleteFeatures_management (in_features)", "name": "Delete Features (Data Management)", "description": "Deletes all or the selected subset of features from the input. If the input features are from a feature class or table, all rows will be deleted. If the input features are from a layer with no selection, all featurs will be deleted.", "example": {"title": "DeleteFeatures example 1 (Python window)", "description": "The following Python window script demonstrates how to use the DeleteFeatures tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CopyFeatures_management ( \"majorrds.shp\" , \"C:/output/output.gdb/majorrds2\" ) arcpy.DeleteFeatures_management ( \"C:/output/output.gdb/majorrds2\" )"}, "usage": ["This tool accepts layers with selections as input, and will delete only those features that are selected. To delete specific features from a feature class, convert the feature class into a layer using ", "Make Feature Layer", " or by adding it to the ArcMap display. A selection can then be applied using the ", "Select Layer By Attribute", " or ", "Select Layer By Location", " tools or by querying a map layer or selecting features with the selection arrow in ArcMap. ", "If a layer is input, and that layer does not have a selection, all features will be deleted.  If a feature class is input, all features will be deleted.", "Deleting all rows from a feature class with a large number of rows can be  slow.  If your intent is to delete all the rows in the feature class you should consider using the ", "Truncate Table", " tool instead.  Please see the ", "Truncate Table", " documentation for important cautionary statements on its use.", "This tool deletes both the geometry and attributes of the ", "Input Features", ".", "The ", "Output Extent", " environment is honored by this tool.  Only the features that are within or intersect the output extent  environment will be deleted.  If the input layer has a selection, only the selected features that are within or intersect the output extent will be deleted.", "When working in ArcMap and using a layer with selections as input, using this  tool in an edit session will allow for the ", "Delete Features", " operation to be undone using undo/redo."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature class, shapefile, or layer containing features to be deleted. ", "dataType": "Feature Layer"}]},
{"syntax": "CopyFeatures_management (in_features, out_feature_class, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})", "name": "Copy Features (Data Management)", "description": "Copies features from the input feature class or layer to a new feature class. If the input is a layer which has a selection, only the selected features will be copied. If the input is a geodatabase feature class or shapefile, all features will be copied.", "example": {"title": "CopyFeatures example (Python window)", "description": "The following Python window script demonstrates how to use the CopyFeatures tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CopyFeatures_management ( \"climate.shp\" , \"C:/output/output.gdb/climate\" )"}, "usage": ["Both the geometry and attributes of the ", "Input Features", " will be copied to the output feature class.", "This tool can be used for data conversion as it can read many feature formats (any you can add to ArcMap) and write these to shapefile or geodatabase (File, Personal, or ArcSDE)."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The features to be copied. ", "dataType": "Feature Layer;Raster Catalog Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class which will be created and to which the features will be copied. If the output feature class already exists and the overwrite option is set to true, the output will be deleted first. If the output feature class already exists and the overwrite option is set to false, the operation will fail. ", "dataType": "Feature Class"}, {"name": "config_keyword", "isOptional": true, "description": "Geodatabase configuration keyword to be applied if the output is an ArcSDE geodatabase or file geodatabase. ", "dataType": "String"}, {"name": "spatial_grid_1", "isOptional": true, "description": "The Spatial Grid 1, 2, and 3 parameters apply only to file geodatabases and certain ArcSDE geodatabase feature classes. If you are unfamiliar with setting grid sizes, leave these options as 0,0,0 and ArcGIS will compute optimal sizes for you. For more information about this parameter, refer to the Add Spatial Index tool documentation. ", "dataType": "Double"}, {"name": "spatial_grid_2", "isOptional": true, "description": " Cell size of the second spatial grid. Leave the size at 0 if you only want one grid. Otherwise, set the size to at least three times larger than Spatial Grid 1. ", "dataType": "Double"}, {"name": "spatial_grid_3", "isOptional": true, "description": " Cell size of the third spatial grid. Leave the size at 0 if you only want two grids. Otherwise, set the size to at least three times larger than Spatial Grid 2. ", "dataType": "Double"}]},
{"syntax": "CheckGeometry_management (in_features, out_table)", "name": "Check Geometry (Data Management)", "description": "Generates a report of the geometry problems in a feature class. Valid input formats are shapefile and feature classes stored in a personal geodatabase or file geodatabase. SDE Geodatabases automatically check the validity of each geometry when they are uploaded; therefore the  Check Geometry  and  Repair Geometry  tools are not for use with SDE. For additional information on geometry problems, its impact on the software, and potential sources, see  Checking and repairing geometries .", "example": {"title": "Check Geometry Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CheckGeometry function in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"c:/data/data.gdb\" arcpy.CheckGeometry_management ([ \"contours\" , \"roads\" , \"vegetation\" ], \"CG_Result\" )"}, "usage": ["The ", "Output Table", " will have a record for each geometry problem discovered. If no problems are found the table will be empty.", "The ", "Output Table", " has the following fields:", "The PROBLEM field will contain one of the following:", "The problem identified by this tool can be addressed in the following ways:", "For point features, only the null geometry problem applies.", "To facilitate the review of the features which are reported to have geometry problems in ArcMap you can join the ", "Input Features", " to the ", "Output Table", " using the ", "Join", " tool. Simply join using the input's ObjectID field, and the output table's FEATURE_ID field. You may also uncheck the ", "Keep All", " option so that only features with geometry problems are displayed.", "Starting with the 10.0 release, a line geometry  is no longer considered \"self-intersecting\" if it crosses itself.  There was no adverse effect to these types of geometry therefore the ", "Check Geometry", " tool will no longer report the feature as a problem, and the ", "Repair Geometry", " tool will no longer perform a \"fix\" on the feature's geometry.  Prior to the 10.0 release ", "Repair Geometry", " would add vertices at the point of intersection of \"self-intersecting\" lines.  If you do want to add vertices when line features intersect themselves use the ", "Integrate", " tool."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "One or more feature classes or feature layers that will be checked for geometry problems. Valid input formats are shapefile and feature classes stored in a personal geodatabase or file geodatabase. ", "dataType": "Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table that will contain the list of problems that were discovered in the input features. ", "dataType": "Table"}]},
{"syntax": "Adjust3DZ_management (in_features, {reverse_sign}, {adjust_value}, {from_units}, {to_units})", "name": "Adjust 3D Z (Data Management)", "description": "Allows the modification of every Z-value in a 3D feature class.", "example": {"title": "Adjust3DZ example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.Adjust3DZ_management ( \"subsurface_pts.shp\" , \"REVERSE\" , 0 , \"METERS\" , \"FEET\" )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "Bathymetry data often has positive Z-values. You may wish to reverse the signs of all the data in the feature class to make the Z-values negative.", "Z-enabled data could be referenced to a vertical datum that is not appropriate for your geoprocessing needs. This tool could apply a bulk shift of all the Z-values in the feature class to adjust the data either up or down vertically.", "The ", "Convert From Units", " and ", "Convert To Units", " parameters  allow you to convert  your Z-values from one common unit of measure to another."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class containing Z-values in the SHAPE field. ", "dataType": "Feature Layer"}, {"name": "reverse_sign", "isOptional": true, "description": "Denotes whether the sign of all Z-values in the feature class will be inverted. REVERSE \u2014 Inverts the sign of Z-values. NO_REVERSE \u2014 Maintains the sign of Z-values. This is the default.", "dataType": "String"}, {"name": "adjust_value", "isOptional": true, "description": "Uniformly adjusts the elevation of all Z-values in the feature class. Entering a negative number will decrease the Z-value, whereas a positive number will increase it. ", "dataType": "Double"}, {"name": "from_units", "isOptional": true, "description": "The existing units of the Z-values. This parameter is used in conjunction with the Convert To Units parameter. MILLIMETERS CENTIMETERS METERS INCHES FEET (US) YARDS FATHOMS", "dataType": "String"}, {"name": "to_units", "isOptional": true, "description": "The units that existing Z-values will be converted to. MILLIMETERS CENTIMETERS METERS INCHES FEET (US) YARDS FATHOMS", "dataType": "String"}]},
{"syntax": "AddXY_management (in_features)", "name": "Add XY Coordinates (Data Management)", "description": "Adds the fields POINT_X and POINT_Y to the point input features and calculates their values. It also appends the POINT_Z and POINT_M fields if the input features are Z- and M-enabled.", "example": {"title": "AddXY Example (Python Window)", "description": "The following Python Window script demonstrates how to use the AddXY function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Copy_management ( \"climate.shp\" , \"climateXYpts.shp\" ) arcpy.AddXY_management ( \"climateXYpts.shp\" )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "Add XY Coordinates is most commonly used to get access to point features to perform analysis or to extract points based on their x,y location.", "If the POINT_X, POINT_Y, POINT_Z, and POINT_M fields exist, their values are recalculated.", "If points are moved after using Add XY Coordinates, their POINT_X and POINT_Y values, and POINT_Z, and POINT_M values\u2014if present\u2014must be recomputed by running Add XY Coordinates again.", "Project", " does not modify the values of POINT_X, POINT_Y, POINT_Z, or POINT_M.", "If the Input Features are in a ", "geographic coordinate system", ", POINT_X and POINT_Y represent the longitude and latitude, respectively.", "If an ArcMap layer is selected as input , the x,y coordinates are based on the input's coordinate system, not that of the data frame."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The point features whose x,y coordinates will be appended as POINT_X and POINT_Y fields. ", "dataType": "Feature Layer"}]},
{"syntax": "UpdateAnnotation_management (in_features, {update_values})", "name": "Update Annotation Feature Class (Data Management)", "description": "Updates the input annotation feature class with text attribute fields and optionally populates the value of each new field for every feature in the feature class.", "example": {"title": "UpdateAnnotation Example (Python Window)", "description": "The following Python Window script demonstrates how to use the UpdateAnnotation tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/Ontario.mdb\" arcpy.UpdateAnnotation_management ( \"ProvParks_anno\" , \"POPULATE\" )"}, "usage": ["This tool can be run on a versioned feature class if the option to populate attribute fields is unchecked. The schema of the feature class will be updated in this case, but the new annotation fields will not be populated. Attribute values for a feature will remain blank until the feature is edited.", "This tool will update the schema of the feature class and, optionally, each annotation feature within the feature class. The schema update will add fields to the feature class (bold, italic, text, and so on) and also ensure that there is a symbol within the symbol collection. Without a symbol in the symbol collection, you can't use the improvements for constructing annotation features.", "Populating the attributes fields is an intensive operation that requires every feature to be updated. Turning off this option will add the fields, but not populate them. If the fields are not populated on update, they will remain blank until the feature is edited."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "Input annotation feature class to which new fields will be added. ", "dataType": "Feature Layer"}, {"name": "update_values", "isOptional": true, "description": "Populates the value of each new field for every feature in the feature class. POPULATE \u2014 Populates the value of each new field for every feature in the feature class. DO_NOT_POPULATE \u2014 Do not populate a value for the fields.", "dataType": "Boolean"}]},
{"syntax": "Integrate_management (in_features, {cluster_tolerance})", "name": "Integrate (Data Management)", "description": "Integrate is used to maintain the integrity of shared feature boundaries by making features coincident if they fall within the specified x,y tolerance.         Features that fall within the specified x,y tolerance are considered identical or coincident. For example, suppose you specify an x,y tolerance of five  units (such as feet or meters) and your data has a parcel boundary that should be shared with the adjacent parcel boundary but is four  units away. After running this tool, the boundaries of the two parcels would be made coincident because they were within the x,y tolerance of five units. Integrate  performs\r\nthe following processing tasks: \r\n", "example": {"title": "Integrate Example (Python Window)", "description": "The following Python window script demonstrates how to use the Integrate function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CopyFeatures_management ( \"Habitat_Analysis.gdb/vegtype\" , \"C:/output/output.gdb/vegtype\" ) arcpy.Integrate_management ( \"C:/output/output.gdb/vegtype\" , 0.01 )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "If there are any input features selected, this tool will execute only on those selected features.", "This tool performs the same kind of work as a topology in that it moves features within an x,y tolerance and inserts vertices where features intersect. Consider using a topology to perform this sort of operation because a topology allows you to  specify rules and conditions about how features relate to each other.  For more information about using topologies, see  ", "Topology_basics", ".", "You would use ", "Integrate", " rather than a topology when:", "The value for ", "XY Tolerance", " is critical\u2014a tolerance that is too large may collapse and delete polygons or lines, or move vertices that should not be moved. To minimize error, the value you choose for x,y tolerance should be as small as possible.", "Integrate", " accepts only simple feature classes as input (point, multipoint, line, or polygon). The input features cannot include annotation features, dimension features, network features, and so on.", "To undo changes to the input features, use ", "Integrate", " in an edit session.", "When processing datasets that contain  individual features with a very large number of vertices (e.g., hundreds of thousands to millions of vertices within a single feature), some geometric processing operations may run out of memory. For more details, see ", "Geoprocessing with large datasets", ".", "The output data element of this tool is a derived multivalue output. To use this tool's output(s) with another tool, use its input(s) directly and set its output as a precondition of the other tool, as illustrated below. ", "Learn more about setting a precondition", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature classes to be integrated. When the distance between features is small in comparison to the tolerance the vertices or points will be clustered (moved to be coincident). The feature class or layer with the lower rank will snap to the feature from the feature class or layer with the higher rank (with 1 being a higher rank than 2). Note that features in the feature class with a rank of 1 may move when a large x,y tolerance is used. ", "dataType": "Value Table"}, {"name": "cluster_tolerance", "isOptional": true, "description": "The distance that determines the range in which feature vertices are made coincident. To minimize undesired movement of vertices, the x,y tolerance should be fairly small. If no value is specified, the xy tolerance from the first dataset in the list of inputs will be used. ", "dataType": "Linear unit"}]},
{"syntax": "CreateRandomPoints_management (out_path, out_name, {constraining_feature_class}, {constraining_extent}, {number_of_points_or_field}, {minimum_allowed_distance}, {create_multipoint_output}, {multipoint_size})", "name": "Create Random Points (Data Management)", "description": "Creates a specified number of random point features. Random points can be generated in an extent window, inside polygon features, on point features, or along line features.  \r\n Learn more about how Create Random Points works \r\n", "example": {"title": "Create Random Points Example (Python Window)", "description": "The following Python window script demonstrates how to use the Create Random Points tool in immediate mode:", "code": "import arcpy arcpy.CreateRandomPoints_management ( \"c:/data/project\" , \"samplepoints\" , \"c:/data/studyarea.shp\" , \"\" , 500 , \"\" , \"POINT\" , \"\" )"}, "usage": ["The area in which random points will be generated can be defined either by constraining polygon, point, or line features or by a constraining extent window.", "The ", "Number of Points", " parameter can be specified as a  number or as a numeric field in the ", "Constraining Feature Class", "  containing values for how many random points to place within each feature. The field option is only valid for polygon or line constraining features. If the number of points is supplied as a number, each feature in the constraining feature class will have that number of random points generated inside or along it.", "To assign random values to randomly placed points, first generate random points using the this tool. Second, use the ", "Add Field", " tool to create a new numeric field in the random points feature class. Suggested field types are long integer or float. Third, use the ", "Calculate Field", " tool to assign random values to the empty field in the random points feature class. To generate a random integer between a and b (inclusive), use the Python expression ", "random.randint(a,b)", ". To generate a random float number between a and b (exclusive), use the  Python expression ", "random.uniform(a,b)", ". Do not forget to set ", "Expression Type", " to PYTHON, replace the a and b values, and import the random module in the ", "Code Block", " section using the expression ", "import random", ".", "The ", "Constraining Extent", " parameter can be entered as a set of minimum and maximum x- and y-coordinates or as equal to the extent of a feature layer or feature class.", "If values for both ", "Constraining Feature Class", " and ", "Constraining Extent", " are specified, the ", "Constraining Feature Class", " value will be used and the ", "Constraining Extent", " value will be ignored.", "In the tool dialog box, the ", "Constraining Extent", " values can be reset using the ", "Clear", " button.", "If you are using a ", "Constraining Feature Class", " that has more than one feature, and you wish to specify the total number of random points to be generated (as opposed to the number of random points to be placed inside each feature), you must first dissolve the constraining feature class with the ", "Dissolve", " tool so it only contains a single feature, then use that dissolved feature class as the ", "Constraining Feature Class", ".", "When unable to place any more random points within a constraining area without breaking the minimum allowed distance specification, the number of random points in the constraining area will be reduced to the maximum possible under the minimum allowed distance.", "The ", "Minimum Allowed Distance", " parameter can be specified as a linear unit or a field from the constraining features containing numeric values. This value will determine the minimum allowed distance between random points within each input feature. The field option is only valid for polygon or line constraining features. Random points may be within the minimum allowed distance if they were generated inside or along different constraining features.", "Using point features as the constraining feature class creates a random subset of the constraining point features. No new point locations are generated.", "Noninteger (whole) positive values for the ", "Number of Points", " and ", "Minimum Allowed Distance", " parameters will be rounded to the nearest whole number. Nonnumeric and negative values are set to 0."], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The location or workspace in which the random points feature class will be created. This location or workspace must already exist. ", "dataType": "Feature Dataset;Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the random points feature class to be created. ", "dataType": "String"}, {"name": "constraining_feature_class", "isOptional": true, "description": "Random points will be generated inside or along the features in this feature class. The constraining feature class can be point, multipoint, line, or polygon. Points will randomly be placed inside polygon features, along line features, or at point feature locations. Each feature in this feature class will have the specified number of points generated inside it (for example, if you specify 100 points, and the constraining feature class has 5 features, 100 random points will be generated in each feature, totaling 500 points). ", "dataType": "Feature Layer"}, {"name": "constraining_extent", "isOptional": true, "description": "Random points will be generated inside the extent. The constraining extent will only be used if no constraining feature class is specified. ", "dataType": "Extent;Feature Layer;Raster Layer"}, {"name": "number_of_points_or_field", "isOptional": true, "description": "The number of points to be randomly generated. The number of points can be specified as a long integer number or as a field from the constraining features containing numeric values for how many random points to place within each feature. The field option is only valid for polygon or line constraining features. If the number of points is supplied as a long integer number, each feature in the constraining feature class will have that number of random points generated inside or along it. ", "dataType": "Field;Long"}, {"name": "minimum_allowed_distance", "isOptional": true, "description": "The shortest distance allowed between any two randomly placed points. If a value of \"1 Meter\" is specified, all random points will be farther than 1 meter away from the closest point. ", "dataType": "Field;Linear unit"}, {"name": "create_multipoint_output", "isOptional": true, "description": "Determines if the output feature class will be a multipart or single part feature. POINT \u2014 The output will be geometry type point (each point is a separate feature). This is the default. MULTIPOINT \u2014 The output will be geometry type multipoint (all points are a single feature).", "dataType": "Boolean"}, {"name": "multipoint_size", "isOptional": true, "description": "If the Create Multipoint Output option is used (checked/MULTIPOINT), this parameter specifies the number of random points to be placed in each multipoint geometry. ", "dataType": "Long"}]},
{"syntax": "CreateFishnet_management (out_feature_class, origin_coord, y_axis_coord, cell_width, cell_height, number_rows, number_columns, {corner_coord}, {labels}, {template}, {geometry_type})", "name": "Create Fishnet (Data Management)", "description": "Creates a fishnet of rectangular cells.  The output can be polyline or polygon features. \r\n Learn more about how Create Fishnet works \r\n", "example": {"title": "CreateFishnet example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CreateFishnet function in immediate mode.", "code": "import arcpy # Create a fishnet with 9 columns and 9 rows # with origin at (1, 1) and output geometry is set to default (POLYLINE) arcpy.CreateFishnet_management ( \"C:/data/output/fishnet1.shp\" , \"1 1\" , \"1 9\" , \"1\" , \"1\" , \"9\" , \"9\" , \"#\" , \"NO_LABELS\" )"}, "usage": ["The coordinate system of the output can be set either by entering a feature class or layer in the ", "Template Extent", "  parameter or by setting  the Output Coordinate System environment variable.", " In addition to creating the output fishnet, a new point feature class is created with label points at the center of each fishnet cell if the ", "Create Label Points", " parameter is set to ", "LABELS", " (Checked). The name of this feature class is the same as the output feature class with a suffix of ", "_label", " and is created in the same location.", "The ", "Geometry Type", " parameter gives the option of creating output polyline (default) or polygon cells. Creating a polygon fishnet may be slow, depending on the number of rows and columns.", "The ", "Cell Size Width", " and the ", "Cell Size Height", " values are in the same units as defined by the output feature class."], "parameters": [{"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class containing the fishnet of rectangular cells. ", "dataType": "Feature Class"}, {"name": "origin_coord", "isOptional": false, "description": "The starting pivot point of the fishnet. ", "dataType": "Point"}, {"name": "y_axis_coord", "isOptional": false, "description": "The Y-axis coordinate is used to orient the fishnet. The fishnet is rotated by the same angle as defined by the line connecting the origin and the y-axis coordinate. ", "dataType": "Point"}, {"name": "cell_width", "isOptional": false, "description": "Determines the width of each cell. If you want the width to be automatically calculated using the value in the Number of Rows parameter, set this value to zero\u2014the width will be calculated when the tool is run. ", "dataType": "Double"}, {"name": "cell_height", "isOptional": false, "description": "Determines the height of each cell. If you want the height to be automatically calculated using the value in the Number of Columns parameter, set this value to zero\u2014the height will be calculated when the tool is run. ", "dataType": "Double"}, {"name": "number_rows", "isOptional": false, "description": "Determines the number of rows the fishnet will have. If you want the number of rows to be automatically calculated using the value in the Cell Size Width parameter, set this value to zero\u2014the number of rows will be calculated when the tool is run. ", "dataType": "Long"}, {"name": "number_columns", "isOptional": false, "description": "Determines the number of columns the fishnet will have. If you want the number of columns to be automatically calculated using the value in the Cell Size Height parameter, set this value to zero\u2014the number of columns will be calculated when the tool is run. ", "dataType": "Long"}, {"name": "corner_coord", "isOptional": true, "description": "The opposite corner of the fishnet set by X-Coordinate and Y-Coordinate values. ", "dataType": "Point"}, {"name": "labels", "isOptional": true, "description": "Specifies whether or not a point feature class will be created containing label points at the center of each fishnet cell. LABELS \u2014 A new feature class is created with label points. This is the default. NO_LABELS \u2014 The label points feature class is not created.", "dataType": "Boolean"}, {"name": "template", "isOptional": true, "description": "Specify the extent of the fishnet. The extent can be entered by specifying the coordinates or using a template dataset. Left\u2014XMin value Right\u2014XMax value Bottom\u2014YMin value Top\u2014YMax value", "dataType": "Extent"}, {"name": "geometry_type", "isOptional": true, "description": "Determines if the output fishnet cells will be polyline or polygon features. POLYLINE \u2014 Output is a polyline feature class. Each cell is defined by four line features. POLYGON \u2014 Output is a polygon feature class. Each cell is defined by a polygon feature. ", "dataType": "String"}]},
{"syntax": "CreateFeatureclass_management (out_path, out_name, {geometry_type}, {template}, {has_m}, {has_z}, {spatial_reference}, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})", "name": "Create Feature Class (Data Management)", "description": "Creates an empty feature class in an ArcSDE, file geodatabase, or personal geodatabase; in a folder it creates a shapefile.", "example": {"title": "CreateFeatureclass Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CreateFeatureclass function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CreateFeatureclass_management ( \"C:/output\" , \"habitatareas.shp\" , \"POLYGON\" , \"study_quads.shp\" , \"DISABLED\" , \"DISABLED\" , \"C:/workspace/landuse.shp\" )"}, "usage": ["The ", "Feature Class Location", " (geodatabase or folder) must already exist.", "This tool creates only simple feature classes such as point, multipoint, polygon, and polyline. Custom feature classes such as annotation, dimensions, and relationship class are created in the ", "Catalog", " window or in ArcCatalog by right-clicking a Geodatabase and selecting the ", "New...", "A shapefile created by this tool has a field named ID of type integer. The ID field is not created when you provide a ", "Template Feature Class", "."], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The ArcSDE, file, or personal geodatabase, or the folder in which the output feature class will be created. This workspace must already exist. ", "dataType": "Workspace; Feature Dataset"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the feature class to be created. ", "dataType": "String"}, {"name": "geometry_type", "isOptional": true, "description": "The geometry type of the feature class. POINT \u2014 MULTIPOINT \u2014 POLYGON \u2014 POLYLINE \u2014 ", "dataType": "String"}, {"name": "template", "isOptional": false, "description": "The feature class used as a template to define the attribute schema of the feature class. ", "dataType": "Feature Layer"}, {"name": "has_m", "isOptional": true, "description": "Determines if the feature class contains linear measurement values (m-values). DISABLED \u2014 The output feature class will not have m-values. ENABLED \u2014 The output feature class will have m-values. SAME_AS_TEMPLATE \u2014 The output feature class will have m-values only if the Template has m-values.", "dataType": "String"}, {"name": "has_z", "isOptional": true, "description": "Determines if the feature class contains elevation values (z-values). DISABLED \u2014 The output feature class will not have z-values. ENABLED \u2014 The output feature class will have z-values. SAME_AS_TEMPLATE \u2014 The output feature class will have z-values only if the Template has z-values.", "dataType": "String"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output feature dataset. You can specify the spatial reference in several ways: When you use a Template Feature Class its spatial reference is ignored. By entering the path to a .prj file, such as C:/workspace/watershed.prj . By referencing a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland . By defining a spatial reference object prior to using this tool, such as sr = arcpy.SpatialReference(\"C:/data/Africa/Carthage.prj\") , which you then use as the spatial reference parameter. ", "dataType": "Spatial Reference"}, {"name": "config_keyword", "isOptional": true, "description": "The configuration keyword applies to ArcSDE data only. It determines the storage parameters of the database table. ", "dataType": "String"}, {"name": "spatial_grid_1", "isOptional": true, "description": "The Spatial Grid 1, 2, and 3 parameters are used to compute a spatial index and only apply to file geodatabases and certain ArcSDE geodatabase feature classes. If you are unfamiliar with setting grid sizes, leave these options as 0,0,0 and ArcGIS will compute optimal sizes for you. Since no features are written by this tool, the spatial index will be in an unbuilt state. The index will be built when features are written to the feature class such as by the Append tool or editing operations. For more information about this parameter, refer to the Add Spatial Index tool documentation. ", "dataType": "Double"}, {"name": "spatial_grid_2", "isOptional": true, "description": " Cell size of the second spatial grid. Leave the size at 0 if you only want one grid. Otherwise, set the size to at least three times larger than Spatial Grid 1. ", "dataType": "Double"}, {"name": "spatial_grid_3", "isOptional": true, "description": " Cell size of the third spatial grid. Leave the size at 0 if you only want two grids. Otherwise, set the size to at least three times larger than Spatial Grid 2. ", "dataType": "Double"}]},
{"syntax": "CalculateDefaultGridIndex_management (in_features)", "name": "Calculate Default Spatial Grid Index (Data Management)", "description": "Calculates a set of valid grid index values (spatial grid 1, 2, and 3) for the input features. Grid index values will be calculated even if the input features do not support spatial grid indexing.", "example": {"title": "Calculate Default Grid Index Example (Python Window)", "description": "The following Python window script demonstrates how to use the CalculateDefaultGridIndex function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CalculateDefaultGridIndex_management ( \"rivers.shp\" )"}, "usage": ["The grid index values will be returned as a message from the tool. The values can be viewed in the geoprocessing ", "Results", " window, or the value can be assigned to a variable in scripting by accessing the tool execution ", "result", " object.. ", "The spatial grid index of the ", "Input Features", " is not updated by this tool. File or SDE geodatabase feature class spatial grid indexes can be modified using the ", "Add Spatial Index", " tool, or in the feature class property page under the ", "Index", " tab.", "The set of values returned by the tool can be used with the ", "Add Spatial Index", " or ", "Copy Features", " tools, or for the ", "Output Spatial Grid", " environment settings.", "The returned grid size is based on the spatial reference, average feature size, and number of features in the input features."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The features for which a valid spatial grid index will be calculated. ", "dataType": "Feature Layer; Raster Catalog Layer"}]},
{"syntax": "CalculateDefaultClusterTolerance_management (in_features)", "name": "Calculate Default XY Tolerance (Data Management)", "description": "Calculates a default  XY tolerance by examining the spatial reference and the extent of the feature class.  With geodatabase feature classes, the value returned by this tool will be identical to the  XY Tolerance property  on a geodatabase feature class or dataset, or the  cluster tolerance of a topology.  With non-geodatabase feature classes such as coverage feature classes, shape files, or CAD feature classes, the value will be based on the default tolerance of the feature class' spatial reference.  XY tolerance is also available in scripting through the  XYTolerance  property of a  SpatialReference  object. A  SpatialReference  object can be created by describing a feature class.", "example": {"title": "Calculate Default XY Tolerance Example (Python Window)", "description": "The following Python window script demonstrates how to use the Calculate Default Cluster Tolerance function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CalculateDefaultClusterTolerance_management ( \"roads.shp\" )"}, "usage": ["This tool cannot be used to reset the value of default XY Tolerance; it can only calculate the XY tolerance value. ", "The result of this tool will be returned as a message from the tool. The values can be viewed in the geoprocessing ", "Results", " window, or the value can be assigned to a variable in scripting by accessing the tool execution ", "result", " object.", "Although the default XY tolerance of non-geodatabase feature classes maybe different depending on the spatial reference, its value is  equivalent to 0.001 meter. This default value cannot be changed. ", "The XY tolerance of a geodatabase feature classes can be specified when a new feature class, or its containing feature dataset or topology, is created  in a geodatabase. If you do not specify a new XY tolerance, the default values of the containing feature classes are used, which usually is equivalent to 0.001 meter. If the feature class is saved in a topology of a  geodatabase, then XY tolerance can be reset using the ", "Set Cluster Tolerance", " tool. For more information, you can right-click any feature class or dataset and click ", "Properties", ", then select the  ", "Tolerance", " tab, then check ", "About Setting Tolerance", " help.", "Most tools that create new feature classes (such as ", "Create Feature Class", " or ", "Copy Features", ") honor the ", "XY_Tolerance", " environment setting.    However, only those tools that use this environment will set the XY tolerance on the output feature class.  The input XY tolerance will not be affected by this environment setting. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature class for which the default XY tolerance will be calculated. ", "dataType": "Feature Layer"}]},
{"syntax": "AppendAnnotation_management (input_features, output_featureclass, reference_scale, {create_single_class}, {require_symbol_from_table}, {create_annotation_when_feature_added}, {update_annotation_when_feature_modified})", "name": "Append Annotation Feature Classes (Data Management)", "description": "Creates a new geodatabase annotation feature class or appends to an existing annotation feature class by combining annotation from multiple input geodatabase annotation feature classes into a single feature class with annotation classes.", "example": {"title": "AppendAnnotation example (Python window)", "description": "The following Python window script demonstrates how to use the AppendAnnotation tool in immediate mode:", "code": "import arcpy arcpy.env.workspace = \"C:/data/Cobourg.gdb\" arcpy.AppendAnnotation_management ( \"highways;roads\" , \"transport_anno\" , 1200 , \"CREATE_CLASSES\" , \"NO_SYMBOL_REQUIRED\" , \"AUTO_CREATE\" , \"AUTO_UPDATE\" )"}, "usage": ["When appending multiple annotation feature classes into a new annotation feature class, the input annotation feature classes must reside in the same database.", "If you select geodatabase annotation features in ArcMap or build a definition query, only those features will be appended to the output feature class.", "When appending feature-linked annotation feature classes, all the input annotation feature classes must be linked to the same feature class.", "If you select an output annotation feature class that already exists, the features will be appended to that feature class and the tool will project the annotation features in the destination spatial reference.", "An ", "ArcGIS for Desktop Standard", " or ", "ArcGIS for Desktop Advanced", " license is required to create an output annotation feature class that is feature-linked."], "parameters": [{"name": "input_features", "isInputFile": true, "isOptional": false, "description": "Input annotation features that will form an annotation class in the output feature class. ", "dataType": "Feature Layer"}, {"name": "output_featureclass", "isOutputFile": true, "isOptional": false, "description": "New annotation feature class that contains an annotation class for each input annotation feature class. ", "dataType": "Feature Class"}, {"name": "reference_scale", "isOptional": false, "description": "Reference scale set in the output feature class. Input features created at a different reference scale will be transformed to match this output reference scale. ", "dataType": "Double"}, {"name": "create_single_class", "isOptional": true, "description": "Specifies how annotation features will be added to the output feature class. ONE_CLASS_ONLY \u2014 All annotation features will be aggregated into one annotation class within the output feature class. CREATE_CLASSES \u2014 Separate annotation classes will be created for each input annotation class within the output feature class.", "dataType": "Boolean"}, {"name": "require_symbol_from_table", "isOptional": true, "description": "Specifies how symbols can be selected for newly created annotation features. REQUIRE_SYMBOL \u2014 Restricts the creation of annotation features to the list of symbols in the symbol collection of the output feature class NO_SYMBOL_REQUIRED \u2014 Allows annotation features to be created with any symbology", "dataType": "Boolean"}, {"name": "create_annotation_when_feature_added", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specifies if feature-linked annotation is created when a feature is added. AUTO_CREATE \u2014 Use the label engine to place feature-linked annotation when a linked feature is created. NO_AUTO_CREATE \u2014 Do not place feature-linked annotation when a feature is created.", "dataType": "Boolean"}, {"name": "update_annotation_when_feature_modified", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Specifies if feature-linked annotation is updated when a linked feature changes. AUTO_UPDATE \u2014 Use the label engine to update feature-linked annotation when a linked feature changes. NO_AUTO_UPDATE \u2014 Do not update feature-linked annotation when a linked feature changes.", "dataType": "Boolean"}]},
{"syntax": "TableToDomain_management (in_table, code_field, description_field, in_workspace, domain_name, {domain_description}, {update_option})", "name": "Table To Domain (Data Management)", "description": "Creates or updates a coded value domain with values from a table.", "example": {"title": "TableToDomain Example (Python Window)", "description": "The following Python window script demonstrates how to use the TableToDomain function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.TableToDomain_management ( \"diameter.dbf\" , \"code\" , \"descript\" , \"montgomery.gdb\" , \"diameters\" , \"Valid pipe diameters\" )"}, "usage": ["A domain can also be created with the ", "Create Domain", " tool.", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The database table from which to derive domain values. ", "dataType": "Table View"}, {"name": "code_field", "isOptional": false, "description": "The field in the database table from which to derive domain code values. ", "dataType": "Field"}, {"name": "description_field", "isOptional": false, "description": "The field in the database table from which to derive domain description values. ", "dataType": "Field"}, {"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The workspace that contains the domain to be created or updated. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the domain to be created or updated. ", "dataType": "String"}, {"name": "domain_description", "isOptional": true, "description": "The description of the domain to be created or updated. Domain descriptions of existing domains are not updated. ", "dataType": "String"}, {"name": "update_option", "isOptional": true, "description": "If the domain already exists, specifies how the domain will be updated. APPEND \u2014 Appends to the domain values from the database table. REPLACE \u2014 Replaces the values in the domain with values from the database table. ", "dataType": "String"}]},
{"syntax": "SetValueForRangeDomain_management (in_workspace, domain_name, min_value, max_value)", "name": "Set Value For Range Domain (Data Management)", "description": "Sets the minimum and maximum values for an existing Range domain.", "example": {"title": "Set Value for Range Domain Example (Python Window)", "description": "The following Python window script demonstrates how to use the SetValueForRangeDomain function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.SetValueForRangeDomain_management ( \"montgomery.gdb\" , \"RotAngle\" , 0 , 359 )"}, "usage": ["A range domain specifies a valid range of values for a numeric attribute. For example, a valid range of water main pressure values might be between 50 and 75 psi.", "Domain management involves the following steps:", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The geodatabase containing the domain to be updated. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the range domain to be updated. ", "dataType": "String"}, {"name": "min_value", "isOptional": false, "description": "The minimum value of the range domain. ", "dataType": "String"}, {"name": "max_value", "isOptional": false, "description": "The maximum value of the range domain. ", "dataType": "String"}]},
{"syntax": "RemoveDomainFromField_management (in_table, field_name, {subtype_code})", "name": "Remove Domain From Field (Data Management)", "description": "Removes an  attribute domain  association from a feature class or table field.", "example": {"title": "Remove Domain from Field Example (Python Window)", "description": "The following Python window script demonstrates how to use the RemoveDomainFromField function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.RemoveDomainFromField_management ( \"montgomery.gdb/water/distribmains\" , \"DIAMETER\" )"}, "usage": ["This tool is the opposite operation from the ", "Assign Domain To Field", " function. Removing a domain from a field removes the association between a field and an attribute domain.", "When a domain is removed from a field, the attribute validation rule for that field is removed from the database.", "The same attribute domain can be associated with multiple fields of the same table, feature class, or subtype as well as with multiple tables and feature classes. Removing a domain from a field will not affect other domain associations.", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box.", "Current map layers may be used to define the Input table.", "The ", "Subtype", " parameter ", "Add Value", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the Subtype parameter may not be populated with values. The ", "Add Value", " button allows you to add expected value(s) so you can complete the ", "Remove Domain From Field", " dialog box and continue to build your model."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input table containing the attribute domain that will be removed. ", "dataType": "Table View"}, {"name": "field_name", "isOptional": false, "description": "The field that will no longer be associated with an attribute domain. ", "dataType": "Field"}, {"name": "subtype_code", "isOptional": false, "description": "Selects the subtype code(s) that will no longer be associated with an attribute domain. ", "dataType": "String"}]},
{"syntax": "DomainToTable_management (in_workspace, domain_name, out_table, code_field, description_field, {configuration_keyword})", "name": "Domain To Table (Data Management)", "description": "Creates a table from an attribute domain.", "example": {"title": "Domain to Table Example (Python Window)", "description": "The following Python window script demonstrates how to use the DomainToTable function.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.DomainToTable_management ( \"montgomery.gdb\" , \"DistDiam\" , \"diameters\" , \"code\" , \"descript\" )"}, "usage": ["Creating a table from an ", "attribute domain", " allows for additional editing of the table in ArcMap. For example, a table could be created from a coded value domain, additional code values could be added to the coded value list, and the ", "Table To Domain", " tool could be used to update the original domain.", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The workspace containing the attribute domain to be converted to a table. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the existing attribute domain. ", "dataType": "String"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table to be created. ", "dataType": "Table"}, {"name": "code_field", "isOptional": false, "description": "The name of the field in the created table that will store code values. ", "dataType": "String"}, {"name": "description_field", "isOptional": false, "description": "The name of the field in the created table that will store code value descriptions. ", "dataType": "String"}, {"name": "configuration_keyword", "isOptional": true, "description": "For SDE tables, the custom storage keywords for creating the table. ", "dataType": "String"}]},
{"syntax": "DeleteDomain_management (in_workspace, domain_name)", "name": "Delete Domain (Data Management)", "description": "Deletes a domain from a workspace.", "example": {"title": "Delete Domain Example (Python Window)", "description": "The following Python window script demonstrates how to use the DeleteDomain function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.DeleteDomain_management ( \"montgomery.gdb\" , \"DistDiam\" )"}, "usage": ["A domain cannot be deleted if it is associated with a feature class or table. Use the ", "Remove Domain From Field", " tool to remove the association between a feature class or table and a domain.", "Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The geodatabase that contains the domain to be deleted. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the domain to be deleted. ", "dataType": "String"}]},
{"syntax": "DeleteCodedValueFromDomain_management (in_workspace, domain_name, code)", "name": "Delete Coded Value From Domain (Data Management)", "description": "Removes a value from a coded value domain.", "example": {"title": "Delete Coded Value from Domain Example (Python Window)", "description": "The following Python window script demonstrates how to use the CreateDomain function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.DeleteCodedValueFromDomain_management ( \"montgomery.gdb\" , \"DistDiam\" , [ \"20\" , \"24\" ])"}, "usage": ["Workspace domains can also be managed in ArcCatalog or the ", "Catalog", " window. Domains can be created and modified through the ", "Domains", " tab on the ", "Database Properties", " dialog box.", "The ", "Code Value", " parameter ", "Add Value", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Code Value", " parameter may not be populated with values. The ", "Add Value", " button allows you to add expected values so you continue to build your model."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The workspace containing the domain to be updated. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the coded value domain to be updated. ", "dataType": "String"}, {"name": "code", "isOptional": false, "description": "The value(s) to be deleted from the specified domain. ", "dataType": "String"}]},
{"syntax": "Warp_management (in_raster, source_control_points, target_control_points, out_raster, {transformation_type}, {resampling_type})", "name": "Warp (Data Management)", "description": "Performs a transformation on the raster based on the source and target control points using a polynomial transformation. This is similar to georeferencing using a text file.", "example": {"title": "Warp example 1 (Python window)", "description": "This is a Python sample for the Warp tool.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" source_pnt = \"'234718 3804287';'241037 3804297';'244193 3801275'\" target_pnt = \"'246207 3820084';'270620 3824967';'302634 3816147'\" arcpy.Warp_management ( \"raster.img\" , source_pnt , target_pnt , \"warp.tif\" , \"POLYORDER1\" , \"BILINEAR\" )"}, "usage": ["You must specify the source and target coordinates. The transformation type (polynomial order) from which to choose is dependent on the number of control points entered.", "The default polynomial order (1) will perform an affine transformation.", "Warp is useful when the raster requires a systematic geometric correction that can be modeled with a polynomial. A spatial transformation can invert or remove a distortion by using polynomial transformation of the proper order. The higher the order, the more complex the distortion that can be corrected. The higher orders of polynomial will involve progressively more processing time.", "To determine the minimum number of links necessary for a given order of polynomial, use the following formula:", "This tool will determine the extent of the warped raster and will set the number of rows and columns to be about the same as in the input raster. Some minor differences may be due to the changed proportion between the output raster's sizes in the x and y directions. The default cell size used will be computed by dividing the extent by the previously determined number of rows and columns. The value of the cell size will be used by the resampling algorithm.", "If you choose to define an ", "output cell size", " in the Environment Settings, the number of rows and columns will be calculated as follows:", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "source_control_points", "isOptional": false, "description": "The source points are the \"from\" coordinates of the links. ", "dataType": "Point"}, {"name": "target_control_points", "isOptional": false, "description": "The target points are the \"to\" coordinates of the links. ", "dataType": "Point"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "Output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "transformation_type", "isOptional": true, "description": "The geometric transformation type. POLYORDER0 \u2014 A zero-order polynomial is used to shift your data. This is commonly used when your data is already georeferenced, but a small shift will better line up your data. Only one link is required to perform a zero-order polynomial shift. POLYORDER1 \u2014 A first-order polynomial (affine) fits a flat plane to the input points. This is the default. POLYORDER2 \u2014 A second-order polynomial fits a somewhat more complicated surface to the input points. POLYORDER3 \u2014 A third-order polynomial fits a more complicated surface to the input points. ADJUST \u2014 A transformation that optimizes for both global and local accuracy. It accomplishes this by first performing a polynomial transformation, then adjusting the control points locally, to better match the target control points, using a triangulated irregular network (TIN) interpolation technique. SPLINE \u2014 A transformation that exactly transforms the source control points to the target control points. This means that the control points will be accurate, but the raster pixels that are between the control points are not. PROJECTIVE \u2014 A transformation that can warp lines so that they remain straight. In doing so, lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products.", "dataType": "String"}, {"name": "resampling_type", "isOptional": true, "description": "The resampling algorithm to be used. The default is NEAREST. The NEAREST and MAJORITY options are used for categorical data, such as a land-use classification. The NEAREST option is the default since it is the quickest and also because it will not change the cell values. Do not use NEAREST or MAJORITY for continuous data, such as elevation surfaces. The BILINEAR option and the CUBIC option are most appropriate for continuous data. It is not recommended that BILINEAR or CUBIC be used with categorical data because the cell values may be altered. NEAREST \u2014 Nearest neighbor assignment BILINEAR \u2014 Bilinear interpolation CUBIC \u2014 Cubic convolution MAJORITY \u2014 Majority resampling ", "dataType": "String"}]},
{"syntax": "Shift_management (in_raster, out_raster, x_value, y_value, {in_snap_raster})", "name": "Shift (Data Management)", "description": "Moves (slides) the raster to a new geographic location, based on x and y shift values. This tool is helpful if your raster dataset needs to be shifted to align with another data file.", "example": {"title": "Shift example 1 (Python window)", "description": "This is a Python sample for the Shift tool.", "code": "import arcpy arcpy.Shift_management ( \"c:/data/image.tif\" , \"c:/output/shift.tif\" , \"100\" , \"150\" , \"snap.tif\" )"}, "usage": ["The cell size of the output raster will be the same as that of the input raster.", "The number of rows and columns in the output raster will be the same as those of the input raster, no matter what parameters are specified.", "The coordinates of the lower left corner of the output raster will be offset from the input raster by the x and y shift coordinate values specified.", "Using a negative shift x-coordinate value will shift the output to the left. A positive shift x-coordinate value will shift the output to the right. Using a negative shift y-coordinate value will shift the output down. A positive shift y-coordinate value will shift the output to the top.", "The output raster dataset is nudged according to the location of the input snap raster, so the new shifted raster dataset can be aligned perfectly with another raster dataset.", "Shift does not perform any resampling or warping.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "Output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "x_value", "isOptional": false, "description": "The value used to shift the x coordinates. ", "dataType": "Double"}, {"name": "y_value", "isOptional": false, "description": "The value used to shift the y coordinates. ", "dataType": "Double"}, {"name": "in_snap_raster", "isInputFile": true, "isOptional": true, "description": "The raster dataset used to align the cells of the output raster dataset. ", "dataType": "Raster Layer"}]},
{"syntax": "Rotate_management (in_raster, out_raster, angle, {pivot_point}, {resampling_type})", "name": "Rotate (Data Management)", "description": "Turns a raster dataset around the specified pivot point by the angle specified angle in degrees; the raster dataset will rotate in a clockwise direction. Valid values for the rotation angle is any number from 0 to 360, including floating point values. A negative value will rotate the image counterclockwise.", "example": {"title": "Rotate example 1 (Python window)", "description": "This is a Python sample for the Rotate tool.", "code": "import arcpy arcpy.Rotate_management ( \"c:/data/image.tif\" , \"c:/output/rotate.tif\" , \"30\" , \"1940000 304000\" , \"BILINEAR\" )"}, "usage": ["Rotation is, by default, around the lower left corner of the raster. The rotation point can be changed with the optional Pivot Point parameter.", "Resampling is only done if the angle is not a multiple of 90.", "The rotation angle specified must be between 0 and 360, and the dataset will be rotated clockwise.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "angle", "isOptional": false, "description": "The angle in degrees to rotate the raster. This can be any floating-point number. ", "dataType": "Double"}, {"name": "pivot_point", "isOptional": true, "description": "The pivot point around which to rotate the raster. The default is the lower left corner of the input raster dataset. To add this variable, press F8 or right-click and click Insert Variable . ", "dataType": "Point"}, {"name": "resampling_type", "isOptional": true, "description": "The resampling algorithm to be used. The default is NEAREST. The NEAREST and MAJORITY options are used for categorical data, such as a land-use classification. The NEAREST option is the default since it is the quickest and also because it will not change the cell values. Do not use NEAREST or MAJORITY for continuous data, such as elevation surfaces. The BILINEAR option and the CUBIC option are most appropriate for continuous data. It is not recommended that BILINEAR or CUBIC be used with categorical data because the cell values may be altered. NEAREST \u2014 Nearest neighbor assignment BILINEAR \u2014 Bilinear interpolation CUBIC \u2014 Cubic convolution MAJORITY \u2014 Majority resampling ", "dataType": "String"}]},
{"syntax": "Rescale_management (in_raster, out_raster, x_scale, y_scale)", "name": "Rescale (Data Management)", "description": "Resizes a raster by the specified x and y scale factors.", "example": {"title": "Rescale example 1 (Python window)", "description": "This is a Python sample for the Rescale tool.", "code": "import arcpy arcpy.Rescale_management ( \"c:/data/image.tif\" , \"c:/output/rescale.tif\" , \"4\" , \"4\" )"}, "usage": ["The output size is multiplied by the scale factor for both the x and y directions. The number of columns and rows stays the same in this process, but the cell size is multiplied by the scale factor.", "The scale factor must be positive.", "A scale factor greater than one means the image will be rescaled to a larger dimension, resulting in a larger extent because of a larger cell size.", "A scale factor less than one means the image will be rescaled to a smaller dimension, resulting in a smaller extent because of a smaller cell size.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "Output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "x_scale", "isOptional": false, "description": "The factor in which to scale the cell size in the x direction. The factor must be greater than zero. ", "dataType": "Double"}, {"name": "y_scale", "isOptional": false, "description": "The factor in which to scale the cell size in the y direction. The factor must be greater than zero. ", "dataType": "Double"}]},
{"syntax": "ProjectRaster_management (in_raster, out_raster, out_coor_system, {resampling_type}, {cell_size}, {geographic_transform}, {Registration_Point}, {in_coor_system})", "name": "Project Raster (Data Management)", "description": "Transforms the raster dataset from one projection to another. \r\n Learn more about how Project Raster works \r\n", "example": {"title": "ProjectRaster example 1 (Python window)", "description": "This is a Python sample for the ProjectRaster tool.", "code": "import arcpy from arcpy import env arcpy.ProjectRaster_management ( \"c:/data/image.tif\" , \"c:/output/reproject.tif\" , \"World_Mercator.prj\" , \"BILINEAR\" , \"5\" , \"NAD_1983_To_WGS_1984_5\" , \"#\" , \"#\" )"}, "usage": ["The coordinate system defines how your raster data is projected.", "This tool guarantees that the error is less than half a pixel.", "You are able to choose a preexisting spatial reference, import it from another dataset, or create a new one.", "You may want to change the coordinate system so your data is all in the same projection.", "This tool can only output a square cell size.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings.", "Projects a raster dataset into a new spatial reference using a bilinear interpolation approximation method, which projects pixels on a coarse mesh grid and uses bilinear interpolation between the pixels.", "The NEAREST option, which performs a nearest neighbor assignment, is the fastest of the four interpolation methods. It is primarily used for categorical data, such as a land-use classification, because it will not change the cell values. It is not recommended that NEAREST be used for continuous data, such as elevation surfaces.", "The BILINEAR option, bilinear interpolation, determines the new value of a cell based on a weighted distance average of surrounding cells. The CUBIC option, cubic convolution, determines the new cell value by fitting a smooth curve through the surrounding points. These are most appropriate for continuous data and may cause some smoothing; also, cubic convolution may result in the output raster containing values outside the range of the input raster. It is not recommended that BILINEAR or CUBIC be used with categorical data because the cell values may be altered.", "The cells of the raster dataset will be square and of equal area in map coordinate space, although the shape and area a cell represents on the surface of the earth will never be constant across a raster. This is because no map projection can preserve both shape and area simultaneously. The area represented by the cells will vary across the raster. Therefore, the cell size and the number of rows and columns in the output raster may change.", "Always specify an output cell size, unless you are projecting between spherical (latitude\u2013longitude) coordinates and a planar coordinate system where you don't know what an appropriate cell size would be.", "The default cell size of the output raster is determined from the projected cell size at the center of the output raster. This is also (usually) the intersection of the central meridian and latitude of true scale and is the area of least distortion. The boundary of the input raster is projected, and the minimum and maximum extents dictate the size of the output raster. Each cell is projected back to the input coordinate system to determine the cell's value.", "The geographic transformation is an optional parameter when the input and output coordinate systems have the same datum. If the input and output datum are different, a geographic transformation needs to be specified.", "The registration point allows you to specify the origin point for anchoring the output cells. All output cells will be an interval of the cell size away from this point. This point does not have to be a corner coordinate or fall within the raster dataset. If a Snap Raster is set in the Environment Settings, the registration point will be ignored.", "CLARKE 1866 is the default spheroid if it is not inherent to the projection (such as NEWZEALAND_GRID) or another is specified with the SPHEROID subcommand.", "The snap raster setting will take priority over the registration point, if both are set."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset to be created. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "out_coor_system", "isOutputFile": true, "isOptional": false, "description": "The coordinate system to which the input raster will be projected. The default value is set based on the Output Coordinate System environment setting. Valid values for this parameter are A file with the \".prj\" extension (the prj files which ship with ArcGIS can be found in \"C:\\Program Files\\ArcGIS\\Coordinate Systems\"). An existing feature class, feature dataset, raster catalog (basically anything with a coordinate system). The string representation of a coordinate system. These lengthy strings can be generated by adding a coordinate system variable to ModelBuilder, setting the variable's value as desired, then exporting the model to a Python script.", "dataType": "Coordinate System"}, {"name": "resampling_type", "isOptional": true, "description": "The resampling algorithm to be used. The default is NEAREST. The NEAREST and MAJORITY options are used for categorical data, such as a land-use classification. The NEAREST option is the default since it is the quickest and also because it will not change the cell values. Do not use NEAREST or MAJORITY for continuous data, such as elevation surfaces. The BILINEAR option and the CUBIC option are most appropriate for continuous data. It is not recommended that BILINEAR or CUBIC be used with categorical data because the cell values may be altered. NEAREST \u2014 Nearest neighbor assignment BILINEAR \u2014 Bilinear interpolation CUBIC \u2014 Cubic convolution MAJORITY \u2014 Majority resampling ", "dataType": "String"}, {"name": "cell_size", "isOptional": true, "description": "The cell size for the new raster dataset. The default cell size is the cell size of the selected raster dataset. ", "dataType": "Analysis Cell Size"}, {"name": "geographic_transform", "isOptional": true, "description": "The transformation method used between two geographic systems or datums. The geographic transformation is optional when the input and output coordinate systems have the same datum. If the input and output datum are different, a geographic transformation needs to be specified. For information on each supported geographic (datum) transformations, see the geographic_transformations.pdf located in <install location>\\ArcGIS\\Desktop 10.1 \\Documentation. ", "dataType": "String"}, {"name": "Registration_Point", "isOptional": true, "description": "The x and y coordinates (in the output space) used for pixel alignment. The registration point works similar to the concept of snap raster. Instead of snapping the output to an existing raster cell alignment, the registration point allows you to specify the origin point for anchoring the output cells. All output cells will be an interval of the cell size away from this point. This point does not have to be a corner coordinate or fall within the raster dataset. The Snap Raster environment setting will take priority over the Registration Point parameter. Therefore, if you want to set the registration point, make sure that Snap Raster is not set. ", "dataType": "Point"}, {"name": "in_coor_system", "isInputFile": true, "isOptional": true, "description": "The coordinate system of the input raster dataset. ", "dataType": "Coordinate System"}]},
{"syntax": "Mirror_management (in_raster, out_raster)", "name": "Mirror (Data Management)", "description": "Reorients the raster by flipping it, from left to right, along the vertical axis through the center of the raster.", "example": {"title": "Mirror example 1 (Python window)", "description": "This is a Python sample for Mirror.", "code": "import arcpy arcpy.Mirror_management ( \"c:/data/image.tif\" , \"c:/data/mirror.tif\" )"}, "usage": ["Mirror flips the raster from left to right along the vertical axis through the center of the region.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "Input raster dataset. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "Output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}]},
{"syntax": "Flip_management (in_raster, out_raster)", "name": "Flip (Data Management)", "description": "Reorients the raster by turning it over, from top to bottom, along the horizontal axis through the center of the raster. This may be useful to correct raster datasets that are upside down.", "example": {"title": "Flip example 1 (Python window)", "description": "This is a Python sample for the Flip tool.", "code": "import arcpy arcpy.Flip_management ( \"c:/data/image.tif\" , \"c:/data/flip.tif\" )"}, "usage": ["This tool flips the grid from top to bottom along the horizontal axis through the center of the region.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "Input raster dataset. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "Output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}]},
{"syntax": "Project_management (in_dataset, out_dataset, out_coor_system, {transform_method}, {in_coor_system})", "name": "Project (Data Management)", "description": "Projects spatial data from one coordinate system to another.  ", "example": {"title": "Project example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Project function in immediate mode.", "code": "import arcpy input_features = \"C:/data/input/projections.gdb/wells\" output_features_class = \"C:/data/output/wells_UTM11N.shp\" out_coordinate_system = arcpy.SpatialReference ( \"NAD 1983 UTM Zone 11N\" ) arcpy.Project_management ( input_features , output_features_class , out_coordinate_system )"}, "usage": ["If the input feature class or dataset has an Unknown or unspecified coordinate system, you can specify the input dataset's coordinate system with the ", "Input Coordinate System", " parameter.  This allows you to specify the data's coordinate system  without having to modify the input data (which may not be possible if the input is a read-only format).  Also you can use the ", "Define Projection", " tool to permanently assign a coordinate system to the dataset.   ", "All types of ", "feature classes", " (geodatabase feature class, coverage feature class, SDC feature class, and shapefile), ", "feature datasets", " in a geodatabase and ", "feature layers", " in ArcGIS applications (ArcMap, ArcScene, and ArcGlobe),   are valid input. ", "Coverages, VPF Coverages, raster datasets, and raster catalogs are not supported as input to this tool. To project a Coverage, use the ", "Project", " tool in Coverage toolbox.  Use the ", "Project Raster", " tool to project raster datasets.", "The tool's ", "Geographic Transformation", " parameter is optional. When no geographic or datum transformation is required, no drop-down list will appear on the parameter and  it is left blank.  When a transformation is required, a drop-down list will be generated based on the input and output datums  and a default transformation will be picked.", "Transformations are bidirectional.  For example, if converting data from WGS 1984 to NAD 1927, you can pick a transformation called NAD_1927_to_WGS_1984_3, and the tool will apply it correctly.", "in_memory", " is not a valid workspace for the output dataset. The output must be written to a feature class on disk.", "When projecting the complex data types listed below, certain operations need to be performed on the resulting data: ", "If the input participates in relationship classes (as with feature-linked annotation), the relationship class will be transferred to the output.  The exception to this rule relates to stand-alone tables.", "Depending on the input feature's coordinates and the horizon (valid extent) of the output coordinate system,  multipoint, line, and polygon  may be clipped or split into more than one part as part of projecting them.", "Feature classes participating in a geometric network cannot be projected independently\u2014the whole Feature Dataset containing the network needs to be projected.", "Many geoprocessing tools honor the ", "output coordinate system", "  environment setting, and in many workflows you can use this environment setting using the ", "Project", " tool. For example, the ", "Union", " tool honors the output coordinate system environment setting, which means you can union several feature classes together, all of which are in a different coordinate system and write the unioned output to a feature class in an entirely different coordinate system.", "Learn more about geoprocessing environments", "Selection and definition query on layers are not supported by this tool: all features in the dataset referenced by the layer will be projected.  If you want to project selected features only, consider using the ", "Copy Features", " tool.  ", "Copy Features", " only copies selected features and honors the ", "output coordinate system", " geoprocessing environment.", "When a feature class within a feature dataset is used as input,  the output cannot be written to the same feature dataset.  This is because feature classes within a feature dataset must all have the same coordinate system. In this case, the output feature class will be written to the geodatabase containing the feature dataset."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The feature class, feature layer, or feature dataset to be projected. ", "dataType": "Feature Layer; Feature Dataset"}, {"name": "out_dataset", "isOutputFile": true, "isOptional": false, "description": "The output dataset to which the results will be written. ", "dataType": "Geodataset"}, {"name": "out_coor_system", "isOutputFile": true, "isOptional": false, "description": " Valid values are a Spatial Reference object , a file with a .prj extension, or a string representation of a coordinate system. ", "dataType": "Coordinate System"}, {"name": "transform_method", "isOptional": true, "description": "This method can be used for converting data between two geographic coordinate systems or datums. This optional parameter may be required if the input and output coordinate systems have different datum. Transformations are bi-directional . For example, if converting data from WGS 1984 to NAD 1927, you can pick a transformation called NAD_1927_to_WGS_1984_3, and the tool will apply it correctly. ", "dataType": "String"}, {"name": "in_coor_system", "isInputFile": true, "isOptional": true, "description": "The coordinate system of the input feature class or dataset. This parameter becomes enabled when the input has an Unknown, or unspecified, coordinate system. This allows you to specify the data's coordinate system without having to modify the input data (which may not be possible if the input is in read-only format). ", "dataType": "Coordinate System"}]},
{"syntax": "CreateSpatialReference_management ({spatial_reference}, {spatial_reference_template}, {xy_domain}, {z_domain}, {m_domain}, {template}, {expand_ratio})", "name": "Create Spatial Reference (Data Management)", "description": "Creates a spatial reference object for use in ModelBuilder and scripting.", "example": {"title": "Create Spatial Reference example (stand-alone script)", "description": "The following stand-alone script uses the CreateSpatialReference function as part of a workflow that loops through a folder and finds all shapefiles that end in \"ST\", create spatial references, and append them into a geodatabase feature class.", "code": "# Name: findSTshp.py # Purpose: Loops through a folder and finds all feature classes within  the geodatabases under it. # Filters only Polygon feature classes and copies them to a new geodatabase. # Import system modules import arcpy import os from arcpy import env try : #Set the workspace env.workspace = r\"C:\\data\\Redlands\" workspaces = arcpy.ListWorkspaces () print workspaces fcList = [] for fd in workspaces : env.workspace = fd fcs = arcpy.ListFeatureClasses ( '*' , 'POLYGON' ) for fc in fcs : fcList.append ( fd + os.sep + fc ) fc1 = fcList [ 0 ] print fc1 print fcList #sr = arcpy.CreateSpatialReference_management(\"\",fc1,\"\",\"\",\"\",fcList) #outFC = arcpy.CreateFeatureclass_management ('c:/data/gdb.mdb', 'gdt2', 'polyline',fc1, \"\",\"\", sr) #arcpy.Append_management(fcList, outFC, 'no_test') except Exception , e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"Line  %i \" % tb.tb_lineno print e.message print \"FINISHED\""}, "usage": ["You can  create a spatial reference object with set ", "coordinate system", ", ", "spatial domains", ", and ", "precision", ". The spatial domains and precision of the output spatial reference can be further modified using ", "XY Domain", ", ", "Z Domain", ", ", "M Domain", ", ", "Template XYDomains", ", and ", "Grow XYDomain By Percentage", " parameters.", "Template XYDomains", " does not have to be in the same coordinate system as that specified in ", "Spatial Reference", " or ", "Spatial Reference Template", ". If they are different, the extents will be projected to match.", "If the ", "Spatial Reference", " and ", "Spatial Reference Template", " parameters are both set, spatial reference parameter will take priority.", "All the parameters of the tool are optional. If no parameters are specified, the spatial reference will be defined as 'Unknown' and the XYDomain will assume standard defaults.", "In ModelBuilder, the output of this tool  can be used as input to tools with a spatial reference parameter (e.g., ", "Create_Feature_Class", ", ", "Create_Feature_Dataset", ", ", "Make_XY_Event_Layer", ")."], "parameters": [{"name": "spatial_reference", "isOptional": true, "description": "Name of the the spatial reference object to be created. ", "dataType": "Spatial Reference"}, {"name": "spatial_reference_template", "isOptional": true, "description": "The feature class or layer to be used as a template to set the value for the spatial reference. ", "dataType": "Feature Layer; Raster Catalog Layer; Raster Dataset"}, {"name": "xy_domain", "isOptional": true, "description": "Allowable coordinate range for x,y coordinates. ", "dataType": "Envelope"}, {"name": "z_domain", "isOptional": true, "description": "Allowable coordinate range for z values. ", "dataType": "String"}, {"name": "m_domain", "isOptional": true, "description": "Allowable coordinate range for m values. ", "dataType": "String"}, {"name": "template", "isOptional": false, "description": "Feature classes or layers that can be used to define the XY Domain. ", "dataType": "Feature Layer"}, {"name": "expand_ratio", "isOptional": true, "description": "Percentage by which the XY Domain will be expanded. ", "dataType": "Double"}]},
{"syntax": "BatchProject_management (Input_Feature_Class_or_Dataset, Output_Workspace, {Output_Coordinate_System}, {Template_dataset}, {Transformation})", "name": "Batch Project (Data Management)", "description": "Changes the coordinate system of a set of input feature classes or feature datasets to a common coordinate system. To change the coordinate system of a single feature class or dataset use the  Project  tool.", "example": {"title": "BatchProject example 1 (Python window)", "description": "The following Python window script demonstrates how to use the BatchProject function in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/input/batchproject\" arcpy.BatchProject_management ([ \"citylim.shp\" , \"flood.shp\" , \"faultzn.shp\" ], \"C:/data/output/batchproject\" , \"\" , \"C:/data/usa.gdb/templatefc\" )"}, "usage": ["Any valid inputs to the ", "Project", " tool, such as all ", "feature classes", " or ", "feature datasets", ", are also valid inputs for this tool.", "Although both the ", "Output Coordinate System", " and ", "Template Dataset", " are optional parameters, one of them must be entered. Keeping both of these parameters empty will cause tool execution to fail.", "As the tool does not validate whether a transformation is required, use the ", "Project", " tool first with one of the inputs to determine whether any transformation is required. The drop-down list of the Project tool's ", "Geographic Transformation", " parameter shows which transformations (if any) are valid.", "A feature class or dataset with an undefined or Unknown coordinate system must first have its coordinate system defined using the ", "Define Projection", " tool before it can be used with the tool.", "The names of the output feature classes will be based on the names of the input feature classes. For instance, if the input is ", "C:\\myworkspace\\Gondor.shp", ", the output feature class will be named ", "Gondor.shp", ". If the name already exists in the output workspace, a number will be appended (for example, ", "_1", ") to the end to make it unique (", "Gondor_1.shp", ") in the output workspace."], "parameters": [{"name": "Input_Feature_Class_or_Dataset", "isInputFile": true, "isOptional": false, "description": "The input feature classes or feature datasets whose coordinates are to be converted. ", "dataType": "Geodataset"}, {"name": "Output_Workspace", "isOutputFile": true, "isOptional": false, "description": "The location of each new output feature class or feature dataset. ", "dataType": "Feature Dataset; Workspace"}, {"name": "Output_Coordinate_System", "isOutputFile": true, "isOptional": true, "description": "The coordinate system to be used to project the inputs. Valid values are a Spatial Reference object , a file with a .prj extension, or a string representation of a coordinate system. ", "dataType": "Coordinate System"}, {"name": "Template_dataset", "isOptional": true, "description": "The feature class or the feature dataset used to specify the output coordinate system used for projection. ", "dataType": "Geodataset"}, {"name": "Transformation", "isOptional": true, "description": "Name of the geographic transformation to be applied to convert data between two geographic coordinate systems (datums). ", "dataType": "String"}]},
{"syntax": "DefineProjection_management (in_dataset, coor_system)", "name": "Define Projection (Data Management)", "description": "This tool overwrites the coordinate system information (map projection and datum) stored with a dataset.  The only use for this tool is for datsets that have an unknown or incorrect coordinate system defined. All geographic datasets have a coordinate system that is used throughout ArcGIS to display, measure, and transform geographic data.  If the coordinate system for a dataset is unknown or incorrect, you can use this tool to specify the correct coordinate system.   You must know the correct coordinate system of the dataset before using this tool.", "example": {"title": "DefineProjection example (Python window)", "description": "The following Python Window script demonstrates how to use the DefineProjection function in immediate mode.", "code": "import arcpy infc = r\"C:\\data\\citylim_unk.shp\" sr = arcpy.SpatialReference ( \"NAD 1983 UTM Zone 11N\" ) arcpy.DefineProjection_management ( infc , sr )"}, "usage": ["This tool only updates the existing coordinate system information\u2014it does not modify any geometry.  If you want to transform the geometry to another coordinate system, use the ", "Project", " tool.", "The most common use for this tool is to assign a known coordinate system to a dataset with an unknown coordinate system (that is, the coordinate system is  \"Unknown\" in the dataset properties).   Another use is to assign the correct coordinate system  for  a dataset that has an incorrect coordinate system defined (for example, the coordinates are in UTM meters but the coordinate system is defined as geographic).  ", "When a dataset with a known coordinate system is input to this tool, the tool will issue a warning but will execute successfully.", "All feature classes in a geodatabase feature dataset will be in the same coordinate system. The coordinate system for a geodatabase dataset should be determined when it is created. Once it contains feature classes, its coordinate system cannot be changed."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "Dataset or feature class whose projection is to be defined. ", "dataType": "Feature Layer;Geodataset"}, {"name": "coor_system", "isOptional": false, "description": " Valid values are a Spatial Reference object , a file with a .prj extension, or a string representation of a coordinate system. ", "dataType": "Coordinate System"}]},
{"syntax": "CreateCustomGeoTransformation_management (geot_name, in_coor_system, out_coor_system, custom_geot)", "name": "Create Custom Geographic Transformation (Data Management)", "description": "Creates a transformation method for converting data between two geographic coordinate systems or datums. The output of this tool can be used as a transformation method for any tool with a parameter that requires a geographic transformation.", "example": {"title": "CreateCustomGeoTransformation Example (Python stand-alone script)", "description": "The following stand-alone script uses the CreateCustomGeoTransformation function to create a custom transformation for a particular use case. The output is a *.gtf file created in the default directory.", "code": "# Name: CreateCustomGeographicTransformation.py # Description: Creates a custom geographic transformation in the default directory. # import system modules import arcpy # set the variables geoTransfmName = \"cgt_geocentric2\" inGCS = \"GEOGCS['GCS_Tokyo',DATUM['D_Tokyo',SPHEROID['Bessel_1841',6377397.155,299.1528128]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]]\" outGCS = \"GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]]\" customGeoTransfm = \"GEOGTRAN[METHOD['Geocentric_Translation'],PARAMETER['X_Axis_Translation',''],PARAMETER['Y_Axis_Translation',''],PARAMETER['Z_Axis_Translation','']]\" arcpy.CreateCustomGeoTransformation_management ( geoTransfmName , inGCS , outGCS , customGeoTransfm )"}, "usage": [" All custom geographic transformation files are saved with a ", ".gtf", " extension and stored in the  ", "ESRI\\<ArcGIS product>\\ArcToolbox\\CustomTransformations", " folder under the user's ", "Application Data", " folder.  The ", "CustomTransformations", " folder is created by the tool if it does not exist. If the ", "Application Data", " folder is read-only or hidden, the output is created in ", "ArcToolbox\\CustomTransformations", " under the user's temp folder. The location or name of the ", "Application Data", " and ", "temp", " folders is dependent on the operating system. ", "Any geoprocessing tool that uses geographic transformations will look at all custom transformations in the default storage location and present them as valid transformation options on the dialog box under the ", "Geographic Transformation", "  parameter's drop-down list.", "Custom transformation files can't be edited. They are binary files that store version and string length information that may be corrupted if edited outside of the geoprocessing framework. To update the file you should create a new custom geographic transformation and overwrite the existing file."], "parameters": [{"name": "geot_name", "isOptional": false, "description": "Name of the custom transformation method. All custom geographic transformation files are saved with a .gtf extension and stored in the ESRI\\<ArcGIS product>\\ArcToolbox\\CustomTransformations folder under the user's Application Data folder. The CustomTransformations folder is created by the tool if it does not exist. If the Application Data folder is read-only or hidden, the output is created in ArcToolbox\\CustomTransformations under the user's temp folder. The location or name of the Application Data and temp folders is dependent on the operating system. In any Windows operating system the Application Data folder is located at %appdata% and the user's Temp folder is located at %temp% . In Unix systems, the tmp and Application Data folders are located in the user's home directory, under $HOME and $TMP , respective.", "dataType": "String"}, {"name": "in_coor_system", "isInputFile": true, "isOptional": false, "description": "The starting geographic coordinate system. ", "dataType": "Coordinate System"}, {"name": "out_coor_system", "isOutputFile": true, "isOptional": false, "description": "The final geographic coordinate system. ", "dataType": "Coordinate System"}, {"name": "custom_geot", "isOptional": false, "description": "Set the METHOD and PARAMETER values wrapped in a string for custom transformation GEOGTRAN. Set the name of the method from the available methods of Geocentric_Translation, Molodensky, Molodensky_Abridged, Position_Vector, Coordinate_Frame, Molodensky_Badekas, NADCON, HARN, NTV2, Longitude_Rotation, Unit_Change, and Geographic_2D_Offset. Each method has its own sets of parameters\u2014you can edit the values of the parameters by entering text next to the name of the parameter within the whole string representation of the custom geographic transformation. See examples in the Python sample below. ", "dataType": "String"}]},
{"syntax": "SelectLayerByLocation_management (in_layer, {overlap_type}, {select_features}, {search_distance}, {selection_type})", "name": "Select Layer By Location (Data Management)", "description": "Selects features in a layer based on a spatial relationship to features in another layer. Each feature in the  Input Feature Layer  is evaluated against the features in the   Selecting Features  layer or feature class; if the specified  Relationship  is met, the input feature is selected. \r\n Graphic examples of relationships \r\n", "example": {"title": "SelectLayerByLocation example 1 (Python window)", "description": "The following Python window script demonstrates how to use the SelectLayerByLocation function in immediate mode.", "code": "import arcpy # First, make a layer from the feature class arcpy.MakeFeatureLayer_management ( \"c:/kamsack.gdb/parcel\" , \"parcel_lyr\" ) # Then add a selection to the layer based on location to features in another feature class  arcpy.SelectLayerByLocation_management ( \"parcel_lyr\" , \"have_their_center_in\" , \"c:/kamsack.gdb/city_limits\" )"}, "usage": ["The input must be a ", "feature layer", "; it cannot be a ", "feature class", ".", "Valid inputs for this tool are layers in the ArcMap, ArcGlobe, or ArcScene table of contents, and also on layers created in ArcCatalog or in scripts using the ", "Make Feature Layer", " tool. ", " The coordinate system in which the spatial relationship is evaluated may affect the result. Features that intersect in one coordinate system may or may not intersect in another. ", "This tool can be used to select features based on their spatial relationships to other features within the same layer. For some examples, see: ", "Select by location within a layer.", "The ", "Get Count", " tool can be used to find the number of features selected by the Select Layer By Location tool. This can be useful for determining if any features matched the desired spatial relationship before proceeding to further analysis as part of an automated workflow (that is, script or model).", "For more information about using the three-dimensional spatial relationships (INTERSECT_3D and WITHIN_A_DISTANCE_3D), see ", "Select by location 3D relationships", "."], "parameters": [{"name": "in_layer", "isInputFile": true, "isOptional": false, "description": "The layer containing the features that will be evaluated against the Selecting Features. The selection will be applied to this layer. The input can be a layer in the ArcMap table of contents, or a layer created in ArcCatalog or in scripts using the Make Feature Layer tool. The input cannot be the path to a feature class on disk. ", "dataType": "Feature Layer; Mosaic Layer; Raster Catalog Layer"}, {"name": "overlap_type", "isOptional": true, "description": "The spatial relationship to be evaluated. INTERSECT \u2014 The features in the input layer will be selected if they intersect a selecting feature. This is the default. INTERSECT_3D \u2014 The features in the input layer will be selected if they intersect a selecting feature in three-dimensional space (x, y, and z). WITHIN_A_DISTANCE \u2014 The features in the input layer will be selected if they are within a specified distance of a selecting feature. Specify a distance in the Search Distance parameter. WITHIN_A_DISTANCE_3D \u2014 The features in the input layer will be selected if they are within a specified distance of a selecting feature in three-dimensional space. Specify a distance in the Search Distance parameter. CONTAINS \u2014 The features in the input layer will be selected if they contain a selecting feature. The input features must be polygons. COMPLETELY_CONTAINS \u2014 The features in the input layer will be selected if they completely contain a selecting feature. The input features must be polygons. CONTAINS_CLEMENTINI \u2014 This spatial relationship yields the same results as COMPLETELY_CONTAINS with one exception. If the selecting feature is entirely on the boundary of the input feature (no part is properly inside or outside), the feature will not be selected. CLEMENTINI defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty. WITHIN \u2014 The features in the input layer will be selected if they are within a selecting feature. The selecting features must be polygons. COMPLETELY_WITHIN \u2014 The features in the input layer will be selected if they are completely within or contained by a selecting feature. The selecting features must be polygons. WITHIN_CLEMENTINI \u2014 The result will be identical to WITHIN except if the entirety of the feature in the input layer is on the boundary of the feature in the selecting layer, the feature will not be selected. CLEMENTINI defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty. ARE_IDENTICAL_TO \u2014 The features in the input layer will be selected if they are identical (in geometry) to a selecting feature. BOUNDARY_TOUCHES \u2014 The features in the input layer will be selected if they have a boundary that touches a selecting feature. The input and selecting features must be lines or polygons. Additionally, the feature in the input layer must be either completely inside or outside the polygon from the selecting layer. SHARE_A_LINE_SEGMENT_WITH \u2014 The features in the input layer will be selected if they share a line segment with a selecting feature. The input and selecting features must be lines or polygons. CROSSED_BY_THE_OUTLINE_OF \u2014 The features in the input layer will be selected if they are crossed by the outline of a selecting feature. The input and selecting features must be lines or polygons. If polygons are used for the input or selecting layer, the polygon's boundary (line) will be used. Lines that cross at a point will be selected, not lines that share a line segment. HAVE_THEIR_CENTER_IN \u2014 The features in the input layer will be selected if their center falls within a selecting feature. The center of the feature is calculated as follows: for polygon and multipoint, the geometry's centroid is used, and for line input, the geometry's midpoint is used. CONTAINED_BY \u2014 This returns the same result as WITHIN. CONTAINED_BY is maintained to support backward compatibility with models and scripts built into releases prior to ArcGIS 9.3.", "dataType": "String"}, {"name": "select_features", "isOptional": true, "description": "The features in the Input Feature Layer will be selected based on their relationship to the features from this layer or feature class. ", "dataType": "Feature Layer"}, {"name": "search_distance", "isOptional": true, "description": "This parameter is only valid if the Relationship parameter is set to one of the following: WITHIN_A_DISTANCE, WITHIN_A_DISTANCE_3D, INTERSECT, INTERSECT_3D, HAVE_THEIR_CENTER_IN, CONTAINS, or WITHIN. ", "dataType": "Linear unit"}, {"name": "selection_type", "isOptional": true, "description": "Determines how the selection will be applied to the input and how to combine with an existing selection. Note that there is no option here to clear an existing selection. To clear a selection, use the CLEAR_SELECTION option on the Select Layer By Attribute tool. NEW_SELECTION \u2014 The resulting selection replaces any existing selection. This is the default. ADD_TO_SELECTION \u2014 The resulting selection is added to an existing selection, if one exists. If no selection exists, this is the same as the NEW_SELECTION option. REMOVE_FROM_SELECTION \u2014 The resulting selection is removed from an existing selection. If no selection exists, the operation will have no effect. SUBSET_SELECTION \u2014 The resulting selection is combined with the existing selection. Only records that are common to both remain selected. SWITCH_SELECTION \u2014 Switches the selection. All records that were selected are removed from the selection, and all records that were not selected are added to the selection. The Selecting Features and Relationship parameters are ignored when this option is selected. ", "dataType": "String"}]},
{"syntax": "SelectLayerByAttribute_management (in_layer_or_view, {selection_type}, {where_clause})", "name": "Select Layer By Attribute (Data Management)", "description": "Adds, updates, or removes a selection on a layer or table view based on an attribute query.", "example": {"title": "Select Layer By Attribute Example (Python Window)", "description": "The following Python window script demonstrates how to use the SelectLayerByAttribute function in immediate mode.", "code": "import arcpy arcpy.MakeFeatureLayer_management ( \"C:/data/data.mdb/states\" , \"stateslyr\" ) arcpy.SelectLayerByAttribute_management ( \"stateslyr\" , \"NEW_SELECTION\" , \" [NAME] = 'California' \" )"}, "usage": ["The input must be a ", "feature layer", " or a ", "table view", ". The input cannot be a ", "feature class", " or ", "table", ".", "This tool works on layers or table views in the ArcMap table of contents, and also on layers or table views created in a scripts using the ", "Make Feature Layer", " or ", "Make Table View", " tools. ", "If an", "Extent environment", " is specified, or if a definition query is present on the ", "Input Layer or Table View", ", only the features or rows within the extent or matching the definition query may be selected. ", "The ", "Get Count", " tool can be used to determine the number of features or rows selected.  This can be especially useful in a script or model to determine if further processing is desired."], "parameters": [{"name": "in_layer_or_view", "isInputFile": true, "isOptional": false, "description": "The feature layer or table view to which the selection will be applied. The input can be a layer or table view in the ArcMap table of contents, or a layer or table view created in ArcCatalog or in scripts using the Make Feature Layer or Make Table View tools. ", "dataType": "Table View; Raster Layer; Mosaic Layer"}, {"name": "selection_type", "isOptional": true, "description": "Determines how the selection will be applied and what to do if a selection already exists. NEW_SELECTION \u2014 The resulting selection replaces any existing selection. This is the default. ADD_TO_SELECTION \u2014 The resulting selection is added to an existing selection if one exists. If no selection exists, this is the same as the NEW_SELECTION option. REMOVE_FROM_SELECTION \u2014 The resulting selection is removed from an existing selection. If no selection exists, this option has no effect. SUBSET_SELECTION \u2014 The resulting selection is combined with the existing selection. Only records that are common to both remain selected. SWITCH_SELECTION \u2014 Switches the selection. All records that were selected are removed from the selection; all records that were not selected are added to the selection. The Expression is ignored when this option is specified. CLEAR_SELECTION \u2014 Clears or removes any selection. The Expression is ignored when this option is specified. ", "dataType": "String"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of records. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, coverages, or dBASE or INFO tables, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}]},
{"syntax": "SaveToLayerFile_management (in_layer, out_layer, {is_relative_path}, {version})", "name": "Save To Layer File (Data Management)", "description": "Creates an output  layer  file (.lyr) that references geographic data stored on disk.", "example": {"title": "SaveToLayerFile Example (Python Window)", "description": "The following Python Window script demonstrates how to use the SaveToLayerFile tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.SaveToLayerFile_management ( \"studyquadsLyr\" , \"C:/output/studyquadsLyr.lyr\" , \"ABSOLUTE\" )"}, "usage": ["This tool is used to save an in-memory layer, a layer file stored on disk, or a feature layer in ArcMap to a layer file (", ".lyr", ") that references geographic data stored on disk.", "This  tool accepts as input  feature layers created by tools such as ", "Make Feature Layer", " or ", "Make XY Event Layer", ".", "If the input layer has a selection applied to it, the output layer file will maintain this selection."], "parameters": [{"name": "in_layer", "isInputFile": true, "isOptional": false, "description": "The in-memory layer, layer file stored on disk, or feature layer in ArcMap to be saved to disk as a layer file ( .lyr ). ", "dataType": "Layer"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": "The output layer file ( .lyr ) to be created. ", "dataType": "Layer File"}, {"name": "is_relative_path", "isOptional": true, "description": " Determines if the output layer file ( .lyr ) will store a relative path to the source data stored on disk, or an absolute path. ABSOLUTE \u2014 The output layer file will store an absolute path to the source data stored on disk. This is the default. RELATIVE \u2014 The output layer file will store a relative path to the source data stored on disk. If the output layer file is moved, its source path will update to where the source data should be in relation to the new path.", "dataType": "Boolean"}, {"name": "version", "isOptional": true, "description": " The version of layer file the output will be saved as. The default is CURRENT. CURRENT \u2014 10.1 \u2014 10 \u2014 9.3 \u2014 9.2 \u2014 9.1 \u2014 9.0 \u2014 8.3 \u2014", "dataType": "String"}]},
{"syntax": "MakeXYEventLayer_management (table, in_x_field, in_y_field, out_layer, {spatial_reference}, {in_z_field})", "name": "Make XY Event Layer (Data Management)", "description": "Creates a new point feature layer based on x- and y-coordinates defined in a source table. If the source table contains z-coordinates (elevation values), that field can also be specified in the creation of the event layer. The layer created by this tool is temporary. Learn more about adding x,y coordinate data to a map", "example": {"title": "MakeXYEventLayer example (Python window)", "description": "The following Python window script demonstrates how to use the MakeXYEventLayer tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.MakeXYEventLayer_management ( \"firestations.dbf\" , \"POINT_X\" , \"POINT_Y\" , \"firestations_points\" , \"\" , \"POINT_Z\" )"}, "usage": ["The output point feature layer created by this  tool is temporary and will not persist after the session ends. You can export this event  layer to a feature class on disk using the ", "Copy Features", ", ", "Feature to Point", ", or ", "Feature Class to Feature Class", " tool.", "It is not possible to interactively move the output layer's points through editing controls, since event layers are not editable. Alternatives to directly moving these points are to change the x- and y-coordinate attributes in the input table, then re-create the event layer, or save the event layer to a feature class on disk, then perform edits on the feature class.", "The standard delimiter for tabular text files with extensions ", ".csv", " or ", ".txt", " is a comma, and for files with a ", ".tab", " extension, a tab. To use an input table with a nonstandard delimiter, you must first specify the correct delimiter  used in the table using a ", "schema.ini", " file.", "Learn more about working with tabular text files", "If the input table does not have an ObjectID field, you will not be able to make selections or add joins to the resulting layer. Many delimited text files or tables from OLE DS connections do not have ObjectID fields. "], "parameters": [{"name": "table", "isOptional": false, "description": "The table containing the X and Y coordinates that define the locations of the point features to create. ", "dataType": "Table View"}, {"name": "in_x_field", "isInputFile": true, "isOptional": false, "description": "The field in the input table that contains the x-coordinates. ", "dataType": "Field"}, {"name": "in_y_field", "isInputFile": true, "isOptional": false, "description": "The field in the input table that contains the y-coordinates. ", "dataType": "Field"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": "The name of the output point event layer. ", "dataType": "Feature Layer"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the coordinates in the X and Y Fields defined above. This will be the output event layer's spatial reference. ", "dataType": "Spatial Reference"}, {"name": "in_z_field", "isInputFile": true, "isOptional": true, "description": "The field in the input table that contains the z-coordinates. ", "dataType": "Field"}]},
{"syntax": "MakeWCSLayer_management (in_wcs_coverage, out_wcs_layer, {template}, {band_index})", "name": "Make WCS Layer (Data Management)", "description": "Creates a temporary raster layer from a WCS service.", "example": {"title": "MakeWCSLayer example 1 (Python window)", "description": "This is a Python sample for MakeWCSLayer.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Workspace\" input1 = \"GIS Servers/File_TIFF_Amberg on server3/090160_1\" arcpy.MakeWCSLayer_management ( input1 , \"wcslayer1\" , \"11.844983 49.445367 11.858321 49.453887\" , \"1;2;3\" )"}, "usage": ["This is one of the few tools that can accept a WCS service as an input; therefore, this tool can be used to convert a WCS service to a raster layer, which can then be used by a geoprocessing tool. The layer that is created by the tool is temporary and will not persist after the session ends unless the document is saved.", "The input can also be a URL to a WCS server. The WCS server URL should also include the coverage and version information. If only URL is entered, the tool will automatically take the first coverage and use default version (1.0.0) to create the WCS layer.", "The output can be the entire image service or a portion of it.", "You can clip out a portion of the image service by choosing an output extent layer or by specifying the rectangle extent. If you choose an output extent layer, the clip extent will be based upon extent of that layer.", "The output can be created with only a subset of the bands. This will help save on time and disk space."], "parameters": [{"name": "in_wcs_coverage", "isInputFile": true, "isOptional": false, "description": "The name of the input WCS service, or the URL that references the WCS service. If a WCS server URL is used, the URL should include the coverage name and version information. If only URL is entered, the tool will automatically take the first coverage and use default version (1.0.0) to create the WCS layer. An example of a URL that includes the coverage name and version is below: http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?coverage=rasterDRGs&version=1.1.1 In this example, \"http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?\" is the URL. The coverage specified is \"coverage=rasterDRGs\", and the version is \"&version=1.1.1\". To get the coverage names on a WCS server, use the WCS GetCapabilities request. An example of WCS request is http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?request=getcapabilities&service=wcs ", "dataType": "WCS Coverage; String"}, {"name": "out_wcs_layer", "isOutputFile": true, "isOptional": false, "description": "The name of the output WCS layer. ", "dataType": "Raster Layer"}, {"name": "template", "isOptional": true, "description": "The output extent of the WCS layer. The output extent can either be specified by defining the area to be clipped (X-Minimum, Y-Minimum, X-Maximum, and Y-Maximum) or by using the extent of an existing layer. ", "dataType": "Extent"}, {"name": "band_index", "isOptional": false, "description": "Choose which bands to export for the layer. If no bands are specified, then all the bands will be used in the output. ", "dataType": "Value Table"}]},
{"syntax": "MakeTableView_management (in_table, out_view, {where_clause}, {workspace}, {field_info})", "name": "Make Table View (Data Management)", "description": "Creates a  table view  from an input  table  or  feature class . The table view that is created by the tool is temporary and will not persist after the session ends unless the document is saved.", "example": {"title": "MakeTableView example 1 (Python window)", "description": "The following Python window script demonstrates how to use the MakeTableView function in immediate mode.", "code": "import arcpy arcpy.MakeTableView_management ( \"C:/data/input/crimefrequency.dbf\" , \"crimefreq_tview\" )"}, "usage": ["This tool is commonly used to create a table view with a selected set of attributes or fields.", "ArcCatalog does not display these table views, but they can be used as inputs to other geoprocessing tools in the current ArcGIS session. Once the ArcGIS application is exited, the table views are deleted.", "Table views created in ArcCatalog cannot be used in ArcMap.", " If an SQL expression is used but returns nothing, the output will be empty.", "Field names can be given a new name by using the ", "Field Info", " control. The second column on the control lists the existing field names from the input. To rename a field, click the field name and type in a new one.", "New field names defined in the ", "Field Info", " control will be honored in subsequent tools. However, if this tool is the last tool in a model, the field names will be obtained from the source data on disk. To maintain new field names, the new layer has to be written out to a new data using ", "Copy Rows", " or ", "Copy Features", " tools.", "The field names will be validated by specifying an input workspace. Thus, if the input is a geodatabase feature class, and the output workspace is a folder, the field names may be truncated, since shapefile attributes can only have names of ten characters or less. The new names may be reviewed and altered using the ", "Field Info", " control.", "A subset of fields can be made unavailable in the new layer by using the ", "Field Info", " control's visible property. The third column in the control provides a dropdown option to specify whether a field will be visible or hidden in the new layer. The default is TRUE. Selecting FALSE will hide that field. You cannot use the hidden fields in a workflow if the newly created layer is input to a subsequent process or tool.  If the output is saved to disk, only the fields listed as visible will appear in the new data.", "The split policy option on the ", "Field Info", " control does not apply to this tool."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input table or feature class. ", "dataType": "Table View;Raster Layer"}, {"name": "out_view", "isOutputFile": true, "isOptional": false, "description": "The name of the table view to be created. ", "dataType": "Table View ;Raster Layer"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of features. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, or coverages, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}, {"name": "workspace", "isOptional": true, "description": "The input workspace used to validate the field names. If the input is a geodatabase table and the output workspace is a dBASE table, the field names may be truncated, since dBASE fields can only have names of ten characters or less. The new names may be reviewed and altered using the field information control. ", "dataType": "Workspace"}, {"name": "field_info", "isOptional": true, "description": "Specifies which fields from the input table to rename and make visible in the output table view. ", "dataType": "Field Info"}]},
{"syntax": "MakeRasterLayer_management (in_raster, out_rasterlayer, {where_clause}, {envelope}, {band_index})", "name": "Make Raster Layer (Data Management)", "description": "Creates a raster layer from an input raster dataset or layer file. The layer that is created by the tool is temporary and will not persist after the session ends unless the layer is saved to disk or the map document is saved. This tool can be used to make a temporary layer, so you can work with a specified subset of bands within a raster dataset.", "example": {"title": "MakeRasterLayer example 1 (Python window)", "description": "This is a Python sample for MakeRasterLayer.", "code": "import arcpy arcpy.MakeRasterLayer_management ( \"c:/workspace/image.tif\" , \"rdlayer\" , \"#\" , \"feature.shp\" , \"1\" )"}, "usage": ["To make your layer permanent, right-click the layer in the table of contents and click ", "Save As Layer File", ", or use the ", "Save To Layer File tool", "."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The path and name of the input raster dataset. ", "dataType": "Composite Geodataset"}, {"name": "out_rasterlayer", "isOutputFile": true, "isOptional": false, "description": "The name of the temporary output raster dataset. ", "dataType": "Raster Layer"}, {"name": "where_clause", "isOptional": true, "description": "A query statement using the fields and values of the raster dataset. To add this variable, press F8 , or right-click and click Insert Variable . ", "dataType": "SQL Expression"}, {"name": "envelope", "isOptional": true, "description": "Using the min x, min y, max x, or max y, you can specify the extents of the raster layer. To add this variable, press F8 , or right-click and click Insert Variable . ", "dataType": "Extent"}, {"name": "band_index", "isOptional": false, "description": "Choose which bands to export for the layer. If no bands are specified, then all the bands will be used in the output. ", "dataType": "Value Table"}]},
{"syntax": "MakeRasterCatalogLayer_management (in_raster_catalog, layer_name, {where_clause}, {workspace}, {field_info})", "name": "Make Raster Catalog Layer (Data Management)", "description": "Creates a raster catalog layer from an input raster catalog. The layer that is created by the tool is temporary and will not persist after the session ends unless the layer is saved to disk or the map document is saved.", "example": {"title": "MakeRasterCatalogLayer example 1 (Python window)", "description": "This is a Python sample for MakeRasterCatalogLayer.", "code": "import arcpy arcpy.MakeRasterCatalogLayer_management ( \"c:/data/fgdb.gdb/catalog\" , \"catlayer\" , \"OBJECTID<3\" , \"SDE94.sde\" , \"#\" )"}, "usage": ["To make your layer permanent, right-click the layer in the ", "ArcMap", " table of contents and click ", "Save As Layer File", ", or use the ", "Save To Layer File tool", "."], "parameters": [{"name": "in_raster_catalog", "isInputFile": true, "isOptional": false, "description": "The raster catalog containing one or more raster catalog items (raster datasets). ", "dataType": "Raster Catalog Layer"}, {"name": "layer_name", "isOptional": false, "description": "Name of the temporary raster catalog layer. ", "dataType": "Raster Catalog Layer"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of raster catalog items. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD]. ", "dataType": "SQL Expression"}, {"name": "workspace", "isOptional": true, "description": "The input workspace used to validate the field names. If the input is from a file or personal geodatabase and the output workspace is an ArcSDE geodatabase, the field names may be truncated, since some database fields can only have names with ten characters or less. The new names may be reviewed and altered using the Field Info parameter. ", "dataType": "Workspace"}, {"name": "field_info", "isOptional": true, "description": "Specifies which fields from the input table to rename and make visible in the output table view. ", "dataType": "Field Info"}]},
{"syntax": "MakeQueryTable_management (in_table, out_table, in_key_field_option, {in_key_field}, {in_field}, {where_clause})", "name": "Make Query Table (Data Management)", "description": "This  tool applies an SQL  query  to a database and the results are represented in a  layer  or  table view . The query can be used to join several tables or return a subset of columns or rows from the original data in the database. This tool accepts data from an ArcSDE geodatabase, a file geodatabase, a personal geodatabase, or an OLE DB connection.", "example": {"title": "MakeQueryTable Example (Python Window)", "description": "The following Python window script demonstrates how to use the MakeQueryTable function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.MakeQueryTable_management ([ \"Counties\" , \"codemog\" ], \"queryout\" , \"ADD_VIRTUAL_KEY_FIELD\" , \"\" , [[ \"Counties.OBJECTID\" , 'ObjectID' ],[ \"Counties.NAME\" , 'Name' ], [ \"codemog.Males\" , 'Males' ], [ \"codemog.Females\" , 'Females' ]], \"Counties.FIPS = codemog.Fips and Counties.STATE_NAME = 'California'\" )"}, "usage": ["The layer that is created by the tool is temporary and will not persist after the session ends unless the document is saved.", "All input feature classes or tables must be from the same input ", "workspace", ".", "If a Shape column is added to the field list, the result is a layer; otherwise, it is a table view.", "If the output result is a layer, it may be persisted to a layer file using the ", "Save To Layer File", " tool or to a ", "feature class", " using the ", "Copy Features", " tool.", "The order of the fields in the field list indicates the order the fields will appear in the output layer or table view.", "The tool allows you to provide a key field option and key fields list. This information defines how rows are uniquely identified and is used to add a dynamically generated ObjectID column to the data. Without an ObjectID column, selections will not be supported.", "The key fields list lets you choose several columns if the combination of these columns is needed to define unique values.", "If an SQL expression is used but returns no matching records, the output feature class will be empty.", "Feature classes can be joined, but the fields list must contain at most one field of type geometry. If you add more than one geometry column, the tool will display an error when you click ", "OK", " and execution will stop.", "For details on the syntax for the ", "Expression", " parameter, see ", "Building an SQL Expression", " or ", "SQL Reference", ".", "The ", "Fields", " and ", "Key Fields", " parameters' ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Fields", " and ", "Key Fields", " parameters may not be populated with field names. The Add Field button allows you to add expected field(s) so you can complete the Make Query Table dialog box and continue to build your model.", "When input tables are from a file geodatabase, tables generally join in the order listed in the Input Tables parameter. For example, if Table1 is listed before Table2, Table2 will be joined by getting a row from Table1, then getting matching rows from Table2. However, if this would result in querying Table2 on an nonindexed field, and reversing the order would result in querying Table1 on an indexed field, the order will be reversed in an attempt to maximize performance. This is the sole query optimization logic at work when you're using file geodatabase data with this tool. In general, joins in file geodatabases perform best when they are one-to-many and one-to-one."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The name of the table or tables to be used in the query. If several tables are listed, the Expression parameter can be used to define how they are to be joined. The input table can be from an ArcSDE geodatabase, a file geodatabase, a personal geodatabase, or an OLE DB connection. ", "dataType": "Table View; Raster Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The name of the layer or table view that will be created by the tool. ", "dataType": "Table View;Raster Layer"}, {"name": "in_key_field_option", "isInputFile": true, "isOptional": false, "description": "Indicates how an ObjectID field will be generated, if at all, for the query. The default is USE_KEY_FIELDS. USE_KEY_FIELDS \u2014 This indicates that the fields chosen in the key fields list should be used to define the dynamic ObjectID column. If there are no fields chosen in the key fields list, the ADD VIRTUAL_KEY_FIELD option is automatically applied. ADD_VIRTUAL_KEY_FIELD \u2014 This option indicates that no key fields have been chosen, but a dynamic ObjectID column is to be generated. This is done by copying the data to a local, system-managed workspace and adding a field with unique values to the copy. The layer or table view can then access the copy and use the added field as the key field. NO_KEY_FIELD \u2014 This option indicates that no dynamic ObjectID column is to be generated. Choosing this option means that selections will not be supported for the table view. If there is already a column of type ObjectID in the fields list, it will be used as the ObjectID even if this option is chosen. ", "dataType": "String"}, {"name": "in_key_field", "isInputFile": true, "isOptional": false, "description": "Specifies a field or combination of fields that can be used to uniquely identify a row in the query. This parameter is used only when the USE_KEY_FIELDS option is set. The Add Field button, which is used only in ModelBuilder, allows you to add expected field(s) so you can complete the dialog and continue to build your model. ", "dataType": "Field"}, {"name": "in_field", "isInputFile": true, "isOptional": false, "description": "The fields to include in the layer or table view. If an alias is set for a field, this is the name that appears. If no fields are specified, all fields from all tables are included. ", "dataType": "Value Table"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of records. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, coverages, or dBASE or INFO tables, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}]},
{"syntax": "MakeImageServerLayer_management (in_image_service, out_imageserver_layer, {template}, {band_index}, {mosaic_method}, {order_field}, {order_base_value}, {lock_rasterid}, {cell_size})", "name": "Make Image Server Layer (Data Management)", "description": "Creates a temporary raster layer from an image service. The layer that is created will not persist after the session ends unless the document is saved. The input can also be a URL to an image server.", "example": {"title": "MakeImageServerLayer example 1 (Python window)", "description": "This is a Python sample for the MakeImageServerLayer tool.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Workspace\" input1 = \"GIS Servers/server3 (admin)/File_TIFF_Amberg.ImageServer\" arcpy.MakeImageServerLayer_management ( input1 , \"islayer\" , \"4488961 5478909 4489227 5479255\" , \"4;6\" , \"#\" , \"#\" , \"#\" , \"#\" )"}, "usage": ["Use this tool to create an image layer from an image service, or a URL that references an image service.", "The output can be the entire image service or a portion of it.", "Use this tool to add an image service to a Python script or model, or when creating a geoprocessing service.", "You can clip out a portion of the image service by choosing an output extent layer or by specifying the rectangle extent. If you choose an output extent layer, the clip extent will be based upon the extent of that layer.", "The output can be created with only a subset of the bands. This will help save on time and disk space.", "The mosaicking options are only available when the image service it contains is generated from a mosaic definition or image service definition."], "parameters": [{"name": "in_image_service", "isInputFile": true, "isOptional": false, "description": "The name of the input image service, or the URL that references the image service. An example of a URL is: http://AGSServer:8399/arcgis/services/ISName/ImageServer ", "dataType": "Image Service; String"}, {"name": "out_imageserver_layer", "isOutputFile": true, "isOptional": false, "description": "The name of the output image layer. ", "dataType": "Raster Layer"}, {"name": "template", "isOptional": true, "description": "The output extent of the image layer. The output extent can either be specified by defining the area to be clipped (X-Minimum, Y-Minimum, X-Maximum, and Y-Maximum) or by using the extent of an existing layer. ", "dataType": "Extent"}, {"name": "band_index", "isOptional": false, "description": "Choose which bands to export for the layer. If no bands are specified, then all the bands will be used in the output. ", "dataType": "Value Table"}, {"name": "mosaic_method", "isOptional": true, "description": "The mosaic method defines how the mosaic is created from different rasters. An image returned to the client can be created from a number of input rasters. The mosaic process has two default options available. Closest_To_Center\u2014Sorts rasters based on a default order where rasters that have their center closest to the view center are placed on top. North_West\u2014Sorts rasters based on an order where rasters that have their center closest to the northwest is placed on top. Lock_Raster\u2014Lock the display of single or multiple rasters based on an ID or name. By_Attribute\u2014Sorts rasters based on an attribute field and its difference from the base value. Closest_To_Nadir\u2014Sorts rasters based on an order where rasters that have their nadir position closest to the view center are placed on top. The nadir point can be different from the center point, especially in oblique imagery. Closest_To_Viewpoint\u2014Sorts rasters based on an order where the nadir position closest to the user-defined view point location is on top. Seamline\u2014Cuts the raster using a predefined seamline shape for each raster using optional feathering along the seams. The ordering is predefined during the seamline generation.", "dataType": "String"}, {"name": "order_field", "isOptional": true, "description": " The default field to use to order the rasters when the mosaic method is By_Attribute. The list of fields is defined as those in the service table that are of type metadata and are integer (for example, the values can represent dates or cloud cover percentage). ", "dataType": "String"}, {"name": "order_base_value", "isOptional": true, "description": "The images are sorted based on the difference between this input value and the attribute value in the specified field. ", "dataType": "String"}, {"name": "lock_rasterid", "isOptional": true, "description": "Raster ID or raster name to which the service should be locked, such that only the specified rasters are displayed. If left blank (undefined), it will be similar to the system default. Multiple IDs can be defined as a semicolon-delimited list. ", "dataType": "String"}, {"name": "cell_size", "isOptional": true, "description": " The cell size for the output image service layer. ", "dataType": "Double"}]},
{"syntax": "MakeFeatureLayer_management (in_features, out_layer, {where_clause}, {workspace}, {field_info})", "name": "Make Feature Layer (Data Management)", "description": "Creates a  feature layer  from an input  feature class  or  layer  file. The layer that is created by the tool is temporary and will not persist after the session ends unless the layer is saved to disk or the map  document is saved.", "example": {"title": "MakeFeatureLayer Example 1 (Python Window)", "description": "The following Python window script demonstrates how to use the MakeFeatureLayer function in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/input\" arcpy.MakeFeatureLayer_management ( \"parcels.shp\" , \"parcels_lyr\" )"}, "usage": ["The temporary feature layer can be saved as a layer file using the ", "Save_To_Layer_File", " tool or can be saved as a new feature class using the ", "Copy_Features", " tool.", "Complex feature classes, such as annotation and dimensions, are not supported by this tool.", "Layers created in ArcCatalog cannot be used in ArcMap unless they are saved to a layer file using the ", "Save_To_Layer_File", " tool.", " If an SQL expression is used but returns nothing, the output will be empty.", "Field names can be given a new name by using the ", "Field Info", " control. The second column on the control lists the existing field names from the input. To rename a field, click the field name and type in a new one.", "New field names defined in the ", "Field Info", " control will be honored in subsequent tools. However, if this tool is the last tool in a model, the field names will be obtained from the source data on disk. To maintain new field names, the new layer has to be written out to a new data using ", "Copy Rows", " or ", "Copy Features", " tools.", "The field names will be validated by specifying an input workspace. Thus, if the input is a geodatabase feature class, and the output workspace is a folder, the field names may be truncated, since shapefile attributes can only have names of ten characters or less. The new names may be reviewed and altered using the ", "Field Info", " control.", "A subset of fields can be made unavailable in the new layer by using the ", "Field Info", " control's visible property. The third column in the control provides a dropdown option to specify whether a field will be visible or hidden in the new layer. The default is TRUE. Selecting FALSE will hide that field. You cannot use the hidden fields in a workflow if the newly created layer is input to a subsequent process or tool.  If the output is saved to disk, only the fields listed as visible will appear in the new data.", "A split policy can be set by using the ", "Field Info", " control ", "Use Ratio Policy", " option. The split policy comes into effect any time the feature layer is being used as an input to a tool and a geometry of the input feature layer is split during processing. When the split geometry is sent to the output, a ratio of the input attribute value is calculated for the output attribute value. When ", "Use Ratio Policy", " is enabled, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value is based on the ratio in which the input feature geometry was divided. For example, if the input geometry was divided equally, each new feature's attribute value is assigned one-half of the value of the input feature's attribute value. Use Ratio Policy only applies to numeric field types.", "The default is NONE (unchecked). This means the attribute of the two resulting features takes on a copy of the original object's attribute value.", "Geoprocessing tools do not honor geodatabase feature class or table field split policies.", "When using ModelBuilder to create a tool, you need to ensure that the input data variable to this tool is not flagged as intermediate. If the input is flagged as intermediate, it will be deleted after the model is run from its dialog and the output layer will not be added to the display."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer used the make the new layer. Complex feature classes, such as annotation and dimensions, are not valid inputs to this tool. ", "dataType": "Feature Layer"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": "The name of the feature layer to be created. The newly created layer can be used as input to any geoprocessing tool that accepts a feature layer as input. ", "dataType": "Feature Layer"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of features. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, or coverages, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in single quotes, then escape the interior single quotes like this: ' \"CITY_NAME\" = \\'Chicago\\' ' Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}, {"name": "workspace", "isOptional": true, "description": "The input workspace used to validate the field names. If the input is a geodatabase table and the output workspace is a dBASE table, the field names may be truncated, since dBASE fields can only have names of ten characters or less. The new names may be reviewed and altered using the field information control. ", "dataType": "Workspace;Feature Dataset"}, {"name": "field_info", "isOptional": true, "description": "Can be used to review and alter the field names and hide a subset of fields in the output layer. A split policy can be specified. See the usages for more information. ", "dataType": "Field Info"}]},
{"syntax": "ApplySymbologyFromLayer_management (in_layer, in_symbology_layer)", "name": "Apply Symbology From Layer (Data Management)", "description": "This tool applies the  symbology  from a layer to the Input Layer. It can be applied to  feature ,  raster ,  network analysis ,   TIN , and geostatistical   layer files or layers in the ArcMap table of contents. This tool is primarily for use in scripts or ModelBuilder.", "example": {"title": "ApplySymbologyFromLayer Example 1 (Python Window)", "description": "The following Python window script demonstrates how to use the ApplySymbologyFromLayer function in immediate mode.", "code": "import arcpy arcpy.ApplySymbologyFromLayer_management ( \"sf_points\" , \"sf_points_water.lyr\" )"}, "usage": ["In model or script use, the ", "Symbology Layer", " most often comes from a layer file. The ", "Make Feature Layer", " tool or the ", "Make Raster Layer", " tool can be used to create a layer to apply symbology. Feature, raster, and TIN layers can also be created by right-clicking the layer in the ArcMap table of contents and clicking ", "Save As Layer File", ". The layer is then saved in the desired location.", "The ", "Symbology Layer", " must match the data type of the ", "Input Layer", "; for example, a feature layer cannot be applied to a raster layer and vice versa.", "The ", "Symbology Layer", " can only be applied to features of the same geometry; for example, a point layer cannot be applied to a polygon layer.", "The field in the ", "Input Layer", " that will be displayed must have the same name as that of the corresponding ", "Symbology Layer ", "field. If this field is missing, the output data is drawn with default symbology.", "3D properties such as extrusion and offset are not supported with this tool.", "Symbology methods can be dynamic; for example, the symbology is updated to reflect the characteristics of the ", "Input Layer", " as shown in the illustration below. In this example, the five class Natural Breaks classification method from the Symbology Layer is applied to the Input Layer and the range values are updated to reflect the Shape_Area values of the Input Layer.", "The following methods are dynamic:"], "parameters": [{"name": "in_layer", "isInputFile": true, "isOptional": false, "description": "The layer to which the symbology will be applied. ", "dataType": "Feature Layer;Raster Layer; TIN Layer;Network Analysis Layer;Geostatistical Layer"}, {"name": "in_symbology_layer", "isInputFile": true, "isOptional": false, "description": "The symbology of this layer is applied to the Input Layer. ", "dataType": "Feature Layer; Raster Layer;TIN Layer; Network Analysis Layer;Geostatistical Layer"}]},
{"syntax": "RemoveJoin_management (in_layer_or_view, {join_name})", "name": "Remove Join (Data Management)", "description": "Removes a  join  from a  feature layer  or  table view .", "example": {"title": "Remove Join Example (Python Window)", "description": "The following Python window script demonstrates how to use the RemoveJoin tool in immediate mode in ArcMap on a feature layer in the TOC named veglayer.", "code": "arcpy.RemoveJoin_management ( \"veglayer\" , \"vegtable\" )"}, "usage": ["The ", "Join", " parameter is the name of the table that was joined to the input layer or table view.", "When a layer is joined to two tables and the first join is removed, then both joins will be removed. For example, ", "Layer1", " is joined to ", "TableA", ". Then ", "Layer1", " is joined to ", "TableB", ". If the join to ", "TableA", " is removed, the join to ", "TableB", " is also removed.", "This tool is not limited to working within ArcMap; it works on layers and table views in other ArcGIS applications and in scripts. The ", "Make Feature Layer", " tool makes a layer from a feature class, and the ", "Make Table View", " tool creates a table view from an input table or feature class. The layer or table view can then be used as the input to the ", "Add Join", " and ", "Remove Join", " tools."], "parameters": [{"name": "in_layer_or_view", "isInputFile": true, "isOptional": false, "description": "The layer or table view from which the join will be removed. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "join_name", "isOptional": true, "description": "The join to be removed. ", "dataType": "String"}]},
{"syntax": "JoinField_management (in_data, in_field, join_table, join_field, {fields})", "name": "Join Field (Data Management)", "description": "Joins  the contents of a table to another table based on a common attribute field. The input table is updated to contain the fields from the join table. You can select which fields from the join table will be added to the input table. The records in the  Input Table  are matched to the records in the  Join Table  based on the values of  Input Join Field  and the  Output Join Field . Optionally, only desired fields can be selected from the  Join Table  and appended to the  Input Table  during the join.", "example": {"title": "JoinField example 1 (Python window)", "description": "The following Python window script demonstrates how to use the JoinField function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.JoinField_management ( \"zion_park\" , \"zonecode\" , \"zion_zoning\" , \"zonecode\" , [ \"land_use\" , \"land_cover\" ])"}, "usage": ["The ", "Input Table", " can be a feature class (including shapefile) or a table.", "All fields in the", " Input Table", " will be kept during the join. Optionally, only selected fields from the ", "Join Table", " will be added to the output. These can be checked under the ", "Join Fields", " parameter.", "Records from the ", "Join Table", " can be matched to more than one record in the ", "Input Table", ". For more information on one-to-one, many-to-one, one-to-many, and many-to-many  joins, see ", "About joining and relating tables", ".", "If no fields are selected for the optional ", "Join Fields", " parameter, all fields from the ", "Join Table", " to the output will be joined.", "Joins can be based on fields of types text, date, or number.", "Joins based on text fields are case-sensitive.", "Fields of different number ", "formats", " can be joined as long as the values are equal. For example, a field of type float can be joined to a short integer field.", "The ", "Input Join Field", " and the", " Output Join Field", " can have different names.", "If a join field has the same name as a field from the input table, the joined field will be appended by _1 (or _2, or _3, and so on) to make it unique.", "If values in the ", "Output Join Field", " are not unique, only the first occurrence of each value will be used."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The table or feature class to which the join table will be joined. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "in_field", "isInputFile": true, "isOptional": false, "description": "The field in the input table on which the join will be based. ", "dataType": "Field"}, {"name": "join_table", "isOptional": false, "description": "The table to be joined to the input table. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "join_field", "isOptional": false, "description": "The field in the join table that contains the values on which the join will be based. ", "dataType": "Field"}, {"name": "fields", "isOptional": false, "description": "The fields from the join table to be included in the join. ", "dataType": "Field"}]},
{"syntax": "AddJoin_management (in_layer_or_view, in_field, join_table, join_field, {join_type})", "name": "Add Join (Data Management)", "description": "Joins  a layer to another layer or table (where layer is a feature layer, table view, or raster layer with a raster attribute table) based on a common field. The records in the  Join Table  are matched to the records in the input  Layer Name . A match is made when the input join field and output join field values are equal. This join is temporary.", "example": {"title": "AddJoin example 1 (Python window)", "description": "The following Python window script demonstrates how to use the AddJoin function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.MakeFeatureLayer_management ( \"Habitat_Analysis.gdb/vegtype\" , \"veg_layer\" ) arcpy.AddJoin_management ( \"veg_layer\" , \"HOLLAND95\" , \"vegtable.dbf\" , \"HOLLAND95\" ) arcpy.CopyFeatures_management ( \"veg_layer\" , \"Habitat_Analysis.gdb/vegjoin\" )"}, "usage": ["The input must be a ", "feature layer", ",  a ", "table view", ", or a ", "raster layer", " that has an attribute table; it cannot be a ", "feature class", " or ", "table", ".", "Records from the ", "Join Table", " can be matched to more than one record in the input layer or table view. For more information on one-to-one, many-to-one, one-to-many, and many-to-many  joins,  see ", "About joining and relating tables", ".", "When joining tables, the default option is to keep all records. If a record in the target table doesn't have a match in the join table, that record is given null values for all the fields being appended into the target table from the join table.", "With the keep only matching records option, if a record in the target table doesn't have a match in the join table, that record is removed from the resultant target table. If the target table is the attribute table of a layer, features that don't have data joined to them are not shown on the map.", "The ", "Join Table", " can be any of the following types of tables: a geodatabase table (ArcSDE, file, or personal), a dBASE file, an INFO table, or an OLE DB table.", "The input layer or table view must have an ObjectID field. The ", "Join Table", " is not required to contain an ObjectID field.", "Field properties, such as aliases, visibility, and number formatting, are maintained when a join is added or removed.", "If a join with the same table name already exists\u2014for example, if the layer A is joined to a table B\u2014running the tool again to join table B will result in a warning that the join already exists.", "The join lasts only for the duration of the session. To persist the join for use in another session, save the layer to a layer file using the ", "Save Layer To File", " tool.  This only applies to layers; table views cannot be saved in this manner.", "In the resulting layer or table view, the fields from the input layer or table view will be prefixed with the input's name and a period (.), and all fields from the join table will be prefixed with the join table name plus a period as the default.", "To make a permanent join, consider using the ", "Join Field", " tool.  Another way to make the join permanent is to save the joined feature layer to a new feature class or the joined table view to a new table. When saving results to a new feature class or table, you can use the ", "Qualified Field Names", " environment to control if the joined output field names will be qualified with the name of the table the field came from.", "Indexing the fields in the input layer or table view and ", "Join Table", " on which the join will be based can improve performance. This can be done with the ", "Add Attribute Index", " tool or by right-clicking the input in ArcCatalog and using the dialog box to add an index to the desired field. ", "Learn more about performance tips for joining data", "If the input layer or table view's fields were modified (renamed or hidden) using the ", "Make Feature Layer", " or ", "Make Table View", " tool ", "Field Info", " parameter, these field modifications will not be carried forward to the output joined layer or table view.", "The ", "Join Table", " name cannot start with a numeric value. ", "Learn about reasons why joining tables may fail", " "], "parameters": [{"name": "in_layer_or_view", "isInputFile": true, "isOptional": false, "description": "The layer or table view to which the join table will be joined. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "in_field", "isInputFile": true, "isOptional": false, "description": "The field in the input layer or table view on which the join will be based. ", "dataType": "Field"}, {"name": "join_table", "isOptional": false, "description": "The table or table view to be joined to the input layer or table view. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "join_field", "isOptional": false, "description": "The field in the join table that contains the values on which the join will be based. ", "dataType": "Field"}, {"name": "join_type", "isOptional": true, "description": "Specifies what will be done with records in the input that match a record in the join table. KEEP_ALL \u2014 All records in the input layer or table view will be included in the output\u2014also known as an outer join. This is the default. KEEP_COMMON \u2014 Only those records in the input that match to a row in the join table will be present in the result\u2014also known as an inner join. ", "dataType": "Boolean"}]},
{"syntax": "RemoveSpatialIndex_management (in_features)", "name": "Remove Spatial Index (Data Management)", "description": "Deletes the spatial index from a shapefile, file geodatabase feature class, or an ArcSDE feature class. The spatial index cannot be deleted from a personal geodatabase feature class. Spatial indexes are used by ArcGIS to quickly locate features that match a spatial query. ", "example": {"title": "RemoveSpatialIndex Example (Python Window)", "description": "The following Python Window script demonstrates how to use the RemoveSpatialIndex function in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"Database Connections/Connection to esoracle.sde\" arcpy.RemoveSpatialIndex_management ( \"LPI.Land/LPI.PLSSFirstDivision\" )"}, "usage": [], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The features from which the spatial index will be removed. ", "dataType": "Feature Layer; Mosaic Layer; Raster Catalog Layer"}]},
{"syntax": "RemoveIndex_management (in_table, index_name)", "name": "Remove Attribute Index (Data Management)", "description": "This tool deletes an attribute index from an existing table, feature class, shapefile, coverage, or attributed relationship class. Attribute indexes are used by ArcGIS to quickly locate records that match an attribute query. ", "example": {"title": "Remove Index Python Example (Python Window)", "description": "The following code demonstrates how to use RemoveIndex function in a Python interactive window.", "code": "import arcpy arcpy.env.workspace = \"C:/data/input/indices.gdb\" arcpy.RemoveIndex_management ( \"lakes\" , [ \"IndexA\" , \"IndexB\" ])"}, "usage": ["This tool accepts as input coverage feature classes, shapefiles, file or personal and SDE geodatabase feature classes, and attribute relationship classes.", "If the ", "Index Name", " parameter is empty, there are no attribute  indexes in the dataset.", "Only feature classes from a file or personal geodatabase or SDE database support more than one attribute index. Therefore, the index list can contain only one index to delete unless the input table is from a file or personal geodatabase or SDE database.", "Once an index has been added, it can be deleted and re-added at any point in the lifetime of the feature class or table.", "The ", "Index Name or Indexed Item", " parameter's ", "Add Value", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Index Name or Indexed Item", " parameter may not be populated with values. The ", "Add Value", " button allows you to add expected value(s) so you can complete the Remove Attribute Index dialog box and continue to build your model.", "Learn more about creating attribute indexes", "."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table containing the index or indexes to be deleted. Table can refer to an actual table, a feature class attribute table, or an attributed relationship class. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "index_name", "isOptional": false, "description": "The name of the index or indexes to be deleted. ", "dataType": "String"}]},
{"syntax": "AddSpatialIndex_management (in_features, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})", "name": "Add Spatial Index (Data Management)", "description": "Adds a spatial index to a shapefile, file geodatabase, or ArcSDE feature class.   Use this tool to either add a spatial index to a shapefile or feature class that does not already have one or to rebuild an existing spatial index.   ", "example": {"title": "AddSpatialIndex Example (Python Window)", "description": "The following Python Window script demonstrates how to use the AddSpatialIndex function in immediate mode.", "code": "import arcpy import arcpy.env as ENV ENV.workspace = \"Database Connections/Connection to esoracle.sde\" arcpy.AddSpatialIndex_management ( \"LPI.Land/LPI.PLSSFirstDivision\" , 500 )"}, "usage": ["ArcGIS uses spatial indexes to quickly locate features in feature classes. Identifying a feature, selecting features by pointing or dragging a box, and panning and zooming all require ArcMap to use the spatial index to locate features.  The spatial index is defined by using a grid-based system  that spans the extent of the features in a feature class, like a locator grid you might find on a common road map. ", "By default, ArcGIS creates and maintains a spatial index for geodatabase feature classes.    For a geodatabase feature class to not have a spatial index, you must  explicitly remove it using the ", "Remove_Spatial_Index", "  tool.", "The ", "Spatial Grid 1", ", ", "2", ", and ", "3", " parameters  only apply to file geodatabases and certain ArcSDE geodatabase feature classes (those that use SQL Server binary storage, DB2, Oracle binary storage, or Oracle ST_Geometry).", "If the ", "Input Features", " already have a spatial index, ", "Spatial Grid 1, 2, 3", " displays the current spatial index grid values.  If you enter 0,0,0 for the spatial grid parameters and execute this tool, the optimal grid size is calculated, and the index rebuilt.  You can then view the calculated grid size by reopening this tool and reentering the feature class or layer (you would then click ", "Cancel", ", since you do not need to rebuild the index).  You can also use the ", "Calculate Default Spatial Grid Index", " tool to calculate optimal grid sizes.", "Adding a new spatial index to an ArcSDE feature class is a server-intensive operation. It should not be done on large feature classes when a large number of users are logged in to the server."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "ArcSDE feature class, file geodatabase feature class, or shapefile to which a spatial index is to be added or rebuilt. ", "dataType": "Feature Layer; Mosaic Layer; Raster Catalog Layer"}, {"name": "spatial_grid_1", "isOptional": true, "description": "The Spatial Grid 1, 2, and 3 parameters apply only to file geodatabase and certain ArcSDE geodatabase feature classes. If you are unfamiliar with setting grid sizes, leave these options as 0,0,0, and ArcGIS will compute optimal sizes for you. ", "dataType": "Double"}, {"name": "spatial_grid_2", "isOptional": true, "description": " Cell size of the second spatial grid. Leave the size at 0 if you only want one grid. Otherwise, set the size to at least three times larger than Spatial Grid 1. ", "dataType": "Double"}, {"name": "spatial_grid_3", "isOptional": true, "description": " Cell size of the third spatial grid. Leave the size at 0 if you only want two grids. Otherwise, set the size to at least three times larger than Spatial Grid 2. ", "dataType": "Double"}]},
{"syntax": "AddIndex_management (in_table, fields, {index_name}, {unique}, {ascending})", "name": "Add Attribute Index (Data Management)", "description": "Adds an attribute index to an existing table, feature class, shapefile, coverage, or attributed relationship class. Attribute indexes are used by ArcGIS to quickly locate records that match an attribute query. For information on attribute indexes in geodatabases, see  Creating attribute indexes .", "example": {"title": "AddIndex example 1 (stand-alone script)", "description": "The following stand-alone script demonstrates how to create an attribute index for specified fields.", "code": "# Name: AddAttIndex.py # Description: Create an attribute Index for specified fields # Import system modules import arcpy # Set a default workspace arcpy.env.workspace = \"c:/data\" try : # Create an attribute index for the few fields listed in command. arcpy.AddIndex_management ( \"counties.shp\" , \"NAME;STATE_FIPS;CNTY_FIPS\" , \"#\" , \"NON_UNIQUE\" , \"NON_ASCENDING\" ) arcpy.AddIndex_management ( \"mexico.mdb/land/lakes\" , \"NAME;geocompID\" , \"NGIndex\" , \"UNIQUE\" , \"ASCENDING\" ) except Exception , e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"Line  %i \" % tb.tb_lineno print e.message"}, "usage": ["Shapefiles don't support multiple indexes, so additional fields will become part of a composite index (that is, an index created on multiple fields in a table).", "ArcSDE and file or personal geodatabases have a new index added for each unique index name. If an index name already exists, it must be dropped before it can be updated.", "The ", "Fields To Index", " parameter's Add Field button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Fields To Index", " parameter may not be populated with field names. The Add Field button allows you to add expected fields so you can complete the Add Attribute Index dialog box and continue to build your model."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table containing the fields to be indexed. ", "dataType": "Mosaic Layer; Raster Catalog Layer; Raster Layer; Table View"}, {"name": "fields", "isOptional": false, "description": "The list of fields that will participate in the index. Any number of fields can be specified. ", "dataType": "Field"}, {"name": "index_name", "isOptional": true, "description": "The name of the new index. An index name is necessary when adding an index to geodatabase feature classes and tables. For other types of input, the Index Name is ignored ", "dataType": "String"}, {"name": "unique", "isOptional": true, "description": "Specifies whether the values in the index are unique. NON_UNIQUE \u2014 All values in the index are not unique. This is the default. UNIQUE \u2014 All values in the index are unique. ", "dataType": "Boolean"}, {"name": "ascending", "isOptional": true, "description": "Specifies whether values are indexed in ascending order. NON_ASCENDING \u2014 Values are not indexed in ascending order. This is the default. ASCENDING \u2014 Values are indexed in ascending order.", "dataType": "Boolean"}]},
{"syntax": "SaveGraph_management (in_graph, out_graph_file, {maintain_image_aspect}, {image_width}, {image_height})", "name": "Save Graph (Data Management)", "description": " Saves a graph to an image, vector, or graph file.", "example": {"title": "SaveGraph example (stand-alone script)", "description": "Save a graph as a Windows Bitmap image.", "code": "# Name: SaveGraph_ExampleBMP.py # Description: Save a graph as an image # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"c:/temp\" #Set local variables graph_grf = \"us_pop_2003.grf\" outputFile = \"outgraph.bmp\" #Execute SaveGraph arcpy.SaveGraph_management ( graph_grf , outputFile , \"MAINTAIN_ASPECT_RATIO\" )"}, "usage": ["Image formats", "Vector formats", "Windows Bitmap (", ".bmp", ")", "Scalable Vector Graphics (", ".svg", ")", "GIF(", ".gif", ")", "Adobe Acrobat PDF (", ".pdf", ")", "JPEG (", ".jpg", ")", "Encapsulated PostsScript (", ".eps", ")", "Portable Network Graphic (", ".png", ")", "Enhanced Metafile (", ".emf", ") ", "Paintbrush (", ".pcx", ")", "Windows Metafile (", ".wmf", ")", "When saving the graph to the desired format, you need to specify the file extension in the output file name. For example, if you want to export a graph to a Windows Bitmap, you need to specify the output file name as ", "C:\\temp\\outGraph.bmp", "."], "parameters": [{"name": "in_graph", "isInputFile": true, "isOptional": false, "description": " Input graph name or location. You can input the location of an existing graph file (*.grf). However, when you are using the tool in an ArcGIS application (ArcMap, ArcGlobe, or ArcScene), you can either input a graph present in the ArcGIS document or you can input the location of an existing graph file (*.grf). ", "dataType": "File"}, {"name": "out_graph_file", "isOutputFile": true, "isOptional": false, "description": "The output image, vector, or graph file. The supported image and vector formats are: Windows Bitmap ( .bmp ) GIF( .gif ) JPEG ( .jpg ) Portable Network Graphic ( .png ) Paintbrush ( .pcx ) Scalable Vector Graphics ( .svg ) Adobe Acrobat PDF ( .pdf ) Encapsulated PostsScript ( .eps ) Enhanced Metafile ( .emf ) Windows Metafile ( .wmf )", "dataType": "File"}, {"name": "maintain_image_aspect", "isOptional": true, "description": " MAINTAIN_ASPECT_RATIO \u2014 The aspect ratio of the output image will be preserved. IGNORE_ASPECT_RATIO \u2014 The aspect ratio of the output image will not be preserved. ", "dataType": "Boolean"}, {"name": "image_width", "isOptional": true, "description": " The width of the output image in pixels. ", "dataType": "Integer"}, {"name": "image_height", "isOptional": true, "description": " The height of the output image in pixels. ", "dataType": "Integer"}]},
{"syntax": "MakeGraph_management (in_graph_template_source, in_datasets, out_graph_name)", "name": "Make Graph (Data Management)", "description": " Creates a graph as a visual output using a graph template or an existing graph.", "example": {"title": "MakeGraph example (stand-alone script)", "description": "Create a vertical bar graph using an existing graph or graph template.", "code": "# Name: MakeGraph_ExampleVerticalBar.py # Description: Creates a graph based on an existing graph or graph template # Author: ESRI # Import system modules import arcpy from arcpy import env # Set environment settings env.workspace = \"c:/data\" # Set local variables graph_grf = \"input_VerticalBar.grf\" # Execute MakeGraph to create a Vertical Bar graph arcpy.MakeGraph_management ( graph_grf , \"SERIES=bar:vertical \" + \"DATA=c:/data.gdb/DischargeLevels \" + \"X=Hours Y=Flow LABEL=Hours SORT=ASC;\" + \"GRAPH=general TITLE=Discharge FOOTER=Station;\" + \"LEGEND=general TITLE=Discharge;\" + \"AXIS=left TITLE=Discharge;AXIS=right;\" + \"AXIS=bottom TITLE=Hours;AXIS=top\" , \"outgraph_Discharge\" )"}, "usage": [], "parameters": [{"name": "in_graph_template_source", "isInputFile": true, "isOptional": false, "description": " The input graph template ( .tee ) or graph file ( .grf ). ", "dataType": "Graph ; File"}, {"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": " The input data for individual series in the graph. The input data varies based on the graph type. To facilitate populating the parameters used for creating the graph series from Python, you can use the Graph class . ", "dataType": "Graph Data Table"}, {"name": "out_graph_name", "isOutputFile": true, "isOptional": false, "description": " The name of the graph to be created. ", "dataType": "Graph"}]},
{"syntax": "EliminatePolygonPart_management (in_features, out_feature_class, {condition}, {part_area}, {part_area_percent}, {part_option})", "name": "Eliminate Polygon Part (Data Management)", "description": "Creates a new output feature class containing the features from the input polygons with some parts or holes of a specified size deleted.", "example": {"title": "EliminatePolygonPart Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Eliminate Polygon Part tool.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.EliminatePolygonPart_management ( \"buildings.shp\" , \"output.gdb/remaining_buildings\" , \"AREA\" , 10 )"}, "usage": ["Since polygon holes are considered parts of the polygon, they can be deleted or filled using this tool. If the hole area is smaller than the specified size, the hole will be eliminated and the space will be filled in the output. Any parts that are inside the deleted hole will also be eliminated in the output.", "Part size can be specified as an area, a percent, or a combination of both. Use the ", "Condition", " parameter to determine how the part size will be specified. The Condition parameter AREA_AND_PERCENT and AREA_OR_PERCENT options are used to eliminate parts using both the area and percent criteria. ", "Polygon part percent is calculated as a percentage of the feature's total outer area, including the area of any holes. For example, if a polygon with a hole has an area of 75 square meters, with the hole covering 25 square meters, the polygons total outer area is 100 square meters. To eliminate this hole, an area greater than 25 square meters or a percentage greater than 25% would need to be specified. If the input is a multipart polygon, a feature's outer area is the sum of the area covered by all polygon parts.", "For multipart polygons, the area of each part will be compared with the specified area. If an individual polygon part is smaller than the specified size, the part will be eliminated in the output.", "If all of a polygon feature's parts are smaller than the specified size, the largest part will be kept in the output while all other parts will be eliminated."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer whose features will be copied to the output feature class, with some parts or holes eliminated. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output polygon feature class containing the remaining parts. ", "dataType": "Feature Class"}, {"name": "condition", "isOptional": true, "description": " Specify how the parts to be eliminated will be determined. AREA \u2014 Parts with an area less than that specified will be eliminated. PERCENT \u2014 Parts with a percent of the total outer area less than that specified will be eliminated. AREA_AND_PERCENT \u2014 Parts with an area and percent less than that specified will be eliminated. Only if a polygon part meets both the area and percent criteria will it be deleted. AREA_OR_PERCENT \u2014 Parts with an area or percent less than that specified will be eliminated. If a polygon part meets either the area or percent criteria, it will be deleted. ", "dataType": "String"}, {"name": "part_area", "isOptional": true, "description": "Eliminate parts smaller than this area. ", "dataType": "Areal Unit"}, {"name": "part_area_percent", "isOptional": true, "description": "Eliminate parts smaller than this percentage of a feature's total outer area. ", "dataType": "Double"}, {"name": "part_option", "isOptional": true, "description": "Determines what parts can be eliminated. CONTAINED_ONLY \u2014 Only parts totally contained by other parts can be eliminated. This is the default. ANY \u2014 Any parts can be eliminated. ", "dataType": "Boolean"}]},
{"syntax": "Eliminate_management (in_features, out_feature_class, {selection}, {ex_where_clause}, {ex_features})", "name": "Eliminate (Data Management)", "description": "Eliminates polygons by merging them with neighboring polygons that have the largest area or the longest shared border.  Eliminate  is often used to remove small sliver polygons that are the result of overlay operations, such as  Intersect  or  Union .", "example": {"title": "Eliminate example (Python window)", "description": "The following Python window script demonstrates how to use the Eliminate tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Portland.gdb/Census\" arcpy.MakeFeatureLayer_management ( \"blockgrp\" , \"blocklayer\" ) arcpy.SelectLayerByAttribute_management ( \"blocklayer\" , \"NEW_SELECTION\" , '\"Area_Sq_Miles\" < 0.15' ) arcpy.Eliminate_management ( \"blocklayer\" , \"C:/output/output.gdb/eliminate_output\" , \"LENGTH\" , '\"OBJECTID\" = 9' )"}, "usage": ["Features to be eliminated are determined by a selected feature set applied to a polygon layer. The selected set must be determined in a previous step by using ", "Select Layer by Attribute", ", using ", "Select Layer by Location", ", or querying a map layer in ArcMap.", "The ", "Input Layer", " must include a selection; otherwise, Eliminate will fail.", "The ", "Exclusion Expression", " and ", "Exclusion Layer", " are not mutually exclusive and can be used together to give full control over what is eliminated."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The layer whose polygons will be merged into neighboring polygons. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to be created. ", "dataType": "Feature Class"}, {"name": "selection", "isOptional": true, "description": "These options specify which method will be used for eliminating features. LENGTH \u2014 Merges a selected polygon with a neighboring unselected polygon by dropping the shared border. The neighboring polygon is the one with the longest shared border. This is the default. AREA \u2014 Merges a selected polygon with a neighboring unselected polygon by dropping the shared border. The neighboring polygon is the one with the largest area. ", "dataType": "Boolean"}, {"name": "ex_where_clause", "isOptional": true, "description": "An expression used to identify input features that should not be eliminated. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, or coverages, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In Python, strings are enclosed in matching single or double quotes. To create a string that contains quotes (as is common with a WHERE clause in SQL expressions), you can escape the quotes (using a backslash) or triple quote the string. For example, if the intended WHERE clause is \"CITY_NAME\" = 'Chicago' you could enclose the entire string in double quotes, then escape the interior double quotes like this: \" \\\"CITY_NAME\\\" = 'Chicago' \" Or you could enclose the entire string in triple quotes without escaping: \"\"\" \"CITY_NAME\" = 'Chicago' \"\"\" For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}, {"name": "ex_features", "isOptional": true, "description": "An input polyline or polygon feature class or layer that defines polygon boundaries, or portions thereof, that should not be eliminated. ", "dataType": "Feature Layer"}]},
{"syntax": "Dissolve_management (in_features, out_feature_class, {dissolve_field}, {statistics_fields}, {multi_part}, {unsplit_lines})", "name": "Dissolve (Data Management)", "description": "Aggregates features based on specified attributes. \r\n Learn more about how Dissolve works \r\n", "example": {"title": "Dissolve example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Dissolve tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Portland.gdb/Taxlots\" arcpy.Dissolve_management ( \"taxlots\" , \"C:/output/output.gdb/taxlots_dissolved\" , [ \"LANDUSE\" , \"TAXCODE\" ], \"\" , \"SINGLE_PART\" , \"DISSOLVE_LINES\" )"}, "usage": ["The attributes of the features that become aggregated by dissolve can be summarized or described using a variety of statistics. The statistic used to summarize attributes is added to the output feature class as a single field with the following naming standard of statistic type + underscore + input field name. For example, if the SUM statistic is used on a field named POP, the output will have a field named SUM_POP.", "Dissolve", " can create very large features in the output feature class. This is especially true when there is a small number of unique values in the ", "Dissolve Field(s)", " or when dissolving all features into a single feature. Very large features may cause processing or display problems and/or have poor performance when drawn on a map or when edited. Problems may also occur if the dissolve output  created a feature at the maximum size on one machine, and then this output was moved to a machine with less available memory. To avoid these potential problems, use the ", "Create multipart features", " parameter's SINGLE_PART option to split potentially larger multipart features into many smaller features. For extremely large features created by the Dissolve tool, the ", "Dice", " tool may have to be used to split the large features in order to solve processing, display, or performance problems.", "Null values are excluded from all statistical calculations. For example, the AVERAGE of 10, 5, and NULL is 7.5 ((10+5)/2). The COUNT tool returns the number of values included in the statistical calculation, which in this case is 2.", "This tool will use a tiling process to handle very large datasets for better performance and scalability. For more details, see ", "Geoprocessing with large datasets", ".", "The availability of physical memory may limit the amount (and complexity) of input features that can be processed and dissolved into a single output feature. This limitation could cause an error to occur, as the dissolve process may require more memory than is available. To prevent this, ", "Dissolve", " may divide and process the  input features using an adaptive tiling algorithm. To determine the features that have been tiled, run the ", "Frequency", " tool on the result of this tool, specifying the same fields used in the dissolve process for the ", "Frequency Field(s)", " parameter. Any record with a frequency value of 2 has been tiled. Tile boundaries are preserved in the output features to prevent the creation of features that are too large to be used by ArcGIS. ", "Running ", "Dissolve", " on the output of a previous dissolve run will rarely reduce the number of features in the output when the original processing divided and processed the inputs using  adaptive tiling.  The maximum size of any output feature is determined by the amount of available memory at run time; therefore, output containing tiles is an indicator that dissolving any further with the available resources would cause an out-of-memory situation or result in a feature that is unusable.   Additionally, running the ", "Dissolve", " tool a second time on output that was created this way may experience very slow performance for little to no gain and may cause an unexpected failure.", "The ", "Dissolve Field(s)", " parameter's ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Dissolve Field(s)", " parameter may not be populated with field names. The ", "Add Field", " button allows you to add expected fields so you can complete the tool's dialog box and continue to build your model.", "The ", "Unsplit lines", " parameter with two options, DISSOLVE_LINES and UNSPLIT_LINES, only applies to line input. When the default DISSOLVE_LINES option is specified, lines are dissolved into a single feature. When UNSPLIT_LINES is specified, only two lines that have a common endpoint (known as pseudonode) are merged into one continuous line. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The features to be aggregated. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to be created that will contain the aggregated features. ", "dataType": "Feature Class"}, {"name": "dissolve_field", "isOptional": false, "description": "The field or fields on which to aggregate features. The Add Field button, which is used only in ModelBuilder, allows you to add expected fields so you can complete the dialog box and continue to build your model. ", "dataType": "Field"}, {"name": "statistics_fields", "isOptional": false, "description": "The fields and statistics with which to summarize attributes. Text attribute fields may be summarized using the statistics FIRST or LAST. Numeric attribute fields may be summarized using any statistic. Nulls are excluded from all statistical calculations. FIRST\u2014Finds the first record in the Input Features and uses its specified field value. LAST\u2014Finds the last record in the Input Features and uses its specified field value. SUM\u2014Adds the total value for the specified field. MEAN\u2014Calculates the average for the specified field. MIN\u2014Finds the smallest value for all records of the specified field. MAX\u2014Finds the largest value for all records of the specified field. RANGE\u2014Finds the range of values (MAX\u2013MIN) for the specified field. STD\u2014Finds the standard deviation on values in the specified field. COUNT\u2014Finds the number of values included in statistical calculations. This counts each value except null values. To determine the number of null values in a field, use the COUNT statistic on the field in question, and a COUNT statistic on a different field which does not contain nulls (for example, the OID if present), then subtract the two values.", "dataType": "Value Table"}, {"name": "multi_part", "isOptional": true, "description": "Specifies whether multipart features are allowed in the output feature class. MULTI_PART \u2014 Specifies multipart features are allowed. This is the default. SINGLE_PART \u2014 Specifies multipart features are not allowed. Instead of creating multipart features, individual features will be created for each part. ", "dataType": "Boolean"}, {"name": "unsplit_lines", "isOptional": true, "description": "Controls how line features are dissolved. DISSOLVE_LINES \u2014 Lines are dissolved into a single feature. This is the default. UNSPLIT_LINES \u2014 Lines are only dissolved when two lines have an end vertex in common. ", "dataType": "Boolean"}]},
{"syntax": "Sort_management (in_dataset, out_dataset, sort_field, {spatial_sort_method})", "name": "Sort (Data Management)", "description": "Reorders, in ascending or descending order, records in a feature class or table based on one or multiple fields. The reordered result is written to a new dataset. Learn more about how Sort works", "example": {"title": "Sort example 1 (Python window)", "description": "The following Python window script demonstrates how to use Sort to order features by the values of a field.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/city.gdb\" arcpy.Sort_management ( \"crime\" , \"crime_Sort\" , [[ \"DATE_REP\" , \"ASCENDING\" ]])"}, "usage": ["Feature classes can be spatially reordered, or sorted. The Shape field must be used as the sort field for spatial sorting. There are a number of spatial sort methods that arrange the features differently based on their location.", "Geodatabase and SDE feature classes, shapefiles, feature layers, tables, and table views are valid inputs. Individual components of CAD and SDC datasets are also valid inputs.", " If any input records are selected, only the subset of selected records are sorted and written to the output.", "If more than one field is set as a sort field, rows are first sorted by the first field and within that order sorted by the second field, and so on.", "Polygon features can be sorted by their area using the Shape_Area field of a geodatabase feature class. Similarly, polyline features can be sorted by their length using the Shape_Length field. If you want to sort polygon features in a Shapefile add a new field, calculate the area into the new field using ", "Calculate Field", " and run ", "Sort", " using the new field.", "For the ", "Field(s)", " parameter, sorting by the Shape field or by multiple fields is only available with an ", "Advanced", " license. Sorting by any single attribute field (excluding Shape) is available at all license levels."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The input dataset whose records will be reordered based on the field values in the sort field(s). ", "dataType": "Table View"}, {"name": "out_dataset", "isOutputFile": true, "isOptional": false, "description": "The output feature class or table. ", "dataType": "Feature Class;Table"}, {"name": "sort_field", "isOptional": false, "description": "Specifies the field(s) whose values will be used to reorder the input records, and the direction the records will be sorted. ASCENDING \u2014 Records are sorted from low value to high value. DESCENDING \u2014 Records are sorted from high value to low value.", "dataType": "Value Table"}, {"name": "spatial_sort_method", "isOptional": true, "description": "Specifies how features are spatially sorted. Sort method is only enabled when 'Shape' is selected as one of the sort fields. UR \u2014 Sorting starts at upper right corner. This is the default. UL \u2014 Sorting starts at upper left corner. LR \u2014 Sorting starts at lower right corner. LL \u2014 Sorting starts at lower left corner. PEANO \u2014 Sorting uses a space filling curve algorithm, also known as a peano curve.", "dataType": "String"}]},
{"syntax": "Rename_management (in_data, out_data, {data_type})", "name": "Rename (Data Management)", "description": "Changes the name of a dataset.  This includes a wide variety of data types, among them feature dataset, raster, table, and shapefile.", "example": {"title": "Rename example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Rename function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Rename_management ( \"customers.dbf\" , \"customers_2010.dbf\" )"}, "usage": ["The output name must be unique. If it is not, an error message is issued, even if the geoprocessing overwrite output environment is set to true.", "Rename", " does not rename fields in the dataset. For example, suppose you have a field named ROADS_ID on a feature class named ROADS. Renaming the ROADS feature class to STREETS does not rename the ROADS_ID field to STREETS_ID.", "Renaming a coverage also renames all region and route subclasses within a coverage.", "This  tool does not work with data stored in a DB2 database because of database constraints. "], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The input data to be renamed. ", "dataType": "Data Element"}, {"name": "out_data", "isOutputFile": true, "isOptional": false, "description": "The name for the output data. ", "dataType": "Data Element"}, {"name": "data_type", "isOptional": true, "description": "The type of the data to be renamed. The only time you need to provide a value is when a geodatabase contains a feature dataset and a feature class with the same name. In this case, you need to select the data type (feature dataset or feature class) of the item you want to rename. ", "dataType": "String"}]},
{"syntax": "Merge_management (inputs, output, {field_mappings})", "name": "Merge (Data Management)", "description": "Combines multiple input datasets of the same data type into a single, new output dataset. This tool can combine point, line, or polygon  feature classes  or  tables . Use the  Append  tool to combine input datasets with an existing dataset.", "example": {"title": "Merge example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Merge tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.Merge_management ([ \"majorrds.shp\" , \"Habitat_Analysis.gdb/futrds\" ], \"C:/output/Output.gdb/allroads\" )"}, "usage": ["Use this tool to combine datasets from multiple sources into a new, single output dataset. All input datasets must be of the same type (that is, several point feature classes can be merged, or several tables can be merged, but a line feature class cannot be merged with a polygon feature class).", "All fields in the output dataset and the contents of those fields can be controlled using the  ", "Field Map controls", ".", "Using the Field Map control, ", "fields", " of type ", "BLOB", " can only be passed into an output field of type BLOB, not into any other field type.", "Learn more about mapping fields in scripts", "This tool will not planarize features from the input datasets. All features from the input datasets will remain intact in the output dataset, even if the features overlap. To combine, or planarize, feature geometries use the ", "Union", " tool.", "This  tool does not perform edge matching\u2014there will be no adjustment to the geometry of features.", "If feature classes are being merged, the output dataset will be in the coordinate system of the first feature class in the ", "Input Datasets", " list, unless the ", "Output Coordinate System", " geoprocessing environment is set.", "Map layers can be used as input datasets. If a layer has a selection, only the selected records (features or table rows) are used by the Merge tool.", "This  tool cannot use multiple input ", "layers", " with the same name. Although ArcMap allows for the display of layers with the same name, these layers may not be used. To work around this limitation, use the  tool dialog box browse button to browse for the full paths of each input dataset.", "The Annotation Feature Class is not a valid input data type.", "Raster datasets are not valid input. Use the ", "Mosaic To New Raster", " tool to merge multiple rasters into a new output raster."], "parameters": [{"name": "inputs", "isOptional": false, "description": "The input datasets that will be merged together into a new output dataset. Input datasets can be point, line, or polygon feature classes or tables. The data type of all input datasets must match. ", "dataType": "Table View"}, {"name": "output", "isOptional": false, "description": "The output dataset that will contain all combined input datasets. ", "dataType": "Feature Class;Table"}, {"name": "field_mappings", "isOptional": true, "description": "The fields and field contents chosen from the inputs. Each of the unique input fields will be listed on the Field Map window, and when expanded, you will see a list of all the input field occurrences. For each Field Map, you can add, rename, or delete output fields as well as set properties, such as data type and merge rule. You can also delete an output field's occurrences, and you can format any output field's values. Merge rules allow you to specify how values from two or more input fields are merged into a single output value. There are several merge rules that you can use: First\u2014Use the first input field's values to populate the output field. Last\u2014Use the last input field's values to populate the output field. Join\u2014Concatenate (join) all input fields' values to populate the output field. Sum\u2014Calculate the total of all input fields' values. Mean\u2014Calculate the mean (average) of all input fields' values. Median\u2014Calculate the median (middle) value. Mode\u2014Use the value with the highest frequency. Min\u2014Use the minimum value of all input fields' values. Max\u2014Use the maximum value of all input fields' values. Standard deviation\u2014Use the standard deviation classification method on all input fields' values. Count\u2014Find the number of records included in the calculation. ", "dataType": "Field Mappings"}]},
{"syntax": "FindIdentical_management (in_dataset, out_dataset, fields, {xy_tolerance}, {z_tolerance}, {output_record_option})", "name": "Find Identical (Data Management)", "description": "Reports any records in a feature class or table that have identical values in a list of fields, and generates a table listing these identical records. If the field Shape is selected, feature geometries are compared. The  Delete Identical  tool can be used to find and delete identical records.", "example": {"title": "FindIdentical example 1 (Python window)", "description": "The following Python window script demonstrates how to use the FindIdentical function in immediate mode.", "code": "import arcpy # Find identical records based on a text field and a numeric field. arcpy.FindIdentical_management ( \"C:/data/fireincidents.shp\" , \"C:/output/duplicate_incidents.dbf\" , [ \"ZONE\" , \"INTENSITY\" ])"}, "usage": ["Records are identical if values in the selected input fields are the same for those records. The values from multiple fields in the input dataset can be compared. If more than one field is specified, records are matched by the values in the first field, then by the values of the second field, and so on.", "With feature class or feature layer input, select the field Shape in the ", "Field(s)", " parameter to compare feature geometries to find identical features by location.  The ", "XY Tolerance", " and ", "Z Tolerance", " parameters are only valid when Shape is selected as one of the input fields.", "If the Shape field is selected and the input features  have M or Z values enabled, then the M or Z values are also used to determine identical features.  ", "Check ", "Output only duplicated records", " parameter if you want only the duplicated records in the output table. The output will have the same number of records as the input dataset If this parameter is unchecked (the default).", "The output table will contain two fields: IN_FID and FEAT_SEQ. "], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The table or feature class for which identical records will be found. ", "dataType": "Table View"}, {"name": "out_dataset", "isOutputFile": true, "isOptional": false, "description": "The output table reporting identical records. The FEAT_SEQ field in the output table will have the same value for identical records. ", "dataType": "Table"}, {"name": "fields", "isOptional": false, "description": "The field or fields whose values will be compared to find identical records. ", "dataType": "Field"}, {"name": "xy_tolerance", "isOptional": true, "description": "The xy tolerance that will be applied to each vertex when evaluating if there is an identical vertex in another feature. This parameter is enabled only when Shape is selected as one of the fields. ", "dataType": "Linear unit"}, {"name": "z_tolerance", "isOptional": true, "description": "The Z tolerance that will be applied to each vertex when evaluating if there is an identical vertex in another feature. This parameter is enabled only when Shape is selected as one of the fields. ", "dataType": "Double"}, {"name": "output_record_option", "isOutputFile": true, "isOptional": true, "description": " Choose if you want only duplicated records in the output table. ALL \u2014 All input records will have corresponding records in the output table. This is the default. ONLY_DUPLICATES \u2014 Only duplicate records will have corresponding records in the output table. The output will be empty if no duplicate is found.", "dataType": "Boolean"}]},
{"syntax": "DeleteIdentical_management (in_dataset, fields, {xy_tolerance}, {z_tolerance})", "name": "Delete Identical (Data Management)", "description": "Deletes records in a feature class or table which have identical values in a list of fields. If the field Shape is selected, feature geometries are compared. The  Find Identical  tool can be used to report which records are considered identical without deleting the identical records.", "example": {"title": "DeleteIdentical example 1 (Python window)", "description": "The following Python window script demonstrates how to use the DeleteIdentical function in a Python window.", "code": "import arcpy arcpy.DeleteIdentical_management ( \"C:/data/fireincidents.shp\" , [ \"ZONE\" , \"INTENSITY\" ])"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "This tool finds identical records based on input field values, then deletes all but one of the identical records from each set of identical records. The values from multiple fields in the input dataset can be compared. If more than one field is specified, records are matched by the values in the first field, then by the values of the second field, and so on.", "With feature class or feature layer input, select the field Shape in the ", "Field(s)", " parameter to compare feature geometries to find identical features by location. XY Tolerance and Z Tolerance parameters are only valid when Shape is selected as one of the input fields."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The table or feature class that will have its identical records deleted. ", "dataType": "Table View"}, {"name": "fields", "isOptional": false, "description": "The field or fields whose values will be compared to find identical records. ", "dataType": "Field"}, {"name": "xy_tolerance", "isOptional": true, "description": "The xy tolerance that will be applied to each vertex when evaluating if there is an identical vertex in another feature. ", "dataType": "Linear unit"}, {"name": "z_tolerance", "isOptional": true, "description": "The z tolerance that will be applied to each vertex when evaluating if there is an identical vertex in another feature. ", "dataType": "Double"}]},
{"syntax": "Delete_management (in_data, {data_type})", "name": "Delete (Data Management)", "description": "Permanently deletes data from disk. All types of geographic data supported by ArcGIS, as well as toolboxes and workspaces (folders, geodatabases), can be deleted. If the specified item is a workspace, all contained items are also deleted. ", "example": {"title": "Delete Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Delete function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Copy_management (( \"majorrds.shp\" ), \"majorrdsCopy.shp\" ) arcpy.Delete_management ( \"majorrdsCopy.shp\" )"}, "usage": ["Data currently in use in another ArcGIS application cannot be deleted\u2014the tool fails with ERROR 000464. ", "Deleting a shapefile also deletes ancillary files such as the metadata, projection, and index files.", "Deleting a folder moves the folder to the system Recycle Bin, where it can be restored or permanently deleted.", "Deleting a geometric network demotes all the feature classes in the network to simple feature types; Edge feature classes become line feature classes; and junction feature classes become point feature classes. Deleting the network also deletes all the related network tables and the orphan junction feature class from the geodatabase.", "Deleting a database connection file does not delete the ArcSDE database. A database connection file is simply a shortcut to the database. ", "Deleting a relationship class deletes the row corresponding to that relationship from the relationship table. "], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The input data to be deleted. ", "dataType": "Data Element; Graph; Layer; Table View"}, {"name": "data_type", "isOptional": true, "description": "Data type of the Input Data Element . Data type is displayed for informative purposes and cannot be changed. ", "dataType": "String"}]},
{"syntax": "Copy_management (in_data, out_data, {data_type})", "name": "Copy (Data Management)", "description": "Copies input data and pastes the output to the same or another location regardless of size. The data type of the input and output data element is identical.", "example": {"title": "Copy example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Copy function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Copy_management ( \"majorrds.shp\" , \"C:/output/majorrdsCopy.shp\" )"}, "usage": ["If a feature class is copied to a feature dataset, the ", "spatial reference", " of the feature class and the feature dataset must match; otherwise, the tool fails with an error message.", "Any data dependent on the input is also copied. For example, copying a feature class or table that is part of a relationship class also copies the relationship class. The same applies to a feature class that has feature-linked annotation, domains, subtypes, and indices\u2014all are copied along with the feature class.  Copying geometric networks, network datasets, and topologies also copies the participating feature classes.", "The ", "Copy", " tool does not allow you to copy a feature dataset  into a file geodatabase containing a feature class of the same name, regardless of whether the feature class is  stand-alone or contained in a feature dataset.   ", "To define  the ", "Output data element", " parameter from the tool dialog, click the browse button ", ", type a name, and choose a type from the ", "Save as type:", " drop down list on the browse dialog.   If you are copying a feature class to a geodatabase, you will need to select ", "Datasets", " in the ", "Save as type:", " drop down list in order to show geodatabases in the browse dialog."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The data to be copied to the same or another location. ", "dataType": "Data Element"}, {"name": "out_data", "isOutputFile": true, "isOptional": false, "description": "The location and name of the output data. The file name extension of the output data must match the extension of the input data. For example, if you are copying a file geodatabase, your output data element must have .gdb as a suffix. ", "dataType": "Data Element"}, {"name": "data_type", "isOptional": true, "description": "The type of the data to be copied. The only time you need to provide a value is when a geodatabase contains a feature dataset and a feature class with the same name. In this case, you need to select the data type (feature dataset or feature class) of the item you want to copy. ", "dataType": "String"}]},
{"syntax": "Append_management (inputs, target, {schema_type}, {field_mapping}, {subtype})", "name": "Append (Data Management)", "description": "Appends multiple input datasets into an existing target dataset. Input datasets can be point, line, or polygon feature classes, tables, rasters, raster catalogs, annotation feature classes, or dimensions feature classes.  To combine input datasets into a new output dataset, use the  Merge  tool.", "example": {"title": "Append example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Append tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/\" arcpy.Append_management ([ \"north.shp\" , \"south.shp\" , \"east.shp\" , \"west.shp\" ], \"wholecity.shp\" , \"TEST\" , \"\" , \"\" )"}, "usage": ["Use this tool to add new features or other data from multiple datasets into an existing dataset. This tool can append point, line, or polygon feature classes, tables, rasters, raster catalogs, annotation feature classes, or dimensions feature classes into an existing dataset of the same type. For example, several tables can be appended to an existing table, or several rasters can be appended to an existing raster dataset, but a line feature class cannot be appended to a point feature class. ", "The Append tool's ", "Field Map control", " can be used to control how the attribute information from the input dataset fields is transferred to the target dataset. The Field Map control can only be used if the ", "Schema Type", " NO_TEST is specified.", "This tool will not planarize features when they are added to the target dataset. All features from both the input feature class and the target feature class will remain intact after the append, even if the features overlap. To combine, or planarize feature geometries, use the ", "Union", " tool.", "If the ", "Schema Type", " TEST is specified, the schema (field definitions) of the input datasets must match that of the target dataset in order for the features to be appended. If the ", "Schema Type", " NO_TEST is specified, input dataset schema (field definitions) do not have to match the target dataset. However, any fields from the input datasets that do not match the fields of the target dataset will not be mapped to the target dataset unless the mapping is explicitly set in the Field Map control. ", "Because the input datasets' data is written into an existing target dataset that has a predefined schema (field definitions), the Field Map control does not allow for fields to be added or removed from the target dataset.", "If the spatial references of an input and target feature class do not match, the Append tool will project the features in the input feature class to the coordinate system used by the target feature class.", "This  tool does not perform edge matching\u2014there will be no adjustment to the geometry of features.", "Map layers can be used as ", "Input Datasets", ". If a layer has a selection, only the selected records (features or table rows) are used by the Append tool.", "This  tool cannot use multiple input ", "layers", " with the same name. Although ArcMap allows for the display of layers with the same name, these layers may not be used. To work around this limitation, use the  tool dialog box browse button to browse for the full paths of each of the ", "Input Datasets", ".", "To use the ", "Subtype", " parameter, the target dataset must have a defined subtype field and assigned subtype codes. In the ", "Subtype", " parameter, provide a subtype description to assign that subtype to all new data that is appended to the target dataset."], "parameters": [{"name": "inputs", "isOptional": false, "description": "The input datasets whose data will be appended into the target dataset. Input datasets can be point, line, or polygon feature classes, tables, rasters, raster catalogs, annotation feature classes, or dimensions feature classes. Each input dataset must match the data type of the target dataset. ", "dataType": "Table View; Raster Layer"}, {"name": "target", "isOptional": false, "description": "The existing dataset that the input datasets' data will be appended into. Each input dataset must match the data type of the target dataset. ", "dataType": "Table View; Raster Layer"}, {"name": "schema_type", "isOptional": true, "description": "Specifies if the schema (field definitions) of the input datasets must match the schema of the target dataset in order for data to be appended. TEST \u2014 Input dataset schema (field definitions) must match the schema of the target dataset. An error will be returned if the schemas do not match. NO_TEST \u2014 Input dataset schema (field definitions) do not have to match that of the target dataset. Any fields from the input datasets that do not match the fields of the target dataset will not be mapped to the target dataset unless the mapping is explicitly set in the Field Map control.", "dataType": "String"}, {"name": "field_mapping", "isOptional": true, "description": "Controls how the attribute information in input datasets' fields is transferred to the target dataset. This parameter can only be used if the Schema Type NO_TEST is specified. Because the input datasets' data is appended into an existing target dataset that has a predefined schema (field definitions), fields cannot be added or removed from the target dataset. Merge rules allow you to specify how values from two or more input fields are merged into a single output value. There are several merge rules that you can use: First\u2014Use the first input field's values to populate the output field. Last\u2014Use the last input field's values to populate the output field. Join\u2014Concatenate (join) all input fields' values to populate the output field. Sum\u2014Calculate the total of all input fields' values. Mean\u2014Calculate the mean (average) of all input fields' values. Median\u2014Calculate the median (middle) value. Mode\u2014Use the value with the highest frequency. Min\u2014Use the minimum value of all input fields' values. Max\u2014Use the maximum value of all input fields' values. Standard deviation\u2014Use the standard deviation classification method on all input fields' values. Count\u2014Find the number of records included in the calculation. ", "dataType": "Field Mapping"}, {"name": "subtype", "isOptional": true, "description": "A subtype description to assign that subtype to all new data that is appended to the target dataset. ", "dataType": "String"}]},
{"syntax": "DeleteColormap_management (in_raster)", "name": "Delete Colormap (Data Management)", "description": "Removes the color map associated with a raster dataset.", "example": {"title": "DeleteColormap Example (Python Window)", "description": "This is a Python sample for the DeleteColormap tool.", "code": "import arcpy arcpy.DeleteColormap_management ( \"c:/data/delcolormap.tif\" )"}, "usage": ["This tool will not work when the color map is internally stored in the attribute table of an IMG or TIFF dataset. If the attribute table contains the fields Red, Green, and Blue, that means this tool cannot be used."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The raster dataset containing the color map to remove. ", "dataType": "Raster Layer"}]},
{"syntax": "CalculateStatistics_management (in_raster_dataset, {x_skip_factor}, {y_skip_factor}, {ignore_values}, {skip_existing}, {area_of_interest})", "name": "Calculate Statistics (Data Management)", "description": "Calculates statistics for a raster dataset or mosaic dataset. Statistics are required for your raster and mosaic datasets to perform certain tasks, such as applying a contrast stretch or classifying your data.", "example": {"title": "CalculateStatistics example 1 (Python window)", "description": "This is a Python sample for CalculateStatistics.", "code": "import arcpy arcpy.CalculateStatistics_management ( \"C:/data/image.tif\" , \"5\" , \"5\" , \"0;255\" , \"SKIP_EXISTING\" , \"c:/data/aoi.shp\" )"}, "usage": ["Calculating ", "statistics", " allows ArcGIS applications to properly stretch and symbolize raster data for display.", "A skip factor controls the portion of the raster dataset that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster dataset.", "The skip factors for raster datasets stored in a file geodatabase or an ArcSDE geodatabase are quite different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid (for example, if the skip factor is 5 and the closest pyramid level is 4 x 4 pixels, which is level 2), the software will round down to the next pyramid level (in this case, 2) and use that value as the skip factor.", "A skip factor is not used for all raster formats.  The raster formats that will calculate statistics and take advantage of the skip factor include TIFF, IMG, NITF, DTED, RAW, ADRG, CIB, CADRG, DIGEST, GIS, LAN, CIT, COT, ERMapper, ENVI DAT, BIL, BIP, BSQ, and geodatabase.", "When using this tool to calculate statistics on a mosaic dataset, the statistics are calculated for the top-level mosaicked image, not for every raster contained within the mosaic dataset.", "Specifying a skip factor for a  mosaic dataset is highly recommended as these datasets tend to be very large.", "The Ignore Values option allows you to exclude a specific value from the calculation of statistics. You may want to ignore a value if it is a NoData value or if it will skew your calculation.", "Calculating statistics on the GRID and the RADARSAT2 formats always uses a skip factor of 1."], "parameters": [{"name": "in_raster_dataset", "isInputFile": true, "isOptional": false, "description": "The input raster dataset or mosaic dataset. ", "dataType": "Mosaic Dataset; Mosaic Layer; Raster Dataset"}, {"name": "x_skip_factor", "isOptional": true, "description": "The number of horizontal pixels between samples. The value must be greater than zero and less than or equal to the number of columns in the raster dataset. The default is 1 or the last skip factor used. The skip factors for raster datasets stored in a file geodatabase or an ArcSDE geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. ", "dataType": "Long"}, {"name": "y_skip_factor", "isOptional": true, "description": "The number of vertical pixels between samples. The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used. The skip factors for raster datasets stored in a file geodatabase or an ArcSDE geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. ", "dataType": "Long"}, {"name": "ignore_values", "isOptional": false, "description": "The pixel values that are not to be included in the statistics calculation. The default is no value, or the last ignore values used. ", "dataType": "Long"}, {"name": "skip_existing", "isOptional": true, "description": "Specify whether to calculate statistics only where they are missing or regenerate them even if they exist. OVERWRITE \u2014 Statistics will be calculated even if they already exist. Therefore, existing statistics will be overwritten. This is the default. SKIP_EXISTING \u2014 Statistics will only be calculated if they do not already exist. ", "dataType": "Boolean"}, {"name": "area_of_interest", "isOptional": true, "description": " Specify a feature class that represents area in the dataset from where you want the statistics to be calculated, so they are not generated from the entire dataset. ", "dataType": "Feature Set"}]},
{"syntax": "BuildRasterAttributeTable_management (in_raster, {overwrite})", "name": "Build Raster Attribute Table (Data Management)", "description": "Adds a raster attribute table to a raster dataset or updates an existing one.", "example": {"title": "BuildRasterAttributeTable example (Python window)", "description": "This is a Python sample for BuildRasterAttributeTable.", "code": "import arcpy arcpy.BuildRasterAttributeTable_management ( \"c:/data/image.tif\" , \"Overwrite\" )"}, "usage": ["If you want to delete an existing table and create a new one, check  ", "Overwrite", ". A new raster attribute table will be created, and the old one will be deleted.", "If you have an existing table and you do not check  ", "Overwrite", ", the table will be updated. No fields will be deleted, but the values in the table will be up to date.", "It is not possible to build a raster attribute table for a raster dataset that is a pixel type of 32-bit floating point."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. This must be a raster dataset with only a single band. It is not possible to build a raster attribute table for a raster dataset that has a pixel type of 32-bit floating point. ", "dataType": "Raster layer"}, {"name": "overwrite", "isOptional": true, "description": "This allows you to overwrite any existing raster attribute table that might exist. NONE \u2014 Existing raster attribute tables will not be overwritten, and any edits will be appended to the current table. This is the default. OVERWRITE \u2014 Delete the existing raster attribute tables and create a new raster attribute table.", "dataType": "Boolean"}]},
{"syntax": "BuildPyramids_management (in_raster_dataset, {pyramid_level}, {SKIP_FIRST}, {resample_technique}, {resample_method}, {compression_type}, {compression_quality}, {skip_existing})", "name": "Build Pyramids (Data Management)", "description": "Builds raster pyramids for a raster dataset. This tool can also be used to delete pyramids. To delete pyramids, set the  Pyramids Levels  parameter to 0.", "example": {"title": "BuildPyramids example 1 (Python window)", "description": "This is a Python sample for BuildPyramids.", "code": "import arcpy arcpy.BuildPyramids_management ( \"C:/data/image.tif\" , \"3\" , \"NONE\" , \"BILINEAR\" , \"JPEG\" , \"50\" , \"SKIP_EXISTING\" )"}, "usage": ["Building ", "pyramids", " improves the display performance of raster datasets.", "You only need to build pyramids once per dataset. The pyramids will be accessed every time you display the raster dataset.", "Pyramids will not be built for raster datasets that have less than 1024 pixels in the row or column. Pyramids are not needed since the raster dataset is small enough, and building pyramids will not help increase the performance.", "Wavelet compressed raster datasets, such as ECW and MrSID, do not need to have pyramids built. These formats have internal pyramids that are created upon encoding.", "You can choose the compression type for your overview pyramid file, in the Raster Storage Environment Settings. Compression will create a smaller .ovr file. The IMAGINE format and older versions of ArcGIS will create reduced resolution dataset (.rrd) files, where compression is not available.", "The default pyramid compression will use the optimal compression type, given the type of data. You can manually choose to have LZ77, JPEG, or no compression.", "JPEG compression can only be used with file formats that can store data according to the JPEG specifications."], "parameters": [{"name": "in_raster_dataset", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. The input should have more than 1024 rows and 1024 columns. ", "dataType": "Raster Dataset"}, {"name": "pyramid_level", "isOptional": true, "description": "Choose the number of reduced-resolution dataset layers that will be built. The default value is \u20131, which will build full pyramids. A value of 0 will result in no pyramid levels. To delete pyramids, set the number of levels to 0. ", "dataType": "Long"}, {"name": "SKIP_FIRST", "isOptional": true, "description": "Choose whether to skip the first pyramid level. Skipping the first level will take up slightly less disk space, but it will slow down the performance at these scales. NONE \u2014 The first pyramid level will be built. This is the default. SKIP_FIRST \u2014 The first pyramid level will not be built.", "dataType": "Boolean"}, {"name": "resample_technique", "isOptional": true, "description": " The resampling technique used to build your pyramids. NEAREST \u2014 The nearest neighbor resampling method uses the value of the closest cell to assign a value to the output cell when resampling. This is the default. BILINEAR \u2014 The bilinear interpolation resampling method determines the new value of a cell based on a weighted distance average of the four nearest input cell centers. CUBIC \u2014 The Cubic convolution resampling method determines the new value of a cell based on fitting a smooth curve through the 16 nearest input cell centers.", "dataType": "String"}, {"name": "resample_method", "isOptional": true, "description": " The compression type to use when building the raster pyramids. DEFAULT \u2014 If the source data is compressed using a wavelet compression, it will build pyramids with the JPEG compression type; otherwise, LZ77 will be used. This is the default compression method. LZ77 \u2014 The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type. JPEG \u2014 The JPEG compression algorithm to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality. JPEG_YCbCr \u2014 A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components. NONE \u2014 No compression will be used when building pyramids.", "dataType": "String"}, {"name": "compression_type", "isOptional": true, "description": " The compression type to use when building the raster pyramids. DEFAULT \u2014 If the source data is compressed using a wavelet compression, it will build pyramids with the JPEG compression type; otherwise, LZ77 will be used. This is the default compression method. LZ77 \u2014 The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type. JPEG \u2014 The JPEG compression algorithm to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality. JPEG_YCbCr \u2014 A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components. NONE \u2014 No compression will be used when building pyramids.", "dataType": "String"}, {"name": "compression_quality", "isOptional": true, "description": " The compression quality to use when pyramids are built with the JPEG compression method. The value must be between 0 and 100. The values closer to 100 would produce a higher quality image, but the compression ratio would be lower. ", "dataType": "Long"}, {"name": "skip_existing", "isOptional": true, "description": "Specify whether to build pyramids only when they are missing or regenerate them even if they exist. OVERWRITE \u2014 Pyramids will be built even if they already exist. Therefore, existing pyramids will be overwritten. This is the default. SKIP_EXISTING \u2014 Pyramids will only be built if they do not already exist.", "dataType": "Boolean"}]},
{"syntax": "BatchCalculateStatistics_management (Input_Raster_Datasets, {Number_of_columns_to_skip}, {Number_of_rows_to_skip}, {Ignore_values}, {Skip_Existing})", "name": "Batch Calculate Statistics (Data Management)", "description": "Calculates statistics for  multiple raster datasets.", "example": {"title": "BatchCalculateStatistics example 1 (Python window)", "description": "This is a Python sample for BatchCalculateStatistics.", "code": "import arcpy arcpy.BatchCalculateStatistics_management ( \"C:/data/img1.tif;C:/data/img2.jp2\" , \"5\" , \"5\" , \"0;255\" , \"SKEP_EXISTING\" )"}, "usage": ["Calculating ", "statistics", " allows ArcGIS applications to properly stretch and symbolize raster data for display.", "A skip factor controls the portion of the raster dataset that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster dataset.", "Calculating statistics on the GRID and the RADARSAT2 formats always uses a skip factor of 1.", "The skip factors for raster datasets stored in a file geodatabase or an ArcSDE geodatabase are quite different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid (for example, if the skip factor is 5 and the closest pyramid level is 4 x 4 pixels, which is level 2), the software will round down to the next pyramid level (in this case, 2) and use that value as the skip factor.", "A skip factor is not used for all raster formats.  The raster formats that will calculate statistics and take advantage of the skip factor include TIFF, IMG, NITF, DTED, RAW, ADRG, CIB, CADRG, DIGEST, GIS, LAN, CIT, COT, ERMapper, ENVI DAT, BIL, BIP, BSQ, and geodatabase.", "The Ignore Values option allows you to exclude a specific value from the calculation of statistics. You may want to ignore a value if it is a NoData value or if it will skew your calculation."], "parameters": [{"name": "Input_Raster_Datasets", "isInputFile": true, "isOptional": false, "description": "The input raster datasets. ", "dataType": "Raster Dataset"}, {"name": "Number_of_columns_to_skip", "isOptional": true, "description": "The number of horizontal pixels between samples. The value must be greater than zero and less than or equal to the number of columns in the raster dataset. The default is 1 or the last skip factor used. The skip factors for raster datasets stored in a file geodatabase or an ArcSDE geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. ", "dataType": "Long"}, {"name": "Number_of_rows_to_skip", "isOptional": true, "description": "The number of vertical pixels between samples. The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used. The skip factors for raster datasets stored in a file geodatabase or an ArcSDE geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. ", "dataType": "Long"}, {"name": "Ignore_values", "isOptional": false, "description": "The pixel values that are not to be included in the statistics calculation. The default is no value. ", "dataType": "Double"}, {"name": "Skip_Existing", "isOptional": true, "description": "Specify whether to calculate statistics only where they are missing or regenerate them even if they exist. OVERWRITE \u2014 Statistics will be calculated even if they already exist. Therefore, existing statistics will be overwritten. This is the default. SKIP_EXISTING \u2014 Statistics will only be calculated if they do not already exist. ", "dataType": "Boolean"}]},
{"syntax": "BatchBuildPyramids_management (Input_Raster_Datasets, {Pyramid_levels}, {Skip_first_level}, {Pyramid_resampling_technique}, {Pyramid_compression_type}, {Compression_quality}, {Skip_Existing})", "name": "Batch Build Pyramids (Data Management)", "description": "Builds pyramids for multiple raster datasets.", "example": {"title": "BatchBuildPyramids example 1 (Python window)", "description": "This is a Python sample for BatchBuildPyramids. ", "code": "import arcpy arcpy.BatchBuildPyramids_management ( \"C:/data/img1.tif;C:/data/img2.img\" , \"6\" , \"SKIP_FIRST\" , \"BILINEAR\" , \"JPEG\" , \"50\" , \"SKIP_EXISTING\" )"}, "usage": ["Building ", "pyramids", " improves the display performance of raster datasets.", "Batch building of pyramids is useful when you have a large directory of raster datasets that do not have pyramids or to build pyramids on the items of a raster catalog (drag them into the dialog box).", "Wavelet compressed raster datasets, such as ECW and MrSID, do not need to have pyramids built. These formats have internal pyramids that are created upon encoding.", "Pyramids will not be built for raster datasets that have less than 1024 pixels in the row or column. Pyramids are not needed since the raster dataset is small enough, and building pyramids will not help increase the performance.", "Pyramids cannot be built for raster catalogs, but they can be built for each raster catalog item.", "You can choose the compression type for your overview pyramid file, in the Raster Storage Environment Settings. Compression will create a smaller .ovr file. The IMAGINE format and older versions of ArcGIS will create reduced resolution dataset (.rrd) files, where compression is not available.", "The default pyramid compression will use the optimal compression type, given the type of data. You can manually choose to have LZ77, JPEG, or no compression.", "JPEG compression can only be used with file formats that can store data according to the JPEG specifications."], "parameters": [{"name": "Input_Raster_Datasets", "isInputFile": true, "isOptional": false, "description": "The input raster datasets for which you want to build raster pyramids. Each input should have more than 1024 rows and 1024 columns. ", "dataType": "Raster Dataset"}, {"name": "Pyramid_levels", "isOptional": true, "description": "Choose the number of reduced-resolution dataset layers that will be built. The default value is \u20131, which will build full pyramids. A value of 0 will result in no pyramid levels. ", "dataType": "Long"}, {"name": "Skip_first_level", "isOptional": true, "description": "Choose whether to skip the first pyramid level. Skipping the first level will take up slightly less disk space, but it will slow down the performance at these scales. NONE \u2014 The first pyramid level will be built. This is the default. SKIP_FIRST \u2014 The first pyramid level will not be built.", "dataType": "Boolean"}, {"name": "Pyramid_resampling_technique", "isOptional": true, "description": " The resampling technique used to build your pyramids. NEAREST \u2014 The nearest neighbor resampling method uses the value of the closest cell to assign a value to the output cell when resampling. This is the default. BILINEAR \u2014 The bilinear interpolation resampling method determines the new value of a cell based on a weighted distance average of the four nearest input cell centers. CUBIC \u2014 The Cubic convolution resampling method determines the new value of a cell based on fitting a smooth curve through the 16 nearest input cell centers.", "dataType": "String"}, {"name": "Pyramid_compression_type", "isOptional": true, "description": " The compression type to use when building the raster pyramids. DEFAULT \u2014 If the source data is compressed using a wavelet compression, it will build pyramids with the JPEG compression type; otherwise, LZ77 will be used. This is the default compression method. LZ77 \u2014 The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type. JPEG \u2014 The JPEG compression algorithm to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality. JPEG_YCbCr \u2014 A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components. NONE \u2014 No compression will be used when building pyramids.", "dataType": "String"}, {"name": "Compression_quality", "isOptional": true, "description": " The compression quality to use when pyramids are built with the JPEG compression method. The value must be between 0 and 100. The values closer to 100 would produce a higher quality image, but the compression ratio would be lower. ", "dataType": "Long"}, {"name": "Skip_Existing", "isOptional": true, "description": "Specify whether to build pyramids only where they are missing or regenerate them even if they exist. OVERWRITE \u2014 Pyramids will be built even if they already exist. Therefore, existing pyramids will be overwritten. This is the default. SKIP_EXISTING \u2014 Pyramids will only be built if they do not exist. ", "dataType": "Boolean"}]},
{"syntax": "AddColormap_management (in_raster, {in_template_raster}, {input_CLR_file})", "name": "Add Colormap (Data Management)", "description": "Adds a color map to a raster dataset if it does not already exist or replaces a color map with the one specified.", "example": {"title": "AddColormap example (Python window)", "description": "This is a Python sample for AddColormap.", "code": "import arcpy arcpy.AddColormap_management ( \"c:/data/nocolormap.img\" , \"#\" , \"colormap_file.clr\" )"}, "usage": ["The color map applied to the input raster dataset can originate from a raster dataset that already has a color map, a .CLR file, or an .ACT file.", "This tool will not work when the color map is internally stored in the attribute table of an IMG or TIFF dataset. If the attribute table contains the fields Red, Green, and Blue, that means this tool cannot be used.", "The input raster dataset must be a single band raster dataset with integer values. Color maps can only be created for single-band raster datasets with a pixel depth of 16-bit unsigned or fewer. Certain formats cannot have a color map associated with them; please consult the ", "Supported raster dataset file formats", "."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset to which you want to add a color map. ", "dataType": "Raster Layer"}, {"name": "in_template_raster", "isInputFile": true, "isOptional": true, "description": "The raster dataset with a color map, that will be applied to the input raster dataset. If this is entered the input_CLR_file cannot be specified. ", "dataType": "Raster Layer"}, {"name": "input_CLR_file", "isInputFile": true, "isOptional": true, "description": " The .clr or .act file, which will be used as the template color map for the input raster dataset. If this is entered the in_template_raster cannot be specified. ", "dataType": "File"}]},
{"syntax": "SplitRaster_management (in_raster, out_folder, out_base_name, split_method, format, {resampling_type}, {num_rasters}, {tile_size}, {overlap}, {units}, {cell_size}, {origin})", "name": "Split Raster (Data Management)", "description": "Creates a tiled output from an input raster dataset.", "example": {"title": "SplitRaster example 1  (Python window)", "description": "This is a Python sample for SplitRaster.", "code": "import arcpy arcpy.SplitRaster_management ( \"c:/source/large.tif\" , \"c:/output/splitras\" , \"ras\" , \"NUMBER_OF_TILES\" , \"TIFF\" , \"NEAREST\" , \"2 2\" , \"#\" , \"10\" , \"PIXELS\" , \"#\" , \"#\" )"}, "usage": ["The output files will share most of the properties of the input source raster, such as the spatial reference, source type, pixel type, pixel depth, and cell size.", "The tiling method determines which of the optional parameters are used to determine the dimensions and location of the output tiles. In both cases, NoData values are used to pad the tiles where there is no corresponding source data. The data format depends on the limitations of the individual format specifications and the source image data type. Invalid combinations result in an appropriate error message.", "If a tile already exists (if there is a file with the same name), then it will not be overwritten.", "If a tile only contains NoData pixel values, then it will not be created."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset to be split into tiles. ", "dataType": "Raster Layer"}, {"name": "out_folder", "isOutputFile": true, "isOptional": false, "description": "The output folder, where the tiles will be created. ", "dataType": "Folder"}, {"name": "out_base_name", "isOutputFile": true, "isOptional": false, "description": "The prefix for each file name. The tile number is then appended to complete the file name, which starts with 0. By default, the prefix is the same name as the input raster. ", "dataType": "String"}, {"name": "split_method", "isOptional": false, "description": "The tiling method to be used when splitting the raster dataset. It will determine the size and number of tiles for each output dataset. SIZE_OF_TILE \u2014 Allows you to specify the tile width and tile height. Then the appropriate number of tiles will be created. This is the default method. You can also specify a different lower left origin, different output pixel sizes, and the amount of overlap between adjoining tiles (in pixels, meters, feet, degrees, miles, or kilometers). NUMBER_OF_TILES \u2014 Allows you to specify the number of raster tiles to create in the horizontal and the vertical direction. Then the appropriate tile size will be created for each dataset. You can also specify a different lower left origin, different output pixel sizes, and the amount of overlap between adjoining tiles (in pixels, meters, feet, degrees, miles, or kilometers).", "dataType": "String"}, {"name": "format", "isOptional": false, "description": "The file format for the output raster datasets. TIFF \u2014 Tagged Image File Format. This is the default. BMP \u2014 Bitmap. ENVI \u2014 ENVI DAT. Esri BIL \u2014 Esri band interleaved by line. Esri BIP \u2014 Esri band interleaved by pixel. Esri BSQ \u2014 Esri band sequential. GIF \u2014 Graphic interchange format. GRID \u2014 Esri Grid. IMAGINE IMAGE \u2014 ERDAS IMAGINE. JP2 \u2014 JPEG 2000. JPEG \u2014 Joint Photographics Experts Group. PNG \u2014 Portable Network Graphics.", "dataType": "String"}, {"name": "resampling_type", "isOptional": true, "description": "Choose the resampling method to use when creating the DTEDs. The default is bilinear interpolation resampling. NEAREST \u2014 Nearest neighbor assignment BILINEAR \u2014 Bilinear interpolation CUBIC \u2014 Cubic convolution MAJORITY \u2014 Majority resampling", "dataType": "String"}, {"name": "num_rasters", "isOptional": true, "description": "Specify the number of tiles in each direction. The default value is 1 tile for each direction. This option is only valid when the tiling method is NUMBER_OF_TILES . ", "dataType": "Point"}, {"name": "tile_size", "isOptional": true, "description": " The x and y dimensions of the output tiles. The units parameter will determine the units that are used for these values. This option is only valid when the tiling method is SIZE_OF_TILE . ", "dataType": "Point"}, {"name": "overlap", "isOptional": true, "description": " The number of pixels of overlap between the adjoining tiles. The overlap value will be determined by the units parameter. ", "dataType": "Double"}, {"name": "units", "isOptional": true, "description": " Determines the units that apply to the tile_size and the overlap parameters. PIXELS \u2014 The unit is in pixels. This is the default. METERS \u2014 The unit is in meters. FEET \u2014 The unit is in feet. DEGREES \u2014 The unit is in decimal degrees. MILES \u2014 The unit is in miles. KILOMETERS \u2014 The unit is in kilometers.", "dataType": "String"}, {"name": "cell_size", "isOptional": true, "description": " Specify the output pixel size in each direction. By default, the output will match the input raster. If the cell size values are changed, the tile size and count are reset to their default values (image size and 1, respectively). This parameter is based on the output spatial reference system, which is set in the Environment Settings. ", "dataType": "Point"}, {"name": "origin", "isOptional": true, "description": "The coordinate for the lower left origin point, where the tiling scheme will begin. By default, the lower left origin would be the same as the input raster. This parameter is based on the output spatial reference system, which is set in the Environment Settings. ", "dataType": "Point"}]},
{"syntax": "Resample_management (in_raster, out_raster, {cell_size}, {resampling_type})", "name": "Resample (Data Management)", "description": "Alters the raster dataset by changing the cell size and resampling method.", "example": {"title": "Resample example 1 (Python window)", "description": "This is a Python sample for the Resample tool.", "code": "import arcpy arcpy.Resample_management ( \"c:/data/image.tif\" , \"resample.tif\" , \"10 20\" , \"NEAREST\" )"}, "usage": ["The cell size can be changed, but the extent of the raster dataset will remain the same.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "The ", "Output Cell Size ", " parameter can resample the output to the same cell size as an existing raster layer, or it  can output a specific X and Y cell size.", "There are four options for the ", "Resampling Technique", " parameter:", "The Bilinear and Cubic options should not be used with categorical data, since the cell values may be altered.", "The lower left corner of the output raster dataset will be the same map space coordinate location as the lower left corner of the input raster dataset.", "The numbers of rows and columns in the output raster are determined as follows:", "If there is any remainder from the above equations, rounding of the number of columns and/or rows is performed."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size for the new raster dataset. You can specify the cell size in 3 different ways: Using a single number specifying a square cell size Using two numbers that specify the X and Y cell size, which is space delimited Using the path of a raster dataset from which the square cell size will be imported", "dataType": "Cell Size XY"}, {"name": "resampling_type", "isOptional": true, "description": "The resampling algorithm to be used. The default is NEAREST. NEAREST \u2014 Nearest neighbor assignment BILINEAR \u2014 Bilinear interpolation CUBIC \u2014 Cubic convolution MAJORITY \u2014 Majority resampling ", "dataType": "String"}]},
{"syntax": "ExtractSubdataset_management (in_raster, out_raster, {subdataset_index})", "name": "Extract Subdataset (Data Management)", "description": "Extracts raster datasets stored within a subdataset raster file.", "example": {"title": "ExtractSubdataset example 1 (Python window)", "description": "This is a Python sample for ExtractSubdataset", "code": "import arcpy arcpy.ExtractSubDataset_management ( \"c:/data/MyNITF.ntf\" , \"extracted.tif\" , \"2\" )"}, "usage": ["This tool is useful since ArcCatalog is only able to preview the first subdataset within the parent file.", "Subdataset file formats can be either Hierarchical Data Format (HDF) or National Imagery Transmission Format (NITF) files", "The data structure allows the file format to consist of multiple datasets in one parent file. In addition, each of the subdatasets can consist of one band or multiple bands.", "If you do not choose any subdatasets, the default will be to only return the first subdataset.", "When storing your raster dataset to a JPEG or a JPEG 2000 file, you can specify the compression quality within the Raster Storage Settings in the Environment Settings dialog box.", "The GIF format does not support multiband; therefore, it is not a valid output format unless your raster dataset is single band."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input subdataset raster format. Valid inputs can either be HDF or NITF files. ", "dataType": "Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The name and extension of the raster dataset to be created. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil\u2014ESRI BIL .bip\u2014ESRI BIP .bmp\u2014BMP .bsq\u2014ESRI BSQ .dat\u2014ENVI DAT .gif\u2014GIF .img\u2014ERDAS IMAGINE file .jpg\u2014JPEG .jp2\u2014JPEG 2000 .png\u2014PNG .tif\u2014TIFF no extension\u2014ESRI GRID.", "dataType": "Raster Dataset"}, {"name": "subdataset_index", "isOptional": false, "description": "Define the subdatasets that you want to extract. ", "dataType": "Value Table"}]},
{"syntax": "CreatePansharpenedRasterDataset_management (in_raster, red_channel, green_channel, blue_channel, {infrared_channel}, out_raster_dataset, in_panchromatic_image, pansharpening_type, {red_weight}, {green_weight}, {blue_weight}, {infrared_weight})", "name": "Create Pan-sharpened Raster Dataset (Data Management)", "description": "Fuses a high-resolution panchromatic raster dataset with a lower-resolution multiband raster dataset to create a red-green-blue (RGB) raster with the resolution of the panchromatic raster. \r\n Learn about panchromatic sharpening \r\n", "example": {"title": "CreatePansharpenedRasterDataset example 1 (Python window)", "description": "This is a Python sample for the CreatePansharpenedRasterDataset tool.", "code": "import arcpy arcpy.CreatePansharpenedRasterDataset_management ( \"c:/data/rgbn.tif\" , \"3\" , \"2\" , \"1\" , \"4\" , \"c:/data/outpan.tif\" , \"c:/data/in_pan.img\" , \"Gram-Schmidt\" , \"\" , \"\" , \"\" , \"\" , \"QuickBird\" )"}, "usage": ["Only the areas that fully overlap will be affected by this tool.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "The four weight values allow you to adjust the pan-sharpening algorithms. ", "Pan-sharpening that is performed on a three-band raster dataset will produce a raster dataset that has three bands.", "Pan-sharpening that is performed on a four-band raster dataset will produce a raster dataset that has four bands."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset that you want to pan-sharpen. Typically, this is a low-resolution multiband raster dataset. ", "dataType": "Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer"}, {"name": "red_channel", "isOptional": false, "description": "The input raster band that you want to display with the red color gun. ", "dataType": "Long"}, {"name": "green_channel", "isOptional": false, "description": "The input raster band that you want to display with the green color gun. ", "dataType": "Long"}, {"name": "blue_channel", "isOptional": false, "description": "The input raster band that you want to display with the blue color gun. ", "dataType": "Long"}, {"name": "infrared_channel", "isOptional": true, "description": "The input infrared raster band. An infrared band does not exist for every raster dataset. ", "dataType": "Long"}, {"name": "out_raster_dataset", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "in_panchromatic_image", "isInputFile": true, "isOptional": false, "description": "The panchromatic raster dataset. Typically, this is a high-resolution grayscale image. ", "dataType": "Raster Layer"}, {"name": "pansharpening_type", "isOptional": false, "description": "The pan-sharpening method: Older scripts and models may use still the old keyword. With the ArcGIS 10.1 release, this keyword was replaced with \"Simple_Mean\". Both keywords will continue to work, but for clarity it may be worthwhile to update it to the new keyword. IHS \u2014 Uses Intensity, Hue, and Saturation color space for data fusion. BROVEY \u2014 Uses the Brovey algorithm based on spectral modeling for data fusion. Esri \u2014 Uses the ESRI algorithm based on spectral modeling for data fusion. SIMPLE_MEAN \u2014 Uses the averaged value between the red, green, and blue values and the panchromatic pixel value. Gram-Schmidt \u2014 Uses the Gram-Schmidt spectral sharpening algorithm to sharpen multispectral data.", "dataType": "String"}, {"name": "red_weight", "isOptional": true, "description": "The weight value for the red band. ", "dataType": "Double"}, {"name": "green_weight", "isOptional": true, "description": "The weight value for the green band. ", "dataType": "Double"}, {"name": "blue_weight", "isOptional": true, "description": "The weight value for the blue band. ", "dataType": "Double"}, {"name": "infrared_weight", "isOptional": true, "description": "The weight value for the infrared band. This parameter is only valid if an infrared band exists and the infrared option is used. ", "dataType": "Double"}]},
{"syntax": "createorthocorrectedrasterdataset_management (in_raster, out_raster_dataset, Ortho_type, constant_elevation, {in_DEM_raster}, {ZFactor}, {ZOffset}, {Geoid})", "name": "Create Ortho Corrected Raster Dataset (Data Management)", "description": "Creates an orthocorrected raster dataset using the rational polynomial coefficients (RPC) associated with a raster dataset. Learn more about orthorectifying a raster dataset", "example": {"title": "CreateOrthoCorrectedRasterDataset example (Python window)", "description": "This is a Python sample for the CreateOrthoCorrectedRasterDataset tool.", "code": "import arcpy arcpy.CreateOrthoCorrectedRasterDataset_management ( \"c:/data/RPCdata.tif\" , \"c:/data/orthoready.tif\" , \"DEM\" , \"#\" , \"c:/data/DEM.img\" , \"#\" , \"10\" , \"GEOID\" )"}, "usage": ["To orthocorrect a raster dataset, the raster must have RPCs associated with it.", "For a more accurate result, you should use the digital elevation model (DEM) option for elevation. A DEM should be used in the orthocorrection process so that the elevation and curvatures of the earth can be taken into account.", "If a DEM is used to orthocorrect the raster dataset, the constant elevation value will not be used.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "Check the ", "Geoid", " parameter if you would like the orthocorrection process to assume the earth is a geoid."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that you want to orthocorrect. This raster must have rational polynomial coefficients associated with it. ", "dataType": "Raster Layer"}, {"name": "out_raster_dataset", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "Ortho_type", "isOptional": false, "description": "The type of elevation to use in the orthorectification process. CONSTANT_ELEVATION \u2014 Uses a specified elevation value. DEM \u2014 Uses a specified digital elevation model raster.", "dataType": "String"}, {"name": "constant_elevation", "isOptional": false, "description": "The constant elevation value to be used when the ortho_type parameter is CONSTANT_ELEVATION. If a DEM is used in the orthocorrection process, this value is not used. ", "dataType": "Double"}, {"name": "in_DEM_raster", "isInputFile": true, "isOptional": true, "description": "The digital elevation model rasterto be used for orthorectification when the ortho_type parameter is DEM. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "ZFactor", "isOptional": true, "description": "The scaling factor used to convert the elevation values in the DEM. This is used for two purposes: first, to convert the elevation units (such as meters or feet) to the horizontal coordinate units of the dataset, which may be feet, meters, or degrees; and second, to add vertical exaggeration for visual effect. Learn more about the z factor ", "dataType": "Double"}, {"name": "ZOffset", "isOptional": true, "description": "The base value to be added to the elevation value in the DEM. This could be used to offset elevation values that do not start at sea level. ", "dataType": "Double"}, {"name": "Geoid", "isOptional": true, "description": "Indicates if you would like the orthocorrection process to assume the earth is a geoid or a sphere. NONE \u2014 No geoid correction is made. This is the default. GEOID \u2014 A geoid correction will be made. This will apply the geoid (EGM96) correction to the z values.", "dataType": "Boolean"}]},
{"syntax": "CompositeBands_management (in_rasters, out_raster)", "name": "Composite Bands (Data Management)", "description": "Creates a single raster dataset from multiple bands and can also create a raster dataset using only  a subset of bands.", "example": {"title": null, "description": "This is a Python sample for the Composite Bands tool.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.CompositeBands_management ( \"band1.tif;band2.tif;band3.tif\" , \"compbands.tif\" )"}, "usage": ["This tool can also create a raster dataset containing subset of the original raster dataset bands. This is useful if you need to create a new raster dataset with a specific band combination and order.", "The order that the bands are listed in the Multi-value Input control box will determine the order of the bands in the output raster dataset.", "This tool can only output a square cell size.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "The output raster dataset takes the cell size from the first raster band in the list.", "By default, the output raster dataset takes the extent and the spatial reference of the first raster band with a spatial reference in the list. You can change this by setting the output extent and output coordinate system in the Environment Settings.", "The following are some examples of why you would want to combine single raster datasets into multiband raster datasets:"], "parameters": [{"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": "The input raster datasets. ", "dataType": "Mosaic Dataset ; Mosaic Layer ; Raster Dataset ; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}]},
{"syntax": "Clip_management (in_raster, rectangle, out_raster, {in_template_dataset}, {nodata_value}, {clipping_geometry})", "name": "Clip (Data Management)", "description": "Creates a spatial subset of a raster, including a raster dataset, mosaic dataset, or image service layer.", "example": {"title": "Clip example 1 (Python window)", "description": "This is a Python sample for the Clip tool.", "code": "import arcpy arcpy.Clip_management ( \"c:/data/image.tif\" , \"1952602 294196 1953546 296176\" , \"c:/data/clip.gdb/clip01\" , \"#\" , \"#\" , \"NONE\" )"}, "usage": ["This  tool allows you to extract a portion of a raster dataset based on a template extent. The clip output includes any pixels that intersect the template extent.", "The clipped area is specified either by a rectangular envelope using minimum and maximum x- and y-coordinates or by using an output extent file. If the clip extent specified is not aligned with the input raster dataset, this tool makes sure that the proper alignment is used. This may cause the output to have a slightly different extent than specified in the tool.", "An existing raster or vector layer can be used as the clip extent. If you are using a feature class as the output extent, you have the option to clip the raster by the minimum bounding rectangle of the feature class or by the polygon geometry of the features. If clipping geometry is used, then the pixel depth of the output may be promoted. Therefore, you need to make sure that the output format can support the proper pixel depth.", "When using ArcMap, you also have the ability to use the selected features as the clipping extent. If a feature within the feature class is selected and ", "Selection Extent", " is checked (", "clipping_geometry", " is set to ClippingGeometry), then the output clips out the areas that are selected. If a feature within the feature class is selected but ", "Selection Extent", " is not checked, then the output clips out the minimum bounding rectangle for that feature.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "If the clip extent specified is not aligned with the input raster dataset, this tool makes sure that the proper alignment is used. This may cause the output to have a slightly different extent than specified in the tool.", "The extent values must be in the same spatial coordinates and units as the raster dataset."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer"}, {"name": "rectangle", "isOptional": false, "description": "The four coordinates defining the minimum bounding rectangle to be clipped are defined in this order: X-Minimum, Y-Minimum, X-Maximum, Y-Maximum. If the clip extent specified is not aligned with the input raster dataset, the Clip tool makes sure that the proper alignment is used. This may cause the output to have a slightly different extent than specified in the tool. ", "dataType": "Envelope"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset. Make sure that this output format is able to support the proper pixel depth. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "in_template_dataset", "isInputFile": true, "isOptional": true, "description": "An existing raster or vector layer that can be used as the clip extent. The clip output includes any pixels that intersect the minimum bounding rectangle. If a feature class is used as the output extent and you want to clip the raster based on the polygon features, use the clipping_geometry option. If this is checked, then the pixel depth of the output may be promoted. Therefore, you need to make sure that the output format can support the proper pixel depth. ", "dataType": "Raster Layer; Feature Layer"}, {"name": "nodata_value", "isOptional": true, "description": "All the pixels with the specified value will be set to NoData in the output raster dataset. ", "dataType": "String"}, {"name": "clipping_geometry", "isOptional": true, "description": "If you are using a feature class as the output extent, you have the option to clip the raster by the extent of the feature class or by its polygon perimeter. If clipping geometry is used, then the pixel depth of the output may be promoted. Therefore, you need to make sure that the output format can support the proper pixel depth. NONE \u2014 The raster dataset is clipped based on the minimum bounding rectangle of the feature class. ClippingGeometry \u2014 The raster dataset is clipped based on the perimeter of the polygon shape. ", "dataType": "Boolean"}]},
{"syntax": "WorkspaceToRasterDataset_management (in_workspace, in_raster_dataset, {include_subdirectories}, {mosaic_type}, {colormap}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {mosaicking_tolerance}, {MatchingMethod}, {colormap_to_RGB})", "name": "Workspace To Raster Dataset (Data Management)", "description": "Mosaics all the raster datasets stored within the specified workspace into one raster dataset.", "example": {"title": "WorkspaceToRasterDataset example 1 (Python Window)", "description": "This is a Python sample for the WorkspaceToRasterDataset tool.", "code": "import arcpy arcpy.WorkspaceToRasterDataset_management ( \"c:/data/WS2RD\" , \"c:/fgdb.gdb/outdats\" , \"INCLUDE_SUBDIRECTORIES\" , \"LAST\" , \"FIRST\" , \"0\" , \"9\" , \"\" , \"\" , \"HISTOGRAM_MATCHING\" , \"\" )"}, "usage": ["The target raster dataset must already exist for the tool to run.", "If a target raster dataset does not already exist, use the ", "Create Raster Dataset", " tool to create a new raster dataset.", "Since mosaicking will take place, you will need to specify the mosaic method and colormap mode to use.", "If the target raster dataset is an empty raster dataset, the cell size and spatial reference of the first input raster dataset will be applied to the mosaic.", "Whenever possible, use the Last ", "Mosaic Operator", " to mosaic raster datasets to an existing raster dataset in a file geodatabase or ArcSDE geodatabase; it is by far the most effective way to mosaic.", "For mosaicking of discrete data, First, Minimum, or Maximum ", "Mosaic Operator", " options will provide the most meaningful results. The Blend and Mean ", "Mosaic Operator", " options are best suited for continuous data.", "The color matching method drop-down arrow allows you to choose an algorithm to color match the datasets in your mosaic.", "For file-based rasters and personal geodatabase rasters, the ", "Ignore Background Value", " must be set to the same value as ", "NoData", " in order for the background value to be ignored. File geodatabase rasters and ArcSDE rasters will work without this extra step.", "For floating-point input raster datasets of different resolutions or when cells are not aligned, it is recommended to resample all the data using bilinear interpolation or cubic convolution before running ", "Mosaic", "; otherwise, ", "Mosaic", " will automatically resample the raster datasets using nearest neighbor resampling, which is not appropriate for continuous data types."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The workspace that contains all the raster datasets to be mosaicked into the raster dataset. ", "dataType": "Workspace"}, {"name": "in_raster_dataset", "isInputFile": true, "isOptional": false, "description": "The raster dataset that will have all the rasters from the workspace mosaicked into it. The raster dataset must already exist. ", "dataType": "Raster Dataset"}, {"name": "include_subdirectories", "isOptional": true, "description": "Specify whether to include subdirectories. NONE \u2014 Does not include subdirectories. This is the default. INCLUDE_SUBDIRECTORIES \u2014 Includes all the raster datasets within the subdirectories when loading. ", "dataType": "Boolean"}, {"name": "mosaic_type", "isOptional": true, "description": "The method used to mosaic overlapping areas. For more information about each mosaic operator, refer to Mosaic Operator . FIRST \u2014 The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location. LAST \u2014 The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default. BLEND \u2014 The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area. MEAN \u2014 The output cell value of the overlapping areas will be the average value of the overlapping cells. MINIMUM \u2014 The output cell value of the overlapping areas will be the minimum value of the overlapping cells. MAXIMUM \u2014 The output cell value of the overlapping areas will be the maximum value of the overlapping cells. ", "dataType": "String"}, {"name": "colormap", "isOptional": true, "description": "The method used to choose which color map from the input rasters will be applied to the mosaic output. For more information about each colormap mode, refer to Mosaic colormap mode . FIRST \u2014 The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default. LAST \u2014 The color map from the last raster dataset in the list will be applied to the output raster mosaic. MATCH \u2014 Will take all the color maps into consideration when mosaicking. If all possible values are already used (for the bit depth), it will attempt to match the value with the closest color that is available. REJECT \u2014 Only the raster datasets that do not have a color map associated with them will be mosaicked.", "dataType": "String"}, {"name": "background_value", "isOptional": true, "description": "Use this option to remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values within the raster dataset. The pixel value specified will be set to NoData in the output raster dataset. For file-based rasters and personal geodatabase rasters, the Ignore Background Value must be set to the same value as NoData in order for the background value to be ignored. ArcSDE and file geodatabase rasters will work without this extra step. ", "dataType": "Double"}, {"name": "nodata_value", "isOptional": true, "description": "All the pixels with the specified value will be set to NoData in the output raster dataset. ", "dataType": "Double"}, {"name": "onebit_to_eightbit", "isOptional": true, "description": "Choose whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to ArcSDE. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in ArcSDE, 1-bit raster datasets can only have 1-bit pyramid layers, which makes the display unpleasant. By converting the data to 8-bit in ArcSDE, the pyramid layers are built as 8-bit instead of 1-bit, resulting in a proper raster dataset in the display. NONE \u2014 No conversion will be done. This is the default. OneBitTo8Bit \u2014 The input raster will be converted.", "dataType": "Boolean"}, {"name": "mosaicking_tolerance", "isOptional": true, "description": "When mosaicking takes place, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, a decision needs to be made whether resampling takes place, or whether the data should be shifted. The mosaicking tolerance controls whether resampling of the pixels take place, or if the pixels should be shifted. If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will take place. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not take place (instead, a shift is performed). The unit of tolerance is a pixel, where the valid value range is 0 to 0.5. A tolerance of 0.5 will guarantee a shift takes place. A tolerance of zero guarantees resampling, if there is a misalignment in pixels. For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, then resampling will take place since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, then the pixels will be shifted. ", "dataType": "Double"}, {"name": "MatchingMethod", "isOptional": true, "description": "Choose the color matching method to apply to the rasters. NONE \u2014 This option will not use the color matching operation when mosaicking your raster datasets. STATISTICS_MATCHING \u2014 This method will match the statistical differences (minimum, maximum, and mean) between the reference overlap area and the source overlap area; the transformation will then be applied to the entire target dataset. HISTOGRAM_MATCHING \u2014 This method will match the histogram from the reference overlap area with the source overlap area; the transformation will then be applied to the entire target. LINEARCORRELATION_MATCHING \u2014 This method will match overlapped pixels and interpolate to the rest of the source; pixels that do not have a one-to-one relationship will use a weighted average. ", "dataType": "String"}, {"name": "colormap_to_RGB", "isOptional": true, "description": "If the input raster dataset has a color map, the output raster dataset can be converted to a three-band output raster dataset. This is useful when mosaicking rasters with different color maps. NONE \u2014 No conversion will occur. This is the default. ColormapToRGB \u2014 The input dataset will be converted.", "dataType": "Boolean"}]},
{"syntax": "RasterCatalogToRasterDataset_management (in_raster_catalog, out_raster_dataset, {where_clause}, {mosaic_type}, {colormap}, {order_by_field}, {ascending}, {pixel_type}, {ColorBalancing}, {MatchingMethod}, {ReferenceRaster}, {OID})", "name": "Raster Catalog To Raster Dataset (Data Management)", "description": "Mosaics the contents of a raster catalog into a new raster dataset.", "example": {"title": "RasterCatalogToRasterDataset example 1 (Python window)", "description": "This is a Python sample for the RasterCatalogToRasterDataset tool.", "code": "import arcpy arcpy.RasterCatalogToRasterDataset_management ( \"c:/data/fgdb.gdb/catalog1\" , \"c:/data/dataset.tif\" , \"OBJECTID>1\" , \"LAST\" , \"FIRST\" , \"\" , \"\" , \"8_BIT_UNSIGNED\" , \"COLOR_BALANCING\" , \"HISTOGRAM_MATCHING\" , \"CALCULATE_FROM_ALL\" , \"\" )"}, "usage": ["This tool allows you to convert your geodatabase raster catalog into a raster dataset; the input is a raster catalog and the output is a new raster dataset. This tool cannot mosaic data into an existing raster dataset.", "There are several advantages of using a mosaicked raster dataset: it tends to display faster at any scale, saves space since there is no overlapping data, and the data tends to display with fewer seams.", "You must set the pixel type to match your existing input raster datasets. If you do not set the pixel type, the 8-bit default will be used and your output might turn out incorrectly.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings.", "The GIF format only supports single-band raster datasets.", "The overlapping areas of the mosaic can be handled in several ways\u2014for example, you can set the tool to keep only the first raster dataset's data, or you can blend the overlapping cell values. There are also several options to determine how to handle a color map, if the raster dataset uses one. For example, you can keep the color map of the last raster dataset used in the mosaic.", "For mosaicking of discrete data, First, Minimum, or Maximum ", "Mosaic Operator", " options will provide the most meaningful results. The Blend and Mean ", "Mosaic Operator", " options are best suited for continuous data.", "Whenever possible, use the Last ", "Mosaic Operator", " to mosaic raster datasets to an existing raster dataset in a file geodatabase or ArcSDE geodatabase; it is by far the most effective way to mosaic.", "When mosaicking with raster datasets containing color maps, it is important to note differences across the color maps for each raster dataset you choose to mosaic. In this situation, use the ", "Mosaic", " tool for raster with different color maps; however, you must choose the proper ", "Mosaic Colormap Mode operator", ". If an improper colormap mode is chosen, your output might not turn out as you expected.", "For floating-point input raster datasets of different resolutions or when cells are not aligned, it is recommended to resample all the data using bilinear interpolation or cubic convolution before running ", "Mosaic", "; otherwise, ", "Mosaic", " will automatically resample the raster datasets using nearest neighbor resampling, which is not appropriate for continuous data types.", "Color matching and color correction can be used to make the raster mosaic more seamless."], "parameters": [{"name": "in_raster_catalog", "isInputFile": true, "isOptional": false, "description": "The raster catalog that will be mosaicked to a raster dataset. ", "dataType": "Raster Catalog Layer"}, {"name": "out_raster_dataset", "isOutputFile": true, "isOptional": false, "description": "The name and extension of the output raster dataset mosaic. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "where_clause", "isOptional": true, "description": "Enter the appropriate SQL statement to select specific rows in the raster catalog. ", "dataType": "SQL Expression"}, {"name": "mosaic_type", "isOptional": true, "description": "The method used to mosaic overlapping areas. For more information about each mosaic operator, refer to Mosaic Operator . FIRST \u2014 The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location. LAST \u2014 The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default. BLEND \u2014 The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area. MEAN \u2014 The output cell value of the overlapping areas will be the average value of the overlapping cells. MINIMUM \u2014 The output cell value of the overlapping areas will be the minimum value of the overlapping cells. MAXIMUM \u2014 The output cell value of the overlapping areas will be the maximum value of the overlapping cells. ", "dataType": "String"}, {"name": "colormap", "isOptional": true, "description": "The method used to choose which color map from the input rasters will be applied to the mosaic output. For more information about each colormap mode, refer to Mosaic colormap mode . FIRST \u2014 The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default. LAST \u2014 The color map from the last raster dataset in the list will be applied to the output raster mosaic. MATCH \u2014 Will take all the color maps into consideration when mosaicking. If all possible values are already used (for the bit depth), it will attempt to match the value with the closest color that is available. REJECT \u2014 Only the raster datasets that do not have a color map associated with them will be mosaicked.", "dataType": "String"}, {"name": "order_by_field", "isOptional": true, "description": "Define the field by which to order the raster catalog items. ", "dataType": "String"}, {"name": "ascending", "isOptional": true, "description": "Choose whether to use the ascending value of the Order By field. If the Ascending option is not used, the descending order will be used. Ascending \u2014 The ascending order of the rows will be followed in the mosaic procedure. This is the default. None \u2014 The descending order of the rows will be followed in the mosaic procedure. ", "dataType": "Boolean"}, {"name": "pixel_type", "isOptional": true, "description": "Determines the bit depth of the output raster dataset. If left unspecified, the output bit depth will be the same as the input. There will be no rescaling of the raster values when a different pixel type is chosen. If the pixel type is demoted (lowered), the raster values outside the valid range for that pixel depth will be truncated and lost. 1_BIT \u2014 A 1-bit unsigned integer. The values can be 0 or 1. 2_BIT \u2014 A 2-bit unsigned integer. The values supported can be from 0 to 3. 4_BIT \u2014 A 4-bit unsigned integer. The values supported can be from 0 to 15. 8_BIT_UNSIGNED \u2014 An unsigned 8-bit data type. The values supported can be from 0 to 255. 8_BIT_SIGNED \u2014 A signed 8-bit data type. The values supported can be from -128 to 127. 16_BIT_UNSIGNED \u2014 A 16-bit unsigned data type. The values can range from 0 to 65,535. 16_BIT_SIGNED \u2014 A 16-bit signed data type. The values can range from -32,768 to 32,767. 32_BIT_UNSIGNED \u2014 A 32-bit unsigned data type. The values can range from 0 to 4,294,967,295. 32_BIT_SIGNED \u2014 A 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647. 32_BIT_FLOAT \u2014 A 32-bit data type supporting decimals. 64_BIT \u2014 A 64-bit data type supporting decimals.", "dataType": "String"}, {"name": "ColorBalancing", "isOptional": true, "description": "Choose whether or not to use a dodging technique to color correct the raster catalog items. All pixels in the raster catalog will be used to determine the gamma and contrast values for the color-balancing algorithm. NONE \u2014 Color balancing will not be performed in the mosaic procedure. This is the default. ColorBalancing \u2014 Color balancing will be performed in the mosaic procedure. ", "dataType": "Boolean"}, {"name": "MatchingMethod", "isOptional": true, "description": "Choose the color matching method to apply to the rasters. NONE \u2014 This option will not use the color matching operation when mosaicking your raster datasets. STATISTICS_MATCHING \u2014 This method will match the statistical differences (minimum, maximum, and mean) between the reference overlap area and the source overlap area; the transformation will then be applied to the entire target dataset. HISTOGRAM_MATCHING \u2014 This method will match the histogram from the reference overlap area with the source overlap area; the transformation will then be applied to the entire target. LINEARCORRELATION_MATCHING \u2014 This method will match overlapped pixels and interpolate to the rest of the source; pixels that do not have a one-to-one relationship will use a weighted average. ", "dataType": "String"}, {"name": "ReferenceRaster", "isOptional": true, "description": "If color matching is applied, choose how to specify the reference raster. Older scripts and models may still use the old keyword. With the ArcGIS 10.1 release, this keyword was replaced with \"CALCULATE_FROM_ALL\" . Both keywords will continue to work, though for clarity it may be worthwhile to update it to the new keyword. CALCULATE_FROM_ALL \u2014 The system will calculate the best raster dataset to use, based on all the raster catalog items. SPECIFY_OID \u2014 The user will type in the Object ID (OID) of the raster catalog item to use as the reference raster. DEFINE_FROM_SELECTION \u2014 The system will calculate the best raster dataset to use, based on the raster catalog items that are selected. ", "dataType": "String"}, {"name": "OID", "isOptional": true, "description": "The object ID (OID) of the reference raster. The OID is a unique key field in the raster catalog. ", "dataType": "Long"}]},
{"syntax": "MosaicToNewRaster_management (input_rasters, output_location, raster_dataset_name_with_extension, {coordinate_system_for_the_raster}, {pixel_type}, {cellsize}, number_of_bands, {mosaic_method}, {mosaic_colormap_mode})", "name": "Mosaic To New Raster (Data Management)", "description": "Mosaics multiple raster datasets into a new raster dataset.", "example": {"title": "MosaicToNewRaster example 1 (Python window)", "description": "This is a Python sample for the MosaicToNewRaster tool.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.MosaicToNewRaster_management ( \"land1.tif;land2.tif\" , \"Mosaic2New\" , \"landnew.tif\" , \"World_Mercator.prj\" , \"8_BIT_UNSIGNED\" , \"40\" , \"1\" , \"LAST\" , \"FIRST\" )"}, "usage": ["The input raster datasets are all the raster datasets you would like to mosaic together. The inputs must have the same number of bands and same bit depth; otherwise, the tool will exit with an error message.", "When working with a large number of raster datasets, the ", "Raster Catalog To Raster Dataset", " tool performs more efficiently. ", "The ", "Mosaic", " tool has more parameters available when combining datasets into an existing raster, such as options to ignore background and nodata values.", "You must set the pixel type to match your existing input raster datasets. If you do not set the pixel type, the 8-bit default will be used and your output may be incorrect.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings.", "The GIF format only supports single-band raster datasets.", "When mosaicking with raster datasets containing color maps, it is important to note differences across the color maps for each raster dataset you choose to mosaic. In this situation, use the ", "Mosaic", " tool for raster with different color maps; however, you must choose the proper ", "Mosaic Colormap Mode operator", ". If an improper colormap mode is chosen, your output might not turn out as you expected.", "This tool does not honor the ", "Output extent", " environment setting for ArcSDE. If you want a specific extent for your output raster, consider using the ", "Clip", " tool. You can either clip the input rasters prior to using this tool, or clip the output of this tool."], "parameters": [{"name": "input_rasters", "isInputFile": true, "isOptional": false, "description": "The input raster datasets. ", "dataType": "Mosaic Dataset; Composite Layer; Raster Dataset; Raster layer"}, {"name": "output_location", "isOutputFile": true, "isOptional": false, "description": "The path to contain the raster dataset. The path can be to a folder or geodatabase. ", "dataType": "Workspace; Raster Catalog"}, {"name": "raster_dataset_name_with_extension", "isOptional": false, "description": "The name and extension of the raster dataset to be created. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "String"}, {"name": "coordinate_system_for_the_raster", "isOptional": true, "description": "Specifies the map projection of the output raster mosaic. ", "dataType": "Coordinate System"}, {"name": "pixel_type", "isOptional": true, "description": "Specifies the bit depth of the output raster dataset. You must set the pixel type to match your existing input raster datasets. If you do not set the pixel type, the 8-bit default will be used and your output may be incorrect. 1_BIT \u2014 A 1-bit unsigned integer. The values can be 0 or 1. 2_BIT \u2014 A 2-bit unsigned integer. The values supported can be from 0 to 3. 4_BIT \u2014 A 4-bit unsigned integer. The values supported can be from 0 to 15. 8_BIT_UNSIGNED \u2014 An unsigned 8-bit data type. The values supported can be from 0 to 255. 8_BIT_SIGNED \u2014 A signed 8-bit data type. The values supported can be from -128 to 127. 16_BIT_UNSIGNED \u2014 A 16-bit unsigned data type. The values can range from 0 to 65,535. 16_BIT_SIGNED \u2014 A 16-bit signed data type. The values can range from -32,768 to 32,767. 32_BIT_UNSIGNED \u2014 A 32-bit unsigned data type. The values can range from 0 to 4,294,967,295. 32_BIT_SIGNED \u2014 A 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647. 32_BIT_FLOAT \u2014 A 32-bit data type supporting decimals. 64_BIT \u2014 A 64-bit data type supporting decimals.", "dataType": "String"}, {"name": "cellsize", "isOptional": true, "description": "The cell size for the new raster dataset. ", "dataType": "Double"}, {"name": "number_of_bands", "isOptional": false, "description": "The number of bands to be contained by the raster dataset. ", "dataType": "Long"}, {"name": "mosaic_method", "isOptional": true, "description": "The method used to mosaic overlapping areas. For more information about each mosaic operator, refer to Mosaic Operator . FIRST \u2014 The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location. LAST \u2014 The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default. BLEND \u2014 The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area. MEAN \u2014 The output cell value of the overlapping areas will be the average value of the overlapping cells. MINIMUM \u2014 The output cell value of the overlapping areas will be the minimum value of the overlapping cells. MAXIMUM \u2014 The output cell value of the overlapping areas will be the maximum value of the overlapping cells. ", "dataType": "String"}, {"name": "mosaic_colormap_mode", "isOptional": true, "description": "Applies when the input raster datasets have a colormap. The method used to choose which color map from the input rasters will be applied to the mosaic output. For more information about each colormap mode, refer to Mosaic colormap mode . FIRST \u2014 The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default. LAST \u2014 The color map from the last raster dataset in the list will be applied to the output raster mosaic. MATCH \u2014 Will take all the color maps into consideration when mosaicking. If all possible values are already used (for the bit depth), it will attempt to match the value with the closest color that is available. REJECT \u2014 Only the raster datasets that do not have a color map associated with them will be mosaicked.", "dataType": "String"}]},
{"syntax": "Mosaic_management (inputs, target, {mosaic_type}, {colormap}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {mosaicking_tolerance}, {MatchingMethod})", "name": "Mosaic (Data Management)", "description": "Mosaics multiple input rasters into an existing raster dataset. \r\n Learn more about mosaicking raster datasets \r\n", "example": {"title": "Mosaic example 1 (Python window)", "description": "This is a Python sample for the Mosaic tool.", "code": "import arcpy from arcpy import env env.workspace = \"c:/data\" arcpy.Mosaic_management ( \"land2.tif;land3.tif\" , \"land1.tif\" , \"LAST\" , \"FIRST\" , \"0\" , \"9\" , \"\" , \"\" , \"\" )"}, "usage": ["The target raster must be an existing raster dataset, which can be an empty raster dataset or one already containing data.", "Mosaic is useful when two or more adjacent raster datasets need to be merged into one entity. Some mosaic techniques can help minimize the abrupt changes along the boundaries of the overlapping rasters.", "The overlapping areas of the mosaic can be handled in several ways\u2014for example, you can set the tool to keep only the first raster dataset's data, or you can blend the overlapping cell values. There are also several options to determine how to handle a color map, if the raster dataset uses one. For example, you can keep the color map of the last raster dataset used in the mosaic.", "The Target layer is considered the first raster in the list of Input Rasters.", "For mosaicking of discrete data, First, Minimum, or Maximum ", "Mosaic Operator", " options will provide the most meaningful results. The Blend and Mean ", "Mosaic Operator", " options are best suited for continuous data.", "Whenever possible, use the Last ", "Mosaic Operator", " to mosaic raster datasets to an existing raster dataset in a file geodatabase or ArcSDE geodatabase; it is by far the most effective way to mosaic.", "The pixel type will be the same as the target raster dataset.", "For file-based rasters and personal geodatabase rasters, the ", "Ignore Background Value", " must be set to the same value as ", "NoData", " in order for the background value to be ignored. File geodatabase rasters and ArcSDE rasters will work without this extra step.", "When mosaicking with raster datasets containing color maps, it is important to note differences across the color maps for each raster dataset you choose to mosaic. You are still able to use the Mosaic tool even if the raster datasets have different color maps; however, you must choose the proper color map mode. If an improper color map mode is chosen, your output might not turn out as you expected.", "The ", "Color Matching Method", " allows you to choose an algorithm to color match the datasets in your mosaic.", "For floating-point input raster datasets of different resolutions or when cells are not aligned, it is recommended to resample all the data using bilinear interpolation or cubic convolution before running ", "Mosaic", "; otherwise, ", "Mosaic", " will automatically resample the raster datasets using nearest neighbor resampling, which is not appropriate for continuous data types.", "The Mosaic tool doesn't use the ", "output extent", " because the tool tends to create very large raster datasets and the output extent setting might accidentally clip your data. If the output extent does need to be adjusted, the ", "Clip tool", " can achieve that operation."], "parameters": [{"name": "inputs", "isOptional": false, "description": "The input raster datasets. ", "dataType": "Mosaic Dataset ; Composite Layer ; Raster Dataset ; Raster Layer"}, {"name": "target", "isOptional": false, "description": "The target raster dataset. This raster dataset must already exist. The Target layer is considered the first raster in the list of Input Rasters. ", "dataType": "Raster Dataset"}, {"name": "mosaic_type", "isOptional": true, "description": "The method used to mosaic overlapping areas. The Target layer is considered the first raster in the list of Input Rasters. For more information about each mosaic operator, refer to Mosaic Operator . FIRST \u2014 The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location. LAST \u2014 The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default. BLEND \u2014 The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area. MEAN \u2014 The output cell value of the overlapping areas will be the average value of the overlapping cells. MINIMUM \u2014 The output cell value of the overlapping areas will be the minimum value of the overlapping cells. MAXIMUM \u2014 The output cell value of the overlapping areas will be the maximum value of the overlapping cells. ", "dataType": "String"}, {"name": "colormap", "isOptional": true, "description": "The method used to choose which color map from the input rasters will be applied to the mosaic output. The Target layer is considered the first raster in the list of Input Rasters. For more information about each colormap mode, refer to Mosaic colormap mode . FIRST \u2014 The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default. LAST \u2014 The color map from the last raster dataset in the list will be applied to the output raster mosaic. MATCH \u2014 Will take all the color maps into consideration when mosaicking. If all possible values are already used (for the bit depth), it will attempt to match the value with the closest color that is available. REJECT \u2014 Only the raster datasets that do not have a color map associated with them will be mosaicked.", "dataType": "String"}, {"name": "background_value", "isOptional": true, "description": "Use this option to remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values within the raster dataset. The pixel value specified will be set to NoData in the output raster dataset. For file-based rasters and personal geodatabase rasters, the Ignore Background Value must be set to the same value as NoData in order for the background value to be ignored. ArcSDE and file geodatabase rasters will work without this extra step. ", "dataType": "Double"}, {"name": "nodata_value", "isOptional": true, "description": "All the pixels with the specified value will be set to NoData in the output raster dataset. ", "dataType": "Double"}, {"name": "onebit_to_eightbit", "isOptional": true, "description": "Choose whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to ArcSDE. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in ArcSDE, 1-bit raster datasets can only have 1-bit pyramid layers, which makes the display unpleasant. By converting the data to 8-bit in ArcSDE, the pyramid layers are built as 8-bit instead of 1-bit, resulting in a proper raster dataset in the display. NONE \u2014 No conversion will be done. This is the default. OneBitTo8Bit \u2014 The input raster will be converted.", "dataType": "Boolean"}, {"name": "mosaicking_tolerance", "isOptional": true, "description": "When mosaicking takes place, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, a decision needs to be made whether resampling takes place, or whether the data should be shifted. The mosaicking tolerance controls whether resampling of the pixels take place, or if the pixels should be shifted. If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will take place. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not take place (instead, a shift is performed). The unit of tolerance is a pixel, where the valid value range is 0 to 0.5. A tolerance of 0.5 will guarantee a shift takes place. A tolerance of zero guarantees resampling, if there is a misalignment in pixels. For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, then resampling will take place since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, then the pixels will be shifted. ", "dataType": "Double"}, {"name": "MatchingMethod", "isOptional": true, "description": "Choose the color matching method to apply to the rasters. NONE \u2014 This option will not use the color matching operation when mosaicking your raster datasets. STATISTICS_MATCHING \u2014 This method will match the statistical differences (minimum, maximum, and mean) between the reference overlap area and the source overlap area; the transformation will then be applied to the entire target dataset. HISTOGRAM_MATCHING \u2014 This method will match the histogram from the reference overlap area with the source overlap area; the transformation will then be applied to the entire target. LINEARCORRELATION_MATCHING \u2014 This method will match overlapped pixels and interpolate to the rest of the source; pixels that do not have a one-to-one relationship will use a weighted average. ", "dataType": "String"}]},
{"syntax": "CreateRasterDataset_management (out_path, out_name, {cellsize}, pixel_type, {raster_spatial_reference}, number_of_bands, {config_keyword}, {pyramids}, {tile_size}, {compression}, {pyramid_origin})", "name": "Create Raster Dataset (Data Management)", "description": "Creates a raster dataset as a file or in a geodatabase.", "example": {"title": "CreateRasterDataset example 1 (Python window)", "description": "This is a Python sample for the CreateRasterDataset tool.", "code": "import arcpy arcpy.CreateRasterDataset_management ( \"c:/data\" , \"EmptyTIFF.tif\" , \"2\" , \"8_BIT_UNSIGNED\" , \"World_Mercator.prj\" , \"3\" , \"\" , \"PYRAMIDS -1 NEAREST JPEG\" , \"128 128\" , \"NONE\" , \"\" )"}, "usage": ["When you create a raster dataset, you are creating an empty location to contain a single raster dataset. You can then mosaic or load raster datasets into this location.", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings.", "The GIF format only supports single-band raster datasets.", "Building ", "pyramids", " improves the display performance of raster datasets.", "Calculating ", "statistics", " allows ArcGIS applications to properly stretch and symbolize raster data for display."], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The output location to contain the raster dataset. ", "dataType": "Workspace; Raster Catalog"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the raster dataset to be created. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "String"}, {"name": "cellsize", "isOptional": true, "description": "The cell size for the new raster dataset. ", "dataType": "Double"}, {"name": "pixel_type", "isOptional": false, "description": "Specifies the data type of the cell values. The default value for this is 8-bit unsigned integer. Not all data types are supported by all raster formats. Check the Supported raster dataset file formats help topic to be sure the format you are using will support the data type you need. 1_BIT \u2014 A 1-bit unsigned integer. The values can be 0 or 1. 2_BIT \u2014 A 2-bit unsigned integer. The values supported can be from 0 to 3. 4_BIT \u2014 A 4-bit unsigned integer. The values supported can be from 0 to 15. 8_BIT_UNSIGNED \u2014 An unsigned 8-bit data type. The values supported can be from 0 to 255. 8_BIT_SIGNED \u2014 A signed 8-bit data type. The values supported can be from -128 to 127. 16_BIT_UNSIGNED \u2014 A 16-bit unsigned data type. The values can range from 0 to 65,535. 16_BIT_SIGNED \u2014 A 16-bit signed data type. The values can range from -32,768 to 32,767. 32_BIT_UNSIGNED \u2014 A 32-bit unsigned data type. The values can range from 0 to 4,294,967,295. 32_BIT_SIGNED \u2014 A 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647. 32_BIT_FLOAT \u2014 A 32-bit data type supporting decimals. 64_BIT \u2014 A 64-bit data type supporting decimals.", "dataType": "String"}, {"name": "raster_spatial_reference", "isOptional": true, "description": "The coordinate system for the raster dataset. If this is not specified, the coordinate system set in the environment settings will be used. ", "dataType": "Coordinate System"}, {"name": "number_of_bands", "isOptional": false, "description": "The number of bands to be contained by the raster dataset. ", "dataType": "Long"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}, {"name": "pyramids", "isOptional": true, "description": "Use this option to create pyramids. For Pyramid Levels, choose a number of -1 or higher. A value of 0 will not build any pyramids, and a value of -1 will automatically choose the correct number of pyramid layers to create. The Pyramid Resampling Technique defines how the data will be resampled when building the pyramids. The Pyramid Compression Type defines the method used when compressing the pyramids. NEAREST\u2014Nearest neighbor should be used for nominal data or raster datasets with color maps, such as land-use or pseudo color images. BILINEAR\u2014Bilinear interpolation is best used with continuous data, such as satellite imagery or aerial photography. CUBIC\u2014Cubic convolution is best used with continuous data, such as satellite imagery or aerial photography. It is similar to bilinear interpolation; however, it resamples the data using a larger matrix. DEFAULT\u2014This uses the compression that is normally used by the raster dataset format. LZ77\u2014A lossless compression. The values of the cells in the raster will not be changed. JPEG\u2014A lossy compression. NONE\u2014No data compression.", "dataType": "Pyramid"}, {"name": "tile_size", "isOptional": true, "description": "The tile width controls the number of pixels you can store in each tile. This is specified as a number of pixels in x. The default tile width is 128. The tile height controls the number of pixels you can store in each tile. This is specified as a number of pixels in y. The default tile height is 128. Only file geodatabases and ArcSDE geodatabases use tile size. ", "dataType": "Tile Size"}, {"name": "compression", "isOptional": true, "description": "This defines the type of data compression that will be used to store the raster dataset. The JPEG and JPEG 2000 compression quality can range from 1 to 100. A higher number means better image quality but less compression. LZ77 \u2014 Lossless. JPEG \u2014 Lossy. Uses compression quality. JPEG 2000 \u2014 Lossy. Uses compression quality. PackBits \u2014 Lossless. Only for the TIFF format. LZW \u2014 Lossless. RLE \u2014 Lossless. CCITT GROUP 3 \u2014 Lossless. Only for the TIFF format. CCITT GROUP 4 \u2014 Lossless. Only for the TIFF format. CCITT (1D) \u2014 Lossless. Only for the TIFF format. NONE \u2014 No data compression.", "dataType": "Compression"}, {"name": "pyramid_origin", "isOptional": true, "description": "This is the origination location of the raster pyramid. It is recommended that you specify this point if you plan on building large mosaics in a file geodatabase or an ArcSDE geodatabase, especially if you plan on mosaicking to them over time (for example, for updating). The pyramid reference point should be set to the upper left corner of your raster dataset. In setting this point for a file geodatabase or an ArcSDE geodatabase, partial pyramiding will be used when updating with a new mosaicked raster dataset. Partial pyramiding updated the parts of the pyramid that do not exist due to the new mosaicked datasets. Therefore, it is good to set your pyramid reference point so that your entire raster mosaic will be below and to the right of this point. However, a pyramid reference point should not be set too large either. ", "dataType": "Point"}]},
{"syntax": "CreateRandomRaster_management (out_path, out_name, {distribution}, {raster_extent}, {cellsize})", "name": "Create Random Raster (Data Management)", "description": "Creates a random raster dataset based on a user-specified distribution and extent.", "example": {"title": "CreateRandomRaster example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CreateRandomRaster function in immediate mode.", "code": "import arcpy arcpy.CreateRandomRaster_management ( \"c:/output\" , \"randrast\" , \"NORMAL 3.0\" , \"0 0 500 500\" , 50 )"}, "usage": ["The values assigned to each cell in the output raster are derived from the random number generator and the selected distribution type. There are several  random number generators available for use, and the one you wish to use is identified in the Environments Settings, in the ", "Random Numbers", " section. The random number generator starts a stream of random numbers based on the generator type and a seed value. These numbers are randomly determined and the values fall between 0 and 1. Each value is independent of the other values.", " Multiple distribution types are available for the random number generators when assigning (or transforming) the values in the output raster. The distributions generally produce different results, and which distribution to select is determined by the end use of the raster. If the random raster is to model some natural phenomenon, the distribution selected should be the best representation of the process of the phenomenon.", " For a description of the distributions and how they are generally used, see ", "Distributions for assigning random values", ".", "The Uniform, Integer, Normal, and Exponential distributions' processing times are independent of their arguments, while the Poisson, Gamma, Binomial, Geometric, and Pascal distributions' processing times can vary considerably when arguments are changed.", "A default value is calculated for the cell size parameter if no value is provided. This value is based on the size of the extent."], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The location of the output raster dataset. ", "dataType": "Workspace;Raster Catalog"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the raster dataset to be created. When not saving to a geodatabase, specify .tif for a TIFF file format, .img for an ERDAS IMAGINE file format, or no extension for a GRID file format. ", "dataType": "String"}, {"name": "distribution", "isOptional": true, "description": "The distribution of random values desired is as follows: The default values are 0.0 for {Minimum} and 1.0 for {Maximum}. The default values are 1 for {Minimum} and 10 for {Maximum}. The default values are 0.0 for {Mean} and 1.0 for {Standard Deviation}. The default value for {Mean} is 1.0. The default value for {Mean} is 1.0. The default values are 10 for {N} and 0.5 for {Probability}. The default value for {Probability} is 0.5. The default values are 10.0 for {r} and 0.5 for {Probability}. UNIFORM {Minimum}, {Maximum}\u2014A uniform distribution with a range defined by the {Minimum} and {Maximum}. Both the {Minimum} and {Maximum} are of type double. The default values are 0.0 for {Minimum} and 1.0 for {Maximum}. INTEGER {Minimum}, {Maximum}\u2014An integer distribution with a range defined by the {Minimum} and {Maximum}. Both the {Minimum} and {Maximum} are of type long. The default values are 1 for {Minimum} and 10 for {Maximum}. NORMAL {Mean}, {Standard Deviation}\u2014A normal distribution with a defined {Mean} and {Standard Deviation}. Both the {Mean} and {Standard Deviation} are of type double. The default values are 0.0 for {Mean} and 1.0 for {Standard Deviation}. EXPONENTIAL {Mean}\u2014An exponential distribution with a defined {Mean}. The {Mean} is of type double. The default value for {Mean} is 1.0. POISSON {Mean}\u2014A Poisson distribution with a defined {Mean}. The {Mean} is of type double. The default value for {Mean} is 1.0. GAMMA {Alpha}, {Beta}\u2014A gamma distribution with a defined {Alpha} and {Beta}. Both the {Alpha} and {Beta} are of type double. The default values are 1.0 for {Alpha} and 1.0 for {Beta}. BINOMIAL {N}, {Probability}\u2014A binomial distribution with a defined {N} and {Probability}. The {N} is of type long and {Probability} is of type double. The default values are 10 for {N} and 0.5 for {Probability}. GEOMETRIC {Probability}\u2014A geometric distribution with a defined {Probability}. The {Probability} is of type double. The default value for {Probability} is 0.5. NEGATIVE BINOMIAL {r}, {Probability}\u2014A Pascal distribution with a defined {r} and {Probability}. The {r} is of type double and {Probability} is of type double. The default values are 10.0 for {r} and 0.5 for {Probability}.", "dataType": "String"}, {"name": "raster_extent", "isOptional": true, "description": "The spatial extent of the random raster dataset. ", "dataType": "Extent"}, {"name": "cellsize", "isOptional": true, "description": "The cell size of the new random raster dataset. ", "dataType": "Double"}]},
{"syntax": "CopyRaster_management (in_raster, out_rasterdataset, {config_keyword}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {colormap_to_RGB}, {pixel_type}, {scale_pixel_value}, {RGB_to_Colormap})", "name": "Copy Raster (Data Management)", "description": "Makes a copy of a raster dataset, loads raster datasets into a raster catalog, or converts a mosaic dataset into a raster dataset.", "example": {"title": "CopyRaster example 1 (Python window)", "description": "This is a Python sample for the CopyRaster tool.", "code": "import arcpy arcpy.CopyRaster_management ( \"c:/data/background.tif\" , \"c:/fdgb/CpRaster.gdb/fgdbRD\" , \"DEFAULTS\" , \"0\" , \"9\" , \"\" , \"\" , \"8_BIT_UNSIGNED\" )"}, "usage": ["You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: ", "This tool can be used to scale your pixel type from one bit depth to another.  When you scale your pixel depth, your raster will display the same, but the values will be scaled to the new bit depth that was specified.", "The output of this tool is always a raster dataset, even if the raster datasets are being loaded into a raster catalog. This tool will accept a mosaic dataset as the input, but the output will still be a raster dataset\u2014the contents of the mosaic dataset will be mosaicked to create a raster dataset.", "This tool can be used to load raster datasets into a raster catalog. If you want to load raster datasets into a raster catalog, you will need to type out the full path of the raster catalog as the output location or drag the raster catalog into the output text box.", "If this tool is used to load raster datasets into a raster catalog, then the ", "Calculate Default Spatial Grid Index tool", " will need to be run after the loading is completed. ", "Learn more about geodatabase items\u2014Spatial index grid size", "For file-based rasters and personal geodatabase rasters, the ", "Ignore Background Value", " must be set to the same value as ", "NoData", " in order for the background value to be ignored. File geodatabase rasters and ArcSDE rasters will work without this extra step.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings.", "The GIF format only supports single-band raster datasets.", "The pixel-type parameter determines the bit depth of the output raster dataset. There is rescaling of the raster values when a different pixel type is chosen. If the pixel type is demoted (lowered), the raster values outside the valid range for that pixel depth will be truncated and lost. To learn about the bit depth capacity for supported export formats, see ", "Supported raster dataset file formats", "."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The name and location of the raster dataset to be copied. ", "dataType": "Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer"}, {"name": "out_rasterdataset", "isOutputFile": true, "isOptional": false, "description": "The name and location of the raster dataset to be created. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset; Raster Catalog"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}, {"name": "background_value", "isOptional": true, "description": "Use this option to remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values within the raster dataset. The pixel value specified will be set to NoData in the output raster dataset. For file-based rasters and personal geodatabase rasters, the Ignore Background Value must be set to the same value as NoData in order for the background value to be ignored. ArcSDE and file geodatabase rasters will work without this extra step. ", "dataType": "Double"}, {"name": "nodata_value", "isOptional": true, "description": "All the pixels with the specified value will be set to NoData in the output raster dataset. ", "dataType": "String"}, {"name": "onebit_to_eightbit", "isOptional": true, "description": "Choose whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to ArcSDE. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in ArcSDE, 1-bit raster datasets can only have 1-bit pyramid layers, which makes the display unpleasant. By converting the data to 8-bit in ArcSDE, the pyramid layers are built as 8-bit instead of 1-bit, resulting in a proper raster dataset in the display. NONE \u2014 No conversion will be done. This is the default. OneBitTo8Bit \u2014 The input raster will be converted.", "dataType": "Boolean"}, {"name": "colormap_to_RGB", "isOptional": true, "description": "If the input raster dataset has a color map, the output raster dataset can be converted to a three-band output raster dataset. This is useful when mosaicking rasters with different color maps. NONE \u2014 No conversion will occur. This is the default. ColormapToRGB \u2014 The input dataset will be converted.", "dataType": "Boolean"}, {"name": "pixel_type", "isOptional": true, "description": "Determines the bit depth of the output raster dataset. If left unspecified, the output bit depth will be the same as the input. There will be no rescaling of the raster values when a different pixel type is chosen. If the pixel type is demoted (lowered), the raster values outside the valid range for that pixel depth will be truncated and lost. 1_BIT \u2014 A 1-bit unsigned integer. The values can be 0 or 1. 2_BIT \u2014 A 2-bit unsigned integer. The values supported can be from 0 to 3. 4_BIT \u2014 A 4-bit unsigned integer. The values supported can be from 0 to 15. 8_BIT_UNSIGNED \u2014 An unsigned 8-bit data type. The values supported can be from 0 to 255. 8_BIT_SIGNED \u2014 A signed 8-bit data type. The values supported can be from -128 to 127. 16_BIT_UNSIGNED \u2014 A 16-bit unsigned data type. The values can range from 0 to 65,535. 16_BIT_SIGNED \u2014 A 16-bit signed data type. The values can range from -32,768 to 32,767. 32_BIT_UNSIGNED \u2014 A 32-bit unsigned data type. The values can range from 0 to 4,294,967,295. 32_BIT_SIGNED \u2014 A 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647. 32_BIT_FLOAT \u2014 A 32-bit data type supporting decimals. 64_BIT \u2014 A 64-bit data type supporting decimals.", "dataType": "String"}, {"name": "scale_pixel_value", "isOptional": true, "description": " When the output is a different pixel type than the input (such as 16-bit to 8-bit) you can choose to have the values scaled to fit into the new range; otherwise, the values that do not fit into the new pixel range will be discarded. If scaling up, such as 8-bit to 16-bit, the minimum and maximum of the 8-bit values will be scaled to the minimum and maximum in the 16-bit range. If scaling down, such as 16-bit to 8-bit, the minimum and maximum of the 16-bit values will be scaled to the minimum and maximum in the 8-bit range. NONE \u2014 The pixel values will remain the same and will not be scaled. Any values that do not fit within the value range will be discarded. This is the default. ScalePixelValue \u2014 The pixel values will be scaled to the new pixel type. When you scale your pixel depth, your raster will display the same, but the values will be scaled to the new bit depth that was specified.", "dataType": "Boolean"}, {"name": "RGB_to_Colormap", "isOptional": true, "description": "You can convert an 8-bit, 3-band (RGB) raster dataset, to a single-band raster dataset with a color map. This operation will suppress color noise that is often found in scanned images by examining the statistics for the raster dataset and classifying the values into 255 quantiles. This is ideal for screen captures, scanned maps, or scanned documents. This is not recommended for satellite or aerial imagery or thematic raster data. NONE \u2014 The output will remain as a 3-band (RGB) raster dataset. No conversion to a color map will occur. This is the default. RGBToColormap \u2014 A single-band raster dataset, with a color map using 255 colors will be created.", "dataType": "Boolean"}]},
{"syntax": "WorkspaceToRasterCatalog_management (in_workspace, in_raster_catalog, {include_subdirectories}, {project})", "name": "Workspace To Raster Catalog (Data Management)", "description": "Loads all the raster datasets stored in the same workspace into an existing raster catalog.", "example": {"title": "WorkspaceToRasterCatalog example (Python window)", "description": "This is a Python sample for the WorkspaceToRaster Catalog tool.", "code": "import arcpy arcpy.WorkspaceToRasterCatalog_management ( \"c:/data\" , \"c:/RC/WS2RC.gdb/emptyRC\" , \"INCLUDE_SUBDIRECTORIES\" , \"PROJECT_ONFLY\" )"}, "usage": ["All raster datasets within the specified workspace will be loaded into the raster catalog.", "The raster catalog must already exist for this tool to run.", "By default, the spatial reference and geotransformation of the raster datasets are persisted in the raster catalog. If you want to project the raster datasets to the spatial reference of the raster column in the raster catalog during loading, use the ", "Project on-the-fly", " parameter.", "When loading raster datasets into a raster catalog with this tool, the spatial grid index is automatically calculated upon completion of the tool.", "\r\n", "Learn about geodatabase items\u2014Spatial indexes and grid size", "\r\n"], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The workspace that contains all the raster datasets to be loaded into the raster catalog. ", "dataType": "Workspace"}, {"name": "in_raster_catalog", "isInputFile": true, "isOptional": false, "description": "An existing raster catalog that will have all the raster datasets from the workspace loaded into it. ", "dataType": "Raster catalog"}, {"name": "include_subdirectories", "isOptional": true, "description": "Specify whether to include subdirectories. NONE \u2014 Does not include subdirectories. This is the default. INCLUDE_SUBDIRECTORIES \u2014 Includes all the raster datasets within the subdirectories when loading. ", "dataType": "Boolean"}, {"name": "project", "isOptional": true, "description": "Specify whether to project the raster datasets projected on the fly. NONE \u2014 All raster datasets are loaded into the raster catalog with the original spatial reference of the raster dataset. This is the default. PROJECT_ONFLY \u2014 All raster datasets will be projected on the fly (to the coordinate system specified in the raster column) when loaded into the raster catalog. ", "dataType": "Boolean"}]},
{"syntax": "RepairRasterCatalogPaths_management (in_raster_catalog, repair_mode, {original_path}, {new_path})", "name": "Repair Raster Catalog Paths (Data Management)", "description": "Repairs broken file paths or deletes broken links within an unmanaged raster catalog or a mosaic dataset.", "example": {"title": "RepairRasterCatalogPaths example (Python window)", "description": "This is a Python sample for the RepairRasterCatalogPaths tool.", "code": "import arcpy arcpy.RepairRasterCatalogPaths_management ( \"c:/data/RepairRC.gdb/bkrnlinks\" , \"FIX\" , \"*\" , \"C:/data/newpath\" )"}, "usage": ["It can also be used to delete any broken links that are no longer needed.", "You need to know the file path location in order to change it. You can use the ", "Export Raster Catalog Paths", " tool to retrieve the original path names.", "You can type an asterisk (*) as the original path if you wish to change all your paths.", "If you need to repair a mosaic dataset, the Repair Mosaic Dataset dialog box can also achieve this task. To learn more about repairing mosaic datasets, see ", "Repairing paths in a mosaic dataset", ". "], "parameters": [{"name": "in_raster_catalog", "isInputFile": true, "isOptional": false, "description": "The unmanaged raster catalog or mosaic dataset to be repaired. ", "dataType": "Raster Catalog Layer; Mosaic Dataset; Group Layer; Composite Layer"}, {"name": "repair_mode", "isOptional": false, "description": "Choose the repair mode you would like to use. FIX \u2014 This option will allow you to fix the paths. REMOVE \u2014 This option will remove any broken links that exist. ", "dataType": "String"}, {"name": "original_path", "isOptional": true, "description": "Type the original path that needs to be repaired. This is a required parameter if the FIX option was chosen. If you want to change all your paths to the new path, you are able to use the asterisk (*) as the original path. ", "dataType": "String"}, {"name": "new_path", "isOptional": true, "description": "Type the new path to use. This is a required parameter if the FIX option was chosen. ", "dataType": "Folder"}]},
{"syntax": "ExportRasterCatalogPaths_management (in_raster_catalog, export_mode, out_table)", "name": "Export Raster Catalog Paths (Data Management)", "description": "Creates a table listing the paths to the raster datasets contained in an unmanaged raster catalog or a mosaic dataset. The table can display all the file paths, or just the ones that are broken.", "example": {"title": "ExportRasterCatalogPaths example (Python window)", "description": "This is a Python sample of the ExportRasterCatalogPaths tool.", "code": "import arcpy arcpy.ExportRasterCatalogPaths_management ( \"c:/data/ExportRC.gdb/bkrnlinks\" , \"BROKEN\" , \"c:/data/brokenpath.dbf\" )"}, "usage": ["The output of this tool is a table, either in a geodatabase or a .DBF file."], "parameters": [{"name": "in_raster_catalog", "isInputFile": true, "isOptional": false, "description": "The input unmanaged raster catalog or mosaic dataset. ", "dataType": "Raster Catalog Layer; Mosaic Dataset; Group Layer; Composite Layer"}, {"name": "export_mode", "isOptional": false, "description": "Choose what paths to output to the table. You can choose to output all the file paths, or just the ones that are broken. Choose the repair mode you would like to use. BROKEN \u2014 This option will export only the broken paths to the table. ALL \u2014 This option will export all the paths to the table. ", "dataType": "String"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output table. This table can be created as a DBF file or within a geodatabase. The output table will have a field that lists the Source OID. This is the OID of the row in the original raster catalog table. ", "dataType": "Table"}]},
{"syntax": "DeleteRasterCatalogItems_management (in_raster_catalog)", "name": "Delete Raster Catalog Items (Data Management)", "description": "Deletes raster catalog items, including all its contents, or a subset of its contents if there is a selection.", "example": {"title": "DeleteRasterCatalogItems example 1 (Python window)", "description": "This is a Python sample for the DeleteRasterCatalogItems tool.", "code": "import arcpy arcpy.DeleteRasterCatalogItems_management ( \"c:/data/DeleteRC.gdb/RC_managed\" )"}, "usage": ["Only geodatabase raster catalogs are valid inputs.", "When using ArcMap, you can use the Selection tool to choose which items to delete."], "parameters": [{"name": "in_raster_catalog", "isInputFile": true, "isOptional": false, "description": "The input raster catalog that will have its items deleted. ", "dataType": "Raster Catalog Layer"}]},
{"syntax": "CreateRasterCatalog_management (out_path, out_name, {raster_spatial_reference}, {spatial_reference}, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3}, {raster_management_type}, {template_raster_catalog})", "name": "Create Raster Catalog (Data Management)", "description": "Creates an empty raster catalog in a geodatabase.", "example": {"title": "CreateRasterCatalog example 1 (Python window)", "description": "This is a Python sample for the CreateRasterCatalog tool.", "code": "import arcpy arcpy.CreateRasterCatalog_management ( \"c:/data/CreateRC.gdb\" , \"fgdb_unman\" , \"Albers_Conical.prj\" , \"Albers_Conical.prj\" , \"MAX_FILE_SIZE_4GB\" , \"1000\" , \"3000\" , \"9000\" , \"UNMANAGED\" , \"\" )"}, "usage": ["Once the raster catalog is created, raster datasets can be loaded into it.", "Raster catalogs can be created in any type of geodatabase: personal, file, or ArcSDE.", "Once a raster catalog is created, raster datasets can be loaded into it by right-clicking the raster catalog and selecting ", "Load Data", ". Raster datasets can also be loaded into a raster catalog using the ", "Workspace To Raster Catalog tool", ".", "Raster catalogs can be managed or unmanaged by the geodatabase. When the raster catalog is managed by the geodatabase means then the raster datasets inside the raster catalog will be physically stored within the geodatabase. When a row or raster is deleted from the catalog, it is deleted from the geodatabase. When the raster catalog is unmanaged, the raster catalog only contains links or pointers connecting a raster catalog row to a raster dataset stored outside the geodatabase. These raster datasets are stored outside the geodatabase. All raster datasets loaded into an unmanaged raster catalog must be a file on disk. Geodatabase raster datasets can only be loaded into raster catalogs that are managed. Raster Catalogs stored in an ArcSDE geodatabase are always managed.", "If the default value of zero is used for the spatial grid index, it is recommended that you load data in the raster catalog using the ", "Workspace To Raster Catalog", " tool. If the ", "Workspace To Raster Catalog", " tool is used to load raster datasets, the spatial grid size will be automatically calculated. If another tool is used to load raster datasets into a raster catalog, the ", "Calculate Default Spatial Grid Index", " tool needs to be used after the loading is completed.", "\r\n", "Learn more about geodatabase items\u2014Spatial indexes and grid size", "\r\n", "When creating a raster catalog in an ArcSDE geodatabase, the raster dataset name cannot have spaces. Use underscores to separate words in raster dataset names."], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The geodatabase to contain the raster catalog. This can be any type of geodatabase: personal, file, or ArcSDE. ", "dataType": "Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the raster catalog to be created. ", "dataType": "String"}, {"name": "raster_spatial_reference", "isOptional": true, "description": "The coordinate system for the raster column in the raster catalog. The spatial reference of the raster column is used during data loading as The default value is the coordinate system set in the environment settings. A default spatial reference for those raster datasets that have an unknown spatial reference A target spatial reference if you choose to project your raster datasets that have different spatial references from the raster column", "dataType": "Coordinate System"}, {"name": "spatial_reference", "isOptional": true, "description": "The coordinate system for the geometry column. The spatial reference for the geometry column defines the spatial reference of the footprints of the raster datasets. The default value is the coordinate system set in the environment settings. ", "dataType": "Spatial Reference"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}, {"name": "spatial_grid_1", "isOptional": true, "description": "The Output Spatial Grid 1, 2, and 3 parameters are used to compute a spatial index and only apply to file geodatabases and ArcSDE geodatabases. If you are unfamiliar with setting grid sizes, leave these options as 0,0,0 and ArcGIS will compute optimal sizes for you. If you use the default spatial grid index (of zero), it is recommended that you load data using the Workspace To Raster Catalog tool . If the Workspace To Raster Catalog tool is used to load raster datasets, the spatial grid size will be automatically calculated. If another tool is used to load raster datasets into a raster catalog, the Calculate Default Spatial Grid Index (Data Management) tool needs to be used after the loading is completed. For more information about this parameter, refer to the Add Spatial Index tool documentation. ", "dataType": "Double"}, {"name": "spatial_grid_2", "isOptional": true, "description": " Cell size of the second spatial grid. Leave the size at 0 if you only want one grid. Otherwise, set the size to at least three times larger than Spatial Grid 1. ", "dataType": "Double"}, {"name": "spatial_grid_3", "isOptional": true, "description": " Cell size of the third spatial grid. Leave the size at 0 if you only want two grids. Otherwise, set the size to at least three times larger than Spatial Grid 2. ", "dataType": "Double"}, {"name": "raster_management_type", "isOptional": true, "description": "Raster datasets within raster catalogs can be managed in two ways: managed or unmanaged (by the geodatabase). MANAGED \u2014 With a managed raster catalog, the raster datasets inside the raster catalog will be physically stored within the geodatabase. When a row (or raster) is deleted from the catalog, it is deleted from the geodatabase. UNMANAGED \u2014 With an unmanaged raster catalog, the raster catalog only contains links or pointers connecting a row to a raster dataset stored outside the geodatabase. All raster datasets loaded into an unmanaged raster catalog must be a file on disk. ", "dataType": "String"}, {"name": "template_raster_catalog", "isOptional": true, "description": "If you want to base your new raster catalog on a template, you can specify a template raster catalog. The new raster catalog will then have the same fields as the template raster catalog. ", "dataType": "Raster Catalog Layer"}]},
{"syntax": "CopyRasterCatalogItems_management (in_raster_catalog, out_raster_catalog, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})", "name": "Copy Raster Catalog Items (Data Management)", "description": "Makes a copy of a raster catalog, including all of its contents, or a subset of its contents if there is a selection.", "example": {"title": "CopyRasterCatalogItems example 1 (Python Window)", "description": "This is a Python sample for CopyRasterCatalogItems.", "code": "import arcpy arcpy.CopyRasterCatalogItems_management ( \"c:/data/CopyRC.mdb/RC1\" , \"c:/data/OutRC.gdb/RC2\" , \"\" , \"\" , \"\" , \"\" )"}, "usage": ["The input and output of this tool is a geodatabase raster catalog.", "You can copy your output raster catalog to any type of geodatabase: personal, file, or ArcSDE.", "When using ArcMap, you can use the Selection tool to choose which raster catalog items to copy.", "The properties of the output raster catalog are the same as the input raster catalog, as well as the properties of the raster datasets in the raster catalog."], "parameters": [{"name": "in_raster_catalog", "isInputFile": true, "isOptional": false, "description": "The name and location of the raster catalog to be copied. ", "dataType": "Raster Catalog Layer"}, {"name": "out_raster_catalog", "isOutputFile": true, "isOptional": false, "description": "The name and location of the output raster catalog. You can copy your output raster catalog to any type of geodatabase: personal, file, or ArcSDE. ", "dataType": "Raster Catalog"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}, {"name": "spatial_grid_1", "isOptional": true, "description": "The Output Spatial Grid 1, 2, and 3 parameters apply only to file geodatabases and ArcSDE geodatabases. If you are unfamiliar with setting grid sizes, leave these options as 0,0,0 and ArcGIS will compute optimal sizes for you. For more information about this parameter, refer to the Add Spatial Index tool documentation. ", "dataType": "Double"}, {"name": "spatial_grid_2", "isOptional": true, "description": " Cell size of the second spatial grid. Leave the size at 0 if you only want one grid. Otherwise, set the size to at least three times larger than Spatial Grid 1. ", "dataType": "Double"}, {"name": "spatial_grid_3", "isOptional": true, "description": " Cell size of the third spatial grid. Leave the size at 0 if you only want two grids. Otherwise, set the size to at least three times larger than Spatial Grid 2. ", "dataType": "Double"}]},
{"syntax": "SynchronizeMosaicDataset_management (in_mosaic_dataset, {where_clause}, {new_items}, {sync_only_stale}, {update_cellsize_ranges}, {update_boundary}, {update_overviews}, {build_pyramids}, {calculate_statistics}, {build_thumbnails}, {build_item_cache}, {rebuild_raster}, {update_fields}, {fields_to_update}, {existing_items}, {broken_items})", "name": "Synchronize Mosaic Dataset (Data Management)", "description": "Rebuilds or updates each raster item in the mosaic dataset and updates affected fields in the attribute table, using the raster type and options that were used when it was \r\noriginally added. Synchronization is a one-way operation: changes in the source data can be \r\nsynchronized to the mosaic dataset\u2019s attribute table, but changes in the attribute table will not affect the source data (but may be overwritten by the synchronization). \r\n", "example": {"title": "SynchronizeMosaicDataset example 1 (Python window)", "description": "This is a Python sample for SynchronizeMosaicDataset.", "code": "import arcpy arcpy.SynchronizeMosaicDataset_management ( \"c:/data/syncmd.gdb/md\" , \"Year>1999\" , \"NO_NEW_ITEMS\" , \"SYNC_STALE\" , \"#\" , \"#\" , \"#\" , \"NO_PYRAMIDS\" , \"NO_STATISTICS\" , \"NO_THUMBNAILS\" , \"NO_ITEM_CACHE\" , \"NO_RASTER\" , \"NO_FIELDS\" , \"#\" )"}, "usage": ["You can use a selection set with this tool to limit the raster items that are updated. When there is a selection or query, only those items will be processed.", "The synchronization can add new items, update existing items, or remove items. ", "Stale item refers to source rasters that have been changed since the mosaic \r\ndataset has been created or the last time the mosaic dataset has been synchronized. For instance, the georeferencing may have been updated or the pyramids may have been built.", "Since the raster items are reconstructed, any modifications made to these items since the last time they were built would be lost, such as editing functions or content in the attribute table.", "Changes made with the synchronization cannot be undone. You may want to create a backup copy if you've made modifications to your mosaic data that could be overwritten.", "If you choose to remove items that have broken data sources, make sure that all your network connections are working properly\u2014this tool will remove any items that cannot be accessed.", "  If the mosaic datasets, added as an input using the Table raster type to a master mosaic dataset, are empty, any subsequent changes performed on the input mosaic datasets will not be synchronized with the master mosaic dataset.", "This tool is particularly useful for keeping mosaic datasets up-to-date. You can use this tool to detect and add new raster datasets that have been added to folder locations from where data in the mosaic dataset was added. Mosaic datasets that are populated using tables that reside in an externally managed database can also be updated with this tool.", "This tool can also build pyramids and calculate statistics on the source rasters, and create thumbnails and raster cache  for the raster items.", "Once the synchronization has completed, the operations listed in ", "Mosaic Dataset Options", " will take place. ", "This tool can be used to generate the cache for an item in the mosaic dataset. Items that can always be  cached are created from the following data: LAS files, LAS dataset, and terrains. Items can also be cached using the ", "Cached Raster function", "."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "The path and name of the mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": "Using SQL, you can define a query or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "new_items", "isOptional": true, "description": " Choose whether to synchronize your mosaic dataset with newly added items. If you choose to use this option, the item's workspace will be searched for new data. When data is added to the mosaic dataset, it will use the same raster type as the other items within the same workspace. If you choose to update with the new items, you can optionally choose whether to update cell size ranges, boundary, or overviews. NO_NEW_ITEMS \u2014 Do not add any new items that may exist. This is the default. UPDATE_WITH_NEW_ITEMS \u2014 Update the mosaic dataset with the new items in the workspaces, where existing mosaic dataset items are stored.", "dataType": "Boolean"}, {"name": "sync_only_stale", "isOptional": true, "description": " Choose whether to update all primary items or only stale primary items. An item is considered stale if the corresponding source files that are used to construct the raster values are modified after the item is created. For instance, the georeferencing may have been updated or the pyramids may have been built. SYNC_STALE \u2014 Only update the stale items. This is the default. SYNC_ALL \u2014 Update all selected items of the mosaic dataset.", "dataType": "Boolean"}, {"name": "update_cellsize_ranges", "isOptional": true, "description": "Choose whether to update the cell size ranges. UPDATE_CELL_SIZES \u2014 The cell size ranges will be recalculated for the entire mosaic dataset, but only the items that have an invalid visibility will be updated. This is the default. NO_CELL_SIZES \u2014 The cell size ranges will not be recalculated.", "dataType": "Boolean"}, {"name": "update_boundary", "isOptional": true, "description": "Choose whether to update the boundary. UPDATE_BOUNDARY \u2014 The boundary will be rebuilt after the mosaic dataset is synchronized. This is the default. NO_BOUNDARY \u2014 The boundary will not be rebuilt.", "dataType": "Boolean"}, {"name": "update_overviews", "isOptional": true, "description": "Choose whether to update any obsolete overviews. The overview becomes obsolete if any underlying rasters have been modified. NO_OVERVIEWS \u2014 The overviews will not be rebuilt. This is the default. UPDATE_OVERVIEWS \u2014 The affected overviews will be rebuilt after the mosaic dataset is synchronized.", "dataType": "Boolean"}, {"name": "build_pyramids", "isOptional": true, "description": "Choose whether to build pyramids for the specified mosaic dataset items. Pyramids can be built for each raster item in the mosaic dataset. Pyramids can improve the speed at which each raster is displayed. Pyramids will not be built for newly added items, if the items are added in the same instance of running this tool. NO_PYRAMIDS \u2014 Pyramids will not be generated. This is the default. BUILD_PYRAMIDS \u2014 Pyramids will be generated for all the raster items specified by this tool. ", "dataType": "Boolean"}, {"name": "calculate_statistics", "isOptional": true, "description": "Choose whether to calculate statistics for the specified mosaic dataset items. Statistics can be calculated for each raster item in the mosaic dataset. Statistics are required for your mosaic dataset when performing certain tasks, such as applying a contrast stretch. Statistics will not be built for newly added items, if the items are added in the same instance of running this tool. NO_STATISTICS \u2014 Statistics will not be calculated. This is the default. CALCULATE_STATISTICS \u2014 Statistics will be calculated for all the raster items specified by this tool.", "dataType": "Boolean"}, {"name": "build_thumbnails", "isOptional": true, "description": "Choose whether to build thumbnails for the specified mosaic dataset items. Thumbnails are small, highly resampled images that can be created for each raster item in the mosaic definition. Thumbnails can be accessed when the mosaic dataset is accessed as an image service and will display as part of the metadata (Item Description). Thumbnails will not be built for newly added items, if the items are added in the same instance of running this tool. NO_THUMBNAILS \u2014 No thumbnails will be created or updated. This is the default. BUILD_THUMBNAILS \u2014 Thumbnails will be generated or updated for all the raster items specified by this tool.", "dataType": "Boolean"}, {"name": "build_item_cache", "isOptional": true, "description": "Choose whether to build a cache for the specified mosaic dataset items. A cache can be created when you've added data using the LAS, Terrain, or LAS Dataset raster types. Items can also be cached using the Cached Raster function. The cache will not be built for newly added items, if the items are added in the same instance of running this tool. NO_ITEM_CACHE \u2014 No cache will be created or updated. This is the default. BUILD_ITEM_CACHE \u2014 The cache will be generated or updated for all the raster items specified by this tool.", "dataType": "Boolean"}, {"name": "rebuild_raster", "isOptional": true, "description": "Choose whether to rebuild the raster items from the data source using the original raster type. This will only affect items that will be synchronized. This parameter is not applicable if UPDATE_WITH_NEW_ITEMS has been chosen. REBUILD_RASTER \u2014 Rebuild the rasters from the source data. You will lose any changes that you have performed on selected items in the mosaic dataset. This is the default. NO_RASTER \u2014 Do not rebuild the rasters. Other primary fields are reset if update_aux_fields is chosen.", "dataType": "Boolean"}, {"name": "update_fields", "isOptional": true, "description": "Choose whether to update the fields within the table. This will only affect items that will be synchronized. If you choose to update the fields, you can control which fields are updates by selecting them in the fields_to_update parameter. If you have made edits to some of the fields you may want to unselect them in the fields_to_update parameter. UPDATE_FIELDS \u2014 Update the fields from the source files. This is the default. NO_FIELDS \u2014 Do not reset the fields within the table from the source.", "dataType": "Boolean"}, {"name": "fields_to_update", "isOptional": false, "description": " Specify which fields should be updated. This parameter is only valid if the option for the update_fields parameter is UPDATE_FIELDS. If you have made edits to some of the fields, you may want to make sure they are not listed. The RASTER column can be refreshed, even if the Rebuild Raster From Data Source check box is unchecked. However, if the Rebuild Raster From Data Source is checked, the RASTER column is rebuilt, even if this option is unchecked here. ", "dataType": "String"}, {"name": "existing_items", "isOptional": true, "description": "Choose whether you would like to update existing items within your mosaic dataset. If you choose this option, you can then choose which existing parameters you would like to update: sync_only_stale, build_pyramids, calculate_statistics, build_thumbnails, build_item_cache, update_raster, update_fields, or fields_to_update. UPDATE_EXISTING_ITEMS \u2014 Existing items will be updated with the parameters that you have chosen to update. This is the default. IGNORE_EXISTING_ITEMS \u2014 Existing items will not be updated.", "dataType": "Boolean"}, {"name": "broken_items", "isOptional": true, "description": " Choose whether you want to remove any broken links. Make sure that all your network connections are working properly\u2014this tool will remove any items that cannot be accessed. IGNORE_BROKEN_ITEMS \u2014 Items that have broken links will not be removed from the mosaic dataset. This is the default. REMOVE_BROKEN_ITEMS \u2014 Items that have broken links will be removed from the mosaic dataset.", "dataType": "Boolean"}]},
{"syntax": "RemoveRastersFromMosaicDataset_management (in_mosaic_dataset, {where_clause}, {update_boundary}, {mark_overviews_items}, {delete_overview_images}, {delete_item_cache}, {remove_items}, {update_cellsize_ranges})", "name": "Remove Rasters From Mosaic Dataset (Data Management)", "description": "Removes selected rasters (items)  from a mosaic dataset. ", "example": {"title": "RemoveRastersFromMosaicDataset example 1 (Python window)", "description": "This is a Python sample for RemoveRastersFromMosaicDataset.", "code": "import arcpy arcpy.RemoveRastersFromMosaicDataset_management ( \"C:/Workspace/remove.gdb/md\" , \"YEAR<1999\" , \"UPDATE_BOUNDARY\" , \"MARK_OVERVIEW_ITEMS\" , \"#\" , \"#\" , \"#\" , \"#\" )"}, "usage": ["There must be a  selection or a query specified; otherwise, the tool will not run. If you want to delete all the records from the mosaic dataset,  specify a query that would select all the rasters, such as ", "\"OBJECTID>=0\"", ".", " You have the option to remove overviews but not to delete them. If the overviews are generated within the mosaic dataset, they will be deleted when they are removed, because they are managed by the mosaic dataset. If you have created the overviews in a folder, or nondefault location, they are not fully managed by the mosaic dataset; therefore, you can remove them but not delete them from disk. You may not want to delete overviews if you are using them elsewhere. ", "If you choose to mark the affected overviews but not delete them, you can use the ", "Build Overviews tool", " to regenerate the affected overviews.", "This tool will also delete the cache created for each item in the mosaic dataset. Both raster cache and LAS cache can be removed. The properties for the cache for these datasets is defined in their functions.", "This tool will not recalculate the extent of the mosaic dataset. To recalculate the extent, you will need to recalculate the feature extent within the ", "Mosaic Dataset Properties", " window."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "Path and name of mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": "Using SQL you can define a query, or use the Query Builder to build a query to define the raster datasets that will be removed from the mosaic dataset. There must be a selection or a query specified; otherwise, the tool will not run. If you want to delete all the records from the mosaic dataset, specify a query that would select all the rasters, such as \"OBJECTID>=0\" . ", "dataType": "SQL Expression"}, {"name": "update_boundary", "isOptional": true, "description": " Updates the boundary polygon of a mosaic dataset. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels. BUILD_BOUNDARY \u2014 The boundary will be updated. This is the default. NO_BUILD_BOUNDARY \u2014 The boundary will not be updated. ", "dataType": "Boolean"}, {"name": "mark_overviews_items", "isOptional": true, "description": "When the rasters in a mosaic catalog have been removed, any overviews created using those rasters may no longer be accurate; therefore, they can be identified so they can be updated or removed if they are no longer needed. MARK_OVERVIEWS \u2014 The affected overviews will be identified. This is the default. NO_MARK_OVERVIEWS \u2014 The affected overviews will not be identified. ", "dataType": "Boolean"}, {"name": "delete_overview_images", "isOptional": true, "description": " Any overviews that are no longer required as a result of the source rasters being removed can also be removed from the mosaic dataset. The overviews are removed only when the overview items are selected. DELETE_OVERVIEW_IMAGES \u2014 The boundary will be updated. This is the default. NO_DELETE_OVERVIEW_IMAGES \u2014 The boundary will not be updated. ", "dataType": "Boolean"}, {"name": "delete_item_cache", "isOptional": true, "description": "Choose whether to delete the cache that is associated with any of the mosaic dataset items that you are removing. DELETE_CACHE \u2014 The mosaic dataset item will be removed and the corresponding cache will be deleted. This is the default. NO_DELETE_CACHE \u2014 Any associated item cache will not be deleted.", "dataType": "Boolean"}, {"name": "remove_items", "isOptional": true, "description": "Choose whether to delete the item from the mosaic dataset, such as a raster dataset, or only delete the associated overviews or cache. REMOVE_MOSAICDATASET_ITEMS \u2014 The item will be removed from the mosaic dataset. This is the default. NO_REMOVE_MOSAICDATASET_ITEMS \u2014 The item will not be removed from the mosaic dataset, but the item's cache and any overviews created from this item will be removed.", "dataType": "Boolean"}, {"name": "update_cellsize_ranges", "isOptional": true, "description": "Choose whether to update the cell size ranges for the mosaic dataset. REMOVE_MOSAICDATASET_ITEMS \u2014 The cell size ranges will be updated. This is the default. NO_REMOVE_MOSAICDATASET_ITEMS \u2014 The cell size ranges will not be modified.", "dataType": "Boolean"}]},
{"syntax": "ImportMosaicDatasetGeometry_management (in_mosaic_dataset, target_featureclass_type, target_join_field, input_featureclass, input_join_field)", "name": "Import Mosaic Dataset Geometry (Data Management)", "description": " Modifies the feature geometry for footprints, the boundary, or seamlines in a mosaic dataset to match those in a polygon feature class.", "example": {"title": "ImportMosaicDatasetGeometry example 1 (Python window)", "description": "This is a Python sample for ImportMosaicDatasetGeometry.", "code": "import arcpy arcpy.ImportMosaicDatasetGeometry_management ( \"c:/workspace/fgdb.gdb/md\" , \"FOOTPRINT\" , \"OBJECTID\" , \"infootprint.shp\" , \"FTID\" )"}, "usage": [" This tool matches the feature in the mosaic dataset with the feature in the feature class based on a common attribute field.", "The footprint is not always used to clip the image in the mosaic dataset. You can change the ", "Always Clip The Image To Its Footprint", " property to ", "Yes", " in the ", "Mosaic Dataset Properties", " dialog box on the ", "Defaults", " tab.", "This tool will not recalculate the extent of the mosaic dataset. To recalculate the extent, you will need to recalculate the feature extent within the ", "Mosaic Dataset Properties", " window."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " Path and name of mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "target_featureclass_type", "isOptional": false, "description": "The polygon feature class in the mosaic dataset whose geometry will be replaced. FOOTPRINT \u2014 The footprint polygons in the mosaic dataset. SEAMLINE \u2014 The seamline polygons in the mosaic dataset. BOUNDARY \u2014 The boundary polygon in the mosaic dataset.", "dataType": "String"}, {"name": "target_join_field", "isOptional": false, "description": " Field in the mosaic dataset that will link to correct geometry in the input feature class. ", "dataType": "Field"}, {"name": "input_featureclass", "isInputFile": true, "isOptional": false, "description": " The polygon feature class with the new geometry for the mosaic dataset. ", "dataType": "Feature Layer ; Raster Catalog Layer"}, {"name": "input_join_field", "isInputFile": true, "isOptional": false, "description": " Field in the input feature class that will link to the correct row in the mosaic dataset. ", "dataType": "Field"}]},
{"syntax": "DefineOverviews_management (in_mosaic_dataset, {overview_image_folder}, {in_template_dataset}, {extent}, {pixel_size}, {number_of_levels}, {tile_rows}, {tile_cols}, {overview_factor}, {force_overview_tiles}, {resampling_method}, {compression_method}, {compression_quality})", "name": "Define Overviews (Data Management)", "description": " Defines the tiling schema and properties of the preprocessed raster datasets that will cover part or all of a mosaic dataset at varying resolutions. ", "example": {"title": "DefineOverviews example 1 (Python window)", "description": "This is a Python sample for DefineOverviews.", "code": "import arcpy arcpy.DefineOverviews_management ( \"c:/workspace/fgdb.gdb/md01\" , \"c:/temp\" , \"#\" , \"#\" , \"30\" , \"6\" , \"4000\" , \"4000\" , \"2\" , \"CUBIC\" , \"JPEG\" , \"50\" )"}, "usage": [" This tool is used when there are specific parameters you need to set to generate your overviews, such as ", "Use the ", "Build Overviews", " tool to generate the overviews after they've been defined with this tool.", "You can  use a polygon feature class to define the shape of the overview. An overview image will be created for every polygon in the feature class. Ideally, your feature class will be used to create one overview image. If you do not wish to use all the polygons in the feature class you can make a selection on the layer in the table of contents or use a tool such as Make Feature Layer to create a temporary layer representing only the desired polygons.", "The default tile size is 128 by 128. The tile size can be changed in the Environment Settings.", "This tool can take a long time to run if the boundary contains a large number of vertices."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The path and name of the mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "overview_image_folder", "isOptional": true, "description": " The folder or geodatabase where the overviews will be stored. Overviews that are stored and managed within personal and file geodatabases have the default overview folder location in the same workspace as the residing geodatabase. ", "dataType": "Workspace"}, {"name": "in_template_dataset", "isInputFile": true, "isOptional": true, "description": " A raster dataset or polygon feature class used to define the extent or shape of the overview. The extent of the raster dataset will be used when using a raster dataset. The shape of the polygon will be used when using a feature class. ", "dataType": "Raster Layer; Feature Layer"}, {"name": "extent", "isOptional": true, "description": " Four coordinates defining the extent of the overview that will be generated. This is specified as space delimited in the following order: X-minimum X-maximum Y-minimum Y-maximum. The mosaic dataset boundary will be used to determine the extent of the overviews if an extent is not defined. ", "dataType": "Envelope"}, {"name": "pixel_size", "isOptional": true, "description": "Base pixel size used to generate the overviews. The default is determined by the software. The units for this parameter is in the same units as the spatial reference of the mosaic dataset. ", "dataType": "Double"}, {"name": "number_of_levels", "isOptional": true, "description": "The number of overview levels that will be generated. For a number greater than 0, it will be the number of overview levels generated. For example, 3 will generate three levels of overviews. If the value is left blank or -1, then an optimal number of overviews will be generated by the system. ", "dataType": "Long"}, {"name": "tile_rows", "isOptional": true, "description": "Optimum number of rows in the overview. The larger the value, the bigger the file, and the more likely it will need to be regenerated if any lower image changes. This number can affect the number of overview images created. If it's a large number, then fewer overviews will be generated. If it's a small number, then more files are generated. ", "dataType": "Long"}, {"name": "tile_cols", "isOptional": true, "description": "Optimum number of columns in the overview. The larger the value, the bigger the file, and the more likely it will need to be regenerated if any lower image changes. This number can affect the number of overview images created. If it's a large number, then fewer overviews will be generated. If it's a small number, then more files are generated. ", "dataType": "Long"}, {"name": "overview_factor", "isOptional": true, "description": "The ratio used to determine the size of the next overview. For example, if the cell size of the first level is x, and the overview factor is 3, then the next overview pixel size will be 3x. ", "dataType": "Long"}, {"name": "force_overview_tiles", "isOptional": true, "description": " Affects the levels at which overviews will be generated. NO_FORCE_OVERVIEW_TILES \u2014 Overviews will only be created at levels above the primary raster pyramids levels. This is the default. FORCE_OVERVIEW_TILES \u2014 Overviews will be created at all levels, even though the primary rasters have pyramids. ", "dataType": "Boolean"}, {"name": "resampling_method", "isOptional": true, "description": " The resampling algorithm used when creating the overviews. NEAREST \u2014 Nearest neighbor assignment. If the Key Metadata Data Type is thematic, then nearest neighbor will be the default. BILINEAR \u2014 Bilinear interpolation. This is the default, unless the Key Metadata Data Type is thematic. CUBIC \u2014 Cubic convolution. ", "dataType": "String"}, {"name": "compression_method", "isOptional": true, "description": "This defines the type of data compression that will be used to store the overview images. JPEG \u2014 A lossy compression. This is the default, unless the Key Metadata Data Type is thematic. JPEG_YCbCr \u2014 A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components. None \u2014 No data compression. LZW \u2014 A lossless compression. If the Key Metadata Data Type is thematic, then nearest neighbor will be the default.", "dataType": "String"}, {"name": "compression_quality", "isOptional": true, "description": " Quality of the compression used with the JPEG compression method. The compression quality can range from 1 to 100. A higher number means better image quality but less compression. ", "dataType": "Long"}]},
{"syntax": "DefineMosaicDatasetNoData_management (in_mosaic_dataset, num_bands, {bands_for_nodata_value}, {bands_for_valid_data_range}, {where_clause}, {Composite_nodata_value})", "name": "Define Mosaic Dataset NoData (Data Management)", "description": " Allows you to specify one or more NoData values for a mosaic dataset. ", "example": {"title": "DefineMosaicDatasetNoData example 1 (Python window)", "description": "This is a Python sample for DefineMosaicDatasetNoData.", "code": "import arcpy arcpy.DefineMosaicDatasetNodata_management ( \"c:/workspace/Nodata.gdb/md\" , \"3\" , \"ALL_BANDS '0 9'\" , \"#\" , \"OBJECTID=2\" , \"COMPOSITE_NODATA\" )"}, "usage": [" NoData can be used to define pixel values that surround an image; however, the mosaic dataset can be made more efficient if the footprints are recomputed to remove these boundary areas. To recompute the footprints you can edit them manually or use the ", "Build Footprints tool", ". ", "You can specify multiple NoData values with the ", "Bands For NoData Value", " parameter. Use a space delimiter between each value you want to define as NoData.", "This tool inserts the Mask function within the function chain for each raster item within a mosaic dataset.", "The Mask function inserted by this tool is inserted before the Composite Bands function in the function chain. Therefore, if the function chain for each raster within the mosaic dataset contains the Composite Bands function, or if your raster data was added with a raster type that adds the Composite Bands function to each raster\u2019s function chain, then any value you specify will apply to all bands."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " Path and name of the mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "num_bands", "isOptional": false, "description": " Defines the number of bands in the mosaic dataset. This value will be populated automatically, based on the mosaic dataset that is added, but you can override it. ", "dataType": "Long"}, {"name": "bands_for_nodata_value", "isOptional": false, "description": " Define values for each or all bands. Each band can have a unique NoData value defined, or the same value can be specified for all bands. If you want to define multiple NoData values for each band selection, use a space delimiter between each NoData value within the bands_for_nodata_value parameter. The Mask function inserted by this tool is inserted before the Composite Bands function in the function chain. Therefore, if the function chain for each raster within the mosaic dataset contains the Composite Bands function, or if your raster data was added with a raster type that adds the Composite Bands function to each raster\u2019s function chain, then any value you specify will apply to all bands. ", "dataType": "Value Table"}, {"name": "bands_for_valid_data_range", "isOptional": false, "description": " The band number and the minimum and maximum pixel value of valid data. The NoData values will be those values outside the range. For example, for an 8-bit image, if you specify band1=10\u2013200, then values 0\u20139 and 201\u2013255 will be defined as NoData. The Mask function inserted by this tool is inserted before the Composite Bands function in the function chain. Therefore, if the function chain for each raster within the mosaic dataset contains the Composite Bands function, or if your raster data was added with a raster type that adds the Composite Bands function to each raster\u2019s function chain, then any value you specify will apply to all bands. ", "dataType": "Value Table"}, {"name": "where_clause", "isOptional": true, "description": " Using SQL you can define a query or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "Composite_nodata_value", "isOptional": true, "description": "Choose whether all bands must be NoData in order for the pixel to be tagged as NoData. NO_COMPOSITE_NODATA \u2014 If any of the bands have pixels of NoData, then the pixel is classified as NoData. This is the default. COMPOSITE_NODATA \u2014 All of the bands must have pixels of NoData in order for the pixel to be classified as NoData.", "dataType": "Boolean"}]},
{"syntax": "CreateReferencedMosaicDataset_management (in_dataset, out_mosaic_dataset, {coordinate_system}, {number_of_bands}, {pixel_type}, {where_clause}, {in_template_dataset}, {extent}, {select_using_features}, {lod_field}, {minPS_field}, {maxPS_field}, {pixelSize}, {build_boundary})", "name": "Create Referenced Mosaic Dataset (Data Management)", "description": "Creates a new mosaic dataset from an existing raster catalog, a selection set from a raster catalog, or a mosaic dataset.", "example": {"title": "CreateReferencedMosaicDataset example 1 (Python window)", "description": "This is a Python sample for CreateReferencedMosaicDataset.", "code": "import arcpy arcpy.CreateReferencedMosaicDataset_management ( \"C:/workspace/RefMD.gdb/md\" , \"ref_md.amd\" , \"GCS_WGS_1984.prj\" , \"1\" , \"#\" , \"#\" , \"ref_md.shp\" , \"#\" , \"SELECT_USING_FEATURES\" , \"#\" , \"#\" , \"#\" , \"#\" , \"NO_BOUNDARY\" )"}, "usage": [" This mosaic dataset can be created within or outside a geodatabase. When not created within a geodatabase, ", " If the input is an RPF raster catalog created using Military Analyst, the ", "Scale Field", " should be specified.", "Overviews cannot be created for a referenced mosaic dataset.", "A referenced mosaic dataset is one option for serving a raster catalog as an image service; however, you can also create a fully capable mosaic dataset from a raster catalog using the ", "Create Mosaic Dataset tool", " followed by the ", "Add Rasters To Mosaic Dataset", " tool.", "You may use this tool to create a mosaic dataset from another mosaic dataset to create a mosaic dataset with a different output. For example, you may create one mosaic dataset with elevation data, then create another that will be used to produce a derived product, such as slope or hillshade."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The input raster catalog or mosaic dataset. ", "dataType": "Mosaic Layer; Mosaic Dataset; Raster Catalog Layer"}, {"name": "out_mosaic_dataset", "isOutputFile": true, "isOptional": false, "description": " The folder location or geodatabase where the mosaic dataset will be created. ", "dataType": "Mosaic Dataset"}, {"name": "coordinate_system", "isOptional": true, "description": "The coordinate system defined for the mosaic dataset, which will be used for all the associated files created. ", "dataType": "Spatial Reference"}, {"name": "number_of_bands", "isOptional": true, "description": "The number of raster dataset bands supported by the mosaic dataset. ", "dataType": "Long"}, {"name": "pixel_type", "isOptional": true, "description": " The bit depth of a cell, used to determine the range of values that the mosaic dataset outputs. For example, an 8-bit mosaic dataset can have 256 unique pixel values, which range from 0 to 255. If it is not defined, it will be taken from the first raster dataset. 1_BIT \u2014 A 1-bit unsigned integer. The values can be 0 or 1. 2_BIT \u2014 A 2-bit unsigned integer. The values supported can be from 0 to 3. 4_BIT \u2014 A 4-bit unsigned integer. The values supported can be from 0 to 15. 8_BIT_UNSIGNED \u2014 An unsigned 8-bit data type. The values supported can be from 0 to 255. 8_BIT_SIGNED \u2014 A signed 8-bit data type. The values supported can be from -128 to 127. 16_BIT_UNSIGNED \u2014 A 16-bit unsigned data type. The values can range from 0 to 65,535. 16_BIT_SIGNED \u2014 A 16-bit signed data type. The values can range from -32,768 to 32,767. 32_BIT_UNSIGNED \u2014 A 32-bit unsigned data type. The values can range from 0 to 4,294,967,295. 32_BIT_SIGNED \u2014 A 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647. 32_BIT_FLOAT \u2014 A 32-bit data type supporting decimals. 64_BIT \u2014 A 64-bit data type supporting decimals.", "dataType": "String"}, {"name": "where_clause", "isOptional": true, "description": "Use SQL to define a query, or use the Query Builder to build a query that identifies the raster datasets to be added to the mosaic dataset from the raster catalog or mosaic definition. ", "dataType": "SQL Expression"}, {"name": "in_template_dataset", "isInputFile": true, "isOptional": true, "description": " Uses the extent of the specified raster dataset or feature class to define the extent of the raster datasets used in the mosaic dataset from the raster catalog or mosaic dataset. Rasters that lay along the defined extent will be included in the mosaic dataset. The input is not limited to a polygon feature class. ", "dataType": "Feature Layer;Raster Layer"}, {"name": "extent", "isOptional": true, "description": " The minimum and maximum x- and y-coordinates used to define the rectangular extent of the raster datasets used in the mosaic dataset from the raster catalog or mosaic dataset. ", "dataType": "Envelope"}, {"name": "select_using_features", "isOptional": true, "description": " If a feature class is specified, you can choose to limit the extent to its envelope or clipped to the feature. To use this, the input must be a polygon feature class. SELECT_USING_FEATURES \u2014 The selection is based on the shape of the feature. This is the default. NO_SELECT_USING_FEATURES \u2014 The selection is based on the extent of the data within the feature class.", "dataType": "Boolean"}, {"name": "lod_field", "isOptional": true, "description": " A field in the raster catalog table defining the map scales at which the mosaic should be displayed; otherwise, a wire frame will be displayed. ", "dataType": "Field"}, {"name": "minPS_field", "isOptional": true, "description": " A field in the raster catalog table defining the minimum cell size of the raster at which the mosaic should be displayed; otherwise, a wire frame will be displayed. ", "dataType": "Field"}, {"name": "maxPS_field", "isOptional": true, "description": " A field in the raster catalog table defining the maximum cell size of the raster at which the mosaic should be displayed; otherwise, a wire frame will be displayed. ", "dataType": "Field"}, {"name": "pixelSize", "isOptional": true, "description": "The maximum cell size in which the mosaic will be displayed. If the mosaic is displayed above this resolution, a wire frame will be displayed. ", "dataType": "Double"}, {"name": "build_boundary", "isOptional": true, "description": " Generates the boundary polygon for the mosaic dataset. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels. This is only available if the mosaic dataset is created within a geodatabase. BUILD_BOUNDARY \u2014 The boundary will be generated or updated. This is the default. NO_BOUNDARY \u2014 The boundary will not be generated.", "dataType": "Boolean"}]},
{"syntax": "CreateMosaicDataset_management (in_workspace, in_mosaicdataset_name, coordinate_system, {num_bands}, {pixel_type}, {product_definition}, {product_band_definitions})", "name": "Create Mosaic Dataset (Data Management)", "description": "Makes an empty mosaic dataset in a geodatabase.", "example": {"title": "CreateMosaicDataset example 1 (Python window)", "description": "This is a Python sample for CreateMosaicDataset.", "code": "import arcpy arcpy.CreateMosaicDataset_management ( \"C:/workspace/CreateMD.gdb\" , \"mosaicds\" , \"C:/workspace/World_Mercator.prj\" , \"3\" , \"8_BIT_UNSIGNED\" , \"False Color Infrared\" , \"#\" )"}, "usage": ["The destination geodatabase can be file, personal, or ArcSDE.", "Once the mosaic dataset is created, you can use the ", "Add Rasters to Mosaic Dataset", " to populate it with rasters.", "The name of the mosaic dataset must keep within the limits of the geodatabase or underlying database. For example, the name cannot start with a number."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The path and name of a geodatabase. ", "dataType": "Workspace"}, {"name": "in_mosaicdataset_name", "isInputFile": true, "isOptional": false, "description": "The name of the mosaic dataset. ", "dataType": "String"}, {"name": "coordinate_system", "isOptional": false, "description": "The coordinate system for the mosaic dataset, which will be used for all the associated files or features created. ", "dataType": "Spatial Reference"}, {"name": "num_bands", "isOptional": true, "description": "The number of raster dataset bands supported by the mosaic dataset. ", "dataType": "Long"}, {"name": "pixel_type", "isOptional": true, "description": " The bit depth of a cell, used to determine the range of values that the mosaic dataset output. For example, an 8-bit mosaic dataset can have 256 unique pixel values, which range from 0 to 255. If it is not defined, it will be taken from the first raster dataset. 1_BIT \u2014 A 1-bit unsigned integer. The values can be 0 or 1. 2_BIT \u2014 A 2-bit unsigned integer. The values supported can be from 0 to 3. 4_BIT \u2014 A 4-bit unsigned integer. The values supported can be from 0 to 15. 8_BIT_UNSIGNED \u2014 An unsigned 8-bit data type. The values supported can be from 0 to 255. 8_BIT_SIGNED \u2014 A signed 8-bit data type. The values supported can be from -128 to 127. 16_BIT_UNSIGNED \u2014 A 16-bit unsigned data type. The values can range from 0 to 65,535. 16_BIT_SIGNED \u2014 A 16-bit signed data type. The values can range from -32,768 to 32,767. 32_BIT_UNSIGNED \u2014 A 32-bit unsigned data type. The values can range from 0 to 4,294,967,295. 32_BIT_SIGNED \u2014 A 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647. 32_BIT_FLOAT \u2014 A 32-bit data type supporting decimals. 64_BIT \u2014 A 64-bit data type supporting decimals.", "dataType": "String"}, {"name": "product_definition", "isOptional": true, "description": "A template defining the number of bands and mid-wavelength ranges for each band. None \u2014 No band ordering is specified for the mosaic dataset. This is the default. NATURAL_COLOR_RGB \u2014 Creates a three-band mosaic dataset, with red, green, and blue wavelength ranges. This is designed for natural color imagery. NATURAL_COLOR_RGBI \u2014 Creates a four-band mosaic dataset, with red, green, blue and near infrared wavelength ranges. FALSE_COLOR_IRG \u2014 Creates a three-band mosaic dataset, with near infrared, red, and green wavelength ranges. FORMOSAT-2_4BANDS \u2014 Creates a four-band mosaic dataset using the FORMOSAT-2 wavelength ranges. GEOEYE-1_4BANDS \u2014 Creates a four-band mosaic dataset using the GeoEye-1 wavelength ranges. IKONOS_4BANDS) \u2014 Creates a four-band mosaic dataset using the IKONOS wavelength ranges. KOMPSAT-2_4BANDS \u2014 Creates a four-band mosaic dataset using the KOMPSAT-2 wavelength ranges. LANDSAT_6BANDS \u2014 Creates a six-band mosaic dataset using the Landsat 5 and 7 wavelength ranges from the TM and ETM+ sensors. LANDSAT_MSS_4BANDS \u2014 Creates a four-band mosaic dataset using the Landsat wavelength ranges from the MSS sensor. QUICKBIRD_4BANDS \u2014 Creates a four-band mosaic dataset using the QuickBird wavelength ranges. RAPIDEYE_5BANDS \u2014 Creates a five-band mosaic dataset using the RapidEye wavelength ranges. SPOT-5_4BANDS \u2014 Creates a four-band mosaic dataset using the SPOT 5 wavelength ranges. WORLDVIEW-2_8BANDS \u2014 Creates an eight-band mosaic dataset using the WorldView-2 wavelength ranges. CUSTOM \u2014 Allows you to define the number of bands and the mid-wavelength ranges for each band.", "dataType": "String"}, {"name": "product_band_definitions", "isOptional": false, "description": "The band and wavelength information for the product_definition can be edited. You can modify the wavelengths, change the band order, or you can add or remove bands when using the CUSTOM product definition. ", "dataType": "Value Table"}]},
{"syntax": "CalculateCellSizeRanges_management (in_mosaic_dataset, {where_clause}, {do_compute_min}, {do_compute_max}, {max_range_factor}, {cell_size_tolerance_factor}, {update_missing_only})", "name": "Calculate Cell Size Ranges (Data Management)", "description": "Computes the minimum and maximum cell sizes for the rasters in a mosaic dataset.", "example": {"title": "CalculateCellSizeRanges example 1 (Python window)", "description": "This is a Python sample for CalculateCellSizeRanges.", "code": "import arcpy arcpy.CalculateCellSizeRanges_management ( \"C:/Workspace/cellsize.gdb/md\" , \"#\" , \"MIN_CELL_SIZES\" , \"MAX_CELL_SIZES\" , \"20\" , \"1\" , \"UPDATE_MISSING_ONLY\" )"}, "usage": [" Cell size ranges are specified in the attribute table, under the minPS and maxPS columns. You can edit these values manually if needed. ", "You cannot calculate cell size ranges for a referenced mosaic dataset."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "Path and name of mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": " Using SQL you can define a query, or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "do_compute_min", "isOptional": true, "description": "Computes the minimum pixel size for all the rasters in the mosaic dataset. MIN_CELL_SIZES \u2014 Computes the minimum pixel size. This is the default. NO_MIN_CELL_SIZES \u2014 The minimum pixel size is not computed. ", "dataType": "Boolean"}, {"name": "do_compute_max", "isOptional": true, "description": "Computes the maximum pixel size for all the rasters in the mosaic dataset. MAX_CELL_SIZES \u2014 Computes the maximum pixel size. This is the default. NO_MAX_CELL_SIZES \u2014 The maximum pixel size is not computed. ", "dataType": "Boolean"}, {"name": "max_range_factor", "isOptional": true, "description": "Controls the factor by which the maximum pixel size of the top-most overview is multiplied to ensure visibility of the mosaic when viewed at small scales. The default is 10. ", "dataType": "Double"}, {"name": "cell_size_tolerance_factor", "isOptional": true, "description": "Allows you to specify a cell size tolerance. This is useful when you have rasters with differing pixel sizes that should be considered the same. The default is 0.8. ", "dataType": "Double"}, {"name": "update_missing_only", "isOptional": true, "description": " Allows you to calculate only the missing cell size range values. UPDATE_ALL \u2014 Cell size minimum and maximum values will be calculated for all rasters within the mosaic dataset. This is the default. UPDATE_MISSING_ONLY \u2014 Cell size minimum and maximum values will only be calculated if they do not exist. ", "dataType": "Boolean"}]},
{"syntax": "BuildSeamlines_management (in_mosaic_dataset, {cell_size}, {sort_method}, {sort_order}, {order_by_attribute}, {order_by_base_value}, {view_point}, {computation_method}, {blend_width}, {blend_type}, {request_size}, {request_size_type})", "name": "Build Seamlines (Data Management)", "description": " Automatically generates seamlines for your mosaic dataset. ", "example": {"title": "BuildSeamlines example 1 (Python window)", "description": "This is a Python sample for BuildSeamlines.", "code": "import arcpy arcpy.BuildSeamlines_management ( \"c:/data/Seamlines.gdb/md\" , \"40\" , \"NORTH_WEST\" , \"#\" , \"#\" , \"#\" , \"#\" , \"RADIOMETRY\" , \"5\" , \"INSIDE\" , \"#\" , \"#\" )"}, "usage": ["The seamlines are generated, so there is one seamline per footprint.", "You cannot build seamlines for a referenced mosaic dataset.", "If you plan on color correcting your mosaic dataset, it is recommended that you color correct before building seamlines, if the computation method is Radiometry. The color correction will be taken into account when seamlines are built.", "To remove seamlines, right-click the mosaic dataset in ", "ArcCatalog", " or the ", "Catalog", " window and click ", "Remove ", ">", " Remove Seamlines", "."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " Path and name of the mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "cell_size", "isOptional": true, "description": " The cell size is used to determine the rasters for which seamlines are generated. This is often used when there are various resolutions of data in a mosaic dataset and you want seamlines generated for only one level. For example, if you mix a high-resolution data source with a low-resolution data source you can specify a cell size that fits within the range of one of these data sources. If you have multiple LOWPS (low pixel size) values, or if you are not sure what cell size to specify, leave this parameter empty. The tool will automatically create seamlines at the appropriate levels. The units for this parameter is in the same units as the spatial reference of the mosaic dataset. ", "dataType": "Double"}, {"name": "sort_method", "isOptional": true, "description": " The sort method is similar to the mosaic method in that it defines the order in which the rasters will be fused together to generate the image used to create the seamlines. NORTH_WEST \u2014 Orders rasters in a view-independent way where rasters with their centers most northwest are displayed on top. This is the default. CLOSEST_TO_VIEWPOINT \u2014 Orders rasters based on a user-defined location and nadir location for the rasters using the Viewpoint tool. BY_ATTRIBUTE \u2014 Orders rasters based on an attribute and its difference from a base value.", "dataType": "String"}, {"name": "sort_order", "isOptional": true, "description": "Choose whether to sort the rasters in ascending order or descending order. ASCENDING \u2014 The rasters will be sorted in ascending order. This is the default. DESCENDING \u2014 The rasters will be sorted in descending order. ", "dataType": "Boolean"}, {"name": "order_by_attribute", "isOptional": true, "description": " The attribute field to order rasters when the sort method is BY_ATTRIBUTE. The default attribute is ObjectID. ", "dataType": "Field"}, {"name": "order_by_base_value", "isOptional": true, "description": " The rasters are sorted based on the difference between their value and the value from the Sort Attribute field. ", "dataType": "Variant"}, {"name": "view_point", "isOptional": true, "description": " The coordinate location to use when the sort method is CLOSEST_TO_VIEWPOINT. ", "dataType": "Point"}, {"name": "computation_method", "isOptional": true, "description": " Choose which computation method to use while building the seamlines. The sort order is applied with each method. GEOMETRY \u2014 Generates seamlines from the footprints, taking the Sort Method into account. This is the default. RADIOMETRY \u2014 Examines the values and patterns within the intersecting areas to compute the seamlines. COPY_FOOTPRINT \u2014 Generates seamlines from the footprints. COPY_TO_SIBLING \u2014 Copies an existing seamline of a raster item to its siblings which share the same group name.This is commonly used with satellite imagery when the panchromatic band does not always share the same extent as the multispectral band. This option makes sure they share the same seamline. ", "dataType": "String"}, {"name": "blend_width", "isOptional": true, "description": " Blending (feathering) occurs along a seamline between pixels where there are overlapping rasters. The blend width defines how many pixels will be blended. If the blend width value is 10, and you use BOTH as the blend type, then 5 pixels will be blended on the inside and outside of the seamline. If the value is 10, and the blend type is INSIDE, then 10 pixels will be blended on the inside of the seamline. ", "dataType": "Double"}, {"name": "blend_type", "isOptional": true, "description": " Blending (feathering) occurs along a seamline between pixels where there are overlapping rasters. The blend type defines where the blending will occur along the seamline. BOTH \u2014 The pixels will be blended on the inside and outside of the seamline. For example, if the blend width value is 10, and you use BOTH, then 5 pixels will be blended on the inside and outside of the seamline.This is the default. INSIDE \u2014 The pixels will be blended on the inside of the seamline. OUTSIDE \u2014 The pixels will be blended on the outside of the seamline.", "dataType": "String"}, {"name": "request_size", "isOptional": true, "description": "The size to which the raster will be resampled when it is examined using this process. The value (such as 1,000) defines the dimension of rows and columns. The maximum value is 5,000. The default values for the Request Size changes based on the option selected in the Request Size Type . The default values are 1000 for PIXELS and 5 for the PIXELSIZE_FACTOR. You can increase or decrease this value based on the complexity of your raster data. Greater image resolution provides more detail in the raster dataset and thereby increases the processing time. ", "dataType": "Long"}, {"name": "request_size_type", "isOptional": true, "description": "The Request Size Type will modify the request size based on the pixel or the pixel size factor selected. Based on the selected request size type, the default values for the Request Size changes using which the raster will be resampled. PIXELS \u2014 The request size will be modified based on the pixel size.This is the default option and resamples the cost image based on the raster pixel size. PIXELSIZE_FACTOR \u2014 The request size will be modified based on the pixel size factor.This option resamples the cost image by multiplying the raster pixel size (from cell size level table) with the pixel size factor.", "dataType": "String"}]},
{"syntax": "BuildOverviews_management (in_mosaic_dataset, {where_clause}, {define_missing_tiles}, {generate_overviews}, {generate_missing_images}, {regenerate_stale_images})", "name": "Build Overviews (Data Management)", "description": "Defines and generates overviews for a mosaic dataset.", "example": {"title": "BuildOverviews example 1 (Python window)", "description": "This is a Python sample for BuildOverviews.", "code": "import arcpy arcpy.BuildOverviews_management ( \"C:/Workspace/Overviews.gdb/md\" , \"OBJECTID<5\" , \"DEFINE_MISSING_TILES\" , \"NO_GENERATE_OVERVIEWS\" , \"IGNORE_MISSING_IMAGES\" , \"IGNORE_STALE_IMAGES\" )"}, "usage": ["This tool will honor the default mosaic dataset settings, not the properties set on a mosaic dataset layer.  For instance, the tool will not accept layer property changes such  as band count, mosaic method, extent, or query\u2014these properties will be determined by the mosaic dataset and default properties of the mosaic dataset.", "You must check on ", "Define missing overview tiles", ", ", "Generate overviews", ", or both.", "If you need more control over the definition of the overviews, use the ", "Define Overviews", " tool first.", "By default, the overviews generated for mosaic datasets in a file geodatabase and personal geodatabase are stored in a folder ", "<gdbname>.Overviews", " in the same location where the geodatabase containing the mosaic dataset exists. The overviews generated for mosaic datasets on the SDE are stored within the geodatabase containing the mosaic dataset. These locations can be changed by first using the ", "Define Overviews", " tool and specifying a location.", "You cannot build overviews for a referenced mosaic dataset."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "Path and name of the mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": " Using SQL, you can define a query or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "define_missing_tiles", "isOptional": true, "description": "Generates overviews if not enough overviews were defined or if new data was added without defining additional overviews. DEFINE_MISSING_TILES \u2014 This will automatically identify where overviews are needed and define them. This is the default. NO_DEFINE_MISSING_TILES \u2014 New overviews will not be defined. ", "dataType": "Boolean"}, {"name": "generate_overviews", "isOptional": true, "description": " All overviews that need to be created or re-created will be generated. This includes missing overviews and stale overviews. GENERATE_OVERVIEWS \u2014 All types and states of overviews will be generated. This is the default. NO_GENERATE_OVERVIEWS \u2014 Only the overviews that have been defined and not generated will be built.", "dataType": "Boolean"}, {"name": "generate_missing_images", "isOptional": true, "description": "Use if overviews have been defined but not generated. GENERATE_MISSING_IMAGES \u2014 Overviews that have been defined but not generated will be generated. This is the default. IGNORE_MISSING_IMAGES \u2014 Overviews that have been defined but not generated will not be generated. ", "dataType": "Boolean"}, {"name": "regenerate_stale_images", "isOptional": true, "description": " If the underlying raster datasets have changed or had their properties modified, the overviews will be identified as stale. REGENERATE_STALE_IMAGES \u2014 Stale overviews will be updated. This is the default. IGNORE_STALE_IMAGES \u2014 Stale overviews will not be updated. ", "dataType": "Boolean"}]},
{"syntax": "BuildFootprints_management (in_mosaic_dataset, {where_clause}, {reset_footprint}, {min_data_value}, {max_data_value}, {approx_num_vertices}, {shrink_distance}, {maintain_edges}, {skip_derived_images}, {update_boundary}, {request_size}, {min_region_size}, {simplification_method})", "name": "Build Footprints (Data Management)", "description": "Computes the footprints for each raster dataset in a mosaic dataset.", "example": {"title": "BuildFootprints example (Python window)", "description": "This is a Python sample for the BuildFootprints tool.", "code": "import arcpy arcpy.BuildFootprints_management ( \"c:/data/Footprints.gdb/md\" , \"#\" , \"RADIOMETRY\" , \"1\" , \"254\" , \"25\" , \"0\" , \"#\" , \"SKIP_DERIVED_IMAGES\" , \"UPDATE_BOUNDARY\" , \"#\" , \"#\" , \"CONVEX_HULL\" )"}, "usage": ["If there is a selection, only those selected footprints will be recalculated.", "The footprint is used to calculate the boundary. If you modify the shape of the footprints along the perimeter of the mosaic dataset, you need to recalculate the boundary. If you don't choose to use this tool, then you can do it later using the ", "Build Boundary tool", ".", "You cannot rebuild footprints for a referenced mosaic dataset.", "The ", "Approximate Number of Vertices", " parameter is used to define the complexity of the footprints. The higher the number of vertices will mean the footprint is more accurate and more irregular. Valid values range from 4 to 10,000. You can set your value to be -1, so that no generalization will take place, but this may mean your footprint will have a very large number of vertices.", "This tool does not support the environment settings."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "The mosaic dataset for which the footprints will be calculated. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": " Using SQL, you can define a query or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "reset_footprint", "isOptional": true, "description": " Choose which method to use when redefining the footprints. RADIOMETRY \u2014 Redefines the shape of the footprint based on a pixel value range. This option is generally used to redefine footprints so that they exclude border areas, which do not define valid data. This is the default. GEOMETRY \u2014 Redefines the shape of the footprint back to its original geometry. COPY_FROM_SIBLING \u2014 When using a pansharpened raster type, the panchromatic item's footprint will be replaced with the multispectral item's footprint. This occurs with some raster types when the panchromatic and multispectral images do not have identical geometries. NONE \u2014 The footprints will not be redefined.", "dataType": "String"}, {"name": "min_data_value", "isOptional": true, "description": "The lowest pixel value representing valid image data. This value is determined by the bit depth of the raster dataset. For example, with 8-bit data, the values can range from 0 to 255. A value around 0 represents very dark colors, like black border pixels. When you specify 1, then the only value less than 1 is 0, so all 0 values will be considered invalid data and will be removed from the perimeter of the footprint. If the imagery is compressed using a lossy compression method, then you should define a value slightly greater than 1 to remove all the black pixels. When dark areas, such as shadows, have been incorrectly removed from the footprint, this value should be decreased. ", "dataType": "Double"}, {"name": "max_data_value", "isOptional": true, "description": "Highest value representing valid data. This value is determined by the bit depth of the raster dataset. For example, with 8-bit data, the values can range from 0 to 255. A value around 255 represents very bright colors, such as white clouds and snow. If you specify 245, then all values between 246 and 255 will be removed from the perimeter of the footprint. ", "dataType": "Double"}, {"name": "approx_num_vertices", "isOptional": true, "description": " Approximate number of vertices with which the new footprint polygon will be created. The minimum value is 4 and the maximum value is 10,000. The greater this value, the more accurate and irregular the polygon and the longer the processing time. A value of -1 will show all the vertices in the footprint, therefore your polygon footprint will not be generalized. ", "dataType": "Long"}, {"name": "shrink_distance", "isOptional": true, "description": "Distance value specified in the units of the mosaic dataset's coordinate system by which the overall polygon will be reduced in size. Shrinking of the polygon is used to counteract effects of lossy compression, which causes edges of the image to overlap into NoData areas. ", "dataType": "Double"}, {"name": "maintain_edges", "isOptional": true, "description": "Use this parameter when using raster datasets that have been tiled and are butt joined (or line up along the seams with little to no overlap). NO_MAINTAIN_EDGES \u2014 All footprints will be altered, regardless of their neighboring footprints. This is the default. MAINTAIN_EDGES \u2014 An analysis of the image edges is performed so that the sheet edges are not removed.", "dataType": "Boolean"}, {"name": "skip_derived_images", "isOptional": true, "description": "Adjust the footprints for derived images, such as service overviews. SKIP_DERIVED_IMAGES \u2014 Derived images, such as service overviews, will not be adjusted. This is the default. NO_SKIP_DERIVED_IMAGES \u2014 Footprints of any derived images will be adjusted along with the base images. ", "dataType": "Boolean"}, {"name": "update_boundary", "isOptional": true, "description": "Generates or updates the boundary polygon of a mosaic dataset. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels. UPDATE_BOUNDARY \u2014 The boundary will be generated or updated. This is the default. NO_BOUNDARY \u2014 The boundary will not be generated or updated.", "dataType": "Boolean"}, {"name": "request_size", "isOptional": true, "description": " The size to which the raster will be resampled when it is examined using this tool. The value (such as 2,000) defines the dimension using rows and columns. You can increase or decrease this value based on the complexity of your raster data. Greater image resolution provides more detail in the raster dataset and thereby increases the processing time. A value of -1 will not resample the footprint, therefore it will compute the footprint at the native pixel size. The request size cannot be greater than the raster contained by the footprints. If this is the case, then the value will automatically be the same as the size of the raster. ", "dataType": "Long"}, {"name": "min_region_size", "isOptional": true, "description": " Determines a filter used to remove holes created in the footprint. This value is specified in pixels, and it is directly related to the Request Size, not to the pixel resolution of the source raster. ", "dataType": "Long"}, {"name": "simplification_method", "isOptional": true, "description": " The simplification will reduce the number of vertices, since dense footprints can affect display performance. Choose which method to use in order to simplify the footprints. NONE \u2014 No simplification method will be implemented. This is the default. CONVEX_HULL \u2014 The minimum bounding geometry for each footprint will be used to simplify the footprint. ENVELOPE \u2014 The envelope of each mosaic dataset item will provide the simplified footprint.", "dataType": "String"}]},
{"syntax": "BuildBoundary_management (in_mosaic_dataset, {where_clause}, {append_to_existing}, {simplification_method})", "name": "Build Boundary (Data Management)", "description": "Generates the boundary polygon for a mosaic dataset. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels.", "example": {"title": "BuildBoundary example (Python window)", "description": "This is a Python sample for the BuildBoundary tool.", "code": "import arcpy arcpy.BuildBoundary_management ( \"c:/workspace/Boundary.gdb/md\" , \"#\" , \"APPEND\" , \"CONVEX_HULL\" )"}, "usage": ["Boundaries can only be generated for mosaic datasets stored within a geodatabase.", "If you remove or add rasters, or modify the extent of the footprints, you should use this tool to rebuild the boundary.", "If you modify the shape of the boundary polygon (using editing tools), you can use this tool to rebuild the boundary to its unmodified shape.", "Use the ", "Append To Existing Boundary", " option to only update the boundary where newly added footprints exist. Since the entire boundary is not recalculated, it will save time."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "The mosaic dataset for which the boundary will be calculated. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": "Enter an SQL query if you would like to create a subset of the data, based on attributes. ", "dataType": "SQL Expression"}, {"name": "append_to_existing", "isOptional": true, "description": "When footprints have been selected, this option determines how the boundary will be modified. OVERWRITE \u2014 Removes any existing boundary, then creates a newly computed boundary. APPEND \u2014 Appends the perimeter of footprints to the existing boundary. This can save time when adding additional raster data to the mosaic dataset, as the entire boundary will not be recalculated. If there are rasters selected, the boundary will be recalculated to include only the selected footprints. This is the default. ", "dataType": "Boolean"}, {"name": "simplification_method", "isOptional": true, "description": "The simplification method reduces the number of vertices, since a dense boundary can affect performance. Choose which simplification method to use in order to simplify the boundary. NONE \u2014 No simplification method will be implemented. This is the default. CONVEX_HULL \u2014 The minimum bounding geometry of the mosaic dataset will be used to simplify the boundary. If there are any footprints that are disconnected, then a minimum bounding geometry for each continuous group of footprints will be used to simplify the boundary. ENVELOPE \u2014 The envelope of the mosaic dataset will provide a simplified boundary. If there are any footprints that are disconnected, then an envelope for each continuous group of footprints will be used to simplify the boundary.", "dataType": "String"}]},
{"syntax": "AddRastersToMosaicDataset_management (in_mosaic_dataset, raster_type, input_path, {update_cellsize_ranges}, {update_boundary}, {update_overviews}, {maximum_pyramid_levels}, {maximum_cell_size}, {minimum_dimension}, {spatial_reference}, {filter}, {sub_folder}, {duplicate_items_action}, {build_pyramids}, {calculate_statistics}, {build_thumbnails}, {operation_description}, {force_spatial_reference})", "name": "Add Rasters To Mosaic Dataset (Data Management)", "description": "Adds raster datasets to a mosaic dataset from many sources, including a file, folder, raster catalog, table, or web service.", "example": {"title": "AddRastersToMosaicDataset example 1 (Python window)", "description": "This is a Python sample for the AddRastersToMosaicDataset tool.", "code": "import arcpy arcpy.AddRastersToMosaicDataset_management ( \"c:/data/AddMD.gdb/md_landsat\" , \"Landsat 7 ETM+\" , \"c:/data/landsat7etm\" , \"UPDATE_CELL_SIZES\" , \"UPDATE_BOUNDARY\" , \"NO_OVERVIEWS\" , \"UPDATE_OVERVIEWS\" , \"2\" , \"#\" , \"#\" , \"GCS_WGS_1984.prj\" , \"*.tif\" , \"SUBFOLDERS\" , \"EXCLUDE_DUPLICATES\" , \"NO_PYRAMIDS\" , \"NO_STATISTICS\" , \"BUILD_THUMBNAILS\" , \"Add Landsat L1G\" , \"FORCE_SPATIAL_REFERENCE\" )"}, "usage": ["Raster data that is added is unmanaged; therefore, if the raster data is deleted or moved the mosaic dataset will be affected.", "You can only add rasters to a mosaic dataset contained within a geodatabase. Those created outside a geodatabase can only contain the contents of a raster catalog or previously created mosaic dataset.", "The ", "Raster Type", " parameter identifies metadata required for loading data into the mosaic dataset.", "Learn more about raster types"], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "The path and name of the mosaic dataset to which the raster data will be added. ", "dataType": "Mosaic Layer"}, {"name": "raster_type", "isOptional": false, "description": "The raster type is specific for imagery products. It identifies metadata, such as georeferencing, acquisition date, and sensor type, along with a raster format. For a list of raster types, see the list of supported raster and image data formats . If you are using a LAS, LAS Dataset, or Terrain raster Type, an *.ART file must be used, where the a cell size is specified. ", "dataType": "Raster Type"}, {"name": "input_path", "isInputFile": true, "isOptional": false, "description": "Path and name of the file, folder, raster dataset, raster catalog, mosaic dataset, table, or service. File \u2014 Allows you to select one or more raster datasets stored in a folder on disk, an image service definition (.ISDef) file, and a raster process definition ( .RPDef ) file. Workspace \u2014 Allows you to select a folder containing multiple raster datasets. The folder can contain subfolders.This is affected by the Include Sub Folders and Filter parameters. Dataset \u2014 Allows you to select an ArcGIS geographic dataset, such as any raster, raster catalog, or mosaic dataset in a geodatabase, or a table. Service \u2014 Allows you to select a WCS, map, or image service, or a web service layer file.", "dataType": "File; Workspace; Raster Dataset; Mosaic Dataset; Table; Raster Layer; Raster Catalog Layer; Mosaic Layer; Terrain Layer; WCS Coverage; Image Service; MapServer; WMS Map; Dataset; Layer File; Terrain; LAS Dataset Layer"}, {"name": "update_cellsize_ranges", "isOptional": true, "description": "Calculates the cell size ranges of each raster in the mosaic dataset. These values are written to the attribute table within the minPS and maxPS columns. UPDATE_CELL_SIZES \u2014 The cell size ranges will be calculated for all the rasters in the mosaic dataset. This is the default. NO_CELL_SIZES \u2014 The cell size ranges will not be calculated.", "dataType": "Boolean"}, {"name": "update_boundary", "isOptional": true, "description": "Generates or updates the boundary polygon of a mosaic dataset. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels. UPDATE_BOUNDARY \u2014 The boundary will be generated or updated. This is the default. NO_BOUNDARY \u2014 The boundary will not be generated or updated.", "dataType": "Boolean"}, {"name": "update_overviews", "isOptional": true, "description": "Defines and generates overviews for a mosaic dataset. UPDATE_OVERVIEWS \u2014 Overviews will be defined and generated. NO_OVERVIEWS \u2014 Overviews will not be defined or generated. This is the default.", "dataType": "Boolean"}, {"name": "maximum_pyramid_levels", "isOptional": true, "description": " Defines the maximum number of pyramid levels that will be used in the mosaic dataset. For example, a value of 2 will use only the first two pyramid levels from the source raster. Leaving this blank or typing a value of -1 will build pyramids for all levels. This value can affect the display and the number of overviews that will be generated. ", "dataType": "Long"}, {"name": "maximum_cell_size", "isOptional": true, "description": " Defines the maximum pyramid cell size that will be used in the mosaic dataset. ", "dataType": "Double"}, {"name": "minimum_dimension", "isOptional": true, "description": " Defines the minimum dimensions of a raster pyramid that will be used in the mosaic dataset. ", "dataType": "Long"}, {"name": "spatial_reference", "isOptional": true, "description": "Spatial reference system of the input data. This should be specified if the data does not have a coordinate system; otherwise, the coordinate system of the mosaic dataset will be used. This can also be used to override the coordinate system of the input data. ", "dataType": "Spatial Reference"}, {"name": "filter", "isOptional": true, "description": "A filter for the data being added to the mosaic dataset. You can use SQL expressions to create the data filter. The wildcards for the filter work on the full path to the input data. If you want to add in only a TIFF image, you can add an asterisk before a file extension. If you want to add in any image with the word 'sensor' in the file path or the file name, you need to add an asterisk before and after the word 'sensor'. Or you can use PERL syntax to create a data filter. REGEX: .*1923.*|.*1922.* REGEX: .*192[34567].*|.*194.*|.*195.* *.TIF *sensor2009* REGEX: .*1923.*|.*1922.* REGEX: .*192[34567].*|.*194.*|.*195.* OBJECTID IN (19745, 19680, 19681, 19744, 5932, 5931, 5889, 5890, 14551, 14552, 14590, 14591)", "dataType": "String"}, {"name": "sub_folder", "isOptional": true, "description": "Recursively explores subfolders. SUBFOLDERS \u2014 All subfolders will be explored for data. This is the default. NO_SUBFOLDERS \u2014 Only the top-level folder will be explored for data.", "dataType": "Boolean"}, {"name": "duplicate_items_action", "isOptional": true, "description": "A check will be performed to see if each raster has already been added, using the original path and file name. Choose which action to perform when a duplicate path and file name have been found. ALLOW_DUPLICATES \u2014 All rasters will be added even if they already exist within the mosaic dataset. This is the default. EXCLUDE_DUPLICATES \u2014 The duplicate raster will not be added. OVERWRITE_DUPLICATES \u2014 The duplicate raster will overwrite the existing one.", "dataType": "String"}, {"name": "build_pyramids", "isOptional": true, "description": "Builds pyramids for each source raster. NO_PYRAMIDS \u2014 Pyramids will not be generated. This is the default. BUILD_PYRAMIDS \u2014 Pyramids will be generated.", "dataType": "Boolean"}, {"name": "calculate_statistics", "isOptional": true, "description": " Calculates statistics for each source raster. NO_STATISTICS \u2014 Statistics will not be generated. This is the default. CALCULATE_STATISTICS \u2014 Statistics will be generated.", "dataType": "Boolean"}, {"name": "build_thumbnails", "isOptional": true, "description": " Builds thumbnails for each source raster. NO_THUMBNAILS \u2014 Thumbnails will not be generated. This is the default. BUILD_THUMBNAILS \u2014 Thumbnails will be generated.", "dataType": "Boolean"}, {"name": "operation_description", "isOptional": true, "description": "A description you want used to represent this operation of adding raster data. It will be added to the raster type table which can be used as part of a search or as a reference at another time. ", "dataType": "String"}, {"name": "force_spatial_reference", "isOptional": true, "description": "Use the coordinate system that is specified for all the rasters when loading data into the mosaic dataset. NO_FORCE_SPATIAL_REFERENCE \u2014 Keep the coordinate system of each raster data when loading data. This is the default. FORCE_SPATIAL_REFERENCE \u2014 Force the coordinate system specified in this tool for each raster when loading data.", "dataType": "Boolean"}]},
{"syntax": "RegisterAsVersioned_management (in_dataset, {edit_to_base})", "name": "Register As Versioned (Data Management)", "description": "Registers ArcSDE dataset as  versioned .", "example": {"title": "RegisterAsVersioned example (stand-alone script)", "description": "\r\nThe following stand-alone script demonstrates how to use the RegisterAsVersioned tool to register a dataset as versioned.", "code": "# Name: RegisterAsVersioned_Example.py # Description: Registers dataset as versioned # Author: ESRI # Import system modules import arcpy # Set local variables datasetName = \"Database Connections/ninefour@gdb.sde/ninefour.GDB.ctgFuseFeature\" # Execute RegisterAsVersioned arcpy.RegisterAsVersioned_management ( datasetName , \"NO_EDITS_TO_BASE\" )"}, "usage": ["Versioning tools only work with datasets in an ArcSDE geodatabase.  File and Personal ", "geodatabases", " don't support versioning. ", "Registering a ", "feature dataset", " as versioned registers all ", "feature classes", " within the feature dataset as versioned.", "Versions are not affected by changes occurring in other versions of the geodatabase."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "Name of the dataset to be registered as versioned. ", "dataType": "Table View; Feature Dataset"}, {"name": "edit_to_base", "isOptional": true, "description": "Determines whether edits to the default version will be moved to the base tables. NO_EDITS_TO_BASE \u2014 Dataset will not be versioned with the option to move edits to base. This is the default. EDITS_TO_BASE \u2014 Dataset will be versioned with the option of moving edits to base. ", "dataType": "Boolean"}]},
{"syntax": "ReconcileVersion_management (in_workspace, version_name, target_name, {conflict_definition}, {conflict_resolution}, {acquire_locks}, {abort_if_conflicts}, {post})", "name": "Reconcile Version (Data Management)", "description": " Reconciles a version against another version in its lineage.", "example": {"title": null, "description": "The following stand-alone Python script uses the ReconcileVersion geoprocessing tool to reconcile a version with a version in the version's lineage and then posts that version.", "code": "# Name: ReconcileVersion_Example.py # Description: Reconciles a version with a version in the version lineage and then posts that version # Author: ESRI # Import system modules import arcpy # Set local variables inWorkspace = \"Database Connections/ninefour@gdb.sde\" versionName = \"myVersion\" targetVersion = \"dbo.DEFAULT\" # Execute ReconcileVersion arcpy.ReconcileVersion_management ( inWorkspace , versionName , targetVersion , \"BY_OBJECT\" , \"FAVOR_TARGET_VERSION\" , \"LOCK_acquireD\" , \"NO_ABORT\" , \"POST\" )"}, "usage": ["The reconcile process requires that you are the only user currently editing the version and the only user able to edit the version throughout the reconcile process until you save or post.", "The reconcile process requires that you have full permissions to all the feature classes that have been modified in the version being edited.", "Versioning tools only work with ArcSDE data.  File and Personal ", "geodatabases", " don't support versioning. ", "The geodatabase is designed to efficiently manage and support long ", "transactions", " using versions.", "The reconcile process detects differences between the edit version and the target version and flags these differences as conflicts. If conflicts exist, they should be resolved."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The ArcSDE geodatabase containing the reconcilable version. The default is to use the workspace defined in the environment. ", "dataType": "Workspace"}, {"name": "version_name", "isOptional": false, "description": "Name of the Edit Version to be reconciled with the Target Version. ", "dataType": "String"}, {"name": "target_name", "isOptional": false, "description": "Name of any version in the direct ancestry of the Edit version, such as the parent version or the default version. ", "dataType": "String"}, {"name": "conflict_definition", "isOptional": true, "description": "Describes the conditions required for a conflict to occur: BY_OBJECT \u2014 Any changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default. BY_ATTRIBUTE \u2014 Only changes to the same attribute of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to different attributes will not be considered a conflict during reconcile. ", "dataType": "String"}, {"name": "conflict_resolution", "isOptional": true, "description": "Describes the behavior if a conflict is detected: FAVOR_TARGET_VERSION \u2014 For all conflicts, resolves in favor of the target version. This is the default. FAVOR_EDIT_VERSION \u2014 For all conflicts, resolves in favor of the edit version. ", "dataType": "String"}, {"name": "acquire_locks", "isOptional": true, "description": "Determines whether feature locks will be acquired. LOCK_ACQUIRED \u2014 Acquires locks when there is no intention of posting the edit session. This is the default. NO_LOCK_ACQUIRED \u2014 No locks are acquired and the edit session will be posted to the target version. ", "dataType": "Boolean"}, {"name": "abort_if_conflicts", "isOptional": true, "description": "Determines if the reconcile process should be aborted if conflicts are found between the target version and the edit version. NO_ABORT \u2014 Does not abort the reconcile if conflicts are found. This is the default. ABORT_CONFLICTS \u2014 Aborts the reconcile if conflicts are found. ", "dataType": "Boolean"}, {"name": "post", "isOptional": true, "description": "Posts the current edit session to the reconciled target version. NO_POST \u2014 Current edits will not be posted to the target version after the reconcile. This is the default. POST \u2014 Current edits will be posted to the target version after the reconcile. ", "dataType": "Boolean"}]},
{"syntax": "PostVersion_management (in_workspace, version_name)", "name": "Post Version (Data Management)", "description": "Posting is the process of applying the current edit session to the reconciled target version during  version  geodatabase editing. Before a version can be posted, it must be  reconciled  with a target version and all conflicts must be resolved.", "example": {"title": null, "description": null, "code": "import arcgisscripting gp = arcgisscripting.create () gp.postversion ( \"Database Connections\\Connection to workspace.sde\" , \"housing\" )"}, "usage": ["Posting synchronizes the edit version with the reconciled version and saves the data.", "Posting can't be undone since you are applying changes to a version that you are not currently editing.", "If the reconciled version is modified between reconciling and posting, you will be notified to reconcile again before posting."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The ArcSDE geodatabase containing the edit version to be posted. The default is to use the workspace defined in the environment. ", "dataType": "Workspace"}, {"name": "version_name", "isOptional": false, "description": "Name of the edit version to be posted to the target version. ", "dataType": "String"}]},
{"syntax": "DeleteVersion_management (in_workspace, version_name)", "name": "Delete Version (Data Management)", "description": "Deletes the specified  version  from the input workspace.", "example": {"title": "DeleteVersion example (stand-alone script)", "description": "\r\n The following stand-alone script demonstrates how use the DeleteVersion tool to delete a version.\r\n", "code": "# Name: DeleteVersion_Example.py # Description: Deletes a version # Import system modules import arcpy # Set local variables inWorkspace = \"Database Connections/whistler@gdb.sde\" newName = \"myVersion2\" # Execute DeleteVersion arcpy.DeleteVersion_management ( inWorkspace , newName )"}, "usage": ["Only the version's owner can rename, delete, or alter the version.", "A parent version cannot be deleted until all dependent child versions are deleted.", "Versions are not affected by changes occurring in other versions of the ", "database", "."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The enterprise geodatabase containing the version to be deleted. The default is to use the workspace defined in the Current Workspace environment. ", "dataType": "Workspace"}, {"name": "version_name", "isOptional": false, "description": "The name of the version to be deleted. ", "dataType": "String"}]},
{"syntax": "CreateVersion_management (in_workspace, parent_version, version_name, {access_permission})", "name": "Create Version (Data Management)", "description": "Creates a new version in the specified geodatabase.", "example": {"title": null, "description": "\r\n The following stand-alone script demonstrates how to create a new version.\r\n", "code": "# Name: CreateVersion_Example.py # Description: Creates a new version # Import system modules import arcpy # Set local variables inWorkspace = \"Database Connections/whistler@gdb.sde\" parentVersion = \"dbo.DEFAULT\" versionName = \"myVersion\" # Execute CreateVersion arcpy.CreateVersion_management ( inWorkspace , parentVersion , versionName , \"PUBLIC\" )"}, "usage": ["The output version name is prefixed by the geodatabase user name\u2014for example, ", "SDE.arctoolbox", ".", "The output version's permissions are set to private by default but can be changed using the ", "Alter Version", " tool.", "Personal and File ", "geodatabases", " do not support versioning. Versioning tools only work with enterprise geodatabases.", "Versions", " are not affected by changes occurring in other versions of the database.", "A version's permission can only be changed by its owner (the user who created it)."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The enterprise geodatabase that contains the parent version and will contain the new version. The default is to use the workspace defined in the environment settings. ", "dataType": "Workspace"}, {"name": "parent_version", "isOptional": false, "description": "The geodatabase, or version of a geodatabase, on which the new version will be based. ", "dataType": "String"}, {"name": "version_name", "isOptional": false, "description": "The name of the version to be created. ", "dataType": "String"}, {"name": "access_permission", "isOptional": true, "description": "The permission access level for the version. ", "dataType": "String"}]},
{"syntax": "ChangeVersion_management (in_features, version_type, {version_name}, {date})", "name": "Change Version (Data Management)", "description": " Each input feature layer or table view will have its workspace modified to connect to the requested version.", "example": {"title": "ChangeVersion Example (Python Window)", "description": "The following Python window script demonstrates how to use the ChangeVersion function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = r'Database Connections\\toolbox.sde' arcpy.MakeFeatureLayer_management ( r'TOOLBOX.Redlands\\TOOLBOX.street' , 'RedlandsStreets' ) arcpy.MakeFeatureLayer_management ( r'TOOLBOX.Redlands\\TOOLBOX.streams' , 'RedlandsStreams' ) arcpy.MakeFeatureLayer_management ( arcpy.SelectLayerByLocation_management ( \"RedlandsStreams\" , \"WITHIN_A_DISTANCE\" , \"RedlandsStreets\" , \"100 Meters\" , \"NEW_SELECTION\" , '#' ), 'StreamsNearStreets' , '' , '' , '' ) arcpy.ChangeVersion_management ( 'RedlandsStreets' , 'TRANSACTIONAL' , 'TOOLBOX.proposedStreets2k9' , '' ) arcpy.MakeFeatureLayer_management ( arcpy.SelectLayerByLocation_management ( \"RedlandsStreams\" , \"WITHIN_A_DISTANCE\" , \"RedlandsStreets\" , \"100 Meters\" , \"NEW_SELECTION\" , '#' ), 'NewStreamsNearStreets' , '' , '' , '' )"}, "usage": [" Only works with feature layers and table views. ", "The ArcSDE connection file used to create the input feature layer or table view will not be edited by this tool. Only the open workspace of the Feature Layer/Table View is changed to connect to the specified version.", "Transactional and historical views are supported."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The Input Feature Layer or Table View to connect to using the specified version. ", "dataType": "Feature Layer"}, {"name": "version_type", "isOptional": false, "description": " The type of version to change to. TRANSACTIONAL \u2014 Connect to a defined state of the database. HISTORICAL \u2014 Connect to a version representing a defined moment in time, often specified by a time or Historical Marker. ", "dataType": "String"}, {"name": "version_name", "isOptional": true, "description": " Name of the version to change to. Optional if using historical versions. ", "dataType": "String"}, {"name": "date", "isOptional": true, "description": "Date of the historical version to change to. ", "dataType": "Date"}]},
{"syntax": "AlterVersion_management (in_workspace, in_version, {name}, {description}, {access})", "name": "Alter Version (Data Management)", "description": "Alters the database version's properties of name, description, and access permissions.", "example": {"title": "AlterVersion example: ", "description": "\r\nThe following stand-alone Python script uses the AlterVersion Goeprocessing tool to alter a version.\r\n", "code": "# Name: AlterVersion_Example.py # Description: Changes the name of a version # Import system modules import arcpy # Set local variables inWorkspace = \"Database Connections/ninefour@gdb.sde\" versionName = \"myVersion\" newName = \"myVersion2\" # Execute AlterVersion arcpy.AlterVersion_management ( inWorkspace , versionName , newName , \"#\" , \"PUBLIC\" )"}, "usage": ["Versions can only be created and edited with the ", "ArcGIS for Desktop Standard", " and ", "ArcGIS for Desktop Advanced", " license levels."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The enterprise geodatabase where the version to be altered is located. ", "dataType": "Workspace"}, {"name": "in_version", "isInputFile": true, "isOptional": false, "description": "Name of the version to be altered. ", "dataType": "String"}, {"name": "name", "isOptional": true, "description": "The new name of the version. ", "dataType": "String"}, {"name": "description", "isOptional": true, "description": "The new description for the version. ", "dataType": "String"}, {"name": "access", "isOptional": true, "description": "Set/change the access permissions. PRIVATE \u2014 Only the owner may view the version and modify available feature classes. This is the default. PUBLIC \u2014 Any user may view the version and modify available feature classes. PROTECTED \u2014 Any user may view the version, but only the owner may modify available feature classes.", "dataType": "String"}]},
{"syntax": "ValidateTopology_management (in_topology, {visible_extent})", "name": "Validate Topology (Data Management)", "description": "Validates a geodatabase topology.   Validate Topology  performs the following operations:", "example": {"title": "ValidateTopology example stand-alone script", "description": "The following stand-alone script demonstrates how to use the ValidateTopology function.", "code": "# Name: ValidateTopology_Example.py # Description: Validates a topology import arcpy from arcpy import env arcpy.ValidateTopology_management ( \"D:\\Calgary\\Trans.mdb\\Streets\\Street_Topo\" )"}, "usage": ["If you validate an enterprise geodatabase topology in ", "ArcCatalog", ", the feature dataset that the topology is within must not be registered as versioned.", "This tool will only process dirty areas.  For details on dirty areas, see ", "Topology_in_ArcGIS", ". ", "If the tool is being used in ArcMap, the ", "Visible Extent", " parameter can be used  to limit validation to the extent visible in the map display."], "parameters": [{"name": "in_topology", "isInputFile": true, "isOptional": false, "description": "The geodatabase topology to be validated. ", "dataType": "Topology Layer"}, {"name": "visible_extent", "isOptional": true, "description": "In ArcMap, determines whether to use the current visible extent of the map or the full extent of the topology. If run in ArcCatalog or in a Python script, the entire extent of the topology will be validated regardless of this parameter setting. FULL_EXTENT \u2014 The entire extent of the topology will be validated. This is the default. VISIBLE_EXTENT \u2014 Only the current visible extent will be validated. ", "dataType": "Boolean"}]},
{"syntax": "SetClusterTolerance_management (in_topology, cluster_tolerance)", "name": "Set Cluster Tolerance (Data Management)", "description": "Sets the cluster tolerance of a topology. ", "example": {"title": "SetClusterTolerance stand-alone script", "description": "The following stand-alone script demonstrates how to use the SetClusterTolerance function.", "code": "# Name: SetClusterTolerance_Example.py # Description: Updates the cluster tolerance property on a topology # Author: ESRI # Import system modules import arcpy from arcpy import env arcpy.SetClusterTolerance_management ( \"D:\\Calgary\\Trans.mdb\\Streets\\Street_Topo\" , 0.00015 )"}, "usage": ["You cannot alter the cluster tolerance for a topology if the topology has been registered as versioned.", "Changing the cluster tolerance will require the entire topology be validated.", "For more information about ", "cluster tolerance", ", see ", "Topology_in_ArcGIS"], "parameters": [{"name": "in_topology", "isInputFile": true, "isOptional": false, "description": "The topology for which you want to change the cluster tolerance. This is the full path to the topology, note the topology's name or the topology layer's name when in ArcMap. ", "dataType": "Topology Layer"}, {"name": "cluster_tolerance", "isOptional": false, "description": "The value to be set as the cluster tolerance property of the selected topology. If you enter a value of zero, the default or minimum cluster tolerance will be applied to the topology. ", "dataType": "Double"}]},
{"syntax": "RemoveRuleFromTopology_management (in_topology, in_rule)", "name": "Remove Rule From Topology (Data Management)", "description": "Removes a rule from a topology.", "example": {"title": "RemoveRuleFromTopology Python Window example", "description": "The following script demonstrates how to use the RemoveRuleFromTopology function in the Python Window.", "code": "import arcpy from arcpy import env arcpy.RemoveRuleFromTopology_management ( \"C:/CityData.mdb/LegalFabric/topology\" , \"Must Not Have Dangles (21)\" )"}, "usage": ["When running this tool using scripting, the feature class ObjectClassID involved in the topology rule to be removed must be specified in parentheses after the rule name.  For example", "Removing a rule will require the entire extent of the  topology to be validated."], "parameters": [{"name": "in_topology", "isInputFile": true, "isOptional": false, "description": "The topology from which to remove a rule. This is the full path to the topology layer on disk, NOT the topology layer name in map. ", "dataType": "Topology Layer"}, {"name": "in_rule", "isInputFile": true, "isOptional": false, "description": "The topology rule to remove from the topology. ", "dataType": "String"}]},
{"syntax": "RemoveFeatureClassFromTopology_management (in_topology, in_featureclass)", "name": "Remove Feature Class From Topology (Data Management)", "description": "Removes a feature class from a topology.", "example": {"title": "RemoveFeatureClassFromTopology Python window example", "description": "The following stand-alone script demonstrates how to use the RemoveFeatureClassFromTopology function in the Python Window.", "code": "import arcpy arcpy.RemoveFeatureClassFromTopology_management ( \"C:/Datasets/TestGPTopology.mdb/LegalFabric/topology\" , \"Parcel_line\" )"}, "usage": ["Removing a feature class from a topology also removes all the topology rules associated with that feature class.", "Removing a feature class from a topology will require  the entire topology to be validated."], "parameters": [{"name": "in_topology", "isInputFile": true, "isOptional": false, "description": "The topology from which to remove the feature class. ", "dataType": "Topology"}, {"name": "in_featureclass", "isInputFile": true, "isOptional": false, "description": "The feature class to remove from the topology. ", "dataType": "String"}]},
{"syntax": "CreateTopology_management (in_dataset, out_name, {in_cluster_tolerance})", "name": "Create Topology (Data Management)", "description": "Creates a topology. The topology will not contain any feature classes or rules.   Use the  Add Feature Class To Topology  and the  Add Rule To Topology  tools to add feature classes and rules to the topology.", "example": {"title": "CreateTopology stand-alone script", "description": "The following Stand-alone script demonstrates how to use the CreateTopology function.", "code": "# Name: CreateTopology_Example.py # Description: Creates a new topology (these must reside within a feature dataset) # Author: ESRI # Import system modules import arcpy from arcpy import env env.workspace = \"h:/workspace\" arcpy.CreateTopology_management ( \"d:/landuse.mdb/landuse\" , \"landuse_Topology\" )"}, "usage": ["If the ", "Cluster Tolerance", " parameter  is blank or set to  0,  the xy tolerance of the feature dataset which contains the topology will be used.", "There is a range allowed for the cluster tolerance value, this range is derived from the precision of the ", "spatial  reference    ", " of the feature dataset in which the topology is contained. If the value entered is larger than the maximum cluster tolerance, the maximum value will be used instead. If the value entered is smaller than the minimum, the minimum value will be used."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The feature dataset in which the topology will be created. ", "dataType": "Feature Dataset"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the topology to be created. This name must be unique across the entire geodatabase. ", "dataType": "String"}, {"name": "in_cluster_tolerance", "isInputFile": true, "isOptional": true, "description": "The cluster tolerance to be set on the topology. The larger the value, the more likely vertices will be to cluster together. ", "dataType": "Double"}]},
{"syntax": "AddRuleToTopology_management (in_topology, rule_type, in_featureclass, {subtype}, {in_featureclass2}, {subtype2})", "name": "Add Rule To Topology (Data Management)", "description": "Adds a new rule to a topology. The rules you choose to add depend on the spatial relationships that you wish to monitor for the feature classes that participate in the topology.   For a complete list and description of the available topology rules see  Geodatabase topology rules and topology error fixes", "example": {"title": "AddFeatureClassToTopology stand-alone script", "description": "The following stand-alone script demonstrates how to use the AddRuleToTopology function.", "code": "# Name: AddRuleToTopology_Example.py # Description: Adds a rule to a topology # Author: ESRI # Import system modules import arcpy # Any intersection of ParcelOutline (BlockLines subtype only) needs to be reviewed arcpy.AddRuleToTopology_management ( \"C:/Landbase.mdb/LegalFabric/topology\" , \"Must Not Intersect (Line)\" , \"C:/Landbase.mdb/LegalFabric/ParcelOutline\" , \"BlockLines\" , \"\" , \"\" , )"}, "usage": ["You can enter the name of the subtype value to which you want a topology rule to be applied."], "parameters": [{"name": "in_topology", "isInputFile": true, "isOptional": false, "description": "The topology to which the new rule will be added. ", "dataType": "Topology Layer"}, {"name": "rule_type", "isOptional": false, "description": "The topology rule to be added. For a complete list of the rules and what they do, see the tool's help page. ", "dataType": "String"}, {"name": "in_featureclass", "isInputFile": true, "isOptional": false, "description": "The input or origin feature class. ", "dataType": "Feature Layer"}, {"name": "subtype", "isOptional": true, "description": "The subtype for the input or origin feature class. Enter the subtype's description (not the code). If subtypes do not exist on the origin feature class, or you want the rule to be applied to all subtypes in the feature class leave this blank. ", "dataType": "String"}, {"name": "in_featureclass2", "isInputFile": true, "isOptional": true, "description": "The destination feature class for the topology rule. ", "dataType": "Feature Layer"}, {"name": "subtype2", "isOptional": true, "description": "The subtype for the destination feature class. Enter the subtype's description (not the code). If subtypes do not exist on the origin feature class, or you want the rule to be applied to all subtypes in the feature class leave this blank. ", "dataType": "String"}]},
{"syntax": "AddFeatureClassToTopology_management (in_topology, in_featureclass, xy_rank, z_rank)", "name": "Add Feature Class To Topology (Data Management)", "description": "Adds a feature class to a topology.", "example": {"title": "AddFeatureClassToTopology example stand-alone script", "description": "The following stand-alone script demonstrates how to use the AddFeatureClassToTopology function.", "code": "# Name: AddFeatureClassToTopology_Example.py # Description: Adds a feature class to participate in a topology # Import system modules import arcpy arcpy.AddFeatureClassToTopology_management ( r\"D:\\Calgary\\Trans.mdb\\Streets\\Street_Topo\" , r\"D:\\Calgary\\Trans.mdb\\Streets\\StreetNetwork\" , 1 , 0.1 )"}, "usage": ["The new feature class must be in the same feature dataset as the topology.", "Adding a new feature class to a topology automatically makes the entire topology dirty, so when you finish adding feature classes, you will need to revalidate the topology. The new features may create errors where previously there were none depending on the topology rules associated with the feature class.", "Feature classes can only be added to topologies with the same versioned status.  For example, a versioned feature class can be added to a versioned topology, but a non-versioned feature class cannot be added to a versioned  topology.", "If the feature class you are adding is z-aware, you can rank the relative accuracy of the feature class by elevation by setting the z rank for the feature class.", "When adding a feature class to a topology, you must specify the rank of the vertices in this feature class relative to those in other feature classes. When validation of the topology cracks and clusters feature vertices, vertices from feature classes assigned a higher rank will not be moved when snapping with vertices with lower-ranked feature classes. The highest rank is 1, and you can assign up to 50 different rank values."], "parameters": [{"name": "in_topology", "isInputFile": true, "isOptional": false, "description": "The topology to which the feature class will participate. ", "dataType": "Topology Layer"}, {"name": "in_featureclass", "isInputFile": true, "isOptional": false, "description": "The feature class to add to the topology. The feature class must be in the same feature dataset as the topology. ", "dataType": "Feature Layer"}, {"name": "xy_rank", "isOptional": false, "description": "The relative degree of positional accuracy associated with vertices of features in the feature class versus those in other feature classes participating in the topology. The feature class with the highest accuracy should get a higher rank (lower number, for example, 1) than a feature class which is known to be less accurate. ", "dataType": "Long"}, {"name": "z_rank", "isOptional": false, "description": "Feature classes that are z-aware have elevation values embedded in their geometry for each vertex. By setting a z rank, you can influence how vertices with accurate z-values are snapped or clustered with vertices that contain less accurate z measurements. ", "dataType": "Long"}]},
{"syntax": "PivotTable_management (in_table, fields, pivot_field, value_field, out_table)", "name": "Pivot Table (Data Management)", "description": "Creates a table from the Input Table by reducing redundancy  in records and flattening one-to-many relationships.    ", "example": {"title": "PivotTable Example (Python Window)", "description": "The following Python Window script demonstrates how to use the PivotTable function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.PivotTable_management ( \"attributes.dbf\" , \"OwnerID\" , \"AttrTagNam\" , \"AttrValueS\" , \"C:/output/attribPivoted.dbf\" )"}, "usage": ["This tool  is typically used to reduce redundant records and flatten one-to-many relationships.", "If the pivot field is a numeric type, its value will be appended to its original field name in the output table.", "The ", "Input Field(s)", " parameter's Add Field button is used only in ModelBuilder to access and load the expected fields of a preceding process that has not yet been run into the ", "Input Field(s)", " list so you can complete the ", "Pivot Table", " dialog box and continue to build the model.", "The number of fields in the output table is determined by the number of Input Fields you choose, plus one field for each unique Pivot Field value. The number of records in the output table is determined by the unique combination of values between your chosen Input Fields and the Pivot Field."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table whose records will be pivoted. ", "dataType": "Table View"}, {"name": "fields", "isOptional": false, "description": "The fields that define records to be included in the output table. ", "dataType": "Field"}, {"name": "pivot_field", "isOptional": false, "description": "The field whose record values are used to generate the field names in the output table. ", "dataType": "Field"}, {"name": "value_field", "isOptional": false, "description": "The field whose values populate the pivoted fields in the output table. ", "dataType": "Field"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table to be created. ", "dataType": "Table"}]},
{"syntax": "GetCount_management (in_rows)", "name": "Get Count (Data Management)", "description": "Returns the total number of rows for a  feature class ,   table ,  layer , or  raster .", "example": {"title": "GetCount example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the GetCount function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.GetCount_management ( \"roads\" )"}, "usage": ["You can view the returned row count in the ", "Results", " window.\r\n\r\n\r\n", "If the input is a layer or table view containing a selected set of records, only the selected records will be counted.", "This tool honors the ", "Output Extent", " environment.  Only those features that are within or intersect the Output Extent environment setting will be counted.", " In ModelBuilder, ", "Get Count", " is typically used to set up a precondition, as illustrated below. In this model, ", "Get Count", " counts the number of records returned by the ", "Select", " tool. If the count is zero, ", "Buffer", " will not run due to the precondition."], "parameters": [{"name": "in_rows", "isInputFile": true, "isOptional": false, "description": "The input table view or raster layer. If a selection is defined on the input, the count of the selected rows is returned. ", "dataType": "Raster Layer; Table View"}]},
{"syntax": "DeleteRows_management (in_rows)", "name": "Delete Rows (Data Management)", "description": "Deletes all or the selected subset of rows from the input. If the input rows are from a feature class or table, all rows will be deleted. If the input rows  are from a layer or table view with no selection, all rows will be deleted.", "example": {"title": "DeleteRows example 1 (Python window)", "description": "The following Python window script demonstrates how to use the DeleteRows function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CopyRows_management ( \"accident.dbf\" , \"C:/output/accident2.dbf\" ) arcpy.DeleteRows_management ( \"C:/output/accident2.dbf\" )"}, "usage": ["The ", "Input Rows", " parameter can be an INFO or dBASE table, ArcSDE, file, or personal geodatabase table or feature class, shapefile, layer, or table view.", "If this tool is used on feature data, the entire row, including the geometry, will be deleted.", "If a layer or table view is input, and the layer or table view does not have a selection, all rows will be deleted.  If a table is input, all rows will be deleted.", " Deleting all rows from a table with a large number of rows can be  slow.  If your intent is to delete all the rows in the table you should consider using the ", "Truncate Table", " tool instead.  Please see the ", "Truncate Table", " documentation for important cautionary statements on its use.", "When working in ArcMap and using a layer or table view with a selection as input, using this  tool in an edit session will allow for the ", "Delete Rows", " operation to be undone using undo/redo."], "parameters": [{"name": "in_rows", "isInputFile": true, "isOptional": false, "description": "The feature class, layer, table, or table view whose rows will be deleted. ", "dataType": "Table View"}]},
{"syntax": "CreateTable_management (out_path, out_name, {template}, {config_keyword})", "name": "Create Table (Data Management)", "description": "Creates an ArcSDE, file, or personal geodatabase table, or an INFO or dBASE table.", "example": {"title": "CreateTable Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CreateTable function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CreateTable_management ( \"C:/output\" , \"habitatTemperatures.dbf\" , \"vegtable.dbf\" )"}, "usage": ["If the output location is a folder, the default output is an INFO table. ", "To create a dBASE table in a folder, you must append the extension ", ".dbf", " to the output table name."], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The ArcSDE, file, or personal geodatabase or workspace in which the output table will be created. ", "dataType": "Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the table to be created. ", "dataType": "String"}, {"name": "template", "isOptional": false, "description": "A table whose attribute schema is used to define the output table. Fields in the template table(s) will be added to the output table. ", "dataType": "Table View"}, {"name": "config_keyword", "isOptional": true, "description": "The configuration keyword that determines the storage parameters of the table in an ArcSDE geodatabase. ", "dataType": "String"}]},
{"syntax": "CopyRows_management (in_rows, out_table, {config_keyword})", "name": "Copy Rows (Data Management)", "description": "Writes the rows from an input table, table view, feature class, or feature layer to a new table. If a selection is defined on a feature class or feature layer in ArcMap, only the selected rows are copied out.", "example": {"title": "CopyRows example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CopyRows function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CopyRows_management ( \"vegtable.dbf\" , \"C:/output/output.gdb/vegtable\" )"}, "usage": ["All rows are copied, if the input is a ", "feature class", " or ", "table", ". If the input rows are from a ", "layer", " or ", "table view", " which has a selection, only the selected features or rows are used.", " If the input rows are a feature class, then only the attributes, and not the geometry, are copied to the output table.", "This tool supports the following table formats as input: ", "For file input (", ".csv", " or ", ".txt", "), the first row of the input file is used as the field names on the output table.  Field names cannot contain spaces or special characters (such as ", "$", " or ", "*", "), and you will  receive an error if the first row of the input file contains spaces or special characters.", "Learn more about table formats supported in ArcGIS", "To add or append the copied rows to an existing table, use the ", "Append", " tool.", "The output table can be saved in a dBASE, ", "ArcSDE geodatabase", ", ", "file geodatabase", ", or ", "personal geodatabase", ", or as an INFO table.", "If the output is an INFO table, neither the output path nor the table name can have spaces."], "parameters": [{"name": "in_rows", "isInputFile": true, "isOptional": false, "description": "The rows from a feature class, layer, table, or table view to be copied. ", "dataType": "Table View ; Raster Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table to which the rows will be written. The output table can be saved in a dBASE, ArcSDE geodatabase, file geodatabase, or personal geodatabase, or as an INFO table. The table to which the rows will be written. The output table can be saved in a dBASE, ArcSDE geodatabase, file geodatabase, or personal geodatabase, or as an INFO table. ", "dataType": "Table"}, {"name": "config_keyword", "isOptional": true, "description": "The config keyword specifies the default storage parameters for an ArcSDE geodatabase. ", "dataType": "String"}]},
{"syntax": "ChangePrivileges_management (in_dataset, user, {View}, {Edit})", "name": "Change Privileges (Data Management)", "description": "Establishes or changes user access privileges to the input ArcSDE datasets, stand-alone feature classes, or tables.", "example": {"title": null, "description": " The following stand-alone script demonstrates how to grants view and edit privileges to WendelClark.", "code": "# Name: GrantPrivileges_Example.py # Description: Grants view and edit privileges to WendelClark # Author: ESRI # Import system modules import arcpy # Set local variables datasetName = \"Database Connections/gdb@production.sde/production.GDB.ctgFuseFeature\" # Execute ChangePrivileges arcpy.ChangePrivileges_management ( datasetName , \"WENDELCLARK\" , \"GRANT\" , \"GRANT\" )"}, "usage": ["To edit ArcSDE datasets, both the View and Edit parameters must be granted. Edit privileges are dependent on the View privilege, since you cannot edit what you cannot see (view).", "Edit privileges may be revoked, but you can still view the dataset. However, if the View privilege is revoked, the Edit privileges will automatically be revoked as well.", "The relational database management system (RDBMS) equivalent command for the View parameter is Select.", "The RDBMS equivalent commands for the Edit parameter are Update, Insert, and Delete. All three are granted or revoked simultaneously by the Edit parameter."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The datasets, feature classes, or tables whose access privileges will be changed. ", "dataType": "Layer; Table View; Dataset"}, {"name": "user", "isOptional": false, "description": "The database username whose privileges are being modified. ", "dataType": "String"}, {"name": "View", "isOptional": true, "description": "Establishes the user's View privileges. AS_IS \u2014 No change to the user's existing view privilege. If the user has view privileges, they will continue to have view privileges. If the user doesn't have view privileges, they will continue to not have view privileges. GRANT \u2014 Allows user to view datasets. REVOKE \u2014 Removes all user privileges to view datasets.", "dataType": "String"}, {"name": "Edit", "isOptional": true, "description": "Establishes the user's Edit privileges. AS_IS \u2014 No change to the user's existing edit privilege. If the user has edit privileges, they will continue to have edit privileges. If the user doesn't have edit privileges, they will continue to not have edit privileges. This is the default. GRANT \u2014 Allows the user to edit the input datasets. REVOKE \u2014 Removes the user's edit privileges. The user may still view the Input dataset. ", "dataType": "String"}]},
{"syntax": "Analyze_management (in_dataset, components)", "name": "Analyze (Data Management)", "description": "Updates database statistics of business tables, feature tables, and delta tables, along with the statistics of those tables' indexes.", "example": {"title": null, "description": "This stand-alone Python script uses the Analyze tool to gather statistics for the indexes on the buisness table of the input dataset.", "code": "# Name: Analyze_Example.py # Description: Gathers statistics for the indexes on the buisness table of the input dataset # Author: ESRI # Import system modules import arcpy # Set local variables inDataset = \"Database Connections/ninefour@gdb.sde/GDB.ctgPrimaryFeature\" # Execute AlterVersion arcpy.Analyze_management ( inDataset , \"BUSINESS\" )"}, "usage": ["This tool can only be used with data stored in an ArcSDE Geodatabase.", "After data loading, deleting, updating, and compressing operations, it is important to update RDBMS statistics in Oracle, SQL Server, DB2, or Informix databases.", "This tool updates the statistics of business tables, feature tables, raster tables, adds table, and deletes table, along with the statistics on those tables' indexes.", "The ", "Components to Analyze", " parameter's ", "Add Value", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the Components to Analyze parameter may not be populated with values. The ", "Add Value", " button allows you to add expected value(s) so you can complete the ", "Analyze", " dialog box and continue to build your model."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "The table or feature class to be analyzed. ", "dataType": "Layer; Table View ; Dataset"}, {"name": "components", "isOptional": false, "description": "The component type to be analyzed. BUSINESS \u2014 Updates business rules statistics. FEATURE \u2014 Updates feature statistics. RASTER \u2014 Updates statistics on raster tables. ADDS \u2014 Updates statistics on added datasets. DELETES \u2014 Updates statistics on deleted datasets.", "dataType": "String"}]},
{"syntax": "SetSubtypeField_management (in_table, field)", "name": "Set Subtype Field (Data Management)", "description": "Defines the field in the input table or feature class that stores the subtype codes.", "example": {"title": "Set Subtype Field Example (Python Window)", "description": "The following Python window script demonstrates how to use the SetSubtypeField function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Montgomery.gdb\" arcpy.SetSubtypeField_management ( \"water/fittings\" , \"TYPECODE\" )"}, "usage": ["A feature class or table can have only one subtype field.", "After a subtype field is set, subtype codes can be added to the feature class or table using the ", "Add Subtype", " tool.", "The subtypes of a feature class or table can also be managed in ArcCatalog. Subtypes can be created and modified using the Subtypes Property page on the dataset's Properties dialog box."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input table or feature class which contains the field to set as a subtype field. ", "dataType": "Table View"}, {"name": "field", "isOptional": false, "description": "An integer field that will store the subtype codes. ", "dataType": "Field"}]},
{"syntax": "SetDefaultSubtype_management (in_table, subtype_code)", "name": "Set Default Subtype (Data Management)", "description": "Sets the default value or code for the input table's subtype.", "example": {"title": "Set Default Subtype Example (Python Window)", "description": "The following Python window script demonstrates how to use the SetDefaultSubtype function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Montgomery.gdb\" arcpy.SetDefaultSubtype_management ( \"water/fittings\" , 5 )"}, "usage": ["The input table must contain subtype codes before setting a default code. Use the ", "Add Subtype", " and ", "Set Subtype Field", " tools to create subtype codes.", "The subtype of a feature class or table can also be managed in ArcCatalog. Subtypes can be created and modified using the Subtype Property page on the dataset's Properties dialog box."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input table or feature class whose subtype default value will be set. ", "dataType": "Table View"}, {"name": "subtype_code", "isOptional": false, "description": "The unique default value for a subtype. ", "dataType": "Long"}]},
{"syntax": "RemoveSubtype_management (in_table, subtype_code)", "name": "Remove Subtype (Data Management)", "description": "Removes a subtype from the input table using its code.", "example": {"title": "Remove Subtype Example (Python Window)", "description": "The following Python window script demonstrates how to use the RemoveSubtype function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Montgomery.gdb\" arcpy.RemoveSubtype_management ( \"water/fittings\" , [ \"4\" , \"7\" ])"}, "usage": ["Subtypes are removed using their integer code.", "The subtypes of a feature class or table can also be managed in ArcCatalog. Subtypes can be created and modified using the ", "Subtypes Property", " page on the dataset's ", "Properties", " dialog box.", "The ", "Subtype Code", " parameter's ", "Add Value", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the ", "Subtype Code", " parameter may not be populated with values. The ", "Add Value", " button allows you to add expected value(s) so you can complete the ", "Remove Subtype", " dialog box and continue to build your model."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The feature class or table containing the subtype definition. ", "dataType": "Table View"}, {"name": "subtype_code", "isOptional": false, "description": "The code used to remove a subtype from the input table or feature class. ", "dataType": "String"}]},
{"syntax": "AddSubtype_management (in_table, subtype_code, subtype_description)", "name": "Add Subtype (Data Management)", "description": "Adds a new subtype to the subtypes in the input table. \r\n Learn more about working with subtypes \r\n", "example": {"title": "Add Subtype example (Python window)", "description": "The following Python window script demonstrates how to use the AddSubtype function in Immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/Montgomery.gdb\" arcpy.SetSubtypeField_management ( \"water/fittings\" , \"TYPECODE\" ) arcpy.AddSubtype_management ( \"water/fittings\" , \"1\" , \"Bend\" )"}, "usage": ["A field in the feature class or table must be assigned as the subtype field before new subtypes can be added. This is done using the ", "Set Subtype Field", " tool.", "If you add a subtype whose code already exists, the new subtype will be ignored.", "If you need to change the description of an existing subtype, you would first have to ", "remove the subtype", ", then add a new subtype with the same code and a new description.", "The subtypes of a feature class or table can also be managed in ArcCatalog. Subtypes can be created and modified using the ", "Subtypes Property", " tab on the dataset's ", "Properties", " dialog box."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The feature class or table containing the subtype definition to be updated ", "dataType": "Table View"}, {"name": "subtype_code", "isOptional": false, "description": "A unique integer value for the subtype to be added ", "dataType": "Long"}, {"name": "subtype_description", "isOptional": false, "description": "A description of the subtype code ", "dataType": "String"}]},
{"syntax": "TableToRelationshipClass_management (origin_table, destination_table, out_relationship_class, relationship_type, forward_label, backward_label, message_direction, cardinality, relationship_table, attribute_fields, origin_primary_key, origin_foreign_key, destination_primary_key, destination_foreign_key)", "name": "Table To Relationship Class (Data Management)", "description": "Creates an attributed relationship class from the origin, destination, and relationship tables.", "example": {"title": "TableToRelationshipClass Example (Python Window)", "description": "The following Python Window script demonstrates how to use the TableToRelationshipClass tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data/Montgomery.gdb\" arcpy.TableToRelationshipClass_management ( \"owners\" , \"Parcels\" , \"ownersParcels_RelClass\" , \"SIMPLE\" , \"Owns\" , \"Is Owned By\" , \"BACKWARD\" , \"MANY_TO_MANY\" , \"owners\" , [ \"OWNER_PERCENT\" , \"DEED_DATE\" ], \"OBJECTID\" , \"owner_id\" , \"OBJECTID\" , \"parcel_id\" )"}, "usage": ["This tool creates a table in the database containing the selected attribute fields of the relationship table. These fields are used to store attributes of the relationship itself that are not attributed to either the origin or destination class. For example, in a parcel database, you may have a relationship class between parcels and owners in which owners \"own\" parcels and parcels are \"owned by\" owners. An attribute of that relationship may be percentage ownership.", "Simple or peer-to-peer relationships are relationships between two or more objects in the database that exist independently of each other. For example, in a railroad network, you may have railroad crossings that have one or more related signal lamps. However, a railroad crossing can exist without a signal lamp, and signal lamps exist on the railroad network where there are no railroad crossings. Simple relationships can have one-to-one, one-to-many, or many-to-many cardinality.", "A composite relationship is a relationship in which the lifetime of one object controls the lifetime of its related objects. For example, power poles support transformers and transformers are mounted on poles. Once a pole is deleted, a delete message is propagated to its related transformers, which are deleted from the transformers' feature class. Composite relationships are always one-to-many.", "Forward and backward path labels describe the relationship when navigating from one object to another. The forward path label describes the relationship navigated from the origin class to the destination class. In the pole-transformer example, a forward path label might be: Poles support transformers. The backward path label describes the relationship navigated from the destination to the origin class. In the pole-transformer example, a backward path label might be: Transformers are mounted on poles.", "Relationship classes can also be created in ArcCatalog. Select the menu item ", "New ", ">", " Relationship Class", " from the context menu of a geodatabase.", "The ", "Attribute Fields", " parameter's ", "Add Field", " button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the", " Attribute Fields", " parameter may not be populated with field names. The ", "Add Field", " button allows you to add expected field(s) so you can complete the ", "Table To Relationship Class", " dialog box and continue to build your model."], "parameters": [{"name": "origin_table", "isOptional": false, "description": "The table or feature class that will be associated to the destination table. ", "dataType": "Table View"}, {"name": "destination_table", "isOptional": false, "description": "The table or feature class that will be associated to the origin table. ", "dataType": "Table View"}, {"name": "out_relationship_class", "isOutputFile": true, "isOptional": false, "description": "The relationship class that will be created. ", "dataType": "Relationship Class"}, {"name": "relationship_type", "isOptional": false, "description": "The type of association to create between the origin and destination tables. SIMPLE \u2014 An association where each object is independent of each other (a parent-to-parent relationship). This is the default. COMPOSITE \u2014 An association where the lifetime of one object controls the lifetime of its related object (a parent-child relationship). ", "dataType": "String"}, {"name": "forward_label", "isOptional": false, "description": "A label describing the relationship as it is traversed from the origin table/feature class to the destination table/feature class. ", "dataType": "String"}, {"name": "backward_label", "isOptional": false, "description": "A label describing the relationship as it is traversed from the destination table/feature class to the origin table/feature class. ", "dataType": "String"}, {"name": "message_direction", "isOptional": false, "description": "The direction messages will be propagated between the objects in the relationship. For example, in a relationship between poles and transformers, when the pole is deleted, it sends a message to its related transformer objects informing them it was deleted. NONE \u2014 No messages propagated. This is the default. FORWARD \u2014 Messages propagated from the origin to the destination. BACKWARD \u2014 Messages propagated from the destination to the origin. BOTH \u2014 Messages propagated from the origin to the destination and from the destination to the origin. ", "dataType": "String"}, {"name": "cardinality", "isOptional": false, "description": "The cardinality of the relationship between the origin and destination. ONE_TO_ONE \u2014 Each object of the origin table/feature class can be related to zero or one object of the destination table/feature class. This is the default. ONE_TO_MANY \u2014 Each object of the origin table/feature class can be related to multiple objects in the destination table/feature class. MANY_TO_MANY \u2014 Multiple objects of the origin table/feature class can be related to multiple objects in the destination table/feature class. ", "dataType": "String"}, {"name": "relationship_table", "isOptional": false, "description": "The table containing attributes that will be added to the relationship class. ", "dataType": "Table View"}, {"name": "attribute_fields", "isOptional": false, "description": "The fields containing attribute values that will be added to the relationship class. ", "dataType": "Field"}, {"name": "origin_primary_key", "isOptional": false, "description": "The field in the origin table that will be used to create the relationship. Generally, this is the object identifier field. ", "dataType": "String"}, {"name": "origin_foreign_key", "isOptional": false, "description": "The name of the Foreign Key field in the relationship table that refers to the Primary Key field in the origin table/feature class. ", "dataType": "String"}, {"name": "destination_primary_key", "isOptional": false, "description": "The field in the destination table that will be used to create the relationship. Generally, this is the object identifier field. ", "dataType": "String"}, {"name": "destination_foreign_key", "isOptional": false, "description": "The field in the relationship table that refers to the Primary Key field in the destination table. ", "dataType": "String"}]},
{"syntax": "CreateRelationshipClass_management (origin_table, destination_table, out_relationship_class, relationship_type, forward_label, backward_label, message_direction, cardinality, attributed, origin_primary_key, origin_foreign_key, {destination_primary_key}, {destination_foreign_key})", "name": "Create Relationship Class (Data Management)", "description": "This tool creates a relationship class to store an association between fields or features in the origin table and the destination table.", "example": {"title": "Create Relationship Class example (Python window)", "description": "The following Python Window script demonstrates how to use the Create Relationship Class tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data/Habitat_Analysis.gdb\" arcpy.CreateRelationshipClass_management ( \"vegtype\" , \"vegtable\" , \"veg_RelClass\" , \"SIMPLE\" , \"Attributes from vegtable\" , \"Attributes and Features from vegtype\" , \"NONE\" , \"ONE_TO_ONE\" , \"NONE\" , \"HOLLAND95\" , \"HOLLAND95\" )"}, "usage": ["Relationships can exist between spatial objects (features in feature classes), nonspatial objects (rows in a table), or spatial and nonspatial objects.", "Once created, a relationship class cannot be modified; you can only add, delete, or refine its ", "rules", ". Relationship classes can be deleted and renamed using ArcCatalog in the same manner as any other object in the database.", "For many-to-many relationship classes, a new table is created in the database to store the foreign keys used to link the origin and destination classes. This table can also have other fields to store attributes of the relationship itself that are not attributed to either the origin or destination class. For example, in a parcel database, you might have a relationship class between parcels and owners in which owners \"own\" parcels and parcels are \"owned by\" owners. An attribute of that relationship could be percentage ownership. One-to-one and one-to-many relationship classes can also have attributes; in this case, a table is created to store the relationships.", "Simple or peer-to-peer relationships involve two or more objects in the database that exist independently of each other. For example, in a railroad network, you might have railroad crossings that have one or more related signal lamps. However, a railroad crossing can exist without a signal lamp, and signal lamps exist on the railroad network where there are no railroad crossings. Simple relationships can have one-to-one, one-to-many, or many-to-many cardinality.", "A composite relationship is one in which the lifetime of one object controls the lifetime of its related objects. For example, power poles support transformers, and transformers are mounted on poles. Once a pole is deleted, a delete message is propagated to its related transformers, which are deleted from the transformers' feature class. Composite relationships are always one-to-many.", "Forward and backward path labels describe the relationship when navigating from one object to another. The forward path label describes the relationship navigated from the origin class to the destination class. In the pole-transformer example, a forward path label might be \"Poles support transformers.\" The backward path label describes the relationship navigated from the destination to the origin class. In the pole-transformer example, a backward path label might be\"Transformers are mounted on poles.\"", "Relationship classes can also be created in ArcCatalog. Select the command ", "New ", ">", " Relationship Class", " from the context menu of a geodatabase."], "parameters": [{"name": "origin_table", "isOptional": false, "description": "The table or feature class that is associated to the destination table. ", "dataType": "Table View"}, {"name": "destination_table", "isOptional": false, "description": "The table that is associated to the origin table. ", "dataType": "Table View"}, {"name": "out_relationship_class", "isOutputFile": true, "isOptional": false, "description": "The relationship class that is created. ", "dataType": "Relationship Class"}, {"name": "relationship_type", "isOptional": false, "description": "The type of relationship to create between the origin and destination tables. SIMPLE \u2014 A relationship between independent objects (parent to parent). This is the default. COMPOSITE \u2014 A relationship between dependent objects where the lifetime of one object controls the lifetime of the related object (parent to child). ", "dataType": "String"}, {"name": "forward_label", "isOptional": false, "description": "A name to uniquely identify the relationship when navigating from the origin table to the destination table. ", "dataType": "String"}, {"name": "backward_label", "isOptional": false, "description": "A name to uniquely identify the relationship when navigating from the destination table to the origin table. ", "dataType": "String"}, {"name": "message_direction", "isOptional": false, "description": "The direction in which messages are passed between the origin and destination tables. For example, in a relationship between poles and transformers, when the pole is deleted, it sends a message to its related transformer objects informing them it was deleted. FORWARD \u2014 Messages are passed from the origin to the destination table. BACK \u2014 Messages are passed from the destination to the origin table. BOTH \u2014 Messages are passed from the origin to the destination table and from the destination to the origin table. NONE \u2014 No messages passed. This is the default. ", "dataType": "String"}, {"name": "cardinality", "isOptional": false, "description": "Determines how many relationships exist between rows or features in the origin and rows or features in the destination table. ONE_TO_ONE \u2014 Each row or feature in the origin table can be related to zero or one row or feature in the destination table. This is the default. ONE_TO_MANY \u2014 Each row or feature in the origin table can be related to one or several rows or features in the destination table. MANY_TO_MANY \u2014 Several fields or features in the origin table can be related to one or several rows or features in the destination table. ", "dataType": "String"}, {"name": "attributed", "isOptional": false, "description": "Specifies if the relationship will have attributes. NONE \u2014 Indicates the relationship class will not have attributes. This is the default. ATTRIBUTED \u2014 Indicates the relationship class will have attributes. ", "dataType": "Boolean"}, {"name": "origin_primary_key", "isOptional": false, "description": "The field in the origin table, typically the OID field, that links it to the Origin Foreign Key field in the relationship class table. ", "dataType": "String"}, {"name": "origin_foreign_key", "isOptional": false, "description": "The field in the relationship class table that links it to the Origin Primary Key field in the origin table. ", "dataType": "String"}, {"name": "destination_primary_key", "isOptional": true, "description": "The field in the destination table, typically the OID field, that links it to the Destination Foreign Key field in the relationship class table. ", "dataType": "String"}, {"name": "destination_foreign_key", "isOptional": true, "description": "The field in the relationship class table that links it to the Destination Primary Key field in the destination table. ", "dataType": "String"}]},
{"syntax": "BuildPyramidsAndStatistics_management (in_workspace, {include_subdirectories}, {build_pyramids}, {calculate_statistics}, {BUILD_ON_SOURCE}, {block_field}, {estimate_statistics}, {x_skip_factor}, {y_skip_factor}, {ignore_values}, {pyramid_level}, {SKIP_FIRST}, {resample_technique}, {compression_type}, {compression_quality}, {skip_existing})", "name": "Build Pyramids And Statistics (Data Management)", "description": "Traverses a folder structure, building pyramids and calculating statistics for all the raster datasets it contains. It can also build pyramids and calculate statistics for all the items in a raster catalog or mosaic dataset. ", "example": {"title": "BuildPyramidsAndStatistics example 1 (Python window)", "description": "This is a Python sample for the BuildPyramidsAndStatistics tool.", "code": "import arcpy arcpy.BuildPyramidsAndStatistics_management ( \"C:/Workspace\" , \"INCLUDE_SUBDIRECTORIES\" , \"BUILD_PYRAMIDS\" , \"CALCULATE_STATISTICS\" , \"BUILD_ON_SOURCE\" , \"BlockField\" , \"ESTIMATE_STATISTICS\" , \"10\" , \"10\" , \"0;255\" , \"-1\" , \"NONE\" , \"BILINEAR\" , \"JPEG\" , \"50\" , \"OVERWRITE\" )"}, "usage": ["Building ", "pyramids", " improves the display performance of raster datasets.", "Calculating ", "statistics", " allows ArcGIS applications to properly stretch and symbolize raster data for display.", "All supported raster formats will be processed.", "Raster catalogs and mosaic datasets must be specified as the input workspace. If the workspace includes a raster catalog or mosaic dataset, then these items will not be included when the tool runs.", "Wavelet compressed raster datasets, such as ECW and MrSID, do not need to have pyramids built. These formats have internal pyramids that are created upon encoding."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": "The workspace that contains all the raster datasets to be processed, a mosaic dataset or a raster catalog. If the workspace includes a raster catalog or mosaic dataset, then these items will not be included when the tool runs. ", "dataType": "Mosaic Dataset; Mosaic Layer; Raster Catalog Layer; Raster Dataset; Text File; Workspace"}, {"name": "include_subdirectories", "isOptional": true, "description": "Specify whether to include subdirectories. Raster catalogs and mosaic datasets must be specified as the input workspace. If the workspace includes a raster catalog or mosaic dataset, then these items will not be included when the tool runs. NONE \u2014 Does not include subdirectories. INCLUDE_SUBDIRECTORIES \u2014 Includes all the raster datasets within the subdirectories when loading. This is the default. ", "dataType": "Boolean"}, {"name": "build_pyramids", "isOptional": true, "description": "Specify whether to build pyramids. NONE \u2014 Does not build pyramids. BUILD_PYRAMIDS \u2014 Builds pyramids. This is the default. ", "dataType": "Boolean"}, {"name": "calculate_statistics", "isOptional": true, "description": "Specify whether to calculate statistics. NONE \u2014 Does not calculate statistics. CALCULATE_STATISTICS \u2014 Calculates statistics. This is the default. ", "dataType": "Boolean"}, {"name": "BUILD_ON_SOURCE", "isOptional": true, "description": "Specify whether to build pyramids and calculate statistics on the source raster datasets or calculate statistics on the raster items in a mosaic dataset. This option only applies to mosaic datasets. NONE \u2014 Statistics will be calculated for each raster item in the mosaic dataset (on each row in the attribute table). Any functions added to the raster item will be applied before generating the statistics. This is the default. BUILD_ON_SOURCE \u2014 Builds pyramids and calculates statistics on the source data of the mosaic dataset. ", "dataType": "Boolean"}, {"name": "block_field", "isOptional": true, "description": " The name of the field within a mosaic dataset's attribute table used to identify items that should be considered one item when performing some calculations and operations. ", "dataType": "String"}, {"name": "estimate_statistics", "isOptional": true, "description": "Specify whether to calculate statistics for the mosaic dataset (not for the rasters within it). The statistics are derived from the existing statistics that have been calculated for each raster in the mosaic dataset. NONE \u2014 Statistics are not calculated for the mosaic dataset. This is the default. ESTIMATE_STATISTICS \u2014 Statistics will be calculated for the mosaic dataset.", "dataType": "Boolean"}, {"name": "x_skip_factor", "isOptional": true, "description": "The number of horizontal pixels between samples. The value must be greater than zero and less than or equal to the number of columns in the raster dataset. The default is 1 or the last skip factor used. ", "dataType": "Long"}, {"name": "y_skip_factor", "isOptional": true, "description": "The number of vertical pixels between samples. The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used. ", "dataType": "Long"}, {"name": "ignore_values", "isOptional": false, "description": "The pixel values that are not to be included in the statistics calculation. The default is no value. ", "dataType": "Long"}, {"name": "pyramid_level", "isOptional": true, "description": "Choose the number of reduced-resolution dataset layers that will be built. The default value is \u20131, which will build full pyramids. A value of 0 will result in no pyramid levels. ", "dataType": "Long"}, {"name": "SKIP_FIRST", "isOptional": true, "description": "Choose whether to skip the first pyramid level. Skipping the first level will take up slightly less disk space, but it will slow down the performance at these scales. NONE \u2014 The first pyramid level will be built. This is the default. SKIP_FIRST \u2014 The first pyramid level will not be built.", "dataType": "Boolean"}, {"name": "resample_technique", "isOptional": true, "description": " The resampling technique used to build your pyramids. NEAREST \u2014 The nearest neighbor resampling method uses the value of the closest cell to assign a value to the output cell when resampling. This is the default. BILINEAR \u2014 The bilinear interpolation resampling method determines the new value of a cell based on a weighted distance average of the four nearest input cell centers. CUBIC \u2014 The Cubic convolution resampling method determines the new value of a cell based on fitting a smooth curve through the 16 nearest input cell centers.", "dataType": "String"}, {"name": "compression_type", "isOptional": true, "description": " The compression type to use when building the raster pyramids. DEFAULT \u2014 If the source data is compressed using a wavelet compression, it will build pyramids with the JPEG compression type; otherwise, LZ77 will be used. This is the default compression method. LZ77 \u2014 The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type. JPEG \u2014 The JPEG compression algorithm to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality. JPEG_YCbCr \u2014 A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components. NONE \u2014 No compression will be used when building pyramids.", "dataType": "String"}, {"name": "compression_quality", "isOptional": true, "description": " The compression quality to use when pyramids are built with the JPEG compression method. The value must be between 0 and 100. The values closer to 100 would produce a higher quality image, but the compression ratio would be lower. ", "dataType": "Long"}, {"name": "skip_existing", "isOptional": true, "description": "Specify whether to calculate statistics only where they are missing or regenerate them even if they exist. OVERWRITE \u2014 Statistics will be calculated even if they already exist. Therefore, existing statistics will be overwritten. This is the default. SKIP_EXISTING \u2014 Statistics will only be calculated if they do not already exist. ", "dataType": "Boolean"}]},
{"syntax": "GetCellValue_management (in_raster, location_point, {band_index})", "name": "Get Cell Value (Data Management)", "description": "Retrieves the pixel value at a specific x,y coordinate. For multiband raster datasets you can specify from which bands to retrieve the cell value. If you do not specify any bands, the pixel value for all the bands will be returned for the input location.", "example": {"title": "GetCellValue example 1 (Python window)", "description": "This is a Python sample for the GetCellValue tool.", "code": "import arcpy result = arcpy.GetCellValue_management ( \"C:/data/rgb.img\" , \"480785 3807335\" , \"2;3\" ) cellSize = int ( result.getOutput ( 0 )) print cellSize"}, "usage": ["This tool is used when you need the pixel value for a geoprocessing model. In ArcMap, ArcScene, or ArcGlobe you can use the Identify tool instead."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Dataset; Mosaic Layer; Raster Layer"}, {"name": "location_point", "isOptional": false, "description": "Type the x and y coordinates of the pixel location. ", "dataType": "Point"}, {"name": "band_index", "isOptional": false, "description": "Define which bands for which you would like to get the pixel value. If you do not define any bands, a pixel value for all the bands in the x,y location will be returned. ", "dataType": "Value Table"}]},
{"syntax": "GetRasterProperties_management (in_raster, {property_type}, {band_index})", "name": "Get Raster Properties (Data Management)", "description": "Returns the properties of a raster dataset, mosaic dataset, or a raster product.", "example": {"title": "GetRasterProperties example 1 (Python window)", "description": "This is a Python sample for GetRasterProperties.", "code": "import arcpy #Get the geoprocessing result object elevSTDResult = arcpy.GetRasterProperties_management ( \"c:/data/elevation\" , \"STD\" ) #Get the elevation standard deviation value from geoprocessing result object elevSTD = elevSTDResult.getOutput ( 0 )"}, "usage": ["The property returned will be displayed in the ", "Results", " window.", "The Python result of this tool returns a geoprocessing ", "Result", " object. In order to obtain the string value, use the Result object's ", "getOutput", " method."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset or mosaic dataset. ", "dataType": "Composite Geodataset"}, {"name": "property_type", "isOptional": true, "description": "The property to be obtained from the raster dataset or mosaic dataset. MINIMUM \u2014 Returns the smallest value of all cells in the input raster. MAXIMUM \u2014 Returns the largest value of all cells in the input raster. MEAN \u2014 Returns the average of all cells in the input raster. STD \u2014 Returns the standard deviation of all cells in the input raster. UNIQUEVALUECOUNT \u2014 Returns the number of unique values in the input raster. TOP \u2014 Returns the top or YMax value of the extent. LEFT \u2014 Returns the left or XMin value of the extent. RIGHT \u2014 Returns the right or XMax value of the extent. BOTTOM \u2014 Returns the bottom or YMin value of the extent. CELLSIZEX \u2014 Returns the cell size in the x-direction. CELLSIZEY \u2014 Returns the cell size in the y-direction. VALUETYPE \u2014 Returns the type of the cell value in the input raster: 0 = 1-bit 1 = 2-bit 2 = 4-bit 3 = 8-bit unsigned integer 4 = 8-bit signed integer 5 = 16-bit unsigned integer 6 = 16-bit signed integer 7 = 32-bit unsigned integer 8 = 32-bit signed integer 9 = 32-bit floating point 10 = 64-bit double precision 11 = 8-bit complex 12 = 64-bit complex 13 = 16-bit complex 14 = 32-bit complex COLUMNCOUNT \u2014 Returns the number of columns in the input raster. ROWCOUNT \u2014 Returns the number of rows in the input raster. BANDCOUNT \u2014 Returns the number of bands in the input raster. ANYNODATA \u2014 Returns whether there is NoData in the raster. ALLNODATA \u2014 Returns whether all the pixels are NoData. This is the same as ISNULL. SENSORNAME \u2014 Returns the name of the sensor. PRODUCTNAME \u2014 Returns the product name related to the sensor. AQUISITIONDATE \u2014 Returns the date that the data was captured. SOURCETYPE \u2014 Returns the source type. CLOUDCOVER \u2014 Returns the amount of cloud cover as a percentage. SUNAZIMUTH \u2014 Returns the sun azimuth, in degrees. SUNELEVATION \u2014 Returns the sun elevation, in degrees. SENSORAZIMUTH \u2014 Returns the sensor azimuth, in degrees. SENSORELEVATION \u2014 Returns the sensor elevation, in degrees. OFFNADIR \u2014 Returns the off-nadir angle, in degrees. WAVELENGTH \u2014 Returns the wavelength range of the band, in nanometers.", "dataType": "String"}, {"name": "band_index", "isOptional": true, "description": "Choose from which band to get the properties. If no band is chosen, then the first band will be used when the tool runs. ", "dataType": "String"}]},
{"syntax": "ExportRasterWorldFile_management (in_raster_dataset)", "name": "Export Raster World File (Data Management)", "description": "Creates a world file based on the geographic information of a raster dataset. The pixel size and the location of the upper left pixel is extracted for the world file.", "example": {"title": "ExportRasterWorldFile example 1 (Python window)", "description": "This is a Python sample for the ExportRasterWorldFile tool.", "code": "import arcpy arcpy.ExportRasterWorldFile_management ( \"c:/data/image.tif\" )"}, "usage": ["Your output world file will depend on the file format you are working with. For the valid world file extensions, refer to ", "world files for raster datasets", ".", "If the transformation cannot be expressed as a world file, this tool will write an approximate affine transformation into the world file, with an x on the end of the extension name. For example, a TIFF image with this approximate affine transformation has the extension .tfwx. This is to signify that this is not a standard world file; it is only an approximation."], "parameters": [{"name": "in_raster_dataset", "isInputFile": true, "isOptional": false, "description": "The raster dataset for which the world file will be created. ", "dataType": "Raster dataset"}]},
{"syntax": "DeleteRasterAttributeTable_management (in_raster)", "name": "Delete Raster Attribute Table (Data Management)", "description": "Removes the raster attribute table associated with a raster dataset. Since raster attribute tables can only be built on single-band raster datasets, this tool is only valid for raster datasets with a single band.", "example": {"title": "DeleteRasterAttributeTable Example (Python Window)", "description": "This is a Python sample for the DeleteRasterAttributeTable tool.", "code": "import arcpy arcpy.DeleteRasterAttributeTable_management ( \"c:/data/delrat.tif\" )"}, "usage": ["The input raster dataset can only have a single band."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The raster dataset containing the raster attribute table to be removed. The raster dataset must be a single-band raster dataset. ", "dataType": "Raster Layer"}]},
{"syntax": "AddEdgeJunctionConnectivityRuleToGeometricNetwork_management (in_geometric_network, in_edge_feature_class, edge_subtype, in_junction_feature_class, junction_subtype, {default_junction}, {edge_min}, {edge_max}, {junction_min}, {junction_max})", "name": "Add Edge-Junction Connectivity Rule To Geometric Network (Data Management)", "description": "\r\n Adds an edge-junction connectivity rule to a geometric network.\r\n", "example": {"title": "AddEdgeJunctionConnectivityRuleToGeometricNetwork example (stand-alone script)", "description": "The following stand-alone Python script demonstrates how to use the \r\nAddEdgeJunctionConnectivityRuleToGeometricNetwork in Python script to add an edge-junction connectivity rule \r\nwith cardinality to a geometric network.", "code": "# Import arcpy module import arcpy # Local variables: Water_Net = \"C: \\\\ testing \\\\ GeometricNetworks \\\\ Montgomery.gdb \\\\ Water \\\\ Water_Net\" # Process: Add Edge-Junction Connectivity Rule To Geometric Network arcpy.AddEdgeJunctionConnectivityRuleToGeometricNetwork ( Water_Net , \"Distribmains\" , \"Distribmains\" , \"Fittings\" , \"Tap\" , \"DEFAULT\" , \"0\" , \"2\" , \"0\" , \"2\" )"}, "usage": ["\r\nThe feature classes specified must reside in the geometric network.", "\r\nIf the edge or junction feature class has subtypes and connectivity rules for multiple subytpes are required, each rule must be added separately.\r\n", "Default junctions are automatically inserted at the free end point when creating new edge features in a network. Only one default junction is allowed per edge-junction rule.\r\n\r\n"], "parameters": [{"name": "in_geometric_network", "isInputFile": true, "isOptional": false, "description": " The geometric network to which the connectivity rule will be added. ", "dataType": "Geometric Network"}, {"name": "in_edge_feature_class", "isInputFile": true, "isOptional": false, "description": " The name of the edge feature class. ", "dataType": "String"}, {"name": "edge_subtype", "isOptional": false, "description": " The subtype description for the edge feature class. If subtypes do not exist on the feature class, use the feature class name. ", "dataType": "String"}, {"name": "in_junction_feature_class", "isInputFile": true, "isOptional": false, "description": "The name of the junction feature class. ", "dataType": "String"}, {"name": "junction_subtype", "isOptional": false, "description": "The subtype description for the junction feature class. If subtypes do not exist on the feature class, use the feature class name. ", "dataType": "String"}, {"name": "default_junction", "isOptional": true, "description": "Indicates if the junction specified in this rule will be created automatically at a dangling endpoint of an edge in the feature class specified as part of the rule. DEFAULT \u2014 Create a junction at a dangling endpoint of edges for this rule. NO_ DEFAULT \u2014 Do not create a junction at a dangling endpoint of edges for this rule. This is the default. ", "dataType": "Boolean"}, {"name": "edge_min", "isOptional": true, "description": "The minimum number of edges that can connect to each junction. If nothing is specified, then it will be valid to have any number of edges connected to a single junction for the feature class or subtype pair. ", "dataType": "Long"}, {"name": "edge_max", "isOptional": true, "description": "The maximum number of edges that can connect to each junction. If nothing is specified, then it will be valid to have any number of edges connected to a single junction for the feature class or subtype pair. ", "dataType": "Long"}, {"name": "junction_min", "isOptional": true, "description": "The minimum number of junctions that can connect to each edge. If nothing is specified, then it will be valid to have any number of junctions connected to a single edge for the feature class or subtype pair. ", "dataType": "Long"}, {"name": "junction_max", "isOptional": true, "description": "The maximum number of junctions that can connect to each edge. If nothing is specified, then it will be valid to have any number of junctions connected to a single edge for the feature class or subtype pair. ", "dataType": "Long"}]},
{"syntax": "AnalyzeDatasets_management (input_database, include_system, {in_datasets}, {analyze_base}, {analyze_delta}, {analyze_archive})", "name": "Analyze Datasets (Data Management)", "description": "\r\nUpdates database statistics of base tables, delta tables, and archive tables, along with the statistics on those tables' indexes. This tool is used in enterprise geodatabases to help get optimal performance from the RDBMS's query optimizer. Stale statistics can lead to poor geodatabase performance.\r\n", "example": {"title": "AnalyzeDatasets example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Analyze Datasets tool in immediate mode. ", "code": "# Import system modules import arcpy arcpy.AnalyzeDatasets_management ( \"Database Connections/tenone@sde.sde\" , \"SYSTEM\" , \"gdb.city;gdb.state;map.lines\" , \"ANALYZE_BASE\" , \"ANALYZE_DELTA\" , \"ANALYZE_ARCHIVE\" )"}, "usage": [], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " The enterprise database that contains the data to be analyzed. ", "dataType": "Workspace"}, {"name": "include_system", "isOptional": false, "description": " Indicates whether statistics will be gathered on the states and state lineages tables. You must be the geodatabase administrator for this option to be activated. This option only applies to geodatabases. If the input workspace is a database, this option will be ignored. NO_SYSTEM \u2014 Statistics will not be gathered on the states and state lineages tables. This is the default. SYSTEM \u2014 Statistics will be gathered on the states and state lineages tables.", "dataType": "Boolean"}, {"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": " Names of the datasets that will be analyzed. An individual dataset or a Python list of datasets is allowed. Dataset names use paths relative to the input workspace; full paths are not accepted as input. Note that the connected user must be the data owner for the datasets provided. ", "dataType": "String"}, {"name": "analyze_base", "isOptional": true, "description": " Indicates whether the selected dataset base tables will be analyzed. This option only applies to geodatabases. If the input workspace is a database, this option will be ignored. ANALYZE_BASE \u2014 Statistics will be gathered on the base tables for the selected datasets. This is the default. NO_ANALYZE_BASE \u2014 Statistics will not be gathered for the base tables for the selected datasets.", "dataType": "Boolean"}, {"name": "analyze_delta", "isOptional": true, "description": " Indicates whether the selected dataset delta tables will be analyzed. This option only applies to geodatabases. If the input workspace is a database, this option will be ignored. ANALYZE_DELTA \u2014 Statistics will be gathered on the delta tables for the selected datasets. This is the default. NO_ANALYZE_DELTA \u2014 Statistics will not be gathered for the delta tables for the selected datasets.", "dataType": "Boolean"}, {"name": "analyze_archive", "isOptional": true, "description": " Indicates whether the selected dataset archive tables will be analyzed. This option only applies to geodatabases. If the input workspace is a database, this option will be ignored. ANALYZE_ARCHIVE \u2014 Statistics will be gathered on the archive tables for the selected datasets. This is the default. NO_ANALYZE_ARCHIVE \u2014 Statistics will not be gathered for the archive tables for the selected datasets.", "dataType": "Boolean"}]},
{"syntax": "RebuildIndexes_management (input_database, include_system, {in_datasets}, delta_only)", "name": "Rebuild Indexes (Data Management)", "description": "Updates indexes of datasets and system tables stored in an enterprise geodatabase. This tool is used in enterprise geodatabases to rebuild existing attribute or spatial indexes. Out-of-date indexes can lead to poor geodatabase performance.", "example": {"title": "RebuildIndexes example 1 (Python window)", "description": "The following example demonstrates how to rebuild indexes using the Python window in ArcGIS.", "code": "# Import system modules import arcpy arcpy.RebuildIndexes_management ( \"Database Connections/GDB@DC@server.sde\" , \"NO_SYSTEM\" , \"db1.GDB.Roads;db1.GDB.Parcels\" , \"ALL\" )"}, "usage": [], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " The enterprise database that contains the data to be updated. ", "dataType": "Workspace"}, {"name": "include_system", "isOptional": false, "description": " Indicates whether indexes will be rebuilt on the states and state lineages tables. You must be the geodatabase administrator for this option to be executed successfully. This option only applies to geodatabases. If the input workspace is a database this option will be ignored. NO_SYSTEM \u2014 Indexes will not be rebuilt on the states and state lineages table. This is the default. SYSTEM \u2014 Indexes will be rebuilt on the states and state lineages tables.", "dataType": "Boolean"}, {"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": " Names of the datasets that will have their indexes rebuilt. Dataset names use paths relative to the input workspace; full paths are not accepted as input. ", "dataType": "Dataset"}, {"name": "delta_only", "isOptional": false, "description": "Indicates how the indexes will be rebuilt on the selected datasets. This option has no effect if input_datasets is empty. This option only applies to geodatabases. If the input workspace is a database this option will be ignored. ALL \u2014 Indexes will be rebuilt on all indexes for the selected datasets. This option can be used for cases where the business tables for the selected datasets are not updated often and there are a high volume of edits in the delta tables. ONLY_DELTAS \u2014 Indexes will only be rebuilt for the delta tables of the selected datasets. This includes spatial indexes as well as user-created attribute indexes and any geodatabase-maintained indexes for the dataset. This is the default.", "dataType": "Boolean"}]},
{"syntax": "AddAttachments_management (in_dataset, in_join_field, in_match_table, in_match_join_field, in_match_path_field, {in_working_folder})", "name": "Add Attachments (Data Management)", "description": "\r\n Adds file attachments to the records of a geodatabase feature class or table. The attachments are stored internally in the geodatabase in a separate attachment table that maintains linkage to the target dataset. Attachments are added to the target dataset using a match table that dictates for each input record (or an attribute group of records) the path to a file to add as an attachment to that record.\r\n Learn more about geodatabase attachments Learn more about working with the attachments geoprocessing tools", "example": {"title": "AddAttachments example (Python window)", "description": "The following code snippet illustrates how to use the AddAttachments tool in the Python window.", "code": "import arcpy arcpy.AddAttachments_management ( r\"C:\\Data\\City.gdb\\Parcels\" , \"ParcelID\" , r\"C:\\Data\\matchtable.csv\" , \"ParcelID\" , \"Picture\" , r\"C:\\Pictures\" )"}, "usage": ["Before attachments can be added using this tool, they must first be enabled using the ", "Enable Attachments", " tool.", "Attachments added using this tool will be copied internally to the geodatabase. The original attachment files will not be affected in any way. If the original files are modified, these changes will not be automatically made to the geodatabase attachment; to synchronize changes to the geodatabase, remove the affected attachments using the ", "Remove Attachments", " tool, then add the modified files back as new attachments.", "If your ", "Input Dataset", " already contains a field that is the path to the attachments to add, and you do not want to use a separate ", "Match Table", ", specify the same dataset for both the ", "Input Dataset", " and ", "Match Table", ". The tool will automatically select the Object ID field for both join fields, and you can specify which field from the input contains the paths to the attachment files.", "\r\nMultiple files can be attached to a single feature class or table record. To accomplish this, the ", "Match Table", " should contain multiple records for that input ID (for example, record 1 has an InputID of ", "1", " and a pathname ", "pic1a.jpg", ", and record 2 has an InputID of ", "1", " and a pathname ", "pic1b.jpg", ").\r\n"], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " Geodatabase table or feature class to add attachments to. Attachments are not added directly to this table, but rather to a related attachment table that maintains linkage to the input dataset. The input dataset must be stored in a version 10.0 or later geodatabase, and the table must have attachments enabled. ", "dataType": "Table View"}, {"name": "in_join_field", "isInputFile": true, "isOptional": false, "description": " Field from the Input Dataset that has values that match the values in the Match Join Field . Records that have join field values that match between the Input Dataset and the Match Table will have attachments added. This field can be an Object ID field or any other identifying attribute. ", "dataType": "Field"}, {"name": "in_match_table", "isInputFile": true, "isOptional": false, "description": " Table that identifies which input records will have attachments added and the paths to those attachments. ", "dataType": "Table View"}, {"name": "in_match_join_field", "isInputFile": true, "isOptional": false, "description": " Field from the match table that indicates which records in the Input Dataset will have specified attachments added. This field can have values that match Input Dataset Object IDs or some other identifying attribute. ", "dataType": "Field"}, {"name": "in_match_path_field", "isInputFile": true, "isOptional": false, "description": " Field from the match table that contains paths to the attachments to add to Input Dataset records. ", "dataType": "Field"}, {"name": "in_working_folder", "isInputFile": true, "isOptional": true, "description": " Folder or workspace where attachment files are centralized. By specifying a working folder, the paths in the Match Path Field can be the short names of files relative to the working folder. For example, if loading attachments with paths like C:\\MyPictures\\image1.jpg , C:\\MyPictures\\image2.jpg , set the Working Folder to C:\\MyPictures , then paths in the Match Path Field can be the short names such as image1.jpg and image2.jpg , instead of the longer full paths. ", "dataType": "Folder"}]},
{"syntax": "RemoveEmptyFeatureClassFromGeometricNetwork_management (in_geometric_network, in_feature_class)", "name": "Remove Empty Feature Class From Geometric Network (Data Management)", "description": "\r\n Removes an empty feature class from a geometric network.", "example": {"title": null, "description": "The following stand-alone Python script demonstrates how to use the RemoveFeatureClassFromGeometricNetwork tool to remove an empty feature class called \"Tanks\" from a geometric network.", "code": "# Import arcpy module import arcpy # Local variables: Water_Net = \"C:/GeometricNetworks/Montgomery.gdb/Water/Water_Net\" # Process: Remove Feature Class From Geometric Network arcpy.RemoveFeatureClassFromGeometricNetwork ( Water_Net , \"Tanks\" )"}, "usage": ["\r\nOnly empty feature classes can be removed; populated feature classes cannot be removed.  Within an enterprise geodatabase, this may require the ", "Compress", " tool to be run in order for the feature class to be recognized as empty.\r\n", "\r\nThis tool will not delete the feature class; it will only remove it from the network.  You must delete the feature class if needed.\r\n", " In an enterprise geodatabase, the feature class being removed and the geometric network may be versioned.\r\n", "You cannot remove the orphan junction feature class from a geometric network.  ", "\r\nA feature class cannot be removed from the geometric network if, besides the orphan junction feature class, it is the sole feature class in the network.\r\n"], "parameters": [{"name": "in_geometric_network", "isInputFile": true, "isOptional": false, "description": " The geometric network from which the feature class will be removed. ", "dataType": "Geometric Network"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": " The name of the feature class to remove. ", "dataType": "String"}]},
{"syntax": "ImportXMLWorkspaceDocument_management (target_geodatabase, in_file, {import_type}, {config_keyword})", "name": "Import XML Workspace Document (Data Management)", "description": "\r\n Imports the contents of an XML workspace document into an existing geodatabase.\r\n Learn more about copying a schema using XML workspaces Learn more about Geodatabase XML", "example": {"title": "ImportXMLWorkspaceDocument example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ImportXMLWorkspaceDocument tool in immediate mode.", "code": "import arcpy arcpy.ImportXMLWorkspaceDocument_management ( \"C:/Data/Target.gdb\" , \"C:/Data/StJohnsData.xml\" , \"SCHEMA_ONLY\" , \"DEFAULTS\" )"}, "usage": ["\r\nThe ", "Target Geodatabase", " must already exist and can be a personal, file, or ArcSDE geodatabase. To create a new, empty geodatabase use the ", "Create File GDB", " or ", "Create Personal GDB", " tools.\r\n", "If you are importing into a file or ArcSDE geodatabase and want to use a configuration keyword, you can choose one from the ", "Configuration Keyword", " drop-down list on the tool dialog box. In a stand-alone script, you will need to know the name of the configuration keyword to use.", "If ", "Overwrite the outputs of geoprocessing operations option", " on the ", "Geoprocessing Options", " dialog box is unchecked and a data element from the input XML workspace document has the same name as a data element in the ", "Target Geodatabase", ", the data element will be imported with a new unique name. If this option is checked, existing datasets will be overwritten. For more information about overwriting tool output, see ", "Using geoprocessing options to control tool execution", ".", "The tool messages will include the list of data element names that were imported."], "parameters": [{"name": "target_geodatabase", "isOptional": false, "description": " The existing geodatabase where the contents of the XML workspace document will be imported. ", "dataType": "Workspace"}, {"name": "in_file", "isInputFile": true, "isOptional": false, "description": " The input XML workspace document file containing geodatabase contents to be imported. This can be an XML file or a compressed ZIP file ( .zip or .z ) containing the XML file. ", "dataType": "File"}, {"name": "import_type", "isOptional": true, "description": " Determines if both data (feature class and table records, including geometry) and schema are imported, or only schema is imported. DATA \u2014 Import the data and schema. This is the default. SCHEMA_ONLY \u2014 Import the schema only.", "dataType": "String"}, {"name": "config_keyword", "isOptional": true, "description": " Geodatabase configuration keyword to be applied if the Target Geodatabase is an ArcSDE or file geodatabase. ", "dataType": "String"}]},
{"syntax": "ExtractPackage_management (in_package, output_folder)", "name": "Extract Package (Data Management)", "description": "\r\nExtracts the contents of a package \r\nto a specified folder.   The output folder will be  updated with the extracted contents of the input package.", "example": {"title": "ExtractPackage example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ExtractPackage tool.", "code": "arcpy.env.workspace = \"C:/arcgis/ArcTutor/Getting_Started/Greenvalley\" arcpy.ExtractPackage_management ( 'WaterUsePackage.lpk' , 'C:/My_Data/Packages/WaterUse_unpacked' )"}, "usage": ["Supported package types include: ", "The output folder can be a new folder or an existing folder.  When extracting to an existing folder, the contents of the package will be appended to existing files and folders.     If the output folder  already contains  the extracted contents of the package, the existing contents will be overwritten. "], "parameters": [{"name": "in_package", "isInputFile": true, "isOptional": false, "description": " The input package that will be extracted. ", "dataType": "File"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder to contain the contents of the package. ", "dataType": "Folder"}]},
{"syntax": "SharePackage_management (in_package, username, password, summary, tags, {credits}, {public}, {groups})", "name": "Share Package (Data Management)", "description": "\r\nShares a package by uploading to ArcGIS online\r\n\r\n ", "example": {"title": "SharePackage example 1 (Python window)", "description": "Code sample that takes a layer package and shares it to ArcGIS Online.", "code": "import arcpy arcpy.SharePackage_management ( r\"C:\\states.lpk\" , \"username\" , \"password\" , \"this is a summary\" , \"tag1, tag2\" , \"Credits\" , \"MYGROUPS\" , \"My_Group\" )"}, "usage": ["If  a package of  the same name already exists on ArcGIS online, it will be overwritten. "], "parameters": [{"name": "in_package", "isInputFile": true, "isOptional": false, "description": " Input layer ( .lpk ), map ( .mpk ), geoprocessing ( .gpk ), map tile ( .tpk ), or address locator ( .gcpk ) package file. ", "dataType": "File"}, {"name": "username", "isOptional": false, "description": " Esri Global Account user name. ", "dataType": "String"}, {"name": "password", "isOptional": false, "description": " Esri Global Account password. ", "dataType": "Encrypted String"}, {"name": "summary", "isOptional": false, "description": "Summary of package. The summary is displayed in the item information of the package on ArcGIS.com. ", "dataType": "String"}, {"name": "tags", "isOptional": false, "description": " Tags used to describe and identify the package. Individual tags are separated using either a comma or semicolon. ", "dataType": "String"}, {"name": "credits", "isOptional": true, "description": "Credits for the package. This is generally the name of the organization that is given credit for authoring and providing the content for the package. ", "dataType": "String"}, {"name": "public", "isOptional": true, "description": "Specifies whether input package will be shared and available to everybody. EVERYBODY \u2014 Package will be shared with everybody. MYGROUPS \u2014 Package will be shared with package owner and any selected group. This is the default.", "dataType": "Boolean"}, {"name": "groups", "isOptional": false, "description": "List of groups to share package with. ", "dataType": "String"}]},
{"syntax": "MakeMosaicLayer_management (in_mosaic_dataset, out_mosaic_layer, {where_clause}, {template}, {band_index}, {mosaic_method}, {order_field}, {order_base_value}, {lock_rasterid}, {sort_order}, {mosaic_operator}, {cell_size})", "name": "Make Mosaic Layer (Data Management)", "description": "Creates a temporary mosaic layer from an mosaic dataset or layer file. The layer that is created by the tool is temporary and will not persist after the session ends unless the layer is saved to disk or the map document is saved. This tool can be used to make a temporary layer, so you can work with a specified subset of bands within a mosaic dataset.", "example": {"title": "MakeMosaicLayer example 1 (Python window)", "description": "This is a Python sample for MakeMosaicLayer.", "code": "import arcpy arcpy.MakeMosaicLayer_management ( \"c:/data/fgdb.gdb/mdsrc\" , \"mdlayer2\" , \"\" , \"clipmd.shp\" , \"3;2;1\" , \"BY_ATTRIBUTE\" , \"Tag\" , \"Dataset\" , \"\" , \"DESCENDING\" , \"LAST\" , \"10\" )"}, "usage": ["To make your layer permanent, right-click the layer in the table of contents and click ", "Save As Layer File", ", or use the ", "Save To Layer File", " tool."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "The path and name of the input mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "out_mosaic_layer", "isOutputFile": true, "isOptional": false, "description": " The name of the temporary output mosaic layer. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": " A query statement using the fields and values of the mosaic dataset. ", "dataType": "SQL Expression"}, {"name": "template", "isOptional": true, "description": " Using the min x, min y, max x, or max y, you can specify the extents of the output mosaic layer. ", "dataType": "Extent"}, {"name": "band_index", "isOptional": false, "description": " Choose which bands to export for the layer. If no bands are specified, then all the bands will be used in the output. ", "dataType": "Value Table"}, {"name": "mosaic_method", "isOptional": true, "description": "Choose the mosaic method. The mosaic method defines how the layer is created from different rasters within the mosaic dataset. CLOSEST_TO_CENTER \u2014 Sorts rasters based on an order where rasters that have their center closest to the view center are placed on top. NORTH_WEST \u2014 Sorts rasters based on an order where rasters that have their center closest to the northwest are placed on top. LOCK_RASTER \u2014 Enables a user to lock the display of single or multiple rasters, based on an ID or name. When you choose this option, you need to specify the Lock Raster ID. BY_ATTRIBUTE \u2014 Sorts rasters based on an attribute field and its difference from the base value. When this option is chosen, the order field and order base value parameters also need to be set. CLOSEST_TO_NADIR \u2014 Sorts rasters based on an order where rasters that have their nadir position closest to the view center are placed on top. The nadir point can be different from the center point, especially in oblique imagery. CLOSEST_TO_VIEWPOINT \u2014 Sorts rasters based on an order where the nadir position is closest to the user-defined viewpoint location and are placed on top. SEAMLINE \u2014 Cuts the rasters using the predefined seamline shape for each raster using optional feathering along the seams. The ordering is predefined during seamline generation. The LAST mosaic operator is not valid with this mosaic method.", "dataType": "String"}, {"name": "order_field", "isOptional": true, "description": " Choose the order field. When the mosaic method is BY_ATTRIBUTE, the default field to use when ordering rasters needs to be set. The list of fields is defined as those in the service table that are of type metadata. ", "dataType": "String"}, {"name": "order_base_value", "isOptional": true, "description": "Type an order base value. The images are sorted based on the difference between this value and the attribute value in the specified field. ", "dataType": "String"}, {"name": "lock_rasterid", "isOptional": true, "description": " Choose the Raster ID or raster name to which the service should be locked and that only the specified rasters are displayed. If left undefined, it will be similar to system default. Multiple IDs can be defined as a semicolon-delimited list. ", "dataType": "String"}, {"name": "sort_order", "isOptional": true, "description": "Choose whether the sort order is ascending or descending. ASCENDING \u2014 The sort order will be ascending. This is the default. DESCENDING \u2014 The sort order will be descending.", "dataType": "String"}, {"name": "mosaic_operator", "isOptional": true, "description": "Choose which mosaic operator to use. When two or more rasters all have the same sort priority, this parameter is used to further refine the sort order. FIRST \u2014 The first raster in the list will be on top. This is the default. LAST \u2014 The last raster in the list will be on top. MIN \u2014 The raster with the lowest value will be on top. MAX \u2014 The raster with the lowest value will be on top. MEAN \u2014 The average pixel value will be on top. BLEND \u2014 The output cell value will be a blend of values; this blend value relies on an algorithm that is weight based and dependent on the distance from the pixel to the edge within the overlapping area.", "dataType": "String"}, {"name": "cell_size", "isOptional": true, "description": " The cell size for the output mosaic layer. ", "dataType": "Double"}]},
{"syntax": "CalculateValue_management (expression, {code_block}, {data_type})", "name": "Calculate Value (Data Management)", "description": " Calculate Value tool returns a value based on a specified Python expression.", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "The ", "Data Type", " parameter is used in ModelBuilder to help chain the output of the ", "Calculate Value", " tool with other tools. For example, if you use the ", "Calculate Value", " tool to calculate a distance for use as input to the ", "Buffer Distance", " parameter of the ", "Buffer", " tool, specify Linear Unit for the ", "Data Type", " parameter.", "Variables created in ModelBuilder can be used by this tool, but variables desired for use in the expression parameter cannot be connected to the ", "Calculate Value", " tool. To use them in the expression, enclose the variable name in percent signs (", "%", "). For example, if you want to divide a variable named 'Input' by 100, your expression would be ", "%Input%/100", ". ", "Note: in the previous expression, if Input = 123, the expression will return 1. To get decimal places, add decimals to the values in the expression. For example: ", "%Input%/100.00", " will return 1.23.  The illustration below shows another example of using variables in the expression.", "In-line variable of type string should be enclosed within quotes (", "\"%string variable%\"", ") in an expression. In-line variables of type numbers (double, long) do not require quotes (", "%double%", ").  ", "Expressions can be created in a standard Python format ONLY. Other scripting languages are not supported.", "The ", "Calculate Value", " tool can evaluates simple mathematical expressions. For example:", "The ", "Calculate Value", " tool allows the use of the Python math module to perform more complex mathematical operations. The math module is accessed by preceding the desired function with \"math\". For example:", "Constants are also supported through the math module. For example:", "The ", "arcgis.rand()", " function is supported. The ", "arcgis.rand()", " function has been created for ArcGIS tools and should not be confused with the Python ", "Rand()", " function. Examples of using the ", "arcgis.rand()", " are as follows:", "The expression ", "arcgis.rand", " must be entered in lowercase characters.", "Generally, you will type the expressions in the ", "Expression", " parameter. More complicated expressions, such as multiline calculations or logical operations (if, then), will require the use of the ", "Code Block", " parameter. The ", "Code Block", " parameter cannot be used on its own; it must be used in conjunction with the ", "Expression", " parameter. ", "Variables defined in the ", "Code Block", " parameter can be referenced from the expression.", "Functions can be defined in the ", "Code Block", " parameter and called from the expression. In the example below, the function returns a wind direction string based on a random input value. In Python, functions are defined using the ", "def", " keyword followed by the name of the function and the function's input parameters. In this case, the function is ", "getWind", " and has one parameter, ", "wind", ". Values are returned from a function using the ", "return", " keyword.", "You can pass variable through the Expression parameter and use if-else logic with inline variables in the code block as shown  below.  The code block checks to see if the ", "Input Cell Size", " variable is empty then returns a value based on the condition.", "Python methods can be used directly in the Expression parameter of the tool. For example, if  you have an input value with a decimal (field value of the input table in this case) and want to use the value in the output name of another tool through ", "inline variable substitution", ", the decimal can be replaced using Python method ", "replace", " in the ", "Calculate Value", " tool expression.", "Python modules can be called and methods such as replace combined or stacked in the code block parameter. In the example below the  ", "time", " module is imported in the code block which returns the current date and time such as ", "Fri Mar 19 2010 09:42:39", ". This returned value is used as name in ", "Create Folder", " tool to name the folder. Since the name of the folder cannot contain spaces or punctuation marks, the ", "replace", " method in Python is used by stacking the method for each element that needs to be replaced. The resulting name of the folder in this example is  ", "FriMar192010094239", ".", "If you are calculating a value in the model and want to use the calculated value with tools such as  ", "Buffer", " that require a buffer distance value as well as a linear unit you have to:", "You can  use the output of ", "Calculate Value", " tool directly in any ", "Spatial Analyst", " tools which accept a raster or a constant value such as ", "Plus", ",", "Greater Than", ", and ", "Less Than", " (these tools are found  in the ", "Spatial Analyst", " toolbox/", "Math toolset", ").  To use the output of ", "Calculate Value", ", change the output data type to Formulated Raster. This output data type format  is a raster surface whose cell values are represented by a formula or constant.", "In Python, part of syntax is proper indentation. Indentation level (two spaces or four spaces) does not matter as long as it is consistent throughout the code block.", "You cannot access model variables from the code block. Such variables must be passed to the code block from the expression. This can be achieved by creating a definition in the ", "Code Block", " and referencing the definition in the ", "Expression", " box.", "  When writing Python scripts, use standard Python statements instead of the ", "Calculate Value", " tool."], "parameters": [{"name": "expression", "isOptional": false, "description": "The Python expression to be evaluated. ", "dataType": "SQL Expression"}, {"name": "code_block", "isOptional": true, "description": "Additional Python code. Code in the code block can be referenced in the Expression parameter. ", "dataType": "String"}, {"name": "data_type", "isOptional": true, "description": "The data type of the output returned from the Python expression. This parameter should be used in ModelBuilder to help chain Calculate Value with other tools. ", "dataType": "String"}]},
{"syntax": "MergeBranch_management ({in_values})", "name": "Merge Branch (Data Management)", "description": "The  Merge Branch  tool merges two or more logical branches into a single output.  Branching in a model is accomplished by creating a script tool that implements the necessary if-then-else logic. It is often the case when branching that you need to merge two branches into a single process. What this means is that if you test an input against a condition (examples: whether the data exists on the disk, whether the cell size is greater than 30 meters, whether the field value is 1), it will create two outputs: True, if the condition is true, and False, if the condition is false. If the condition is True, you want to run some processes and if the condition is False, you want different processes to run, as illustrated below.   At any point, only one of the branches will run depending on the condition and the input. The  Merge Branch  tool is used in such cases where it is not possible to say which branch will run and produce results. The output of both branches becomes the input for the  Merge Branch  tool. The tool looks at the inputs and passes the last output of a branch that has-been-run to the next tool. The  Merge Branch  tool allows any number of inputs and uses the multivalue parameter control.", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "The tool examines the list of input variables and returns the first variable that is in the has-been-run state.", "Merge Branch", " accepts any data type in its list of values. The output data type is Any Value, which is a generic data type. This means you can connect the output of ", "Merge Branch", " to any parameter of any tool. When the connected tool is run, it expects the contents of an Any Value variable to be of the correct data type; it is up to you to make sure that the contents are correct for the tool's parameter.", "\r\n", "All the tools in ArcGIS are empty (without color) when added in a model except ", "Merge Branch", " and ", "Collect Values", ". Unlike other system tools, ", "Merge Branch", " is always a ready-to-run state (colored in). This is because the input to ", "Merge Branch", " is a multiple value data type, and an empty multiple value is considered a valid input."], "parameters": [{"name": "in_values", "isInputFile": true, "isOptional": false, "description": "List of values from different branches. The first ready-to-run state value in the list will be the output of the tool. ", "dataType": "Multiple Value"}]},
{"syntax": "SelectData_management (in_dataelement, {out_dataelement})", "name": "Select Data (Data Management)", "description": " The  Select Data  tool selects data in a parent data element such as a folder, geodatabase, feature dataset, or coverage. The tool allows access to the data stored inside a parent container, such as feature classes or tables inside a geodatabase. \r\n Learn how Select Data works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "Selecting the child from the parent using this tool enables you to continue processing after performing a task where the output data is a container, such as a feature dataset, and the next tool in the model requires a feature class.", "The tool's output always includes the full path to the child dataset.", "When building models where the input to ", "Select Data", " does not exist, the name of the child data element may need to be typed in."], "parameters": [{"name": "in_dataelement", "isInputFile": true, "isOptional": false, "description": "The input data element can be a folder, geodatabase, feature dataset, or coverage. ", "dataType": "Data Element; Composite Layer"}, {"name": "out_dataelement", "isOutputFile": true, "isOptional": true, "description": "The child data element is contained by the input data element. Once the input data element is specified, the child data element control contains a drop-down list of the data elements contained in the input data element. For example, if the input is a feature dataset, all the feature classes within the feature dataset are included in the drop-down list. A single element is selected from this list. ", "dataType": "String"}]},
{"syntax": "TransposeFields_management (in_table, in_field, out_table, in_transposed_field_name, in_value_field_name, {attribute_fields})", "name": "Transpose Fields (Data Management)", "description": "\r\nShifts  data entered in fields or columns into rows in a table or feature class. This tool is useful when your table or feature class stores values in field names (such as Field1, Field2, Field3) and you want to transpose the field names and the corresponding data values in the fields into a row format.", "example": {"title": "TransposeFields example (Python window)", "description": "The following Python window script demonstrates how to use the TransposeFields tool in immediate mode.", "code": "import arcpy arcpy.TransposeFields_management ( \"C:/Data/TemporalData.gdb/Input\" , \"Field1 newField1;Field2 newField2;Field3 newField3\" , \"C:/Data/TemporalData.gdb/Output_Time\" , \"Transposed_Field\" , \"Value\" , \"Shape;Type\" )"}, "usage": ["By default, the  output is a  table. However, if you want to transpose the fields in a feature class, you can choose to output either a  table or a feature class with the transposed fields.  To output a feature class, you should choose the Shape field under Attribute Fields.", " If the feature class is an input, the only way you can get a feature class output is to select the Shape field in the Attribute Fields parameter."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input feature class or table for which the fields containing data values will be transposed. ", "dataType": "Table View"}, {"name": "in_field", "isInputFile": true, "isOptional": false, "description": "The fields or columns containing data values in the input table that need to be transposed. Depending on your needs, you can select multiple fields that need to be transposed. By default, the value is the same as the field name. However, you can choose to specify your own value. For example, if the field names of the fields you want to transpose are Pop1991, Pop1992, and so on, by default, the values for these field will be the same ( Pop1991, Pop1992, and so forth). However, you can choose to specify your own values such as 1991 and 1992. ", "dataType": "Value Table"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output feature class or table. The output feature class or table will contain the transposed field, a value field, and any number of attribute fields specified that need to be inherited from the input table. What is specified for out_table will be a table, unless the in_table value is a feature class and the Shape field is selected in the attribute_fields parameter. ", "dataType": "Table"}, {"name": "in_transposed_field_name", "isInputFile": true, "isOptional": false, "description": "The name of the field that will be created to store field name values of the fields that are selected to be transposed. Any valid field name can be used. ", "dataType": "String"}, {"name": "in_value_field_name", "isInputFile": true, "isOptional": false, "description": "The name of the value field that will be created to store the values from the input table. Any valid field name can be set, as long as it does not conflict with existing field names from the input table or feature class. ", "dataType": "String"}, {"name": "attribute_fields", "isOptional": false, "description": "Attribute fields from the input table to be included in the output table. If you want to output a feature class, choose the Shape field. ", "dataType": "Field"}]},
{"syntax": "ConvertTimeZone_management (in_table, input_time_field, input_time_zone, output_time_field, output_time_zone, {input_dst}, {output_dst})", "name": "Convert Time Zone (Data Management)", "description": "\r\n\r\nConverts time values recorded in a date field from one time zone to another time zone. Converting time values from one time zone to another helps normalize temporal data from different time zones. This improves display and query performance for visualizing temporal data from different time zones using the Time Slider. Learn more about setting a temporal reference on your temporal data Learn more about visualizing temporal data using the Time Slider", "example": {"title": "ConvertTimeZone example (Python window)", "description": "The following Python window script demonstrates how to use the\r\nConvertTimeZone tool in immediate mode.", "code": "import arcpy arcpy.ConvertTimeZone_management ( \"C:/Data/TemporalData.gdb/InputData\" , \"Input_Time\" , \"Pacific_Standard_Time\" , \"Output_Time\" , \"Eastern_Standard_Time\" , \"INPUT_ADJUSTED_FOR_DST\" , \"OUTPUT_ADJUSTED_FOR_DST\" )"}, "usage": ["\r\nThe input time values to be converted must be stored in a Date field.", "The added output time field will be a  Date type field.", "If your data was collected in a time zone which observes Daylight Saving, then you should choose the parameters to honor Daylight Saving time in the input and output fields. ", "Learn more about Daylight Saving Time", "."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " The input feature class or table that contains the time stamps which will be transformed to a different time zone. ", "dataType": "Table View"}, {"name": "input_time_field", "isInputFile": true, "isOptional": false, "description": " The input field that contains the time stamps which will be transformed to a different time zone. ", "dataType": "Field"}, {"name": "input_time_zone", "isInputFile": true, "isOptional": false, "description": " The input time zone in which the time stamps were collected. ", "dataType": "String"}, {"name": "output_time_field", "isOutputFile": true, "isOptional": false, "description": " The output field in which the time stamps transformed to the desired output time zone will be stored. ", "dataType": "String"}, {"name": "output_time_zone", "isOutputFile": true, "isOptional": false, "description": " The time zone to which the time stamps will be transformed. By default, the output time zone is the same as the input time zone. ", "dataType": "String"}, {"name": "input_dst", "isInputFile": true, "isOptional": true, "description": "Indicates whether the time stamps were collected while observing Daylight Saving Time rules in the input time zone. When reading the time values to convert the time zone, the time values will be adjusted to account for the shift in time during Daylight Saving Time. By default, the input time values are adjusted to account for the shift in time due to the Daylight Saving Time rules in the input time zone. INPUT_ADJUSTED_FOR_DST \u2014 The input time values are adjusted for Daylight Saving Time. INPUT_NOT_ADJUSTED_FOR_DST \u2014 The input time values are not adjusted for Daylight Saving Time.", "dataType": "Boolean"}, {"name": "output_dst", "isOutputFile": true, "isOptional": true, "description": "Indicates whether the output time values will account for the shift in time due to the Daylight Saving Time rules observed in the output time zone. By default, the output time values are adjusted to account for the shift in time due to the Daylight Saving Time rules observed in the output time zone. OUTPUT_ADJUSTED_FOR_DST \u2014 The output time values will be adjusted for Daylight Saving Time in the out time zone. OUTPUT_NOT_ADJUSTED_FOR_DST \u2014 The output time values will not be adjusted for Daylight Saving Time in the out time zone.", "dataType": "Boolean"}]},
{"syntax": "ConvertTimeField_management (in_table, input_time_field, {input_time_format}, output_time_field, {output_time_type}, {output_time_format})", "name": "Convert Time Field (Data Management)", "description": "\r\nConverts time values stored in a string or numeric field to a date field. The tool can also be used to convert time values stored in string, numeric, or date fields into custom formats such as day of the week, month of the year, and so on.", "example": {"title": "ConvertTimeField example 1 (Python window)", "description": "The following Python window script demonstrates how to use the\r\nConvertTimeField tool in immediate mode.", "code": "import arcpy arcpy.ConvertTimeField_management ( \"C:/Data/TemporalData.gdb/Input_Table\" , \"Input_Time\" , \"1033;MMMM dd, yyyy HH:mm:ss;AM;PM\" , \"Output_Time\" )"}, "usage": ["\r\nIf the input time field selected is a string or text field, the input time format can be selected from a list of ", "supported time field formats", ". or you can define a custom time field format to interpret custom date and/or time values in the  string field.  For more information about custom formats for string fields, see ", "converting string time values into date format", ".", "If the input time field selected is numeric (short, long, float, or double), the input time format can be selected from a list of ", "supported standard time field formats", ". Custom time formats are not supported  with numeric fields."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " The layer or table that contains the field containing the time values that need to be converted. ", "dataType": "Table View"}, {"name": "input_time_field", "isInputFile": true, "isOptional": false, "description": " The field containing the time values. May be of type short, long, float, double, text, or date. ", "dataType": "Field"}, {"name": "input_time_format", "isInputFile": true, "isOptional": true, "description": "The format in which the time values were stored in the input time field. Either a standard time format can be selected from the drop-down list, or a custom format can be entered. If the data type of the time field is numeric (Short, Long, Float, or Double), a list of standard numeric time formats is provided in the drop-down list. If the data type of the time field is string, a list of standard string time formats is provided in the drop-down list. For string fields, you can also choose to specify a custom time format. For example, the time values may have been stored in a string field in one of the standard formats such as yyyy/MM/dd HH:mm:ss or in a custom format such as dd/MM/yyyy HH:mm:ss. For the custom format, you can also specify the a.m., p.m. designator. If the data type of the time field is date, then no time format is required. Learn more about custom dates and time formats ", "dataType": "String"}, {"name": "output_time_field", "isOutputFile": true, "isOptional": false, "description": " The name of output field in which the converted time values will be stored. ", "dataType": "String"}, {"name": "output_time_type", "isOutputFile": true, "isOptional": true, "description": " The data type of the output time field. May be of type short, long, float, double, text, or date. ", "dataType": "String"}, {"name": "output_time_format", "isOutputFile": true, "isOptional": true, "description": " The format in which the output time values will be saved. The list of output time formats depends on the output data type specified for the output time field. ", "dataType": "String"}]},
{"syntax": "CalculateEndTime_management (in_table, start_field, end_field, {fields})", "name": "Calculate End Time (Data Management)", "description": "Calculates the end time of features based on the time values stored in another field. In the illustration below, the end time values in the  End_Time  field are calculated using the time values in the  Start_Time  field. The end time value for a feature is equal to the start time of the next feature. However, for the last feature in the table, the end time value is calculated to be the same as the start time value of the feature. ", "example": {"title": "CalculateEndTime example (Python window)", "description": "The following Python window script demonstrates how to use the CalculateEndTime tool in immediate mode.", "code": "import arcpy arcpy.CalculateEndTime_management ( \"C:/Data/TemporalData.gdb/CalculateEndTime\" , \"Start_Time\" , \"End_Time\" , \"\" )"}, "usage": ["\r\nThe table is first sorted by the fields in the ", "ID Fields", " parameter, then by the ", "Start_Time ", " field.  After this sort,  the end time of any row is the same as the start time of the next row.", "This tool is useful when the intervals between Start_Time field values are not regular and you want to visualize the feature class or table through time using the time slider. ", "Learn more about visualizing temporal data", " and ", "irregularly spaced temporal data", ".", "The End_Time field value for the last row will be the same as the Start_Time field value for that row."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The feature class or table for which an End_Time field is calculated based on the Start_Time field specified. ", "dataType": "Table View"}, {"name": "start_field", "isOptional": false, "description": "The field containing values that will be used to calculate values for the End_Time field. The Start_Time field and the End_Time field must be of the same type. For example, if the Start_Time field is of type LONG, then the End_Time field should be of type LONG as well. ", "dataType": "Field"}, {"name": "end_field", "isOptional": false, "description": "The field that will be populated with values based on the Start_Time field specified. The Start_Time field and the End_Time field must be of the same format. ", "dataType": "Field"}, {"name": "fields", "isOptional": false, "description": "The name of the field or fields that can be used to uniquely identify spatial entities. These fields are used to first sort based on entity type if there is more than one entity. For instance, for a feature class representing population values per state over time, the state name could be the unique value field (the entity). If population figures are per county, you would need to set the county name and state name as the unique value fields, since some county names are the same for different states. If there is only one entity, this parameter can be ignored. ", "dataType": "Field"}]},
{"syntax": "ComputeDirtyArea_management (in_mosaic_dataset, {where_clause}, timestamp, out_feature_class)", "name": "Compute Dirty Area (Data Management)", "description": "\r\nIdentifies an area within a mosaic dataset that has changed since a specified point in time.\r\n", "example": {"title": "ComputeDirtyArea example 1 (Python window)", "description": "A Python sample for ComputeDirtyArea.", "code": "import arcpy arcpy.ComputeDirtyArea_management ( \"c:/workspace/fgdb.gdb/md\" , \"#\" , \"2010-01-12T18:00:00.00-08:00\" , \"dirtyarea.shp\" )"}, "usage": ["\r\nThis tool constructs a polygon that defines regions containing one or more mosaic dataset items that have been modified since a specified point in time.  ", "This allows tools and applications that depend on the mosaic dataset for the construction of derived products, such as cache, to perform partial updates since the last time the derived products were synchronized with the mosaic dataset.\r\n", "\r\nThe date and time parameter can be specified in one of two ways: ", "A valid XML time string must be in one of the following formats:", "The last possible part of the XML time strings is the time zone. The time zones specified with a Z refers to Zulu Time, or Greenwich Mean Time. You can also specify a time zone by using the positive or negative hours from Zulu Time.  If you do not specify a time zone, then the local time zone will be used.", "Valid Non-XML time strings can take on any format shown below:"], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The input mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": " An optional SQL query that can be specified to only process a specific subset of data. ", "dataType": "SQL Expression"}, {"name": "timestamp", "isOptional": false, "description": " Specify a date and time. All mosaic items modified after this date will be used to compute the dirty area. Currently, there are two types of date formats supported: For more information about the formatting of the time string, refer to the Usage Tips. XML time syntax: YYYY-MM-DD T hh:mm:ss , YYYY-MM-DD T hh:mm:ss.ssssZ , 2002-10-10 T 12:00:00.ssss-00:00 , 2002-10-10 T 12:00:00+00:00 Non-XML time syntax: 2002/12/25 23:59:58.123", "dataType": "String"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output feature class where the dirty area polygons will be created. ", "dataType": "Feature Class"}]},
{"syntax": "ConvertCoordinateNotation_management (in_table, out_featureclass, x_field, y_field, input_coordinate_format, output_coordinate_format, {id_field}, {spatial_reference})", "name": "Convert Coordinate Notation (Data Management)", "description": "Converts coordinate notations representing locations from one format to another. Learn more about supported notation formats", "example": {"title": "ConvertCoordinateNotation example 1 (stand-alone script)", "description": "ConvertCoordinateNotation usage with one input format field.", "code": "#Imports import arcpy #Locals in_tab = r\"c:\\workspace\\inmed.gdb\\loc_mgrs\" out_pts = r\"c:\\workspace\\inmed.gdb\\loc_final\" #Convert Coordinate Notation with MGRS as input field. arcpy.ConvertCoordinateNotation_management ( in_tab , out_pts , \"m10d\" , \"#\" , \"MGRS\" , \"DD_1\" )"}, "usage": ["The input table can be a text file or any table supported by ArcGIS.", "The output is a point feature class where each input location with a valid notation is represented as a point. The records with invalid notations will not have any geometry and newly added output fields will be empty.", "The IDs of the records with invalid notations that fail to convert are listed in a text file named  ", "ConvertCoordinateNotation<x>.log", " (", "<x>", " is an arbitrary number). The file is stored in the user's ", "Temp", " folder. For example, in Windows 7, the directory is ", "Users\\<user>\\AppData\\Local\\Temp", ". In UNIX systems, the file is located in the user's home directory, under ", "$TMP", ".", "The following formats are supported: Decimal degrees (DD), Degrees decimal minutes (DDM), Degrees-minutes-seconds (DMS), Global Area Reference System (GARS), GEOREF (World Geographic Reference System), Universal Transverse Mercator (UTM), United States National Grid (USNG), and Military Grid Reference System (MGRS). ", "Any format can be used as the input or output format. For example,  DMS can be used as both input and output format to convert values and obtain a point feature class for the locations.", "DD, DDM, and DMS require two values to represent a location: one for latitude and the other for longitude. The two values can be in two different fields or the two values can be concatenated into a single string and stored in a single field.", "GARS, GEOREF, UTM, USNG, and MGRS are single-string coordinate formats, meaning only one field contains the coordinate.", "See the ", "Input Coordinate Format", " parameter explanation below for more information.", "If DD_2, DDM_2, or DMS_2 is chosen as the output format, two new fields are added to the output point feature class. For any other format, a single field is added to the output feature class.", "Output field names are matched to the name of the output coordinate notation. For example, if the output format is MGRS, then the new output field name will be MGRS.", "Fields containing the input coordinates  are copied to the output feature class to help compare the input values to the value of the output format. If a field with the same name as the input field already exists in the output, the name of the copied field is appended with a unique number.", "The ", "Add XY Coordinates", " tool can be used to add two fields\u2014POINT_X and POINT_Y\u2014to the output point feature class. These fields contain the coordinates of a point in the unit of the coordinate system of the feature class."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " The table containing the fields with coordinate notations to convert. ", "dataType": "Table View"}, {"name": "out_featureclass", "isOutputFile": true, "isOptional": false, "description": "The output feature class of points. The attribute table will contain fields storing converted values in the output format. Fields from the input table with original input format values will also be added to the output table. ", "dataType": "Feature Class"}, {"name": "x_field", "isOptional": false, "description": " A field from the input table containing the longitude value. For DD_2, DDM_2, and DMS_2, this is the longitude field. For DD_1, DDM_1, DMS_1, GARS, GEOREF, UTM, USNG, and MGRS, this is the field containing both latitude and longitude. ", "dataType": "Field"}, {"name": "y_field", "isOptional": false, "description": "A field from the input table containing the latitude value. For DD_2, DDM_2, and DMS_2, this is the latitude field. This parameter is ignored for DD_1, DDM_1, DMS_1, GARS, GEOREF, UTM, USNG, and MGRS. ", "dataType": "Field"}, {"name": "input_coordinate_format", "isInputFile": true, "isOptional": false, "description": " Coordinate format of the input fields. The default is DD_2. DD, DDM, and DMS are also valid keywords; they can be used just by typing in (on dialog) or passing the value in scripting. However, keywords with underscore and a number tell whether the values are coming from one field or two fields. DD_1 \u2014 Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DD_2 \u2014 Longitude and latitude values are in two separate fields. DDM_1 \u2014 Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DDM_2 \u2014 Longitude and latitude values are in two separate fields. DMS_1 \u2014 Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DMS_2 \u2014 Longitude and latitude values are in two separate fields. GARS \u2014 Global Area Reference System. Based on latitude and longitude, it divides and subdivides the world into cells. GEOREF \u2014 World Geographic Reference System. A grid-based system that divides the world into 15-degree quadrangles and then subdivides into smaller quadrangles. UTM \u2014 Based on the Universal Transverse Mercator projection. It divides the world into 6-degree slices of longitude, which are divided into 20 latitude bands. These grids are further described using eastings and northings to locate any point within the grid. USNG \u2014 United States National Grid. Almost exactly the same as MGRS but uses North American Datum 1983 (NAD83) as its datum. MGRS \u2014 Military Grid Reference System. Follows the UTM coordinates and divides the world into 6-degree longitude and 20 latitude bands, but MGRS then further subdivides the grid zones into smaller 100,000-meter grids. These 100,000-meter grids are then divided into 10,000-meter, 1,000-meter, 100-meter, 10-meter, and 1-meter grids. SHAPE \u2014 Only available when a point feature layer is selected as input. The coordinates of each point are used to define the output format.", "dataType": "String"}, {"name": "output_coordinate_format", "isOutputFile": true, "isOptional": false, "description": " Coordinate format to which the input notations will be converted. The default is DD_2. DD, DDM, and DMS are also valid keywords; they can be used just by typing in (on dialog) or passing the value in scripting. However, keywords with underscore and a number tell whether the values are coming from one field or two fields. DD_1 \u2014 Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DD_2 \u2014 Longitude and latitude values are in two separate fields. DDM_1 \u2014 Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DDM_2 \u2014 Longitude and latitude values are in two separate fields. DMS_1 \u2014 Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DMS_2 \u2014 Longitude and latitude values are in two separate fields. GARS \u2014 Global Area Reference System. Based on latitude and longitude, it divides and subdivides the world into cells. GEOREF \u2014 World Geographic Reference System. A grid-based system that divides the world into 15-degree quadrangles and then subdivides into smaller quadrangles. UTM \u2014 Based on the Universal Transverse Mercator projection. It divides the world into 6-degree slices of longitude, which are divided into 20 latitude bands. These grids are further described using eastings and northings to locate any point within the grid. USNG \u2014 United States National Grid. Almost exactly the same as MGRS but uses North American Datum 1983 (NAD83) as its datum. MGRS \u2014 Military Grid Reference System. Follows the UTM coordinates and divides the world into 6-degree longitude and 20 latitude bands, but MGRS then further subdivides the grid zones into smaller 100,000-meter grids. These 100,000-meter grids are then divided into 10,000-meter, 1,000-meter, 100-meter, 10-meter, and 1-meter grids.", "dataType": "String"}, {"name": "id_field", "isOptional": true, "description": " Any field from the input table. The selected field will be copied to the output table. If the values of this field are unique, this may be used to join the output records back to the input table. ", "dataType": "Field"}, {"name": "spatial_reference", "isOptional": true, "description": " Spatial reference of the output point feature class. The default is GCS_WGS_1984. If the output has a different coordinate system than the input, then the tool projects the data. If the input and output are in different datum, then a default transformation is calculated based on the coordinate systems of the input and the output, and the extent of the data. ", "dataType": "Spatial Reference"}]},
{"syntax": "XYToLine_management (in_table, out_featureclass, startx_field, starty_field, endx_field, endy_field, {line_type}, {id_field}, {spatial_reference})", "name": "XY To Line (Data Management)", "description": "Creates a new feature class containing geodetic line features constructed based on the values in a start x-coordinate field, start y-coordinate field, end x-coordinate field, and end y-coordinate field of a table.\r\n", "example": {"title": "XYToLine example (stand-alone script)", "description": "This sample converts a DBF table to two-point Geodesic lines.", "code": "# Import system modules import arcpy from arcpy import env # Set local variables input_table = r\"c:\\workspace\\city2city.dbf\" out_lines = r\"c:\\workspace\\flt4421.gdb\\routing001\" #XY To Line arcpy.XYToLine_management ( input_table , out_lines , \"LOND1\" , \"LATD1\" , \"LOND2\" , \"LATD2\" , \"GEODESIC\" , \"idnum\" )"}, "usage": ["If you use text files and ", ".csv", " (comma-separated value) files as input, make sure that they follow the file structure specified in  ", "About_tabular_data_sources", ".\r\n", "Each geodetic line is constructed using a particular set of field values representing the x and y coordinates of a starting point and the x and y coordinates of an ending point. These fields and values will be included in the output.", "A geodetic line is a curve on the surface of the earth. However, a geodetic line feature is not stored as a parametric (true) curve in the output, but as a densified polyline representing the path of the geodetic line. If the length of a geodetic line is relatively short, it may be represented by a straight line in the output. As the length of the line increases, more vertices are used to represent the path.", "When the output is a feature class in a file geodatabase or a personal geodatabase, the values in the Shape_Length field are always in the units of the output coordinate system specified by the ", "Spatial Reference", " parameter; and they are the planar lengths of the polylines. To measure a geodesic length or distance, use the ArcMap Measure tool; make sure to choose the Geodesic, Loxodrome, or Great Elliptic option accordingly before taking a measurement."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " The input table that can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. ", "dataType": "Table View"}, {"name": "out_featureclass", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing densified geodetic lines. ", "dataType": "Feature Class"}, {"name": "startx_field", "isOptional": false, "description": "A numerical field in the input table containing the x coordinates (or longitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. ", "dataType": "Field"}, {"name": "starty_field", "isOptional": false, "description": "A numerical field in the input table containing the y coordinates (or latitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. ", "dataType": "Field"}, {"name": "endx_field", "isOptional": false, "description": " A numerical field in the input table containing the x coordinates (or longitudes) of the ending points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. ", "dataType": "Field"}, {"name": "endy_field", "isOptional": false, "description": " A numerical field in the input table containing the y coordinates (or latitudes) of the ending points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. ", "dataType": "Field"}, {"name": "line_type", "isOptional": true, "description": " The type of geodetic line to construct. GEODESIC \u2014 A type of geodetic line which most accurately represents the shortest distance between any two points on the surface of the earth. The mathematical definition of the geodesic line is quite lengthy and complex and therefore omitted here. This line type is the default. GREAT_CIRCLE \u2014 A type of geodetic line which represents the path between any two points along the intersection of the surface of the earth and a plane that passes through the center of the earth. Depending on the output coordinate system specified by the Spatial Reference parameter, in a spheroid-based coordinate system, the line is a great elliptic; in a sphere-based coordinate system, the line is uniquely called a great circle\u2014a circle of the largest radius on the spherical surface. RHUMB_LINE \u2014 A type of geodetic line, also known as a loxodrome line, which represents a path between any two points on the surface of a spheroid defined by a constant azimuth from a pole. A rhumb line is shown as a straight line in the Mercator projection. NORMAL_SECTION \u2014 A type of geodetic line which represents a path between any two points on the surface of a spheroid defined by the intersection of the spheroid surface and a plane that passes through the two points and is normal (perpendicular) to the spheroid surface at the starting point of the two points. Therefore, the normal section line from point A to point B is different from the one from point B to point A.", "dataType": "String"}, {"name": "id_field", "isOptional": true, "description": " A field in the input table; this field and the values are included in the output and can be used to join the output features with the records in the input table. ", "dataType": "Field"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output feature class. You can specify the spatial reference in several ways: By entering the path to a .prj file, such as C:/workspace/watershed.prj . By referencing a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland . By defining a spatial reference object prior to using this tool, such as sr = arcpy.SpatialReference(\"C:/data/Africa/Carthage.prj\") , which you then use as the spatial reference parameter. ", "dataType": "Spatial Reference"}]},
{"syntax": "TableToEllipse_management (in_table, out_featureclass, x_field, y_field, major_field, minor_field, distance_units, {azimuth_field}, {azimuth_units}, {id_field}, {spatial_reference})", "name": "Table To Ellipse (Data Management)", "description": "\r\nCreates a new feature class containing geodetic ellipse features constructed based on the values in an x-coordinate field, y-coordinate field, major-axis field, minor-axis field, and azimuth field of a table.", "example": {"title": "TableToEllipse example (stand-alone script)", "description": "Creates polyline ellipse features from a table.", "code": "# Import system modules import arcpy from arcpy import env # Set local variables input_table = r\"c:\\workspace\\SGS\\eltop.gdb\\elret\" output_ellipse = r\"c:\\workspace\\SGS\\eltop.gdb\\Eplyln_001\" #Table To Ellipse arcpy.TableToEllipse_management ( input_table , output_ellipse , \"lond\" , \"latd\" , \"mjerr\" , \"mnerr\" , \"KILOMETERS\" , \"orient\" , \"DEGREES\" , \"LinkID\" )"}, "usage": ["If you use text files and ", ".csv", " (comma-separated value) files as input, make sure that they follow the file structure specified in  ", "About_tabular_data_sources", ".\r\n", "Each geodetic ellipse is constructed using a particular set of field values representing the x and y coordinates of a center point, major and minor axis lengths, and azimuth angle measured from North. These fields and values will be included in the output.", "A geodetic ellipse is a curve on the surface of the earth. However, a geodetic ellipse feature is not stored as a parametric (true) curve\r\n in the output, but rather a densified polyline representing the path of the geodetic ellipse.", "If you specify the same field for both Major Field and Minor Field, or if the values in both fields are equal, then the output features represent geodetic circles.", "When the output is a feature class in a file geodatabase or a personal geodatabase, the values in the Shape_Length field are always in the units of the output coordinate system specified by the ", "Spatial Reference", " parameter; and they are the planar lengths of the polylines. To measure a geodesic length or distance, use the ArcMap Measure tool; make sure to choose the Geodesic, Loxodrome, or Great Elliptic option accordingly before taking a measurement.", "When needed, you can use the ", "Feature To Polygon", " tool to convert the output polylines to polygons."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " The input table that can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. ", "dataType": "Table View"}, {"name": "out_featureclass", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing geodetic ellipses as densified polylines. ", "dataType": "Feature Class"}, {"name": "x_field", "isOptional": false, "description": " A numerical field in the input table containing the x coordinates (or longitudes) of the center points of ellipses to be positioned in the output coordinate system specified by the Spatial Reference parameter. ", "dataType": "Field"}, {"name": "y_field", "isOptional": false, "description": " A numerical field in the input table containing the y coordinates (or latitudes) of the center points of ellipses to be positioned in the output coordinate system specified by the Spatial Reference parameter. ", "dataType": "Field"}, {"name": "major_field", "isOptional": false, "description": " A numerical field in the input table containing major axis lengths of the ellipses. ", "dataType": "Field"}, {"name": "minor_field", "isOptional": false, "description": " A numerical field in the input table containing minor axis lengths of the ellipses. ", "dataType": "Field"}, {"name": "distance_units", "isOptional": false, "description": " The units for the values in Major Field and Minor Field. METERS \u2014 Values in meters; this is the default. KILOMETERS \u2014 Values in kilometers. MILES \u2014 Values in miles. NAUTICAL_MILES \u2014 Values in nautical miles. FEET \u2014 Values in International feet. US_SURVEY_FEET \u2014 Values in U.S. Survey feet.", "dataType": "String"}, {"name": "azimuth_field", "isOptional": true, "description": " A numerical field in the input table containing azimuth angle values for the major axis rotations of the output ellipses. The values are measured clockwise from North. ", "dataType": "Field"}, {"name": "azimuth_units", "isOptional": true, "description": "The units of the values in the Azimuth Field. DEGREES \u2014 Values in decimal degrees; this is the default. MILS \u2014 Values in mils. RADS \u2014 Values in radians. GRADS \u2014 Values in gradians.", "dataType": "String"}, {"name": "id_field", "isOptional": true, "description": " A field in the input table; this field and the values are included in the output and can be used to join the output features with the records in the input table. ", "dataType": "Field"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output feature class. You can specify the spatial reference in several ways: By entering the path to a .prj file, such as C:/workspace/watershed.prj . By referencing a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland . By defining a spatial reference object prior to using this tool, such as sr = arcpy.SpatialReference(\"C:/data/Africa/Carthage.prj\") , which you then use as the spatial reference parameter. ", "dataType": "Spatial Reference"}]},
{"syntax": "BearingDistanceToLine_management (in_table, out_featureclass, x_field, y_field, distance_field, {distance_units}, bearing_field, {bearing_units}, {line_type}, {id_field}, {spatial_reference})", "name": "Bearing Distance To Line (Data Management)", "description": "\r\nCreates a new feature class containing geodetic line features constructed based on the values in an x-coordinate field, y-coordinate field, bearing field, and distance field of a table.\r\n\r\n", "example": {"title": "BearingDistanceToLine example (stand-alone script)", "description": "Converts bearing and distance info into a line.", "code": "# Import system modules import arcpy from arcpy import env # Local variables input_table = r\"c:\\workspace\\LOBtraffic.dbf\" output_fc = r\"c:\\workspace\\SOPA.gdb\\lob_traf001\" #BearingDistanceToLine arcpy.BearingDistanceToLine_management ( input_table , output_fc , \"X\" , \"Y\" , \"NAUTICAL_MILES\" , \"azim\" , \"DEGREES\" , \"GEODESIC\" , \"recnum\" )"}, "usage": ["If you use text files and ", ".csv", " (comma-separated value) files as input, make sure that they follow the file structure specified in  ", "About_tabular_data_sources", ".\r\n", "Each geodetic line is constructed using a particular set of field values representing the x and y coordinates of a starting point, distance from the starting point, and bearing angle measured from North. These fields and values will be included in the output.", "A geodetic line is a curve on the surface of the earth. However, a geodetic line feature is not stored as a parametric (true) curve in the output, but as a densified polyline representing the path of the geodetic line. If the length of a geodetic line is relatively short, it may be represented by a straight line in the output. As the length of the line increases, more vertices are used to represent the path.", "When the output is a feature class in a file geodatabase or a personal geodatabase, the values in the Shape_Length field are always in the units of the output coordinate system specified by the ", "Spatial Reference", " parameter; and they are the planar lengths of the polylines. To measure a geodesic length or distance, use the ArcMap Measure tool; make sure to choose the Geodesic, Loxodrome, or Great Elliptic option accordingly before taking a measurement."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " The input table that can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. ", "dataType": "Table View"}, {"name": "out_featureclass", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing densified geodetic lines. ", "dataType": "Feature Class"}, {"name": "x_field", "isOptional": false, "description": "A numerical field in the input table containing the x coordinates (or longitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. ", "dataType": "Field"}, {"name": "y_field", "isOptional": false, "description": "A numerical field in the input table containing the y coordinates (or latitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. ", "dataType": "Field"}, {"name": "distance_field", "isOptional": false, "description": "A numerical field in the input table containing the distances from the starting points for creating the output lines. ", "dataType": "Field"}, {"name": "distance_units", "isOptional": true, "description": " The units for the values in the Distance Field. METERS \u2014 Values in meters; this is the default. KILOMETERS \u2014 Values in kilometers. MILES \u2014 Values in miles. NAUTICAL_MILES \u2014 Values in nautical miles. FEET \u2014 Values in International feet. US_SURVEY_FEET \u2014 Values in U.S. Survey feet.", "dataType": "String"}, {"name": "bearing_field", "isOptional": false, "description": " A numerical field in the input table containing bearing angle values for the output line rotation. The angles are measured clockwise from North. ", "dataType": "Field"}, {"name": "bearing_units", "isOptional": true, "description": " The units of the values in the Bearing Field. DEGREES \u2014 Values in decimal degrees; this is the default. MILS \u2014 Values in mils. RADS \u2014 Values in radians. GRADS \u2014 Values in gradians.", "dataType": "String"}, {"name": "line_type", "isOptional": true, "description": " The type of geodetic line to construct. GEODESIC \u2014 A type of geodetic line which most accurately represents the shortest distance between any two points on the surface of the earth. The mathematical definition of the geodesic line is quite lengthy and complex and therefore omitted here. This line type is the default. GREAT_CIRCLE \u2014 A type of geodetic line which represents the path between any two points along the intersection of the surface of the earth and a plane that passes through the center of the earth. Depending on the output coordinate system specified by the Spatial Reference parameter, in a spheroid-based coordinate system, the line is a great elliptic; in a sphere-based coordinate system, the line is uniquely called a great circle\u2014a circle of the largest radius on the spherical surface. RHUMB_LINE \u2014 A type of geodetic line, also known as a loxodrome line, which represents a path between any two points on the surface of a spheroid defined by a constant azimuth from a pole. A rhumb line is shown as a straight line in the Mercator projection. NORMAL_SECTION \u2014 A type of geodetic line which represents a path between any two points on the surface of a spheroid defined by the intersection of the spheroid surface and a plane that passes through the two points and is normal (perpendicular) to the spheroid surface at the starting point of the two points. Therefore, the normal section line from point A to point B is different from the one from point B to point A.", "dataType": "String"}, {"name": "id_field", "isOptional": true, "description": " A field in the input table; this field and the values are included in the output and can be used to join the output features with the records in the input table. ", "dataType": "Field"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output feature class. You can specify the spatial reference in several ways: By entering the path to a .prj file, such as C:/workspace/watershed.prj . By referencing a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland . By defining a spatial reference object prior to using this tool, such as sr = arcpy.SpatialReference(\"C:/data/Africa/Carthage.prj\") , which you then use as the spatial reference parameter. ", "dataType": "Spatial Reference"}]},
{"syntax": "RasterToDTED_management (in_raster, out_folder, dted_level, {resampling_type})", "name": "Raster To DTED (Data Management)", "description": " Splits a raster dataset into files based on the DTED tiling structure. ", "example": {"title": "RasterToDTED example 1 (Python window)", "description": "This is a Python sample for RasterToDTED.", "code": "import arcpy RasterToDTED_management ( \"C:/workspace/image1.img\" , \"C:/workspace/outputDTED\" , \"DTED_0\" , \"BILINEAR\" )"}, "usage": [" There are three levels of the DTED tiling scheme available: DTED level 0, DTED level 1, and DTED level 2.", "The input can only be a single band raster dataset.", "The output spatial reference will be GCS_WGS84. Each tile's extent is one degree in each direction, plus a half pixel on each edge so adjacent tiles have one column and row of overlap. The output pixel size is dictated by the DTED level, and the data is converted and stored as signed, 16-bit integers. ", "The DTED format is intended to be used with one band data that represents elevation, so this tool cannot be used for multiband images."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": " The input raster dataset. The input must be a single band raster dataset representing elevation. ", "dataType": "Raster Layer"}, {"name": "out_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder where the folder structure and DTED files will be created. ", "dataType": "Folder"}, {"name": "dted_level", "isOptional": false, "description": "The DTED level to use when creating your tiles. DTED_0 \u2014 DTED level 0 specification will be used to split the raster dataset. DTED_1 \u2014 DTED level 1 specification will be used to split the raster dataset. This is the default. DTED_2 \u2014 DTED level 2 specification will be used to split the raster dataset. ", "dataType": "String"}, {"name": "resampling_type", "isOptional": true, "description": "Choose the resampling method to use when creating the DTEDs. The default is bilinear interpolation resampling. NEAREST \u2014 Nearest neighbor assignment BILINEAR \u2014 Bilinear interpolation. This is the default. CUBIC \u2014 Cubic convolution", "dataType": "String"}]},
{"syntax": "GenerateExcludeArea_management (in_raster, out_raster, pixel_type, generate_method, {max_red}, {max_green}, {max_blue}, {max_white}, {max_black}, {max_magenta}, {max_cyan}, {max_yellow}, {percentage_low}, {percentage_high})", "name": "Generate Exclude Area (Data Management)", "description": " Allows you to set the exclude area, based on color mask or histogram percentage. The output of this tool is used within the Color Balance Mosaic Dataset tool.", "example": {"title": "GenerateExcludeArea example 1 (Python window)", "description": "This is a Python sample for GenerateExcludeArea.", "code": "import arcpy GenerateExcludeArea_management ( \"C:/workspace/fgdb.gdb/mosdata\" , \"C:/workspace/excludeArea.tif\" , \"8_BIT\" , \"COLOR_MASK\" , \"255\" , \"255\" , \"255\" , \"255\" , \"15\" , \"255\" , \"255\" , \"255\" , \"0\" , \"100\" )"}, "usage": ["This tool is used to exclude areas that will be difficult to color correct, such as water, clouds, and anomalous areas.", "The output of this tool is used in the ", "Color Balance Mosaic Dataset tool", " (but is not required by the tool) to exclude pixels (and colors) from the algorithm used to color correct the\r\nmosaic dataset.\r\n"], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": " The file path and file name of the input raster. Valid inputs include raster layers and mosaic datasets layers. ", "dataType": "Mosaic Dataset; Composite Layer; Raster Dataset; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": " The file path and file name of the input raster. The output will be a raster dataset. The output can then be used as the Exclude Area Raster parameter within the Color Balance Mosaic Dataset tool. ", "dataType": "Raster Dataset"}, {"name": "pixel_type", "isOptional": false, "description": " Choose the pixel depth of your input raster dataset. This parameter is important, since any pixel depth above 8-bit will need to have the color mask and histogram values adjusted. 8_BIT \u2014 Your input raster dataset has values from 0 to 255. This is the default. 11_BIT \u2014 Your input raster dataset has values from 0 to 2047. 12_BIT \u2014 Your input raster dataset has values from 0 to 4095. 16_BIT \u2014 Your input raster dataset has values from 0 to 65535.", "dataType": "String"}, {"name": "generate_method", "isOptional": false, "description": " Choose which method you want to use to exclude areas of your input. COLOR_MASK \u2014 Allows you to set the maximum color values to exclude in the output. This is the default. HISTOGRAM_PERCENTAGE \u2014 Allows you to set the minimum and maximum histogram percentage of pixels.", "dataType": "String"}, {"name": "max_red", "isOptional": true, "description": " This is the maximum red value to exclude. The default is 255. ", "dataType": "Double"}, {"name": "max_green", "isOptional": true, "description": " This is the maximum green value to exclude. The default is 255. ", "dataType": "Double"}, {"name": "max_blue", "isOptional": true, "description": " This is the maximum blue value to exclude. The default is 255. ", "dataType": "Double"}, {"name": "max_white", "isOptional": true, "description": " This is the maximum white value to exclude. The default is 255. ", "dataType": "Double"}, {"name": "max_black", "isOptional": true, "description": " This is the maximum black value to exclude. The default is 0. ", "dataType": "Double"}, {"name": "max_magenta", "isOptional": true, "description": " This is the maximum magenta value to exclude. The default is 255. ", "dataType": "Double"}, {"name": "max_cyan", "isOptional": true, "description": " This is the maximum cyan value to exclude. The default is 255. ", "dataType": "Double"}, {"name": "max_yellow", "isOptional": true, "description": " This is the maximum yellow value to exclude. The default is 255. ", "dataType": "Double"}, {"name": "percentage_low", "isOptional": true, "description": " This is the minimum percentage of the histogram to exclude. The default is 0. Define this value when there are extreme pixel values\u2014very low and high pixel values\u2014because they tend to be trouble areas for color correction. ", "dataType": "Double"}, {"name": "percentage_high", "isOptional": true, "description": "This is the maximum percentage of the histogram to exclude. The default is 100. Define this value when there are extreme pixel values\u2014very low and high pixel values\u2014because they tend to be trouble areas for color correction. ", "dataType": "Double"}]},
{"syntax": "ColorBalanceMosaicDataset_management (in_mosaic_dataset, {balancing_method}, {color_surface_type}, {target_raster}, {gamma}, {exclude_raster}, {stretch_type}, {block_field})", "name": "Color Balance Mosaic Dataset (Data Management)", "description": " Color balances a mosaic dataset so that the tiles appear seamless. ", "example": {"title": "ColorBalanceMosaicDataset example 1 (Python window)", "description": "This is a Python sample for ColorBalanceMosaicDataset.", "code": "import arcpy arcpy.ColorBalanceMosaicDataset_management ( \"C:/workspace/CC.gdb/cc1\" , \"DODGING\" , \"SINGLE_COLOR\" , \"C:/workspace/Aerial.lyr\" , \"#\" , \"STANDARD_DEVIATION\" , \"3\" , \"BLOCKNAME\" )"}, "usage": ["Color balancing can only take place if the following is true about your data:", "The ", "Pre-processing Options", " are performed before any color balancing takes place.", "There are three balancing algorithms available: Dodging, Histogram, and Standard deviation.", "The target color surface is only available if the dodging balancing technique is chosen. When using the dodging technique, each pixel needs a target color, which is picked up from the target color surface. There are five types of target color surfaces that you can choose from: single color, color grid, first order surface, second order surface, and third order surface.", "Target Raster", " allows you to specify the target to balance your rasters.", "To remove a color correction, right-click the mosaic dataset in ArcCatalog or the Catalog window and click ", "Remove ", ">", " Remove Color Balancing", "."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " Type or navigate to the mosaic dataset that you want to color balance. ", "dataType": "Mosaic Layer"}, {"name": "balancing_method", "isOptional": true, "description": " Choose the balancing algorithm to use. DODGING \u2014 This is the traditional dodging photogrammetric technique, where each of the pixel's values change toward the target color. These values are used to determine the output value for each pixel. If you choose Dodging, then you must also choose the type of target color surface to use, which will affect what the target color is. Dodging balancing tends to give the best result in most cases. HISTOGRAM \u2014 This technique will change each of the pixel's values according to the target histogram. The target histogram can be automatically calculated from all the rasters, or a target raster can be specified. Histogram balancing works well when all the rasters have a similar histogram shape. STANDARD DEVIATION \u2014 This technique will change each of the pixel's values according to the standard deviation calculation. The standard deviation value can be automatically calculated from all the rasters or from a specified target raster. Standard deviation balancing works best when all the rasters have the same histogram distribution of the normalized values.", "dataType": "String"}, {"name": "color_surface_type", "isOptional": true, "description": " The Target Color Surface Type is only available if the dodging balancing technique is chosen. When using the dodging technique, each pixel needs a target color, which is determined by which surface type is chosen. SINGLE_COLOR \u2014 All the pixels are dodged toward a single color point, which is the mean. A single color surface works well when there are only a small number of rasters and there are only a few different types of ground objects. If there are too many rasters or there are too many types of ground surfaces, then the output color may become blurred. COLOR_GRID \u2014 All the input pixels are dodged toward a multiple set of points, which are distributed all over the mosaic dataset. Color grid produces a good output for a large number of rasters, or areas with a large number of diverse ground objects. FIRST_ORDER \u2014 All input pixels are dodged toward many points, which are obtained from the two-dimensional polynomial slanted plane. Compared to the Color Grid surface, a polynomial order surface tends to be a smoother color change and uses less storage in the auxiliary table but tends to take longer to process. SECOND_ORDER \u2014 All input pixels are dodged toward a set of multiple points, which is obtained from the two-dimensional polynomial parabolic surface. Compared to the Color Grid surface, a polynomial order surface tends to be a smoother color change and uses less storage in the auxiliary table but tends to take longer to process. THIRD_ORDER \u2014 All input pixels are dodged toward multiple points, which are obtained from the cubic surface. Compared to the Color Grid surface, a polynomial order surface tends to be a smoother color change and uses less storage in the auxiliary table but tends to take longer to process.", "dataType": "String"}, {"name": "target_raster", "isOptional": true, "description": " The target raster image allows you to specify the target raster to balance your rasters. When dodging balancing is used, the target color that will be derived depends on the target color surface type that was chosen. For Single Color, the average value of the reference target image is used. For Color Grid, the reference target image is resampled to a suitable grid. For the polynomial order surfaces, the coefficients of the polynomial are obtained by the Least Squares Fitting from the reference target image. When Histogram Balancing is used, the target histogram is obtained from the reference target image. When Standard Deviation balancing is used, the target standard deviation is obtained from the reference target image. ", "dataType": "Raster Layer"}, {"name": "gamma", "isOptional": true, "description": " Type the gamma value to use in the pre-processing stretch. This is only available if the Standard Deviation or Minimum-Maximum prestretch type is chosen. By applying the gamma stretch, you can control the overall brightness of an image. If the gamma coefficient is set too low, the middle tones appear too dark; however, if the gamma coefficient is set too high, the middle tones appear too light. Gamma changes not only the brightness but also the ratios of red to green to blue. ", "dataType": "Double"}, {"name": "exclude_raster", "isOptional": true, "description": " Navigate to the raster you would like to use as an exclude layer. The Generate Exclude Area tool can be used to create the exclude area. The exclude area is taken into consideration before any color balancing takes place. ", "dataType": "Raster Layer"}, {"name": "stretch_type", "isOptional": true, "description": "Prestretching can be performed on each raster catalog item before any other color correction takes place. This means that the original raster catalog item will be using its stretched pixel values, rather than its raw pixel values, in the color correction process. You may want to use this option to change the color to an expected distribution before applying color correction. NONE \u2014 No prestretch will be applied. This is the default ADAPTIVE \u2014 An adaptive prestretch will be applied before any processing takes place. MINIMUM_MAXIMUM \u2014 A minimum-maximum prestretch will be applied before any processing takes place. STANDARD DEVIATION \u2014 A standard deviation prestretch will be applied before any processing takes place.", "dataType": "String"}, {"name": "block_field", "isOptional": true, "description": " The name of the field within a mosaic dataset's attribute table used to identify items that should be considered one item when performing some calculations and operations. ", "dataType": "String"}]},
{"syntax": "UpgradeGDB_management (input_workspace, input_prerequisite_check, input_upgradegdb_check)", "name": "Upgrade Geodatabase (Data Management)", "description": " Upgrades a geodatabase to the latest release to take advantage of new functionality available in the latest release of ArcGIS. Valid input is a personal or file geodatabase, an sde connection file, or connection information for an ArcSDE geodatabase.  You must have the current release of  ArcGIS for Desktop Standard ,  ArcGIS for Desktop Advanced ,  ArcGIS Engine Geodatabase Update , or  ArcGIS for Server Advanced  or  ArcGIS for Server Standard   installed on the computer from which you will run the upgrade. For enterprise geodatabases, a direct connection to the geodatabase is necessary to run the upgrade.", "example": {"title": "UpgradeGeodatabase example 1 (Python Window)", "description": "The following Python window script demonstrates how to use the UpgradeGDB function in immediate mode to upgrade a file geodatabase:", "code": "import arcpy Output_Workspace = \"C: \\\\ temp \\\\ Default.gdb\" Default_gdb = \"C: \\\\ temp \\\\ Default.gdb\" arcpy.UpgradeGDB_management ( Default_gdb , \"PREREQUISITE_CHECK\" , \"UPGRADE\" )"}, "usage": ["Before you upgrade your enterprise geodatabase, be sure you have read and performed the steps in the applicable topics:"], "parameters": [{"name": "input_workspace", "isInputFile": true, "isOptional": false, "description": " The personal, file, or ArcSDE geodatabase to be upgraded ", "dataType": "Workspace"}, {"name": "input_prerequisite_check", "isInputFile": true, "isOptional": false, "description": " Specifies whether the prerequisite check is run prior to upgrading the geodatabase NO_ PREREQUISITE_CHECK \u2014 Prerequisite check is not run. PREREQUISITE_CHECK \u2014 Prerequisite check is run. This is the default.", "dataType": "Boolean"}, {"name": "input_upgradegdb_check", "isInputFile": true, "isOptional": false, "description": " Specifies whether the upgrade is performed NO_UPGRADE \u2014 The upgrade is not run. UPGRADE \u2014 The upgrade is run. This is the default.", "dataType": "Boolean"}]},
{"syntax": "PackageMap_management (in_map, output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {arcgisruntime}, {reference_all_data}, {version}, {additional_files}, {summary}, {tags})", "name": "Package Map (Data Management)", "description": " Packages a map document and all referenced data sources to create a single compressed  .mpk  file. ", "example": {"title": "PackageMap example 1 (Python window)", "description": "The following Python script demonstrates how to use the Package Map tool from the Python window:", "code": "import arcpy arcpy.env.workspace = \"C:/arcgis/ArcTutor/Editing\" arcpy.PackageMap_management ( 'Exercise1.mxd' , 'EditingExercise1.mpk' , \"PRESERVE\" , \"CONVERT_ARCSDE\" , \"#\" , \"ALL\" )"}, "usage": [" A warning is issued when this tool encounters an unsupported layer type (a schematics or tool layer).   The unsupported layer will not be written to the output.", " The input layer must have a description in order for the tool to execute. To add a description, right-click the layer, click ", "Properties", ", and enter a description.", "To unpack a map package, drag the ", ".mpk", " file into ", "ArcMap", " or right-click the ", ".mpk", " file and click ", "Unpack", ".   Alternatively, you can use the ", "Extract Package", " tool and specify an output folder.", "   By default, packages will be extracted into your user profile. ", "To change the default location of where your packages will be unpacked, open ", "ArcMap Options", " from the ", "Customize", " menu.  From the ", "Sharing", " tab find the ", "Packaging", " section and check ", "Use user specified location", " and browse  to the new folder location. ", "When ", "Convert data to file geodatabase", " is checked", "When ", "Convert data to file geodatabase", " is unchecked", "For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. ", "For feature layers,   the ", "Extent", " parameter is used to select the features that will be consolidated. For raster layers, the ", "Extent", " parameter is used to clip the raster datasets.", "Some datasets reference other datasets.  For example, you may have a topology dataset that references four feature classes. Other examples of datasets that reference other datasets include Geometric Networks, Networks, and Locators.  When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.", "The ", "Schema only", " parameter, if checked, will only consolidate  or package the schema of the input data sources.  A schema is the structure or design of a  feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on.  Data or records will not be consolidated or packaged.    ", "   Data sources that do not support schema only will not be consolidated or packaged.    If the ", "Schema only", " parameter  is checked and the tool encounters a layer that is not supported for schema only, a warning message is displayed, and that layer will be skipped.  If the only layer specified is unsupported for schema only, the tool will fail. "], "parameters": [{"name": "in_map", "isInputFile": true, "isOptional": false, "description": " The map document to be packaged. ", "dataType": "ArcMap Document"}, {"name": "output_file", "isOutputFile": true, "isOptional": false, "description": " The output map package ( .mpk ). ", "dataType": "File"}, {"name": "convert_data", "isOptional": true, "description": "Specifies whether input layers will be converted into a file geodatabase or preserve their original format. This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). CONVERT \u2014 Data will be converted to a file geodatabase. Note: This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. PRESERVE \u2014 Data formats will be preserved when possible. This is the default. Note: The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). ", "dataType": "Boolean"}, {"name": "convert_arcsde_data", "isOptional": true, "description": "Specifies whether input enterprise geodatabase layers will be converted into a file geodatabase or preserve their original format. CONVERT_ARCSDE \u2014 Enterprise geodatabase data will be converted to a file geodatabase and included in the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. ", "dataType": "Boolean"}, {"name": "extent", "isOptional": true, "description": "Specify the extent by manually entering the coordinates in the extent parameter using the format X-Min Y-Min X-Max Y-Max . To use the extent of a specific layer, specify the layer name. MAXOF \u2014 Union of inputs MINOF \u2014 Intersection of inputs DISPLAY \u2014 Same extent as current display <Layer> \u2014 Same extent as specified layer", "dataType": "Extent"}, {"name": "apply_extent_to_arcsde", "isOptional": true, "description": "Determines whether specified extent will be applied to all layers or only to enterprise geodatabase layers. ALL \u2014 Specified extent is applied to all layers. This is the default. ARCSDE_ONLY \u2014 Specified extent is applied to enterprise geodatabase layers only. ", "dataType": "Boolean"}, {"name": "arcgisruntime", "isOptional": true, "description": "Specifies whether the package will support ArcGIS Runtime. To support ArcGIS Runtime, all data sources will be converted to a file geodatabase and a .msd will be created in the output package. DESKTOP \u2014 Output package will not support ArcGIS Runtime. Unless otherwise specified, data sources will not be converted to a file geodatabase and a .msd will not be created. RUNTIME \u2014 Output package will support ArcGIS Runtime. All data sources will be converted to a file geodatabase and a .msd will be created in the output package.", "dataType": "Boolean"}, {"name": "reference_all_data", "isOptional": true, "description": " Setting this option to REFERENCED will create a package that references the data needed rather than copying the data. This is valuable when trying to package large datasets that are available from a central location within an organization. REFERENCED \u2014 Creates a package that references the data rather than copying the data. NOT_REFERENCED \u2014 Creates a package that contains all needed data. This is the default.", "dataType": "Boolean"}, {"name": "version", "isOptional": false, "description": "Specifies the version of the geodatabases that will be created in the resulting package. Specifying a version allows packages to be shared with previous versions of ArcGIS and supports backward compatibility. ALL \u2014 Package will contain geodatabases and a map document compatible with all versions. (10.0 and higher) CURRENT \u2014 Package will contain geodatabases and a map document compatible with the version of the current release. 10.1 \u2014 Package will contain geodatabases and a map document compatible with version 10.1. 10 \u2014 Package will contain geodatabases and a map document compatible with version 10.0.", "dataType": "String"}, {"name": "additional_files", "isOptional": true, "description": "Adds additional files to a package. Additional files, such as .doc , .txt , .pdf , and so on, are used to provide more information about the contents and purpose of the package. ", "dataType": "File"}, {"name": "summary", "isOptional": true, "description": "Adds Summary information to the properties of the package. ", "dataType": "String"}, {"name": "tags", "isOptional": true, "description": "Adds Tag information to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. ", "dataType": "String"}]},
{"syntax": "PackageLayer_management (in_layer, output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {version}, {additional_files}, {summary}, {tags})", "name": "Package Layer (Data Management)", "description": " Packages one or more layers and all referenced data sources to create a single compressed  .lpk  file. ", "example": {"title": "PackageLayer example 1 (Python window)", "description": "The following Python script demonstrates how to use the PackageLayer tool from within the Python window.", "code": "import arcpy arcpy.env.workspace = \"C:/arcgis/ArcTutor/BuildingaGeodatabase/Layers\" arcpy.PackageLayer_management ( 'Parcels.lyr' , 'Parcel.lpk' , \"PRESERVE\" , \"CONVERT_ARCSDE\" , \"#\" , \"ALL\" , \"AlL\" , \"CURRENT\" , \"C:/readme.docx\" , \"Summary of package\" , \"parcel,montgomery\" )"}, "usage": [" A warning is issued when this tool encounters an unsupported layer type (a schematics or tool layer).  The unsupported layer will not be written to the output.", "All input layers must have a description in order for the tool to execute.  To add a description, right-click the layer, click ", "Properties", ", and enter a description. ", "When consolidating or packaging layers, the resulting layers will be renamed using the convention ", "<integer><layername>.lyr", " (for example, ", "0000roads.lyr", ").        This renaming is needed  to ensure all layers that reference unique data sources, and have the same layer name within ArcMap,  have unique layer names in the consolidated folder.", "  ", "Layer packages are backwards compatible with ArcGIS 10 and ArcGIS 9.3.1.  To create a layer package that is compatible with previous versions, use the ", "Package version", " parameter.  It is important to note that due to updates and enhanced functionality for some geodatabase elements, not all layer packages will be backwards compatible.", "When ", "Convert data to file geodatabase", " is checked", "When ", "Convert data to file geodatabase", " is unchecked", "For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. ", "For feature layers,   the ", "Extent", " parameter is used to select the features that will be consolidated. For raster layers, the ", "Extent", " parameter is used to clip the raster datasets.", "Some datasets reference other datasets.  For example, you may have a topology dataset that references four feature classes. Other examples of datasets that reference other datasets include Geometric Networks, Networks, and Locators.  When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.", "The ", "Schema only", " parameter, if checked, will only consolidate  or package the schema of the input data sources.  A schema is the structure or design of a  feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on.  Data or records will not be consolidated or packaged.    ", "   Data sources that do not support schema only will not be consolidated or packaged.    If the ", "Schema only", " parameter  is checked and the tool encounters a layer that is not supported for schema only, a warning message is displayed, and that layer will be skipped.  If the only layer specified is unsupported for schema only, the tool will fail. ", "Consolidating or packaging Coverage or VPF layers will copy the entire Coverage or VPF dataset into the consolidated folder or package.", "To unpack a layer package, either drag the ", ".lpk", " file into ArcMap or right-click the ", ".lpk", " file and select ", "Unpack", ".  Alternatively, you can use the ", "Extract_Package", "  tool and specify an output folder.  ", "By default,   ", "Unpack", " will always extract the package into your user profile under: ", "To change the default location of where your packages will be unpacked, open ", "ArcMap Options", " from the ", "Customize", " menu.  From the ", "Sharing", " tab find the ", "Packaging", " section and check ", "Use user specified location", " and browse  to the new folder location. "], "parameters": [{"name": "in_layer", "isInputFile": true, "isOptional": false, "description": " The layers to package. ", "dataType": "Layer"}, {"name": "output_file", "isOutputFile": true, "isOptional": false, "description": " The location and name of the output package file ( .lpk ) to create. ", "dataType": "File"}, {"name": "convert_data", "isOptional": true, "description": "Specifies whether input layers will be converted into a file geodatabase or preserve their original format. This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). CONVERT \u2014 Data will be converted to a file geodatabase. Note: This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. PRESERVE \u2014 Data formats will be preserved when possible. This is the default. Note: The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). ", "dataType": "Boolean"}, {"name": "convert_arcsde_data", "isOptional": true, "description": "Specifies whether input enterprise geodatabase layers will be converted into a file geodatabase or preserve their original format. CONVERT_ARCSDE \u2014 Enterprise geodatabase data will be converted to a file geodatabase and included in the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. ", "dataType": "Boolean"}, {"name": "extent", "isOptional": true, "description": "Specify the extent by manually entering the coordinates in the extent parameter using the format X-Min Y-Min X-Max Y-Max . To use the extent of a specific layer, specify the layer name. MAXOF \u2014 Union of inputs MINOF \u2014 Intersection of inputs DISPLAY \u2014 Same extent as current display <Layer> \u2014 Same extent as specified layer", "dataType": "Extent"}, {"name": "apply_extent_to_arcsde", "isOptional": true, "description": "Determines whether specified extent will be applied to all layers or only to enterprise geodatabase layers. ALL \u2014 Specified extent is applied to all layers. This is the default. ARCSDE_ONLY \u2014 Specified extent is applied to enterprise geodatabase layers only. ", "dataType": "Boolean"}, {"name": "schema_only", "isOptional": true, "description": "Specifies whether only the schema of the input layers will be consolidated or packaged. ALL \u2014 All features and records will be consolidated or packaged. This is the default. SCHEMA_ONLY \u2014 Only the Schema of input layers will be consolidated or packaged.", "dataType": "Boolean"}, {"name": "version", "isOptional": false, "description": "Specifies the version of the geodatabases that will be created in the resulting package. Specifying a version allows packages to be shared with previous versions and supports backward compatibility. ALL \u2014 Package will contain geodatabases and layer files compatible with all versions. (9.3.1 and higher) CURRENT \u2014 Package will contain geodatabases and layer files compatible with the version of the current release. 10.1 \u2014 Package will contain geodatabases and layer files compatible with version 10.1. 10 \u2014 Package will contain geodatabases and layer files compatible with version 10.0. 9.3.1 \u2014 Pacakge will contain geodatabases and layer files compatible with version 9.3.1.", "dataType": "String"}, {"name": "additional_files", "isOptional": true, "description": "Adds additional files to a package. Additional files, such as .doc , .txt , .pdf , and so on, are used to provide more information about the contents and purpose of the package. ", "dataType": "File"}, {"name": "summary", "isOptional": true, "description": "Adds Summary information to the properties of the package. ", "dataType": "String"}, {"name": "tags", "isOptional": true, "description": "Adds Tag information to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. ", "dataType": "String"}]},
{"syntax": "ConsolidateMap_management (in_map, output_folder, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde})", "name": "Consolidate Map (Data Management)", "description": " Consolidates a map document and all referenced data sources to a specified output folder.", "example": {"title": "ConsolidateMap example 1 (Python window)", "description": "The following Python script demonstrates how to use the ConsolidateMap tool from the Python window:", "code": "import arcpy arcpy.env.workspace = \"C:/arcgis/ArcTutor/Editing\" arcpy.ConsolidateMap_management ( 'Exercise1.mxd' , 'Consolidate_folder' , \"PRESERVE\" , \"CONVERT_ARCSDE\" , \"#\" , \"ALL\" )"}, "usage": [" A warning is issued when this tool encounters an unsupported layer type (a schematics or tool layer). The unsupported layer will not be written to the output.", "When ", "Convert data to file geodatabase", " is checked", "When ", "Convert data to file geodatabase", " is unchecked", "For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. ", "Some datasets reference other datasets.  For example, you may have a topology dataset that references four feature classes. Other examples of datasets that reference other datasets include Geometric Networks, Networks, and Locators.  When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.", "For feature layers,   the ", "Extent", " parameter is used to select the features that will be consolidated. For raster layers, the ", "Extent", " parameter is used to clip the raster datasets."], "parameters": [{"name": "in_map", "isInputFile": true, "isOptional": false, "description": " The map document ( .mxd ) to be consolidated. ", "dataType": "ArcMap Document"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder that will contain the consolidated map document and data. ", "dataType": "Folder"}, {"name": "convert_data", "isOptional": true, "description": "Specifies whether input layers will be converted into a file geodatabase or preserve their original format. This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). CONVERT \u2014 Data will be converted to a file geodatabase. Note: This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. PRESERVE \u2014 Data formats will be preserved when possible. This is the default. Note: The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). ", "dataType": "Boolean"}, {"name": "convert_arcsde_data", "isOptional": true, "description": "Specifies whether input enterprise geodatabase layers will be converted into a file geodatabase or preserve their original format. CONVERT_ARCSDE \u2014 Enterprise geodatabase data will be converted to a file geodatabase and included in the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. ", "dataType": "Boolean"}, {"name": "extent", "isOptional": true, "description": "Specify the extent by manually entering the coordinates in the extent parameter using the format X-Min Y-Min X-Max Y-Max . To use the extent of a specific layer, specify the layer name. MAXOF \u2014 Union of inputs MINOF \u2014 Intersection of inputs DISPLAY \u2014 Same extent as current display <Layer> \u2014 Same extent as specified layer", "dataType": "Extent"}, {"name": "apply_extent_to_arcsde", "isOptional": true, "description": "Determines whether specified extent will be applied to all layers or only to enterprise geodatabase layers. ALL \u2014 Specified extent is applied to all layers. This is the default. ARCSDE_ONLY \u2014 Specified extent is applied to enterprise geodatabase layers only. ", "dataType": "Boolean"}]},
{"syntax": "ConsolidateLayer_management (in_layer, output_folder, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only})", "name": "Consolidate Layer (Data Management)", "description": " Consolidates one or more layers by copying all referenced data sources  into a single folder. ", "example": {"title": "ConsolidateLayer example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ConsolidateLayer tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/arcgis/ArcTutor/BuildingaGeodatabase/Layers\" arcpy.ConsolidateLayer_management ( 'Parcels.lyr' , 'Consolidated_folder' , \"PRESERVE\" , \"CONVERT_ARCSDE\" , \"#\" , \"ALL\" , \"ALL\" )"}, "usage": [" A warning is issued when this tool encounters an unsupported layer type (a schematics or tool layer). The unsupported layer will not be written to the output.", "When consolidating or packaging layers, the resulting layers will be renamed using the convention ", "<integer><layername>.lyr", " (for example, ", "0000roads.lyr", ").        This renaming is needed  to ensure all layers that reference unique data sources, and have the same layer name within ArcMap,  have unique layer names in the consolidated folder.", "  ", "When ", "Convert data to file geodatabase", " is checked", "When ", "Convert data to file geodatabase", " is unchecked", "For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. ", "For feature layers,   the ", "Extent", " parameter is used to select the features that will be consolidated. For raster layers, the ", "Extent", " parameter is used to clip the raster datasets.", "Some datasets reference other datasets.  For example, you may have a topology dataset that references four feature classes. Other examples of datasets that reference other datasets include Geometric Networks, Networks, and Locators.  When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.", "The ", "Schema only", " parameter, if checked, will only consolidate  or package the schema of the input data sources.  A schema is the structure or design of a  feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on.  Data or records will not be consolidated or packaged.    ", "   Data sources that do not support schema only will not be consolidated or packaged.    If the ", "Schema only", " parameter  is checked and the tool encounters a layer that is not supported for schema only, a warning message is displayed, and that layer will be skipped.  If the only layer specified is unsupported for schema only, the tool will fail. ", "Consolidating or packaging Coverage or VPF layers will copy the entire Coverage or VPF dataset into the consolidated folder or package."], "parameters": [{"name": "in_layer", "isInputFile": true, "isOptional": false, "description": " The input layers to be consolidated. ", "dataType": "Layer"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder that will contain the layer files and consolidated data. ", "dataType": "Folder"}, {"name": "convert_data", "isOptional": true, "description": "Specifies whether input layers will be converted into a file geodatabase or preserve their original format. This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). CONVERT \u2014 Data will be converted to a file geodatabase. Note: This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. PRESERVE \u2014 Data formats will be preserved when possible. This is the default. Note: The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). ", "dataType": "Boolean"}, {"name": "convert_arcsde_data", "isOptional": true, "description": "Specifies whether input enterprise geodatabase layers will be converted into a file geodatabase or preserve their original format. CONVERT_ARCSDE \u2014 Enterprise geodatabase data will be converted to a file geodatabase and included in the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. ", "dataType": "Boolean"}, {"name": "extent", "isOptional": true, "description": "Specify the extent by manually entering the coordinates in the extent parameter using the format X-Min Y-Min X-Max Y-Max . To use the extent of a specific layer, specify the layer name. MAXOF \u2014 Union of inputs MINOF \u2014 Intersection of inputs DISPLAY \u2014 Same extent as current display <Layer> \u2014 Same extent as specified layer", "dataType": "Extent"}, {"name": "apply_extent_to_arcsde", "isOptional": true, "description": "Determines whether specified extent will be applied to all layers or only to enterprise geodatabase layers. ALL \u2014 Specified extent is applied to all layers. This is the default. ARCSDE_ONLY \u2014 Specified extent is applied to enterprise geodatabase layers only. ", "dataType": "Boolean"}, {"name": "schema_only", "isOptional": true, "description": "Specifies whether only the schema of the input layers will be consolidated or packaged. ALL \u2014 All features and records will be consolidated or packaged. This is the default. SCHEMA_ONLY \u2014 Only the Schema of input layers will be consolidated or packaged.", "dataType": "Boolean"}]},
{"syntax": "CreatePersonalGDB_management (out_folder_path, out_name, {out_version})", "name": "Create Personal GDB (Data Management)", "description": "Creates a personal geodatabase in a folder.", "example": {"title": "CreatePersonalGDB Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CreatePersonalGDB function in immediate mode.", "code": "import arcpy from arcpy import env arcpy.CreatePersonalGDB_management ( \"C:/output\" , \"pGDB.mdb\" )"}, "usage": [], "parameters": [{"name": "out_folder_path", "isOutputFile": true, "isOptional": false, "description": "Location (folder) where the personal geodatabase will be created. ", "dataType": "Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "Name of the geodatabase to be created. ", "dataType": "String"}, {"name": "out_version", "isOutputFile": true, "isOptional": true, "description": "The ArcGIS version for the geodatabase to be created. CURRENT \u2014 Creates a geodatabase compatible with the currently installed version of ArcGIS 10.0 \u2014 Creates a geodatabase compatible with ArcGIS version 10 9.3 \u2014 Creates a geodatabase compatible with ArcGIS version 9.3 9.2 \u2014 Creates a geodatabase compatible with ArcGIS version 9.2 9.1 \u2014 Creates a geodatabase compatible with ArcGIS version 9.1", "dataType": "String"}]},
{"syntax": "CreateFolder_management (out_folder_path, out_name)", "name": "Create Folder (Data Management)", "description": "Creates a folder in the specified location.", "example": {"title": "CreateFolder Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CreateFolder function in immediate mode.", "code": "import arcpy from arcpy import env arcpy.CreateFolder_management ( \"C:/output\" , \"folder1\" )"}, "usage": ["The output folder name must be unique."], "parameters": [{"name": "out_folder_path", "isOutputFile": true, "isOptional": false, "description": "The disk location where the folder is created. ", "dataType": "Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The folder to be created. ", "dataType": "String"}]},
{"syntax": "CreateFileGDB_management (out_folder_path, out_name, {out_version})", "name": "Create File GDB (Data Management)", "description": "Creates a file geodatabase in a folder.", "example": {"title": "CreateFileGDB Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CreateFileGDB function in immediate mode.", "code": "import arcpy from arcpy import env arcpy.CreateFileGDB_management ( \"C:/output\" , \"fGDB.gdb\" )"}, "usage": [], "parameters": [{"name": "out_folder_path", "isOutputFile": true, "isOptional": false, "description": "The location (folder) where the file geodatabase will be created. ", "dataType": "Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the file geodatabase to be created. ", "dataType": "String"}, {"name": "out_version", "isOutputFile": true, "isOptional": true, "description": " The ArcGIS version for the geodatabase to be created. CURRENT \u2014 Creates a geodatabase compatible with the currently installed version of ArcGIS 10.0 \u2014 Creates a geodatabase compatible with ArcGIS version 10 9.3 \u2014 Creates a geodatabase compatible with ArcGIS version 9.3 9.2 \u2014 Creates a geodatabase compatible with ArcGIS version 9.2", "dataType": "String"}]},
{"syntax": "CreateFeatureDataset_management (out_dataset_path, out_name, {spatial_reference})", "name": "Create Feature Dataset (Data Management)", "description": "Creates a feature dataset in the output location\u2014an existing ArcSDE, file geodatabase, or personal geodatabase.", "example": {"title": "CreateFeatureDataset Example 1 (Python Window)", "description": "The following Python Window script demonstrates how to use the CreateFeatureDataset function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CreateFileGDB_management ( \"C:/output\" , \"HabitatAnalysis.gdb\" ) arcpy.CreateFeatureDataset_management ( \"C:/output/HabitatAnalysis.gdb\" , \"analysisresults\" , \"C:/workspace/landuse.prj\" )"}, "usage": [], "parameters": [{"name": "out_dataset_path", "isOutputFile": true, "isOptional": false, "description": "The ArcSDE, file geodatabase, or personal geodatabase in which the output feature dataset will be created. ", "dataType": "Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the feature dataset to be created. ", "dataType": "String"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output feature dataset. You can specify the spatial reference in several ways: By entering the path to a .prj file, such as C:/workspace/watershed.prj . By referencing a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland . By defining a spatial reference object prior to using this tool, such as sr = arcpy.SpatialReference(\"C:/data/Africa/Carthage.prj\") , which you then use as the spatial reference parameter. ", "dataType": "Spatial Reference"}]},
{"syntax": "CreateArcSDEConnectionFile_management (out_folder_path, out_name, server, service, {database}, {account_authentication}, {username}, {password}, {save_username_password}, {version}, {save_version_info})", "name": "Create ArcSDE Connection File (Data Management)", "description": " Creates an ArcSDE connection file for use in connecting to ArcSDE geodatabases. ", "example": {"title": "CreateArcSDEConnectionFile Example (Python Window)", "description": "The following Python window script demonstrates how to use the CreateArcSDEConnectionFile function in immediate mode.", "code": "import arcpy arcpy.CreateArcSDEConnectionFile_management ( r'c:\\connectionFiles' , 'gpserver' , '5151' , '' , 'toolbox' , 'toolbox' , 'SAVE_USERNAME' , 'SDE.DEFAULT' , 'SAVE_VERSION' )"}, "usage": ["Although you can enter any file extension for the", " ArcSDE Connection File Name", ", you must use the standard file extension ", ".sde", " in order for it to be recognised correctly by ArcGIS.", "When valid connection information is entered the tool will connect to the ArcSDE server in order to populate the versions list with versions the connected user has permissions to connect to.", "Please see ", "A quick tour of connections to ArcSDE geodatabases", " for a more complete explanation of ArcSDE connection properties.", "This tool should only be used to create application server connections to your geodatabase. If you want to create direct connections then you should use the ", "Create Database Connection", " tool."], "parameters": [{"name": "out_folder_path", "isOutputFile": true, "isOptional": false, "description": " The folder path where the .sde file will be stored. ", "dataType": "Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": " The name of the ArcSDE Connection file. Use the .sde file extension. ", "dataType": "String"}, {"name": "server", "isOptional": false, "description": " The ArcSDE Server machine name. ", "dataType": "String"}, {"name": "service", "isOptional": false, "description": " The ArcSDE Service name or TCP port number. ", "dataType": "String"}, {"name": "database", "isOptional": true, "description": "For non-Oracle databases only. The DBMS database to connect to. ", "dataType": "String"}, {"name": "account_authentication", "isOptional": true, "description": " DATABASE_AUTH \u2014 Database Authentication. Uses an internal database username and password to connect to the DBMS. You aren't required to type your username and password to create a connection; however, if you don't, you will be prompted to enter them when a connection is established. OPERATING_SYSTEM_AUTH \u2014 Use Operating system authentication. You do not need to type in a username and password. The connection will be made with the username and password used to log in to the operating system. If the login used for the operating system is not a valid geodatabase login, the connection will fail. Also note if you are creating a connection to a geodatabase stored in Oracle, DB2, or Informix you have to use a direct connection to the database.", "dataType": "Boolean"}, {"name": "username", "isOptional": true, "description": " Database username to connect with using Database Authentication. ", "dataType": "String"}, {"name": "password", "isOptional": true, "description": " The database user password when using Database Authentication. ", "dataType": "Encrypted String"}, {"name": "save_username_password", "isOptional": true, "description": " SAVE_USERNAME \u2014 Save the username and password in the connection file. DO_NOT_SAVE_USERNAME \u2014 Do not save the username and password in the file. Every time you attempt to connect using the file you will be prompted for the username and password.", "dataType": "Boolean"}, {"name": "version", "isOptional": true, "description": " The Geodatabase version to connect to. ", "dataType": "String"}, {"name": "save_version_info", "isOptional": true, "description": " SAVE_VERSION \u2014 Save the version name in the connection file. DO_NOT_SAVE_VERSION \u2014 Do not save the version name in the connection file. Without the version name being saved with the file you will be prompted for the version to connect to each time you access the connection file.", "dataType": "Boolean"}]},
{"syntax": "CreateArcInfoWorkspace_management (out_folder_path, out_name)", "name": "Create ArcInfo Workspace (Data Management)", "description": "Creates a workspace with an INFO subdirectory.", "example": {"title": "CreateArcInfoWorkspace example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the CreateArcInfoWorkspace function in immediate mode.", "code": "import arcpy from arcpy import env arcpy.CreateArcInfoWorkspace_management ( \"C:/output\" , \"aiworkspace\" )"}, "usage": ["The workspace name must be unique."], "parameters": [{"name": "out_folder_path", "isOutputFile": true, "isOptional": false, "description": "Location where the ArcInfo workspace will be created. ", "dataType": "Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "Name of the ArcInfo workspace to be created. ", "dataType": "String"}]},
{"syntax": "UnregisterAsVersioned_management (in_dataset, {keep_edit}, {compress_default})", "name": "Unregister As Versioned (Data Management)", "description": "Unregisters an ArcSDE dataset as  versioned .", "example": {"title": "UnregisterAsVersioned example (stand-alone script)", "description": "\r\n The following stand-alone script demonstrates how to use the UnregisterAsVersioned tool to unregister a dataset as versioned.\r\n", "code": "# Name: UnregisterAsVersioned_Example.py # Description: Unregisters a dataset as versioned # Author: ESRI # Import system modules import arcpy # Set local variables datasetName = \"Database Connections/ninefour@gdb.sde/ninefour.GDB.ctgFuseFeature\" # Execute UnregisterAsVersioned arcpy.UnregisterAsVersioned_management ( datasetName , \"KEEP_EDIT\" , \"COMPRESS_DEFAULT\" )"}, "usage": ["Versioning tools only work with ArcSDE data.  File and personal ", "geodatabases", " do not support versioning. ", "Unregistering a  dataset as versioned without first compressing the geodatabase could lead to loss of edited data.", "Versions are not affected by changes occurring in other versions of the database."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "Name of the ArcSDE dataset to be unregistered as versioned. ", "dataType": "Table View; Feature Dataset"}, {"name": "keep_edit", "isOptional": true, "description": "Specifies whether edits made of the version should be maintained. KEEP_EDIT \u2014 If there are existing edits in the delta tables the tool will fail with an error message. Do not use this option if you intend to compress your edits from the Default version in the compress_default parameter. This is the default. NO_KEEP_EDIT \u2014 If there are existing edits in the delta tables the tool will allow deletion of these edits.", "dataType": "Boolean"}, {"name": "compress_default", "isOptional": true, "description": "Determines whether edits will be compressed and unused data will be removed. This option is ignored if the KEEP_EDIT keyword is used when specifying the keep_edit parameter. COMPRESS_DEFAULT \u2014 Edits in the Default version are compressed to the base table. This is the default. NO_COMPRESS_DEFAULT \u2014 Any edits remaining in the delta tables are Edits are not compressed.", "dataType": "Boolean"}]},
{"syntax": "RemoveFilesFromLasDataset_management (in_las_dataset, {in_files}, {in_surface_constraints})", "name": "Remove Files From LAS Dataset (Data Management)", "description": "Removes one or more LAS files and surface constraint features from a LAS dataset.", "example": {"title": "RemoveFilesFromLasDataset example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.RemoveFilesFromLasDataset_management ( \"test.lasd\" , \"LA_N; LA_S/LA_5S4E.las\" , \"boundary.shp; streams.shp\" )"}, "usage": ["When a folder is specified for removal, all LAS files that reside in the folder will be removed from the LAS dataset.", "Surface constraint features only have to be cited by their name. For example, ", "'boundary.shp'", " and ", "'sample.gdb/boundary'", " would just be referred to as ", "'boundary'", "."], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "in_files", "isInputFile": true, "isOptional": false, "description": "The LAS files or folders containing LAS files that will be removed from the LAS dataset. ", "dataType": "Folder; File"}, {"name": "in_surface_constraints", "isInputFile": true, "isOptional": false, "description": " The name of the surface constraint features that will be removed from the LAS dataset. ", "dataType": "String"}]},
{"syntax": "LasDatasetStatistics_management (in_las_dataset, calculation_type, {out_file}, {summary_level}, {delimiter}, {decimal_separator})", "name": "LAS Dataset Statistics (Data Management)", "description": "\r\nCalculates or updates statistics for a LAS dataset and generates an optional statistics report.\r\n", "example": {"title": "LasDatasetStatistics example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.LASDatasetStatistics_3d ( \"test.lasd\" , \"NO_FORCE\" , \"LAS_FILE\" , \"DECIMAL_POINT\" , \"SPACE\" , \"LAS_summary.txt\" )"}, "usage": ["Statistics calculate average point spacing for each LAS file by using a binning method that incrementally evaluates small areas of the file to determine an estimate.", "Statistics enable filtering options for a LAS  dataset layer to automatically display the available class codes and return values found in the referenced LAS files. The filtering options can be specified through the Layer Properties dialog box in ArcMap and ArcScene.", "\r\nThe following information about the LAS files will be stored in the LAS dataset and can be reviewed through the LAS Dataset Properties dialog box in ArcCatalog:", "The optional statistics report file provides an overview of LAS properties of all files in the LAS dataset or each individual LAS file in a text format that can be imported into any number of applications."], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "calculation_type", "isOptional": false, "description": " Specifies whether statistics will be calculated for all lidar files or only for those that do not have statistics: SKIP_EXISTING_STATS \u2014 LAS files with up-to-date statistics will be skipped, and statistics will only be calculated for newly added LAS files or ones that were updated since the initial calculation. This is the default. OVERWRITE_EXISTING_STATS \u2014 Statistics will be calculated for all LAS files, including ones that have up-to-date statistics. This is useful if the LAS files were modified in an external application that went undetected by ArcGIS.", "dataType": "Boolean"}, {"name": "out_file", "isOutputFile": true, "isOptional": true, "description": "The output text file that will contain the summary of the LAS dataset statistics. ", "dataType": "Text File"}, {"name": "summary_level", "isOptional": true, "description": " Specify the type of summary contained in the report. DATASET \u2014 The report will summarize statistics for the entire LAS dataset. This is the default. LAS_FILES \u2014 The report will summarize statistics for the LAS files referenced by the LAS dataset.", "dataType": "String"}, {"name": "delimiter", "isOptional": true, "description": "The field delimeter used in the text file. SPACE \u2014 A space will be used to delimit field values. This is the default. COMMA \u2014 A comma will be used to delimit field values. This option is not applicable if the decimal separator is also a comma.", "dataType": "String"}, {"name": "decimal_separator", "isOptional": true, "description": "The decimal character used in the text file to differentiate the integer of a number from its fractional part. DECIMAL_POINT \u2014 A point is used as the decimal character. This is the default. DECIMAL_COMMA \u2014 A comma is used as the decimal character.", "dataType": "String"}]},
{"syntax": "TruncateTable_management (in_table)", "name": "Truncate Table (Data Management)", "description": "\r\nRemoves all rows from a\r\ndatabase table or feature class using truncate procedures in the database.", "example": {"title": "TruncateTable example 1 (Python window)", "description": "The following Python window script demonstrates how to use the TruncateTable tool in immediate mode.", "code": "import arcpy arcpy.TruncateTable_management ( \"Database Connections/whistler.sde/function.junction.table\" )"}, "usage": ["If a selection is applied to a layer or table view, the selection will be ignored\u2014all records will be truncated.", "Versioned data is not supported as input. Data must be unregistered as versioned before the tool will execute successfully.", "Truncate commands do not utilize database transactions and are unrecoverable. This improves performance over row-by-row deletion. ", "For workflows where all rows are removed from a table or feature class and there is no need to back up the transactions, such as nightly reloading of data, it is recommended that this tool is used to perform the task."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " Input database table or feature class that will be truncated. ", "dataType": "Table View"}]},
{"syntax": "CreateLasDataset_management (input, out_las_dataset, {folder_recursion}, {in_surface_constraints}, {spatial_reference}, {compute_stats}, {relative_paths})", "name": "Create LAS Dataset (Data Management)", "description": "\r\nCreates a LAS dataset referencing one or more LAS files and optional surface constraint features.", "example": {"title": "CreateLasDataset example 1 (Python window)", "description": "The following sample demonstrates how to use this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CreateLasDataset_management ( \"folder_a; folder_b/5S4E.las\" , \"test.lasd\" , \"RECURSION\" , \"LA/boundary.shp <None> Softclip;\" \"LA/ridges.shp Elevation hardline\" , \"\" , \"COMPUTE_STATS\" , \"RELATIVE_PATHS\" )"}, "usage": ["The LAS dataset is designed for use with  airborne lidar data in the ", "*.las", "  format. Supported LAS file  versions are 1.0, 1.1, 1.2, and 1.3.", "Consider creating and managing the LAS dataset through the ", "ArcCatalog", " window for a more interactive experience. See ", "Creating a LAS dataset", " for more information. ", "Surface constraint features can be used to enforce feature-derived elevation values that represent surface characteristics in the LAS dataset.", "Each LAS file typically contains spatial reference information in its header which is read by  the LAS dataset. If this information is missing or incorrectly written, the LAS file can be properly georeferenced by creating an auxiliary *.prj file that shares its name, co-resides in its folder, and contains the string representation of the LAS file's coordinate system, similar to the *.prj file of a shapefile.", "LAS points can be classified into a number of categories that describe the material encountered by the lidar return, such as  ground, building, or water. The American Society for Photogrammetry and Remote Sensing (ASPRS) defined the following class codes for LAS file versions  1.1, 1.2, and 1.3:", "Classification Value ", "Classification Type", "0", "Never Classified", "1", "Unassigned", "2", "Ground", "3", "Low Vegetation", "4", "Medium Vegetation", "5", "High Vegetation", "6", "Building", "7", "Noise", "8", "Model Key", "9", "Water", "10", "Reserved for ASPRS Definition", "11", "Reserved for ASPRS Definition", "12", "Overlap", "13\u201331", "Reserved for ASPRS Definition", "While the LAS 1.0 specifications provide class codes ranging  from 0 to 255, it does not have a standardized classification scheme. Any class codes used in 1.0 files would typically be defined by the data vendor and provided through auxiliary information."], "parameters": [{"name": "input", "isOptional": false, "description": "The LAS files and folders containing LAS files that will be referenced by the LAS dataset. This information can be supplied as a string containing all the input data or a list of strings containing specific data elements (for example, \"lidar1.las; lidar2.las; folder1; folder2\" or [\"lidar1.las\", \"lidar2.las\", \"folder1\", \"folder2\"] ). Refer to the help topic Understanding tool syntax for more information on specifying lists for input. ", "dataType": "File; Folder"}, {"name": "out_las_dataset", "isOutputFile": true, "isOptional": false, "description": "The LAS dataset that will be created. ", "dataType": "LAS Dataset"}, {"name": "folder_recursion", "isOptional": true, "description": "Specifies whether lidar data residing in the subdirectories of an input folder would get added to the LAS dataset. NO_RECURSION \u2014 Only lidar files found in an input folder will be added to the LAS dataset. This is the default. RECURSION \u2014 All LAS files residing in the subdirectories of an input folder will be added to the LAS dataset.", "dataType": "Boolean"}, {"name": "in_surface_constraints", "isInputFile": true, "isOptional": false, "description": "The feature classes that will be referenced by the LAS dataset. Each feature will need the following properties defined: in_feature_class \u2014The feature class to be referenced by the LAS dataset. height_field \u2014The field that specifies the source of elevation values for the features. Any numeric field in the feature's attribute table can be used. If the feature supports z-values, the feature geometry can be read by selecting the Shape.Z option. If no height is desired, specify the keyword <None> to create Z-less features whose elevation would be interpolated from the surface. SF_type \u2014The surface feature type that defines how the feature geometry gets incorporated into the triangulation for the surface. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change. anchorpoints \u2014Elevation points that never get thinned away. This option is only available for single-point feature geometry. hardline or softline \u2014Breaklines that enforce a height value. hardclip or softclip \u2014Polygon dataset that defines the boundary of the LAS dataset. harderase or softerase \u2014Polygon dataset that defines holes in the LAS dataset. hardreplace or softreplace \u2014Polygon dataset that defines areas of constant height.", "dataType": "Value Table"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the LAS dataset. If no spatial reference is explicitly assigned, the LAS dataset will use the coordinate system of the first input LAS file. If the input files do not contain any spatial reference information and the Input Coordinate System is not set, then the LAS dataset's coordinate system will be listed as unknown. ", "dataType": "Coordinate System"}, {"name": "compute_stats", "isOptional": true, "description": " Specifies whether statistics should be computed for the LAS files referenced by the LAS dataset. The presence of statistics allows the LAS dataset layer's filtering and symbology options to only show LAS attribute values that exist in the LAS files. COMPUTE_STATS \u2014 Statistics will be computed. NO_COMPUTE_STATS \u2014 Statistics will not be computed. This is the default.", "dataType": "Boolean"}, {"name": "relative_paths", "isOptional": true, "description": "Specifies whether lidar files and surface constraint features will be referenced by the LAS dataset through relative or absolute paths. Using relative paths may be convenient for cases where the LAS dataset and its associated data will be relocated in the file system using the same relative location to one another. ABSOLUTE_PATHS \u2014 Absolute paths will be used for the data referenced by the LAS dataset. This is the default. RELATIVE_PATHS \u2014 Relative paths will be used for the data referenced by the LAS dataset.", "dataType": "Boolean"}]},
{"syntax": "AddFilesToLasDataset_management (in_las_dataset, {in_files}, {folder_recursion}, {in_surface_constraints})", "name": "Add Files To LAS Dataset (Data Management)", "description": "Adds references for one or more LAS files and  surface constraint features to a LAS dataset.", "example": {"title": "AddFilesToLasDataset example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.AddFilesToLasDataset_management ( \"test.lasd\" , [ \"LA_N\" , \"LA_S/LA_5S4E.las\" ], \"RECURSION\" , [ \"boundary.shp <None> Soft_Clip\" , \"breakline.shp Shape.Z Hard_Line\" ])"}, "usage": ["The LAS dataset is designed for use with  airborne lidar data in the ", "*.las", "  format. Supported LAS file  versions are 1.0, 1.1, 1.2, and 1.3.", "Consider creating and managing the LAS dataset through the ", "ArcCatalog", " window for a more interactive experience. See ", "Creating a LAS dataset", " for more information. ", "Surface constraint features can be used to enforce feature-derived elevation values that represent surface characteristics in the LAS dataset.", "Each LAS file typically contains spatial reference information in its header which is read by  the LAS dataset. If this information is missing or incorrectly written, the LAS file can be properly georeferenced by creating an auxiliary *.prj file that shares its name, co-resides in its folder, and contains the string representation of the LAS file's coordinate system, similar to the *.prj file of a shapefile.", "LAS points can be classified into a number of categories that describe the material encountered by the lidar return, such as  ground, building, or water. The American Society for Photogrammetry and Remote Sensing (ASPRS) defined the following class codes for LAS file versions  1.1, 1.2, and 1.3:", "Classification Value ", "Classification Type", "0", "Never Classified", "1", "Unassigned", "2", "Ground", "3", "Low Vegetation", "4", "Medium Vegetation", "5", "High Vegetation", "6", "Building", "7", "Noise", "8", "Model Key", "9", "Water", "10", "Reserved for ASPRS Definition", "11", "Reserved for ASPRS Definition", "12", "Overlap", "13\u201331", "Reserved for ASPRS Definition", "While the LAS 1.0 specifications provide class codes ranging  from 0 to 255, it does not have a standardized classification scheme. Any class codes used in 1.0 files would typically be defined by the data vendor and provided through auxiliary information."], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "in_files", "isInputFile": true, "isOptional": false, "description": " Input files can reference any combination of individual LAS files and folders containing LAS data. ", "dataType": "Folder; File"}, {"name": "folder_recursion", "isOptional": true, "description": "Specifies whether lidar data residing in the subdirectories of an input folder would get added to the LAS dataset. NO_RECURSION \u2014 Only lidar files found in an input folder will be added to the LAS dataset. This is the default. RECURSION \u2014 All LAS files residing in the subdirectories of an input folder will be added to the LAS dataset.", "dataType": "Boolean"}, {"name": "in_surface_constraints", "isInputFile": true, "isOptional": false, "description": "The feature classes that will be referenced by the LAS dataset. Each feature will need the following properties defined: in_feature_class \u2014The feature class to be referenced by the LAS dataset. height_field \u2014The field that specifies the source of elevation values for the features. Any numeric field in the feature's attribute table can be used. If the feature supports z-values, the feature geometry can be read by selecting the Shape.Z option. If no height is desired, specify the keyword <None> to create Z-less features whose elevation would be interpolated from the surface. SF_type \u2014The surface feature type that defines how the feature geometry gets incorporated into the triangulation for the surface. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change. anchorpoints \u2014Elevation points that never get thinned away. This option is only available for single-point feature geometry. hardline or softline \u2014Breaklines that enforce a height value. hardclip or softclip \u2014Polygon dataset that defines the boundary of the LAS dataset. harderase or softerase \u2014Polygon dataset that defines holes in the LAS dataset. hardreplace or softreplace \u2014Polygon dataset that defines areas of constant height.", "dataType": "Value Table"}]},
{"syntax": "SortCodedValueDomain_management (in_workspace, domain_name, sort_by, sort_order)", "name": "Sort Coded Value Domain (Data Management)", "description": "\r\nSorts the code or description of a coded value domain in either ascending or descending order.\r\n", "example": {"title": "Sort Coded Value Domain Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.SortCodedValueDomain_management ( \"montgomery.gdb\" , \"material\" , \"CODE\" , \"ASCENDING\" )"}, "usage": ["Only supports sorting coded value domains stored in version", "10.0", " and later geodatabases."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": " The geodatabase containing the domain to be sorted. Must be a version 10.0 geodatabase or later. ", "dataType": "Workspace"}, {"name": "domain_name", "isOptional": false, "description": "The name of the coded value domain to be sorted. ", "dataType": "String"}, {"name": "sort_by", "isOptional": false, "description": "Specifies whether the code or description will be used to sort the domain. CODE \u2014 Records are sorted based on the code value for the domain. DESCRIPTION \u2014 Records are sorted based on the description value for the domain.", "dataType": "String"}, {"name": "sort_order", "isOptional": false, "description": "Specifies the direction the records will be sorted. ASCENDING \u2014 Records are sorted from low value to high value. DESCENDING \u2014 Records are sorted from high value to low value.", "dataType": "String"}]},
{"syntax": "DeleteMosaicDataset_management (in_mosaic_dataset, {delete_overview_images}, {delete_item_cache})", "name": "Delete Mosaic Dataset (Data Management)", "description": "\r\nRemoves a mosaic dataset, its overviews, and its item cache from disk.", "example": {"title": "DeleteMosaicDataset example 1 (Python window)", "description": "This is a Python sample for the DeleteMosaicDataset tool.", "code": "import arcpy arcpy.DeleteMosaicDataset_management ( \"C:/Workspace/fileGDB.gdb/md2delete\" , \"DELETE_OVERVIEW_IMAGES\" , \"NO_DELETE_ITEM_CACHE\" )"}, "usage": ["\r\nThis tool is used to delete a mosaic dataset in its entirety, including any of the tables within the database and, optionally, any overviews or caches created with it.", "You may not want to delete the overviews or cache when these are used in other mosaic datasets. The cache is usually created when LAS data, LAS datasets, or terrains are used in the mosaic dataset."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The mosaic dataset that you want to delete from the geodatabase. ", "dataType": "Mosaic Dataset ; Mosaic Layer"}, {"name": "delete_overview_images", "isOptional": true, "description": "Choose whether to delete the overviews that are associated with the mosaic dataset. DELETE_OVERVIEW_IMAGES \u2014 The overviews associated with the mosaic dataset will be deleted. This is the default. NO_DELETE_OVERVIEW_IMAGES \u2014 The overviews will not be deleted.", "dataType": "Boolean"}, {"name": "delete_item_cache", "isOptional": true, "description": "Choose whether to delete the raster item cache that is associated with the mosaic dataset. DELETE_ITEM_CACHE \u2014 The cache associated with the mosaic dataset will be deleted. This is the default. NO_DELETE_ITEM_CACHE \u2014 The cache will not be deleted.", "dataType": "Boolean"}]},
{"syntax": "SetFlowDirection_management (in_geometric_network, flow_option)", "name": "Set Flow Direction (Data Management)", "description": "\r\nSets the flow direction for a geometric network based on either the digitized direction or the source/sink settings in the geometric network.\r\n", "example": {"title": "SetFlowDirection example (stand-alone script)", "description": "The following stand-alone Python script demonstrates how to use \r\nthe SetFlowDirection in a Python script to set flow direction for a geometric network.", "code": "# Import arcpy module import arcpy # Local variables: Water_Net = \"C:/testing/GeometricNetworks/Montgomery.gdb/Water/Water_Net\" # Process: Set Flow Direction arcpy.SetFlowDirection_management ( Water_Net , \"WITH_DIGITIZED_DIRECTION\" )"}, "usage": ["\r\nUnlike the ", "Utility Network Analysis", " toolbar, this tool allows you to set flow direction based on the digitized direction of network edges in \r\naddition to the sources/sinks defined in your network.", "\r\nFlow direction must be set within an edit session.  If one is not active when the tool is run, this tool will start an edit session.\r\n", "\r\nGeometric networks in enterprise geodatabases must be registered as versioned in order for this tool to be used.", "This  tool does not honor selections; flow is set on every edge in the geometric network.", "This  tool can only be run with Foreground processing, it cannot be run using Background processing.", "Unlike other datasets, such as topology or network datasets, geometric networks do not have an associated layer. Therefore, when using this tool in ArcMap, the geometric network must be selected from disk.  There is no option to select the geometric network from a layer drop-down list as input to the tool."], "parameters": [{"name": "in_geometric_network", "isInputFile": true, "isOptional": false, "description": " The geometric network for which flow will be set. ", "dataType": "Geometric Network"}, {"name": "flow_option", "isOptional": false, "description": "Indicates method by which flow direction will be established; there is no default value. WITH_DIGITIZED_DIRECTION \u2014 Establish flow direction along the digitized direction of edges. AGAINST_DIGITIZED_DIRECTION \u2014 Establish flow direction against the digitized direction of edges. WITH_SOURCES_SINKS \u2014 Establish flow direction using sources and sinks. RESET_FLOW_DIRECTION \u2014 Reset flow direction on all edges to be indeterminate.", "dataType": "String"}]},
{"syntax": "CreateDatabaseConnection_management (out_folder_path, out_name, database_platform, instance, {account_authentication}, {username}, {password}, {save_user_pass}, {database}, {schema}, {version_type}, {version}, {date})", "name": "Create Database Connection (Data Management)", "description": "\r\nCreates a connection file that can be used to connect to an enterprise database or ArcSDE geodatabase.", "example": {"title": "CreateDatabaseConnection example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CreateDatabaseConnection tool in immediate mode.", "code": "import arcpy arcpy.CreateDatabaseConnection_management ( \"Database Connections\" , \"utah.sde\" , \"SQL_SERVER\" , \"utah\" , \"garfield\" , \"DATABASE_AUTH\" , \"gdb\" , \"gdb\" , \"SAVE_USERNAME\" , \"#\" , \"TRANSACTIONAL\" , \"sde.DEFAULT\" )"}, "usage": ["When using the tool dialog box,  if valid connection information is entered the tool will connect to the database in order to determine if the database contains the geodatabase schema. If the schema is found, the server will automatically populate the ", "Schema", " parameter with the SDE schema (for Oracle user schema geodatabases only), the ", "Version Type", " parameter with the TRANSACTIONAL keyword, and the version name parameter with the Default version name. ", "If you want to prevent your connection information from being saved in the ", "Results", " window or stored in the geoprocessing history log files, you will need to disable ", "history logging", " and save the  connection file without saving the connection information you wish to hide. ", "All of the parameters in the ", "Geodatabase Connection Properties", " section of the tool dialog box are ignored when connecting to an enterprise database that is not a geodatabase."], "parameters": [{"name": "out_folder_path", "isOutputFile": true, "isOptional": false, "description": " The folder path where the .sde file will be stored. ", "dataType": "Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": " The name of the .sde file. The output file extension must end with .sde . ", "dataType": "String"}, {"name": "database_platform", "isOptional": false, "description": " The DBMS platform that will be connected to. Valid options are: SQL_SERVER \u2014 For connecting to Microsoft SQL Server ORACLE \u2014 For connecting to Oracle DB2 \u2014 For connecting to IBM DB2 on Linux, UNIX, or Windows DB2ZOS \u2014 For connecting to IBM DB2 on z/OS INFORMIX \u2014 For connecting to IBM Informix NETEZZA \u2014 For connecting to Netezza POSTGRESQL \u2014 For connecting to PostgreSQL", "dataType": "String"}, {"name": "instance", "isOptional": false, "description": " The server or instance to connect to. The value you choose from the Connection type drop-down list indicates the type of database to which you want to connect. The information you provide for this parameter will vary, depending the connection type you choose. See below for further information specific to each DBMS platform. db2\u2014The name of the cataloged DB2 database. db2zos\u2014The name of the cataloged DB2 database. informix\u2014The Open Database Connectivity (ODBC) data source name for the Informix database. oracle\u2014Either the TNS name or the Oracle Easy Connection string. netezza\u2014The ODBC data source name for the Netezza database. postgresql\u2014The name of the server where PostgreSQL is installed. sqlserver\u2014The name of the SQL Server instance.", "dataType": "String"}, {"name": "account_authentication", "isOptional": true, "description": " DB_AUTHENTICATION \u2014 Database Authentication. Uses an internal database user name and password to connect to the DBMS. You aren't required to type your user name and password to create a connection; however, if you don't, you will be prompted to enter them when a connection is established. OS_AUTHENTICATION \u2014 Use operating system authentication. You do not need to type in a user name and password. The connection will be made with the user name and password used to log in to the operating system. If the login used for the operating system is not a valid geodatabase login, the connection will fail. Also note, if you are creating a connection to a geodatabase stored in Oracle, DB2, or Informix, you must use a direct connection to the database.", "dataType": "Boolean"}, {"name": "username", "isOptional": true, "description": " The database user name to connect with using Database Authentication. ", "dataType": "String"}, {"name": "password", "isOptional": true, "description": " The database user password when using Database Authentication. ", "dataType": "Encrypted String"}, {"name": "save_user_pass", "isOptional": true, "description": " SAVE_USERNAME \u2014 Save the user name and password in the connection file. This is the default. DO_NOT_SAVE_USERNAME \u2014 Do not save the user name and password in the file. Every time you attempt to connect using the file, you will be prompted for the user name and password.", "dataType": "Boolean"}, {"name": "database", "isOptional": true, "description": " The name of the database that you will be connecting to. This parameter only applies to PostgreSQL and SQL Server platforms. ", "dataType": "String"}, {"name": "schema", "isOptional": true, "description": "The user schema geodatabase to connect to. This option only applies to Oracle databases that contain at least one user-schema geodatabase. The default value for this parameter is to use the Default version. ", "dataType": "String"}, {"name": "version_type", "isOptional": true, "description": "The type of version you wish to connect to. If TRANSACTIONAL or HISTORICAL is used, the date parameter will be ignored. If HISTORICAL is used and a name is not provided in the version_name parameter, the Default transactional version will be used. If POINT_IN_TIME is used and a date is not provided in the date parameter, the Default transactional version will be used. TRANSACTIONAL \u2014 Use to connect to a transactional version. HISTORICAL \u2014 Use to connect to an historical marker. POINT_IN_TIME \u2014 Use to connect to a specific point in time. If POINT_IN_TIME is used, the Version Name parameter will be ignored.", "dataType": "String"}, {"name": "version", "isOptional": true, "description": " The geodatabase transactional version or historical marker to connect to. The default option will use the Default version. ", "dataType": "String"}, {"name": "date", "isOptional": true, "description": "The value representing the date and time used to connect to the database. For working with archiving enabled data. Dates can be entered in the following formats: 6/9/2011 4:20:15 PM 6/9/2011 16:20:15 6/9/2011 4:20:15 PM 16:20:15 If a time is entered without a date, the default date of December 30, 1899, will be used. If a date is entered without a time, the default time of 12:00:00 AM will be used.", "dataType": "Date"}]},
{"syntax": "EnableEditorTracking_management (in_dataset, {creator_field}, {creation_date_field}, {last_editor_field}, {last_edit_date_field}, {add_fields}, {record_dates_in})", "name": "Enable Editor Tracking (Data Management)", "description": "Enables editor tracking for a feature class, table, mosaic dataset, or raster catalog.\r\n Learn more about editor tracking", "example": {"title": "EnableEditorTracking example (Python window)", "description": "The following Python window script demonstrates how to use the EnableEditorTracking tool in immediate mode.", "code": "import arcpy arcpy.EnableEditorTracking_management ( \"d:/RC.gdb/Buildings\" , \"Creator\" , \"Created\" , \"Editor\" , \"Edited\" , \"ADD_FIELDS\" , \"UTC\" )"}, "usage": ["This tool can add fields if they do not already exist. If fields do exist, they must be of the correct type. ", "Creator Field", " and ", "Editor Field", "  must be string fields, and ", "Creation Date Field", " and ", "Last Edit Date Field", "  must be date fields.", "Editor tracking applies to operations on existing datasets only. It\r\ndoes not apply to operations that create new datasets. For example,\r\nif you copy and paste a dataset to create a new one, tracking values\r\nwill not update in the new dataset.", "You can record\r\nthe dates of edits in either the database's time zone or in\r\nUTC (Coordinated Universal Time).", "If you're\r\nplanning on copying or replicating data across time zones, or editing through a feature service, use UTC. Since editors can\r\napply edits from potentially anywhere in the world, UTC works well because it ensures times are recorded in a universally accepted and consistent way.", "Configuring your\r\neditor tracking to use the database's time zone is only recommended\r\nif you are certain that all edits will be performed within the same\r\ntime zone. ", "You can run this tool on a dataset that already has editor tracking enabled, but only to enable tracking of additional information. For example, if a dataset is only tracking the creator and creation date, you can run this tool to add the tracking of the editor and last edited date. This tool cannot disable tracking on a field, cannot switch tracking from one field to another, nor can it switch between UTC and database time. If you need to perform any of these operations, disable editor tracking on the input dataset before you run this tool.", "You can enable editor tracking on several or all the datasets in a geodatabase by using this  tool in ModelBuilder.  The Catalog window allows you to enable tracking on more than one dataset at a time. See ", "Enabling editor tracking on multiple datasets", " for more information.", "Editor tracking works in 10.1 and later release\r\nclients only. ArcGIS 10.0 and 10.0 SP1 clients can access\r\ndatasets that have editor tracking enabled, however, any edits these\r\nclients make are not tracked. Making schema changes with these\r\nclients should be avoided, as this disables editor tracking on the\r\ndataset. ArcGIS 10.0 SP2 and subsequent 10.0 service pack clients\r\ncannot access datasets that have editor tracking\r\nenabled."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " The feature class, table, mosaic dataset, or raster catalog that will have editor tracking enabled. ", "dataType": "Dataset"}, {"name": "creator_field", "isOptional": true, "description": " The name of the field that will store the names of users who created features or records. If this field already exists, it must be a String field. ", "dataType": "String"}, {"name": "creation_date_field", "isOptional": true, "description": " The name of the field that will store the dates features or records were created. If this field already exists, it must be a Date field. ", "dataType": "String"}, {"name": "last_editor_field", "isOptional": true, "description": " The name of the field that will store the names of users who last edited features or records. If this field already exists, it must be a String field. ", "dataType": "String"}, {"name": "last_edit_date_field", "isOptional": true, "description": " The name of the field that will store the dates features or records were last edited. If this field already exists, it must be a Date field. ", "dataType": "String"}, {"name": "add_fields", "isOptional": true, "description": "Specifies whether to add fields if they don't already exist. NO_ADD_FIELDS \u2014 Fields will not be added. Fields specified must already exist. This is the default. ADD_FIELDS \u2014 Fields will be added if they don't already exist.", "dataType": "Boolean"}, {"name": "record_dates_in", "isOptional": true, "description": " The time created date and last edited date will be recorded in. The default is UTC (Coordinated Universal Time). UTC \u2014 Record dates in UTC (Coordinated Universal Time). This is the default. DATABASE_TIME \u2014 Record dates in the time zone in which the database is located.", "dataType": "String"}]},
{"syntax": "CreateSpatialType_management (input_database, sde_user_password, {tablespace_name}, {st_shape_library_path})", "name": "Create Spatial Type (Data Management)", "description": "\r\nThe Create Spatial Type tool adds the ST_Geometry SQL type, subtypes, and functions to an Oracle or PostgreSQL database. This allows you to use the ST_Geometry SQL type to store geometries in a database that does not contain a geodatabase.", "example": {"title": "CreateSpatialType example 1", "description": "Executes the tool on a Windows computer to create or upgrade the ST_Geometry type in an Oracle database named db_ora using an existing connection file. The password for the sde user is specified, a tablespace named sdetbsp is created, and the location of the st_shape file on the Oracle server is specified.", "code": "##Import Arcpy module import arcpy arcpy.CreateSpatialType ( \"C:\\Documents and Settings \\a dministrator\\Application Data\\ESRI\\ArcCatalog\\connection_to_db_ora.sde\" , \"ed$pwd\" , \"sdetbsp\" , \"//ora/userfiles\" )"}, "usage": [], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " The input_database is the database connection file (.sde) that connects to the Oracle or PostgreSQL database. You must connect as a database administrator user; in Oracle, you must connect as the sys user. ", "dataType": "Workspace"}, {"name": "sde_user_password", "isOptional": false, "description": " The password for the sde database user. If the sde user does not exist in the database, it will be created and will use the password provided. The password policy of the underlying database will be enforced. If the sde user already exists in the database or database cluster, this password must match the existing password. ", "dataType": "Encrypted String"}, {"name": "tablespace_name", "isOptional": true, "description": " For Oracle, you can provide the name for a tablespace to be set as the default tablespace for the sde user. If the tablespace does not already exist, it will be created in the Oracle default storage location. If a tablespace with the specified name already exists, it will be set as the sde user's default. ", "dataType": "String"}, {"name": "st_shape_library_path", "isOptional": true, "description": " For Oracle, provide the location on the Oracle server where you placed the st_shape library. ", "dataType": "File"}]},
{"syntax": "DisableEditorTracking_management (in_dataset, {creator}, {creation_date}, {last_editor}, {last_edit_date})", "name": "Disable Editor Tracking (Data Management)", "description": "\r\nDisables editor tracking on a feature class, table, mosaic dataset, or raster catalog.\r\n\r\n", "example": {"title": "DisableEditorTracking example (stand-alone script)", "description": "The following stand-alone script demonstrates how to disable editor tracking on a feature class.", "code": "# Name: DisableEditorTracking.py # Description: Disables editor tracking on a feature class. # Import arcpy module import arcpy # Local variables: Buildings = \"d: \\\\ RC.gdb \\\\ RC \\\\ Buildings\" # Process: Disable Editor Tracking arcpy.DisableEditorTracking_management ( Buildings , \"DISABLE_CREATOR\" , \"DISABLE_CREATION_DATE\" , \"DISABLE_LAST_EDITOR\" , \"DISABLE_LAST_EDIT_DATE\" )"}, "usage": ["When editor tracking is disabled on a field, editor tracking information will no longer be recorded in these fields when edits are made.  You can reenable editor tracking with the ", "Enable Editor Tracking", " tool. ", "This tool operates on feature classes, tables, mosaic datasets, and\r\nraster catalogs in a 10.0 or later release geodatabase."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " The feature class, table, mosaic dataset, or raster catalog that will have editor tracking disabled. ", "dataType": "Dataset"}, {"name": "creator", "isOptional": true, "description": "Indicates whether to disable editor tracking for the creator field. DISABLE_CREATOR \u2014 Editor tracking for the creator field will be disabled. This is the default. NO_DISABLE_CREATOR \u2014 Editor tracking for the creator field will not be disabled.", "dataType": "Boolean"}, {"name": "creation_date", "isOptional": true, "description": "Indicates whether to disable editor tracking for the creation date field. DISABLE_CREATION_DATE \u2014 Editor tracking for the creation date field will be disabled. This is the default. NO_DISABLE_CREATION_DATE \u2014 Editor tracking for the creation date field will not be disabled.", "dataType": "Boolean"}, {"name": "last_editor", "isOptional": true, "description": "Indicates whether to disable editor tracking for the last editor field. DISABLE_LAST_EDITOR \u2014 Editor tracking for the last editor field will be disabled. This is the default. NO_DISABLE_LAST_EDITOR \u2014 Editor tracking for the last editor field will not be disabled.", "dataType": "Boolean"}, {"name": "last_edit_date", "isOptional": true, "description": "Indicates whether to disable editor tracking for the last edit date field. DISABLE_LAST_EDIT_DATE \u2014 Editor tracking for the last edit date field will be disabled. This is the default. NO_DISABLE_LAST_EDIT_DATE \u2014 Editor tracking for the last edit date field will not be disabled.", "dataType": "Boolean"}]},
{"syntax": "ConsolidateResult_management (in_result, output_folder, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only})", "name": "Consolidate Result (Data Management)", "description": "\r\nConsolidates one or more geoprocessing results into a specified output folder. \r\nIf  the specified folder does not exist, a new folder will be created. ", "example": {"title": "ConsolidateResult example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ConsolidateResult tool in the Python window.", "code": "import arcpy arcpy.ConsolidateResult_management ( r'C:\\ResultFiles\\BufferPoints.rlt' , r'C:\\project\\Buffer_Pnts' , \"PRESERVE\" , \"CONVERT_ARCSDE\" , \"#\" , \"ALL\" , \"ALL\" )"}, "usage": ["When a  tool is executed, information about the execution is written as a result in the \r\n\r\n", "Results", " window.  Results can be added as input directly from the ", "Results", " window using drag and drop.  Alternatively, results that have been saved as a result file ", "(.rlt)", " can  be added as input.", "Learn more about working with results", "When ", "Convert data to file geodatabase", " is checked", "When ", "Convert data to file geodatabase", " is unchecked", "For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. ", "For feature layers,   the ", "Extent", " parameter is used to select the features that will be consolidated. For raster layers, the ", "Extent", " parameter is used to clip the raster datasets.", "Some datasets reference other datasets.  For example, you may have a topology dataset that references four feature classes. Other examples of datasets that reference other datasets include Geometric Networks, Networks, and Locators.  When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.", "The ", "Schema only", " parameter, if checked, will only consolidate  or package the schema of the input and output data sources.  A schema is the structure or design of a  feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on.  Data or records will not be consolidated or packaged.    ", "   Data sources that do not support schema only will not be consolidated or packaged.    If the ", "Schema only", " parameter  is checked and the tool encounters a layer that is not supported for schema only, a warning message is displayed, and that layer will be skipped.  If the only layer specified is unsupported for schema only, the tool will fail. "], "parameters": [{"name": "in_result", "isInputFile": true, "isOptional": false, "description": " The geoprocessing result to be consolidated. Results that are added as input can either be a result file (.rlt) or a result from the Results window. ", "dataType": "File; String"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder that will contain the consolidated tools and data. ", "dataType": "Folder"}, {"name": "convert_data", "isOptional": true, "description": "Specifies whether input layers will be converted into a file geodatabase or preserve their original format. This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). CONVERT \u2014 Data will be converted to a file geodatabase. Note: This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. PRESERVE \u2014 Data formats will be preserved when possible. This is the default. Note: The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). ", "dataType": "Boolean"}, {"name": "convert_arcsde_data", "isOptional": true, "description": "Specifies whether input enterprise geodatabase layers will be converted into a file geodatabase or preserve their original format. CONVERT_ARCSDE \u2014 Enterprise geodatabase data will be converted to a file geodatabase and included in the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. ", "dataType": "Boolean"}, {"name": "extent", "isOptional": true, "description": "Specify the extent by manually entering the coordinates in the extent parameter using the format X-Min Y-Min X-Max Y-Max . To use the extent of a specific layer, specify the layer name. MAXOF \u2014 Union of inputs MINOF \u2014 Intersection of inputs DISPLAY \u2014 Same extent as current display <Layer> \u2014 Same extent as specified layer", "dataType": "Extent"}, {"name": "apply_extent_to_arcsde", "isOptional": true, "description": "Determines whether specified extent will be applied to all layers or only to enterprise geodatabase layers. ALL \u2014 Specified extent is applied to all layers. This is the default. ARCSDE_ONLY \u2014 Specified extent is applied to enterprise geodatabase layers only. ", "dataType": "Boolean"}, {"name": "schema_only", "isOptional": true, "description": "Specifies whether only the schema of input and output datasets will be consolidated or packaged. ALL \u2014 All records for input and output datasets will be consolidated or packaged. This is the default. SCHEMA_ONLY \u2014 Only the Schema of input and output datasets will be consolidated pr packaged.", "dataType": "Boolean"}]},
{"syntax": "PackageResult_management (in_result, output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {arcgisruntime}, {additional_files}, {summary}, {tags})", "name": "Package Result (Data Management)", "description": "Packages one or more geoprocessing results, including all tools and\r\ninput and output datasets, into a single compressed file  (.gpk) .\r\n", "example": {"title": "PackageResult example 1 (Python window)", "description": "The following Python script demonstrates how to use the PackageResult tool from within the Python window:", "code": "import arcpy arcpy.env.workspace = \"C:/ResultFiles\" arcpy.PackageResult_management ( 'Parcel.rlt' , 'Parcel.gpk' , \"PRESERVE\" , \"CONVERT_ARCSDE\" , \"#\" , \"ALL\" , \"ALL\" , \"DESKTOP\" , r\"C:\\docs\\readme.txt\" , \"Summary text\" , \"Tag1; tag2; tag3\" )"}, "usage": ["When a  tool is executed, information about the execution is written as a result in the \r\n\r\n", "Results", " window.  Results can be added as input directly from the ", "Results", " window using drag and drop, or results that have been saved as a result file ", "(.rlt)", " can also be added as input.", "Learn more about working with results", "When ", "Support ArcGIS Runtime", " the geoprocessing package that is created can be used in the ArcGIS Runtime environment.  To support the runtime environment", "When ", "Convert data to file geodatabase", " is checked", "When ", "Convert data to file geodatabase", " is unchecked", "For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. ", "For feature layers,   the ", "Extent", " parameter is used to select the features that will be consolidated. For raster layers, the ", "Extent", " parameter is used to clip the raster datasets.", "Some datasets reference other datasets.  For example, you may have a topology dataset that references four feature classes. Other examples of datasets that reference other datasets include Geometric Networks, Networks, and Locators.  When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.", "The ", "Schema only", " parameter, if checked, will only consolidate  or package the schema of the input and output data sources.  A schema is the structure or design of a  feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on.  Data or records will not be consolidated or packaged.    ", "   Data sources that do not support schema only will not be consolidated or packaged.    If the ", "Schema only", " parameter  is checked and the tool encounters a layer that is not supported for schema only, a warning message is displayed, and that layer will be skipped.  If the only layer specified is unsupported for schema only, the tool will fail. ", "You cannot create a package from a failed result.  You can, however, use the ", "Consolidate Result", " tool to create a folder containing all data and tools used by the failed result.", "To unpack a geoprocessing package, drag the ", ".gpk", " file into ", "ArcMap", " or right-click the ", ".gpk", " file and click ", "Unpack", ".   Alternatively, you can use the ", "Extract Package", " tool and specify an output folder.", "   By default, packages will be extracted into your user profile. ", "To change the default location of where your packages will be unpacked, open ", "ArcMap Options", " from the ", "Customize", " menu.  From the ", "Sharing", " tab find the ", "Packaging", " section and check ", "Use user specified location", " and browse  to the new folder location. ", "You cannot create a package from a failed result .  You can, however, use the ", "Consolidate Result", " tool to create a folder containing all data and tools used by the failed result."], "parameters": [{"name": "in_result", "isInputFile": true, "isOptional": false, "description": " The result that will be packaged. The input can be either a result added by dragging and dropping directly from the Results window or by browsing to a result file (.rlt) . ", "dataType": "File; String"}, {"name": "output_file", "isOutputFile": true, "isOptional": false, "description": " The name and location of the output package file (.gpk) . ", "dataType": "File"}, {"name": "convert_data", "isOptional": true, "description": "Specifies whether input layers will be converted into a file geodatabase or preserve their original format. This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). CONVERT \u2014 Data will be converted to a file geodatabase. Note: This parameter does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set convert_arcsde_data to CONVERT_ARCSDE. PRESERVE \u2014 Data formats will be preserved when possible. This is the default. Note: The exception to this rule are formats that are not supported in a 64x environment (personal geodatabase (.mdb) data, VPF data, and tables based on Excel spreadsheets or OLEDB connections) and raster formats that ArcGIS cannot write out natively (ADRG, CADRG/ECRG, CIB, and RPF). ", "dataType": "Boolean"}, {"name": "convert_arcsde_data", "isOptional": true, "description": "Specifies whether input enterprise geodatabase layers will be converted into a file geodatabase or preserve their original format. CONVERT_ARCSDE \u2014 Enterprise geodatabase data will be converted to a file geodatabase and included in the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. ", "dataType": "Boolean"}, {"name": "extent", "isOptional": true, "description": "Specify the extent by manually entering the coordinates in the extent parameter using the format X-Min Y-Min X-Max Y-Max . To use the extent of a specific layer, specify the layer name. MAXOF \u2014 Union of inputs MINOF \u2014 Intersection of inputs DISPLAY \u2014 Same extent as current display <Layer> \u2014 Same extent as specified layer", "dataType": "Extent"}, {"name": "apply_extent_to_arcsde", "isOptional": true, "description": "Determines whether specified extent will be applied to all layers or only to enterprise geodatabase layers. ALL \u2014 Specified extent is applied to all layers. This is the default. ARCSDE_ONLY \u2014 Specified extent is applied to enterprise geodatabase layers only. ", "dataType": "Boolean"}, {"name": "schema_only", "isOptional": true, "description": "Specifies whether only the schema of input and output datasets will be consolidated or packaged. ALL \u2014 All records for input and output datasets will be consolidated or packaged. This is the default. SCHEMA_ONLY \u2014 Only the schema of input and output datasets will be consolidated or packaged.", "dataType": "Boolean"}, {"name": "arcgisruntime", "isOptional": true, "description": "Specifies whether the package will support ArcGIS Runtime. To support ArcGIS Runtime, all data sources will be converted to a file geodatabase and a .msd will be created in the package. DESKTOP \u2014 Output package will not support ArcGIS Runtime. RUNTIME \u2014 Output package will support ArcGIS Runtime. ", "dataType": "Boolean"}, {"name": "additional_files", "isOptional": true, "description": "Adds additional files to a package. Additional files, such as .doc , .txt , .pdf , and so on, are used to provide more information about the contents and purpose of the package. ", "dataType": "File"}, {"name": "summary", "isOptional": true, "description": "Adds Summary information to the properties of the package. ", "dataType": "String"}, {"name": "tags", "isOptional": true, "description": "Adds Tag information to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. ", "dataType": "String"}]},
{"syntax": "CreateVersionedView_management (in_dataset, {in_name})", "name": "Create Versioned View (Data Management)", "description": "\r\nCreates a versioned view on a table or feature class.\r\n", "example": {"title": null, "description": null, "code": "arcpy.CreateVersionedView_management ( \"Database Connections \\\\ admin.sde \\\\ bender.GDB.cities\" , \"cities_MV_view\" )"}, "usage": ["\r\nThe table or feature class must be registered as versioned. \r\n"], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " Input table or feature class for which a versioned view will be created. ", "dataType": "Table View"}, {"name": "in_name", "isInputFile": true, "isOptional": true, "description": " Name for the versioned view that is created. If nothing is specified the output versioned view name is the name of the table or feature class with _vw appended to the end. ", "dataType": "String"}]},
{"syntax": "MatchPhotosToRowsByTime_management (Input_Folder, Input_Table, Time_Field, Output_Table, {Unmatched_Photos_Table}, {Add_Photos_As_Attachments}, {Time_Tolerance}, {Clock_Offset})", "name": "Match Photos To Rows By Time (Data Management)", "description": "\r\nMatches photo files to table or feature class rows according to the photo and row time stamps. The row with the time stamp closest to the capture time of a photo will be matched to that photo. Creates a new table containing the ObjectIDs from the input rows and their matching photo paths. Optionally adds matching photo files to the rows of the input table as geodatabase attachments.", "example": {"title": "MatchPhotosToRowsByTime example (Python window)", "description": "The following Python window snippet demonstrates how to use the MatchPhotosToRowsByTime tool.", "code": "import arcpy arcpy.MatchPhotosToRowsByTime_management ( \"c:/data/photos\" , \"c:/data/city.gdb/gps_points\" , \"DateTime\" , \"c:/data/city.gdb/output_table\" , \"\" , \"ADD_ATTACHMENTS\" , \"\" , 20 )"}, "usage": ["\r\nThis tool can be used to match GPS-captured features to digital photographs taken at the same time the GPS feature was captured.\r\n", "The output table will contain four attribute fields:", "While shapefile and dBASE data is supported for the input table, it is recommended to use geodatabase data, because a Date field in a shapefile or dBASE table cannot store both date and time information.", " ", "Learn more about shapefile limitations", "Since a single input row may have a time stamp that matches the time stamp of multiple photographs, the output table may have multiple rows that have the same ", "IN_FID", " (each row in the output refers to a match between one photo and one input row).", "The output table can be joined to the input table using the  output ", "IN_FID", " field and the OBJECTID of the input. In the case where the output table has multiple rows with the same ", "IN_FID", " (one input row matches multiple photos), use a relate or a relationship class to link the output to the input.", " ", "Learn more about joining and relating tables", "The time field must be type ", "Date", ". To convert your text or numeric fields to a ", "Date", " field, use the ", "Convert Time Field", " tool.", "Even if a GPS point and a digital photograph are captured at exactly the same time, the times recorded by the devices may be in different time zones. For example, GPS devices often record times in Coordinated Universal Time (UTC), or Greenwich Mean Time (GMT), while digital cameras often record times in a local time zone. To reconcile time-stamp differences resulting from different time zones, use the ", "Convert Time Zone", " tool to change the time field of the input table to match the time zone of the photo file time stamp.", "Likewise, the clock of the GPS may be out of sync with the clock of the digital camera. To make a good match between the photo and GPS point when these clocks are out of sync, determine the difference between the two clocks, then use this value with the ", "Clock Offset", " parameter.", "The ", "Time Tolerance", " and ", "Clock Offset", " parameters must be specified in seconds. There are a number of utilities on the Internet for calculating how many seconds a different unit of time equals. For example, 3 minutes and 12 seconds is equal to 192 seconds."], "parameters": [{"name": "Input_Folder", "isInputFile": true, "isOptional": false, "description": " The folder where photo files are located. This folder is scanned recursively for photo files; any photos in the base level of the folder, as well as in any subfolders, will be added to the output. ", "dataType": "Folder"}, {"name": "Input_Table", "isInputFile": true, "isOptional": false, "description": "The table or feature class whose rows will be matched with photo files. The input table will typically be a point feature class representing GPS recordings. ", "dataType": "TableView"}, {"name": "Time_Field", "isOptional": false, "description": " The date/time field from the input table that indicates when each row was captured or created. Must be a date field; cannot be a string or numeric field. ", "dataType": "Field"}, {"name": "Output_Table", "isOutputFile": true, "isOptional": false, "description": "The output table containing the OBJECTIDs from the input table that match a photo, and the matching photo path. Only OBJECTIDs from the input table that are found to match a photo are included in the output table. ", "dataType": "Table"}, {"name": "Unmatched_Photos_Table", "isOptional": true, "description": "The optional output table that will list any photo files in the input folder with an invalid time stamp or any photos that cannot be matched because there is no input row within the time tolerance. If no path is specified, this table will not be created. ", "dataType": "Table"}, {"name": "Add_Photos_As_Attachments", "isOptional": true, "description": " Specifies if photo files will be added to the rows of the input table as geodatabase attachments. Adding attachments requires at minimum an ArcGIS for Desktop Standard license, and the output feature class must be in a version 10 or higher geodatabase. ADD_ATTACHMENTS \u2014 Photo files will be added to the rows of the input table as geodatabase attachments. Geodatabase attachments are copied internally to the geodatabase. This is the default. NO_ATTACHMENTS \u2014 Photo files will not be added to the rows of the input table as geodatabase attachments.", "dataType": "Boolean"}, {"name": "Time_Tolerance", "isOptional": true, "description": " The maximum difference (in seconds) between the date/time of an input row and a photo file that will be matched. If an input row and a photo file have time stamps that are different by more than this tolerance, no match will occur. To match a photo file to a row with the closest time stamp, regardless of how large the date/time difference might be, set the tolerance to 0. The sign of this value (- or +) is irrelevant; the absolute value of the number specified will be used. Do not use this parameter to adjust for consistent shifts or offsets between the times recorded by the GPS and the digital camera. Use the Clock Offset parameter, or the Convert Time Zone tool to shift the time stamps of the input rows to match those of the photos. ", "dataType": "Double"}, {"name": "Clock_Offset", "isOptional": true, "description": " The difference (in seconds) between the internal clock of the digital camera used to capture the photos and the GPS unit. If the clock of the digital camera is behind the clock of the GPS unit, use a positive value; if the clock of the digital camera is ahead of the clock of the GPS unit, use a negative value. For example, if a photo with a time stamp of 11:35:17 should match a row with a time stamp of 11:35:32 , use a Clock Offset of 15 . ", "dataType": "Double"}]},
{"syntax": "GeoTaggedPhotosToPoints_management (Input_Folder, Output_Feature_Class, {Invalid_Photos_Table}, {Include_Non-GeoTagged_Photos}, {Add_Photos_As_Attachments})", "name": "GeoTagged Photos To Points (Data Management)", "description": "\r\nCreates points from the x-, y-, and z-coordinate information stored in geotagged photos. Optionally adds photo files to features in the output feature class as geodatabase attachments.\r\n", "example": {"title": "GeoTaggedPhotosToPoints example (Python window)", "description": "The following Python window snippet demonstrates how to use the GeoTaggedPhotosToPoints tool.", "code": "import arcpy arcpy.GeoTaggedPhotosToPoints_management ( \"c:/data/photos\" , \"c:/data/city.gdb/photo_points\" , \"\" , \"ONLY_GEOTAGGED\" , \"ADD_ATTACHMENTS\" )"}, "usage": ["This tool reads the longitude, latitude, and altitude coordinate information from ", "JPEG", " and ", "TIFF", " photo files with valid Exif (exchangeable image file format) metadata and writes these coordinates and associated attributes to an output point feature class. These photos are often captured using digital cameras with built-in or accessory GPS units or with smartphones.", "\r\nThe output feature class will have three attribute fields: ", "The output ", "DateTime", " field is a text field with the timestamp in the format ", "yyyy:MM:dd HH:mm:ss", ". Use the ", "Convert Time Field", " tool to convert this text field to a true datetime field that can be used to analyze and map the data with ", "time", ".", "If the output ", "DateTime", " field has a null or empty value, this may be an indication that your device does not capture  a useable timestamp with the geotagged photo. Photo files may have a date taken or date modified property, but these often do not represent the date and time the photo was taken.", "If the x,y coordinates of a photo are ", "0,0", ", no point will be generated for that photo. These empty coordinates often occur because the camera GPS does not have an adequate signal to capture real coordinates. If the ", "Include Non-GeoTagged Photos", " parameter is checked (", "ALL_PHOTOS", " in scripting), the photo will be added as an output record with a null geometry. ", "The output feature class will have a GCS_WGS_1984 spatial reference, since that is the coordinate system used by GPS receivers.  "], "parameters": [{"name": "Input_Folder", "isInputFile": true, "isOptional": false, "description": " The folder where photo files are located. This folder is scanned recursively for photo files; any photos in the base level of the folder, as well as in any subfolders, will be added to the output. ", "dataType": "Folder"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": " The output point feature class. ", "dataType": "Feature Class"}, {"name": "Invalid_Photos_Table", "isOptional": true, "description": "The optional output table that will list any photo files in the input folder with invalid Exif metadata or empty GPS coordinates. If no path is specified, this table will not be created. ", "dataType": "Table"}, {"name": "Include_Non-GeoTagged_Photos", "isOptional": true, "description": " Specifies if all photo files should be added as records to the output feature class or only those with valid GPS coordinates. ALL_PHOTOS \u2014 All photo files will be added as records to the output feature class. If a photo file does not have GPS coordinate information, it will be added as a record with null geometry. This is the default. ONLY_GEOTAGGED \u2014 Only photo files with valid GPS coordinate information will have records in the output feature class.", "dataType": "Boolean"}, {"name": "Add_Photos_As_Attachments", "isOptional": true, "description": " Specifies if photo files will be added to the output feature class as geodatabase attachments. Adding attachments requires at minimum an ArcGIS for Desktop Standard license, and the output feature class must be in a version 10 or higher geodatabase. ADD_ATTACHMENTS \u2014 Photo files will be added to the output feature class records as geodatabase attachments. Geodatabase attachments are copied internally to the geodatabase. This is the default. NO_ATTACHMENTS \u2014 Photo files will not be added to the output feature class records as geodatabase attachments.", "dataType": "Boolean"}]},
{"syntax": "EditRasterFunction_management (in_mosaic_dataset, {edit_mosaic_dataset_item}, {edit_options}, {function_chain_definition}, {location_function_name})", "name": "Edit Raster Function (Data Management)", "description": "Adds, replaces, or removes a raster function template to a mosaic dataset, items in a mosaic dataset, or a raster layer that contains a raster function. A raster function template defines a raster function chain. The template is stored as an rft.xml file, which can be exported from the  Raster Function Editor .", "example": {"title": "AddRasterFunction example 1 (Python window)", "description": "This is a Python sample for AddRasterFunction.", "code": "import arcpy arcpy.EditRasterFunction_management ( \"C:/Workspace/editfunction.gdb/md\" , \"EDIT_MOSAIC_DATASET\" , \"INSERT\" , \"C:/workspace/hillshade.rft.xml\" , \"Stretch Function\" )"}, "usage": ["To apply the rft.xml to items in a mosaic dataset, you must select the items in the attribute table or define a query using the ", "Make Mosaic Layer tool", ". ", "The  ", "Raster Function Template Editor", " tool can be added to any toolbar Customized Mode window. The tool is located under the ", "Command", " tab within the Raster category. To save out a raster function template, click ", "File", ", then click ", "Save", "; this will save an rft.xml file."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The input can be a mosaic dataset or a raster layer that contains a raster function. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "edit_mosaic_dataset_item", "isOptional": true, "description": "Choose whether to edit the mosaic dataset items. EDIT_MOSAIC_DATASET \u2014 The edits will affect the functions associated with the mosaic dataset. This is the default. EDIT_MOSAIC_DATASET_ITEM \u2014 The edits will affect the functions associated with the mosaic dataset items.", "dataType": "Boolean"}, {"name": "edit_options", "isOptional": true, "description": "Choose whether to insert a new raster function, replace an existing raster function chain, or remove the existing one. INSERT \u2014 Insert the functions above the location_function_name of the existing chain. The Raster Function Chain must be supplied in order to use this option. This is the default. REPLACE \u2014 Replace the existing function chain with the Raster Function Template specified in this tool. The Raster Function Chain must be supplied in order to use this option. REMOVE \u2014 Remove the existing raster function chain.", "dataType": "String"}, {"name": "function_chain_definition", "isOptional": true, "description": " The raster function template file (rft.xml). A raster function chain can be exported from the Raster Function Template Editor. ", "dataType": "File"}, {"name": "location_function_name", "isOptional": true, "description": " The name of the function where the edits will take place. If you INSERT the function, then it will be inserted above the location_function_name . If you REPLACE the function, then it will replace the function that has been selected. If you REMOVE the function, then it will remove the function that has been selected. ", "dataType": "String"}]},
{"syntax": "CreateEnterpriseGeodatabase_management (database_platform, instance_name, {database_name}, {account_authentication}, {database_admin}, {database_admin_password}, {sde_schema}, {gdb_admin_name}, {gdb_admin_password}, {tablespace_name}, authorization_file)", "name": "Create Enterprise Geodatabase (Data Management)", "description": "\r\nThe  Create Enterprise Geodatabase  tool creates a database, storage locations, and a database user to be used as the geodatabase administrator and owner of the geodatabase depending on the database management system (DBMS) used. It grants the geodatabase administrator privileges required to create a geodatabase, then creates a geodatabase in the database. ", "example": {"title": "CreateGeodatabase example 1", "description": "The following script creates a geodatabase in an Oracle database. It creates an sde user and a default tablespace, sdetbs, for the sde user. The keycodes file is on a remote Linux server.", "code": "#Import arcpy module import arcpy arcpy.CreateEnterpriseGeodatabase ( \"ORACLE\" , \"ora11g:1521/elf\" , \"\" , \"DATABASE_AUTH\" , \"sys\" , \"manager\" , \"\" , \"sde\" , \"supersecret\" , \"sdetbs\" , \"//myserver/mymounteddrive/arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License10.1/sysgen/keycodes\" )"}, "usage": ["The following table indicates what the tool can do for each type of DBMS:", "Function", "DBMS", "Creates a database", "PostgreSQL and Microsoft SQL Server", "Creates a tablespace", "Oracle", "Creates a geodatabase administrator user in the database", "Oracle, PostgreSQL, and SQL Server (if creating an sde-schema geodatabase)", "Grants the geodatabase administrator the privileges required to create a geodatabase, upgrade a geodatabase, and kill database connections", "Oracle and PostgreSQL", "Grants the geodatabase administrator the privileges required to create a geodatabase and kill database connections", "SQL Server (if creating an sde-schema geodatabase)", "Creates a geodatabase in the specified database", "Oracle, PostgreSQL, and SQL Server", "You must have ", "ArcGIS for Desktop", " (Standard or Advanced), ArcGIS Engine Runtime with the Geodatabase Update extension, or ", "ArcGIS for Server", " (Standard or Advanced) installed on the computer from which you will create the geodatabase. You must be able to connect directly to the DBMS to create the database objects and a geodatabase. This requires that you install and configure a DBMS client on the computer where the ArcGIS client is installed."], "parameters": [{"name": "database_platform", "isOptional": false, "description": " Specify the type of database management system to which you will connect to create a geodatabase. Oracle \u2014 Indicates you are connecting to an Oracle instance PostgreSQL \u2014 Indicates you are connecting to a PostgreSQL database cluster SQL_Server \u2014 Indicates you are connecting to a SQL Server instance", "dataType": "String"}, {"name": "instance_name", "isOptional": false, "description": "For SQL Server, provide the SQL Server instance name. For Oracle, provide either the TNS name or Oracle Easy Connection string. For PostgreSQL, provide the name of the server where PostgreSQL is installed. ", "dataType": "String"}, {"name": "database_name", "isOptional": true, "description": " This parameter is valid only for PostgreSQL and SQL Server DBMS types. Either type the name of an existing, preconfigured database, or type a name for a database to be created. If you let the tool create the database in SQL Server, the file sizes will either be the same as you have defined for the SQL Server model database or 500 MB for the MDF file and 125 MB for the LDF file, whichever is greater. Both the MDF and LDF files will be created in the default SQL Server location on the database server. If you let the tool create the database in PostgreSQL, the template1 database will be used as the template for your database. ", "dataType": "String"}, {"name": "account_authentication", "isOptional": true, "description": " Specify what type of authorization to use for the database connection. OPERATING_SYSTEM_AUTH \u2014 The login information you provided when you logged in to the computer from which you are running the tool will be used to authenticate your database connection. If your DBMS is not configured to allow operating system authentication, authentication will fail. DATABASE_AUTH \u2014 You must provide a valid database user name and password for authentication in the database. This is the default authentication method. If your DBMS is not configured to allow database authentication, authentication will fail.", "dataType": "Boolean"}, {"name": "database_admin", "isOptional": true, "description": " If you use database authentication, you must specify a database administrator user. For Oracle, the database administrator is sys. For Postgres, it is the postgres superuser. For SQL Server, it is a member of the sysadmin fixed server role. ", "dataType": "String"}, {"name": "database_admin_password", "isOptional": true, "description": " Type the password for the database administrator. If you use database authentication, you must specify the database administrator user password. ", "dataType": "Encrypted String"}, {"name": "sde_schema", "isOptional": true, "description": " This parameter is only relevant for SQL Server and indicates whether the geodatabase is to be created in the schema of a user named sde or in the dbo schema in the database. If creating a dbo-schema geodatabase, you must connect as a user who is dbo in the SQL Server instance. Therefore, if you use operating system authentication, the login used must be dbo in the SQL Server instance. SDE_SCHEMA \u2014 The geodatabase repository is owned by and stored in the schema of a user named sde. This is the default. DBO_SCHEMA \u2014 The geodatabase repository is stored in the dbo schema in the database.", "dataType": "Boolean"}, {"name": "gdb_admin_name", "isOptional": true, "description": "If you are using PostgreSQL, this value must be sde. If the sde login role does not exist, this tool creates it and grants it superuser privileges. It also creates an sde schema in the database. If the sde login role exists, this tool will grant it superuser privileges if it does not already have them. If you are using Oracle, the default value is sde. However, if you are creating a user-schema geodatabase inside a master sde geodatabase, specify the name of the user who will own the geodatabase. If the user does not exist in the DBMS, the Create Enterprise Geodatabase tool creates the user and grants it the privileges required to create and upgrade a geodatabase and kill user connections to the DBMS. If the user already exists, the tool will grant the required privileges to the user. If you are using SQL Server and specified an sde-schema geodatabase, this value must be sde. The tool will create an sde login, database user, and schema and grant it privileges to create a geodatabase and kill connections to the SQL Server instance. If you specified a dbo schema, do not provide a value for this parameter. ", "dataType": "String"}, {"name": "gdb_admin_password", "isOptional": true, "description": " Provide the password for the geodatabase administrator user. If the geodatabase administrator user already exists in the DBMS, the password you type must match the existing password. If the geodatabase administrator user does not already exist, type a valid database password for the new user. The password must meet the password policy enforced by your DBMS. The password is a geoprocessing-encrypted string. ", "dataType": "Encrypted String"}, {"name": "tablespace_name", "isOptional": true, "description": " This parameter is only valid for Oracle and PostgreSQL DBMS types. For Oracle, do one of the following: For PostgreSQL, you must either provide the name of an existing tablespace to be used as the default tablespace for the database or leave this parameter blank. This tool does not create a tablespace in PostgreSQL. If you do not provide a value for this parameter, the database is created in the pg_default tablespace in PostgreSQL. Provide the name of an existing tablespace to be used as the default tablespace for the sde user. Type a valid name and a 400 MB tablespace will be created in the Oracle default storage location and set as the sde user's default tablespace. Leave the tablespace blank, and tablespace SDE_TBS (400 MB) will be created and set as the default tablespace for the sde user.", "dataType": "String"}, {"name": "authorization_file", "isOptional": false, "description": " Provide the path and file name of the keycodes file that was created when you authorized ArcGIS for Server Enterprise. This file is in the \\\\Program Files\\ESRI\\License<release#>\\sysgen folder on Windows and /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen directory on Linux. If you have not already done so, authorize ArcGIS for Server to create this file. ", "dataType": "File"}]},
{"syntax": "SetMosaicDatasetProperties_management (in_mosaic_dataset, {rows_maximum_imagesize}, {columns_maximum_imagesize}, {allowed_compressions}, {default_compression_type}, {JPEG_quality}, {LERC_Tolerance}, {resampling_type}, {clip_to_footprints}, {footprints_may_contain_nodata}, {clip_to_boundary}, {color_correction}, {allowed_mensuration_capabilities}, {default_mensuration_capabilities}, {allowed_mosaic_methods}, {default_mosaic_method}, {order_field}, {order_base}, {sorting_order}, {mosaic_operator}, {blend_width}, {view_point_x}, {view_point_y}, {max_num_per_mosaic}, {cell_size_tolerance}, {cell_size}, {metadata_level}, {transmission_fields}, {use_time}, {start_time_field}, {end_time_field}, {time_format}, {geographic_transform}, {max_num_of_download_items}, {max_num_of_records_returned})", "name": "Set Mosaic Dataset Properties (Data Management)", "description": "\r\n Sets the properties of a mosaic dataset. Many of these properties define the defaults used when displaying the mosaic dataset or how it can be used when it's published as an image service.\r\n", "example": {"title": "SetMosaicDatasetProperties example 1 (Python window)", "description": "This is a Python sample for SetMosaicDatasetProperties.", "code": "import arcpy arcpy.SetMosaicDatasetProperties_management ( \"c:/workspace/mdproperties.gdb/md\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"0.2\" , \"BASIC\" , \"NAME;MINPS;MAXPS;GROUPNAME;PRODUCTNAME;ZORDER;YEARS;YEARE\" , \"ENABLED\" , \"YEARS\" , \"YEARE\" , \"YYYYMM\" , \"NAD_1983_HARN_To_WGS_1984_2\" , \"10\" , \"500\" )"}, "usage": ["\r\nThis tool allows you to set some mosaic dataset properties within a geoprocessing or Python environment."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The mosaic dataset that will have its properties set. ", "dataType": "Mosaic Layer"}, {"name": "rows_maximum_imagesize", "isOptional": true, "description": "Set the maximum number of rows for the mosaicked image, generated by the mosaic dataset for each request. By increasing the rows and columns for the Maximum Image Size of Requests, you will be increasing the time it takes to process the mosaic. However, you may want to increase these numbers if you are printing very large, high-resolution plots. By making these numbers too small, no image may display. For example, if you changed the numbers to 10, then the display window could only be 10 pixels in dimension, or smaller, to display an image. ", "dataType": "Long"}, {"name": "columns_maximum_imagesize", "isOptional": true, "description": " Set the maximum number of columns for the mosaicked image, generated by the mosaic dataset for each request. By increasing the rows and columns for the Maximum Image Size of Requests, you will be increasing the time it takes to process the mosaic. However, you may want to increase these numbers if you are printing very large, high-resolution plots. By making these numbers too small, no image may display. For example, if you changed the numbers to 10, then the display window could only be 10 pixels in dimension, or smaller, to display an image. ", "dataType": "Long"}, {"name": "allowed_compressions", "isOptional": false, "description": " Set the mosaic dataset's Allowed Compression Methods property, which defines the method of compression used to transmit the mosaicked image from the computer to the display (or from the server to the client). NONE \u2014 No compression will be used. JPEG \u2014 JPEG compression will be used. LZW \u2014 LZW compression will be used. LERC \u2014 The Limited Error Raster Compression will be used.", "dataType": "String"}, {"name": "default_compression_type", "isOptional": true, "description": " Set the default compression type. The default compression must be in the allowed_compressions list or already set in the mosaic dataset's Allowed Compression Methods property. ", "dataType": "String"}, {"name": "JPEG_quality", "isOptional": true, "description": " Set the compression quality that will be used when JPEG compression is used. Compression quality can range from 1 to 100. A higher number means better image quality but less compression. ", "dataType": "Long"}, {"name": "LERC_Tolerance", "isOptional": true, "description": " Specify the LERC tolerance, when the LERC compression method is used. The tolerance represents the maximum error value that is applicable per pixel (not an average for the image). This value is specified in the units of the mosaic dataset. For example, if the error is 10 cm and the mosaic dataset is in meter, enter 0.1. LERC is an efficient lossless compression method recommended for data with a large pixel depth, such as float, 32-bit, 16-bit, or 12-bit data. LERC compresses better (5-10 times) and faster (5-10 times) than LZ77 for float data and is better with integer data. For integer data, if the error limit specified is 0.5 or less, LERC is lossless. ", "dataType": "Double"}, {"name": "resampling_type", "isOptional": true, "description": " Set the default resampling method used to display the mosaicked image. NEAREST \u2014 The nearest neighbor resampling method will be used. BILINEAR \u2014 The bilinear interpolation resampling method will be used. CUBIC \u2014 The cubic convolution resampling method will be used. MAJORITY \u2014 The majority resampling method will be used.", "dataType": "String"}, {"name": "clip_to_footprints", "isOptional": true, "description": "Specify whether you want to limit the extent of each raster to its footprint or to use the entire raster to create the mosaicked image. Many times, the raster and the footprint will be the same, but when they are different, you can specify whether to clip the raster or not. NOT_CLIP \u2014 Do not clip the raster to the footprint. This is the default. CLIP \u2014 Clip the rasters to the footprint.", "dataType": "Boolean"}, {"name": "footprints_may_contain_nodata", "isOptional": true, "description": "Specify whether the footprints of the mosaic dataset contain pixels of NoData. FOOTPRINTS_MAY_CONTAIN_NODATA \u2014 Your footprints contain pixels of NoData. Check this on to ensure that there are no holes in your mosaic dataset. This is the default. FOOTPRINTS_DO_NOT_CONTAIN_NODATA \u2014 Your footprints do not contain pixels of NoData. If you specify this setting, you may have a slight performance benefit. However if there are really pixels of NoData in your foorprints, you will see holes in your mosaic dataset.", "dataType": "Boolean"}, {"name": "clip_to_boundary", "isOptional": true, "description": " Specify whether you want to clip the mosaic dataset's image to the boundary, or to display the entire mosaicked image. Often, the boundary will represent the entire mosaic dataset; however, you may modify it to exclude part of the dataset. CLIP \u2014 Clip the mosaicked image to the boundary. This is the default. NOT_CLIP \u2014 Do not clip the mosaicked image to the boundary.", "dataType": "Boolean"}, {"name": "color_correction", "isOptional": true, "description": "Choose whether to enable the color correction that has been set up for the mosaic dataset. NOT_APPLY \u2014 Keep color correction off. This is the default. APPLY \u2014 Use the color correction that has been set up for the mosaic dataset.", "dataType": "Boolean"}, {"name": "allowed_mensuration_capabilities", "isOptional": false, "description": " Choose the allowable mensuration capabilities for this mosaic dataset. Basic \u2014 Distance, point, centroid, and area calculations are allowed. Base-Top Height \u2014 Base to top height measurements are allowed. In order to perform Base-Top Height measurements, rational polynomial coefficients (RPCs) information is required. Base-Top Shadow Height \u2014 Base to top shadow height measurements are allowed. In order to perform Base-Top Shadow Height measurements, sun azimuth and sun elevation information is required. Top-Top Shadow Height \u2014 Top to top shadow height measurements are allowed. In order to perform this measurement, sun azimuth, sun elevation, and rational polynomial coefficients (RPCs) information is required. 3D \u2014 3D measurements are allowed when a DEM is available.", "dataType": "String"}, {"name": "default_mensuration_capabilities", "isOptional": true, "description": " Choose the default mensuration capability for this mosaic dataset. The default must be set in the allowed_mensuration_capabilities parameter list or already set in the mosaic dataset's Mensuration Capabilities property. ", "dataType": "String"}, {"name": "allowed_mosaic_methods", "isOptional": false, "description": " Choose which mosaic methods will be allowed by this mosaic dataset. The mosaicked image can be created from a number of input rasters. The mosaic method defines how the mosaicked image is created from the different rasters in the mosaic dataset. Center \u2014 Enables rasters to be sorted based on a default order where rasters that have their centers closest to the view center are placed on top. NorthWest \u2014 Enables raster ordering in a view-independent way, where rasters with their centers to the northwest are displayed on top. LockRaster \u2014 Enables a user to lock the display of a single or multiple rasters based on the ObjectID. ByAttribute \u2014 Enables raster ordering based on a defined metadata attribute and its difference from a base value. Nadir \u2014 Enables rasters to be sorted by the distance between the nadir position and view center. This is similar to the Closest to Center method but uses the nadir point to a raster, which may be different than the center, especially for oblique imagery. Viewpoint \u2014 Orders rasters based on a user-defined location and nadir location for the rasters using the Viewpoint tool. Seamline \u2014 Cuts the rasters using the predefined seamline shape for each raster using optional feathering along the seams. The ordering is predefined during seamline generation. The LAST mosaic operator is not valid with this mosaic method. None \u2014 Orders rasters based on the order (ObjectID) in the mosaic dataset attribute table.", "dataType": "String"}, {"name": "default_mosaic_method", "isOptional": true, "description": " Choose the default mosaic method for this mosaic dataset. The default must be set in the allowed_mosaic_methods parameter list or already set in the mosaic dataset's Allowed Mosaic Methods property. ", "dataType": "String"}, {"name": "order_field", "isOptional": true, "description": " Choose the default field to use when ordering rasters using the By Attribute mosaic method. The list of fields is defined as those in the attribute table that are of type metadata and are integer. This list can include, but is not limited to: If your field is a numeric or date field, then the Base Order parameter needs to be set. This parameter is not needed if By Attribute is not an allowed mosaic method. Name MinPS MaxPS LowPS HighPS Tag GroupName ProductName CenterX CenterY ZOrder Shape_Length Shape_Area", "dataType": "String"}, {"name": "order_base", "isOptional": true, "description": "The images are sorted based on the difference between this value or the others in the Order Field of the attribute table. If you are using a Date attribute, then it needs to be in one of the following formats: This parameter is not needed if By Attribute is not an allowed mosaic method. YYYY/MM/DD HH:mm:ss.s YYYY/MM/DD HH:mm:ss YYYY/MM/DD HH:mm YYYY/MM/DD HH YYYY/MM/DD YYYY/MM YYYY ", "dataType": "String"}, {"name": "sorting_order", "isOptional": true, "description": " Choose whether to sort your rasters in an ascending or descending order. This parameter is not needed if By Attribute is not an allowed mosaic method. ASCENDING \u2014 Use an ascending order for your rasters. This is the default setting. DESCENDING \u2014 Use a descending order for your rasters.", "dataType": "Boolean"}, {"name": "mosaic_operator", "isOptional": true, "description": " Choose which mosaic operator to use to resolve the overlapping cells. FIRST \u2014 The overlapping areas will contain the cells from the first raster dataset listed in the source. LAST \u2014 The overlapping areas will contain the cells from the last raster dataset listed in the source. MIN \u2014 The overlapping areas will contain the minimum cell values from all the overlapping cells. MAX \u2014 The overlapping areas will contain the maximum cell values from all the overlapping cells. MEAN \u2014 The overlapping areas will contain the mean cell values from all the overlapping cells. BLEND \u2014 The overlapping areas will be a blend of the cell values that overlap along the edge of each raster dataset in the mosaicked image. By default, the edge is defined by the footprint or the seamline for each raster. ", "dataType": "String"}, {"name": "blend_width", "isOptional": true, "description": " Defines the distance in pixels (at the display scale) used by the Blend mosaic operator. ", "dataType": "Long"}, {"name": "view_point_x", "isOptional": true, "description": "Defines an x-offset that is used to calculate where the center of the display is. This value is calculated in the unit of the spatial reference system of the mosaic dataset. This value will be used when the Closest to Viewpoint mosaic method is used. ", "dataType": "Double"}, {"name": "view_point_y", "isOptional": true, "description": "Defines a y-offset that is used to calculate where the center of the display is. This value is calculated in the unit of the spatial reference system of the mosaic dataset. This value will be used when the Closest to Viewpoint mosaic method is used. ", "dataType": "Double"}, {"name": "max_num_per_mosaic", "isOptional": true, "description": "The maximum number of rasters that can used to create the mosaicked image. ", "dataType": "Long"}, {"name": "cell_size_tolerance", "isOptional": true, "description": " Specify the cell size tolerance factor. The tolerance factor must be greater than or equal to 0.0. A factor of 0.1 means that all the LowPS values that are 10 percent larger than the lowest pixel size are considered to be the same\u2014for tools and operations that use pixel (cell) sizes. ", "dataType": "Double"}, {"name": "cell_size", "isOptional": true, "description": "Specify the output cell size. You can choose a layer as the cell size template, or you can specify the actual cell size. If you specify the cell size, you can use a single value for a square cell size, or X and Y values for a rectangular cell size. ", "dataType": "Cell Size XY"}, {"name": "metadata_level", "isOptional": true, "description": " Choose the level of metadata to expose from the server to a client when publishing the mosaic dataset. FULL \u2014 All metadata, including the basic raster dataset information and the function chain's details will be transmitted. This is the default. NONE \u2014 No metadata will be exposed to the client. BASIC \u2014 The raster dataset level of information will be transmitted, such as the columns and rows, cell size, and spatial reference information.", "dataType": "String"}, {"name": "transmission_fields", "isOptional": false, "description": "Choose the fields that users can access. This list can include fields that are not added by default to the mosaic dataset's attribute table. By default, the list includes: Name MinPS MaxPS LowPS HighPS Tag GroupName ProductName CenterX CenterY ZOrder Shape_Length Shape_Area", "dataType": "String"}, {"name": "use_time", "isOptional": true, "description": " Specify whether to make the mosaic dataset time-aware. If time is activated, then you need to specify the start and end fields, and the time format. DISABLED \u2014 The mosaic dataset will not be time-aware. This is the default. ENABLED \u2014 The mosaic dataset will be time-aware. This allows the client to use the Time Slider.", "dataType": "Boolean"}, {"name": "start_time_field", "isOptional": true, "description": " Specify the field in the attribute table to use as the start time. ", "dataType": "String"}, {"name": "end_time_field", "isOptional": true, "description": "Specify the field in the attribute table to use as the end time. ", "dataType": "String"}, {"name": "time_format", "isOptional": true, "description": " Specify the format of the time field. YYYY \u2014 Year YYYYMM \u2014 Year and month YYYY/MM \u2014 Year and month YYYY-MM \u2014 Year and month YYYYMMDD \u2014 Year, month, and day YYYY/MM/DD \u2014 Year, month, and day YYYY-MM-DD \u2014 Year, month, and day YYYYMMDDhhmmss \u2014 Year, month, day, hour, minute, and seconds YYYY/MM/DD hh:mm:ss \u2014 Year, month, day, hour, minute, and seconds YYYY-MM-DD hh:mm:ss \u2014 Year, month, day, hour, minute, and seconds YYYYMMDDhhmmss.s \u2014 Year, month, day, hour, minute, seconds, and fraction of seconds YYYY/MM/DD hh:mm:ss.s \u2014 Year, month, day, hour, minute, seconds, and fraction of seconds YYYY-MM-DD hh:mm:ss.s \u2014 Year, month, day, hour, minute, seconds, and fraction of seconds", "dataType": "String"}, {"name": "geographic_transform", "isOptional": false, "description": " Choose which geographic transformations are associated with this mosaic dataset. ", "dataType": "String"}, {"name": "max_num_of_download_items", "isOptional": true, "description": "Choose the maximum number of items that can be downloaded per request. ", "dataType": "Long"}, {"name": "max_num_of_records_returned", "isOptional": true, "description": " Choose the maximum number of records returned per request. ", "dataType": "Long"}]},
{"syntax": "EnableEnterpriseGeodatabase_management (input_database, authorization_file)", "name": "Enable Enterprise Geodatabase (Data Management)", "description": "\r\nThe  Enable Enterprise Geodatabase  tool creates geodatabase system tables, stored procedures, functions, and types in an existing enterprise database, thereby enabling geodatabase functionality in the database.", "example": {"title": "EnableGeodatabase example 1", "description": "This script uses an existing database connection file (my_db_connection.sde) located in the default database connection location to enable geodatabase functionality with a keycodes file on a remote server.", "code": "##Import Arcpy module import arcpy arcpy.EnableEnterpriseGeodatabase ( \"Database Connections\\my_db_connection.sde\" , \"//myagsserver/Program Files/ESRI/License10.1/sysgen/keycodes\" )"}, "usage": ["If you are connecting to an IBM DB2, Informix, Oracle, or PostgreSQL database, you must connect as a user named sde. If you connect to a Microsoft SQL Server database, you can connect as a user named sde or as a user who is dbo in the SQL Server instance. See the topic  appropriate to your database for information on required privileges to create a geodatabase:"], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " Provide the path and connection file name for the database in which geodatabase functionality is to be enabled. The connection must be made as a user that qualifies as a geodatabase administrator. ", "dataType": "Workspace"}, {"name": "authorization_file", "isOptional": false, "description": " Provide the path and file name of the keycodes file that was created when you authorized ArcGIS for Server Enterprise. This file is in the \\\\Program Files\\ESRI\\License<release#>\\sysgen folder on Windows and /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen directory on Linux. If you have not already done so, authorize ArcGIS for Server to create this file. ", "dataType": "File"}]},
{"syntax": "SetRasterProperties_management (in_raster, {data_type}, {statistics}, {stats_file}, {nodata})", "name": "Set Raster Properties (Data Management)", "description": "Sets some properties on a raster dataset or mosaic dataset, such as data type, statistics, and nodata values.  Learn more about raster statistics and properties", "example": {"title": "SetRasterProperties example 1 (Python window)", "description": "This is a Python sample for SetRasterProperties.", "code": "import arcpy arcpy.SetRasterProperties_management ( \" \\\\ cpu\\data\\srtm.tif\" , \"ELEVATION\" , \"1 50 400 5 28\" , \"#\" , \"#\" )"}, "usage": ["This tool allows you to define the statistics for a raster or mosaic dataset.  Typically, you use this tool  if\r\nyou do not want to have ArcGIS calculate them for you. Statistics you can set are  the minimum, maximum, standard deviation, and mean\r\nvalues for each band.  These statistics can be read from an XML file. This XML file can be created by exporting\r\nthe statistics from another raster or mosaic dataset, as follows:", "The properties that can be set with this tool determine the default rendering settings in ArcMap, as well as statistics that are used by other tools.", "Properties you can set include:", "These properties can also be set in the ", "Properties", " window, by right-clicking a raster in the ", "Catalog", " window and selecting ", "Properties", ", or by right-clicking a raster layer in ArcMap and selecting ", "Properties", ".   The ", "Set Raster Properties", " tool allows you to set these properties within a geoprocessing model or in Python."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": " The input raster dataset or mosaic dataset. ", "dataType": "Mosaic Layer ; Raster Layer"}, {"name": "data_type", "isOptional": true, "description": " The type of data this dataset contains. These settings control the symbology applied when these datasets are rendered. GENERIC \u2014 This is a raster that does not fit any other data source type. ArcGIS will try to use the most appropriate renderer to display this data. ELEVATION \u2014 The data is elevation data. A minimum-maximum stretch will be applied when displaying this data. THEMATIC \u2014 The data is categorical; therefore, no stretching will be used. This data should probably be displayed with the colormap or the unique value renderer. PROCESSED \u2014 The data has already been enhanced; therefore, no stretch will be applied when displaying this dataset.", "dataType": "String"}, {"name": "statistics", "isOptional": false, "description": "Enter the band number, minimum statistics value, maximum statistics value, mean statistics value, and standard deviation value. ", "dataType": "Value Table"}, {"name": "stats_file", "isOptional": true, "description": "An XML file that contains the statistics. This file can be created by exporting the statistics from another raster or mosaic dataset. ", "dataType": "File"}, {"name": "nodata", "isOptional": false, "description": " Define values for each or all bands. Each band can have a unique NoData value defined, or the same value can be specified for all bands. If you want to define multiple NoData values for each band selection, use a space delimiter between each NoData value within the bands_for_nodata_value parameter. ", "dataType": "Value Table"}]},
{"syntax": "ExportTopologyErrors_management (in_topology, out_path, out_basename)", "name": "Export Topology Errors (Data Management)", "description": "\r\nExports the errors from a geodatabase topology to the target geodatabase.  All information associated with the errors and exceptions, such as the features referenced by the error or exception, are exported.\r\nOnce they are exported, the feature classes can be accessed using any license level of ArcGIS.  The feature classes can be used with the  Select by Location  dialog box or the  Select Layer By Location  tool and can be shared with other users who do not have access to the topology itself.", "example": {"title": "ExportTopologyErrors example (stand-alone script)", "description": "The following stand-alone script demonstrates how to use the ExportTopologyErrors function.", "code": "# Set the necessary product code import arceditor # Import arcpy module import arcpy # Local variables: co_topo_FD_Topology = \"C: \\\\ Testing \\\\ topology.mdb \\\\ my_topo_FD \\\\ my_topo_FD_Topology\" # Process: Export Topology Errors arcpy.ExportTopologyErrors_management ( co_topo_FD_Topology , \"C: \\\\ Testing \\\\ topology.mdb \\\\ my_topo_FD\" , \"my_topo_FD_Topology\" )"}, "usage": ["\r\nThe default output location is the location of the specified  topology.", "\r\n The output of the tool consists of three feature classes, one for each supported geometry type of topology errors: points, lines, and polygons. The names of each feature class are created by combining a user-defined base name appended with either PointsErrors, LineErrors, or PolygonErrors.", "\r\nThe default ", "Base Name", " for the three output feature classes is the name of the specified topology.", "\r\nThree output feature classes are always created, even if there are no topology errors of each geometry type.  Use the ", "Get_Count", " tool to determine if any of the feature classes are empty.", "\r\nThe field collection of the output feature classes is fixed and can only be modified after exporting. Along with the standard geodatabase feature class fields (ObjectID, Shape, and optional shape length and area fields), the following fields are included and contain information concerning each topology error:", "Field Name", "Type", "Description", "OriginObjectClassName", "String", "Origin Class Name", "OriginObjectID", "Integer", "Origin Feature's Object ID", "DestinationObjectClassName", "String", "Destination Class Name", "DestinationObjectID", "Integer", "Destination Feature's Object ID", "RuleType", "String", "Description of the rule that was violated, obtained from the esriTopologyRuleType enumeration", "RuleDescription", "String", "User-friendly description of the rule that was violated. This description is the same as one provided in the Topology Error Inspector. ", "IsException", "Integer", "Indicates if this error is an exception.  A value of 1 identifies the error as being an exception."], "parameters": [{"name": "in_topology", "isInputFile": true, "isOptional": false, "description": "The topology from which the errors will be exported. ", "dataType": "Topology Layer"}, {"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The output workspace to which the feature classes will be created. The default is the workspace where the topology is located. ", "dataType": "Feature Dataset; Workspace"}, {"name": "out_basename", "isOutputFile": true, "isOptional": false, "description": "Name to prepend to each output feature class. This allows you to specify unique output names when running multiple exports to the same workspace. The default is the topology name. ", "dataType": "String"}]},
{"syntax": "WarpFromFile_management (in_raster, out_raster, link_file, {transformation_type}, {resampling_type})", "name": "Warp From File (Data Management)", "description": "Performs a transformation on the raster based on a link file, using a polynomial transformation. The link file contains the source and target control points.", "example": {"title": "WarpFromFile example 1 (Python window)", "description": "This is a Python sample for the WarpFromFile tool.", "code": "import arcpy arcpy.WarpFromFile_management ( \" \\\\ cpu\\data \\r aster.img\" , \" \\\\ cpu\\data\\warp_out.tif\" , \" \\\\ cpu\\data\\gcpfile.txt\" , \"POLYORDER2\" , \"BILINEAR\" )"}, "usage": ["Warp is useful when the raster requires a systematic geometric correction that can be modeled with a polynomial. A spatial transformation can invert or remove a distortion by using polynomial transformation of the proper order. The higher the order, the more complex the distortion that can be corrected. The higher orders of polynomial will involve progressively more processing time.", "The default polynomial order (1) will perform an affine transformation.", "To determine the minimum number of links necessary for a given order of polynomial, use the following formula:", "where ", "n", " is the minimum number of links required for a transformation of polynomial order ", "p", ". It is strongly suggested to use more than the minimum number of links. ", "This tool will determine the extent of the warped raster and will set the number of rows and columns to be about the same as in the input raster. Some minor differences may be due to the changed proportion between the output raster's sizes in the x and y directions. The default cell size used will be computed by dividing the extent by the previously determined number of rows and columns. The value of the cell size will be used by the resampling algorithm.", "If you choose to define an ", "output cell size", " in the Environment Settings, the number of rows and columns will be calculated as follows:", "You can save your output to BIL, BIP, BMP, BSQ, DAT, GIF, Esri Grid, IMG, JPEG, JPEG 2000, PNG, TIFF, or any geodatabase raster dataset.", "When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a ", "Compression", " type and ", "Compression Quality", " within the Environment Settings."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset. ", "dataType": "Mosaic Layer; Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "Output raster dataset. When storing the raster dataset in a file format, you need to specify the file extension: When storing a raster dataset in a geodatabase, no file extension should be added to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality. .bil \u2014Esri BIL .bip \u2014Esri BIP .bmp \u2014BMP .bsq \u2014Esri BSQ .dat \u2014ENVI DAT .gif \u2014GIF .img \u2014ERDAS IMAGINE .jpg \u2014JPEG .jp2 \u2014JPEG 2000 .png \u2014PNG .tif \u2014TIFF no extension for Esri Grid", "dataType": "Raster Dataset"}, {"name": "link_file", "isOptional": false, "description": "The link file that will be used to warp the raster. Each row in the input link file should have the following values, each delimited by a TAB: <Optional ID> <From X> <From Y> <To X> <To Y>. ", "dataType": "Text File"}, {"name": "transformation_type", "isOptional": true, "description": "The geometric transformation type. POLYORDER0 \u2014 A zero-order polynomial is used to shift your data. This is commonly used when your data is already georeferenced, but a small shift will better line up your data. Only one link is required to perform a zero-order polynomial shift. POLYORDER1 \u2014 A first-order polynomial (affine) fits a flat plane to the input points. This is the default. POLYORDER2 \u2014 A second-order polynomial fits a somewhat more complicated surface to the input points. POLYORDER3 \u2014 A third-order polynomial fits a more complicated surface to the input points. ADJUST \u2014 A transformation that optimizes for both global and local accuracy. It accomplishes this by first performing a polynomial transformation, then adjusting the control points locally, to better match the target control points, using a triangulated irregular network (TIN) interpolation technique. SPLINE \u2014 A transformation that exactly transforms the source control points to the target control points. This means that the control points will be accurate, but the raster pixels that are between the control points are not. PROJECTIVE \u2014 A transformation that can warp lines so that they remain straight. In doing so, lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products.", "dataType": "String"}, {"name": "resampling_type", "isOptional": true, "description": "The resampling algorithm to be used. The default is NEAREST. The NEAREST and MAJORITY options are used for categorical data, such as a land-use classification. The NEAREST option is the default since it is the quickest and also because it will not change the cell values. Do not use NEAREST or MAJORITY for continuous data, such as elevation surfaces. The BILINEAR option and the CUBIC option are most appropriate for continuous data. It is not recommended that BILINEAR or CUBIC be used with categorical data because the cell values may be altered. NEAREST \u2014 Nearest neighbor assignment BILINEAR \u2014 Bilinear interpolation CUBIC \u2014 Cubic convolution MAJORITY \u2014 Majority resampling ", "dataType": "String"}]},
{"syntax": "CreateGeometricNetwork_management (in_feature_dataset, out_name, in_source_feature_classes, {snap_tolerance}, {weights}, {weight_associations}, {z_snap_tolerance}, {preserve_enabled_values})", "name": "Create Geometric Network (Data Management)", "description": "\r\n Creates a geometric network in a geodatabase using the specified feature classes, role for each feature class, and the specified weights with weight associations.\r\n", "example": {"title": "CreateGeometricNetwork example (stand-alone script)", "description": "The following Python window script demonstrates how to use the CreateGeometricNetwork function in \r\nimmediate mode to create a geometric with six feature classes, one weight associated with two of those classes \r\nand a snapping tolerance.", "code": "# Import arcpy module import arcpy # Local variables: Water = \"C:/arcgis/ArcTutor/BuildingaGeodatabase/Montgomery.gdb/Water\" # Process: Create Geometric Network arcpy.CreateGeometricNetwork ( Water , \"Water_Net\" , \"Distribmains COMPLEX_EDGE NO;Fittings SIMPLE_JUNCTION NO;Hydrants SIMPLE_JUNCTION NO;Sysvalves SIMPLE_JUNCTION NO;Tanks SIMPLE_JUNCTION YES;Transmains COMPLEX_EDGE NO\" , \"0.5\" , \"Friction_Factor DOUBLE #\" , \"Distribmains FRICTION_FACTOR Friction_Factor;Transmains FRICTION_FACTOR Friction_Factor\" , \"\" , \"PRESERVE_ENABLED\" )"}, "usage": ["The feature classes must reside in the same feature dataset as the geometric network.", "Only point and line feature classes are supported as input. The feature classes cannot participate in another geometric network or other advanced geodatabase dataset such as a topology, network dataset, terrain, or fabric.", " Connectivity in geometric network is based  on the geometric coincidence of features.  Ideally, your data should be clean before you build a network.  However, if this is not the case, the data may be snapped during the network building process. While the snapping available within the geometric network wizard\r\ncan help ensure coincidence, it should not be the only solution\r\nused. There are other options available for ensuring clean data,\r\nsuch as Topology, which can help clean up your data in\r\npreparation for use in a geometric network.  The snapping performed during geometric network creation cannot be undone. If the geometric network is deleted, snapped features do not revert to their \r\noriginal locations.", "When building a geometric network from existing feature\r\nclasses, certain geometries that are illegal within the geometric\r\nnetwork, or conditions that you should be aware of, may be\r\nencountered in some of the input feature classes. When this is encountered, a warning message\r\nis displayed at the end of the network building process and a\r\ntable with the name <geometricnetwork_name>_BUILDERR is created in the database containing  a record of these errors."], "parameters": [{"name": "in_feature_dataset", "isInputFile": true, "isOptional": false, "description": "The feature dataset in which the geometric network will be created. In an enterprise geodatabase, the feature dataset and feature classes that will participate in the geometric network cannot be versioned. ", "dataType": "Feature Dataset"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "The name of the geometric network to be created. ", "dataType": "String"}, {"name": "in_source_feature_classes", "isInputFile": true, "isOptional": false, "description": " The input feature classes to be added to the geometric network and the role the feature class should play in the geometric network. Roles can be: For each simple junction feature class, whether it will participate in flow direction with Sources and Sinks. SIMPLE_JUNCTION\u2014The only option for point feature classes. SIMPLE_EDGE\u2014Used for line feature classes and only allows resources to flow from one end of the edge and out the other end. COMPLEX_EDGE\u2014Used for line feature classes and allows resources to be siphoned off along the length of the edge. YES\u2014Simple junction feature class will act as a source or a sink for establishing flow direction. NO\u2014Simple junction feature class will not act as a source or a sink for establishing flow direction.", "dataType": "Value Table"}, {"name": "snap_tolerance", "isOptional": true, "description": "The snapping tolerance to be set on the geometric network. The larger the value, the more likely vertices will be to snap together. The default value is empty, which means that no snapping will be performed during geometric network creation. The snapping performed during geometric network creation cannot be undone. ", "dataType": "Double"}, {"name": "weights", "isOptional": false, "description": " Weights are the cost of traveling along an edge in a network. For example, in a water network, a weight can be the length of the pipe. Indicate the weight name, weight type, and for bitgate weights, the size. The type of the weight determines which feature class fields can be associated with the weight. Types can be one of the following: Integer\u2014Can be associated with fields of type Short Integer or Long Integer. Single\u2014Can be associated with fields of type Float. Double\u2014Can be associated with fields of type Float or Double. Bitgate\u2014Can be associated with fields of type Short Integer or Long Integer. Only values from 0 to 31 are supported.", "dataType": "Value Table"}, {"name": "weight_associations", "isOptional": false, "description": " Specifies the weight associations for each field and feature class. When adding a new network weight, it must be associated with a field in a feature class which will provide the values to determine the weight for the features. ", "dataType": "Value Table"}, {"name": "z_snap_tolerance", "isOptional": true, "description": " The snapping tolerance to be set on the geometric network with z-coordinate based snapping. The larger the value, the more likely vertices will be to snap together. The default value is empty which means that no snapping will be performed during geometric network creation, and the geometric network will not support Zs. A value of zero indicates that no snapping will be performed during the geometric network creation, but the geometric network will support Zs. ", "dataType": "Double"}, {"name": "preserve_enabled_values", "isOptional": true, "description": " Specifies whether to preserve the values in any existing enabled fields or whether the values should be reset to their default value of True. PRESERVE_ENABLED \u2014 Valid values (either True or False) in the existing enabled fields are preserved. This is the default. NO_PRESERVE_ENABLED \u2014 Valid values (either True or False) in the existing enabled fields are not preserved.", "dataType": "Boolean"}]},
{"syntax": "ExportXMLWorkspaceDocument_management (in_data, out_file, {export_type}, {storage_type}, {export_metadata})", "name": "Export XML Workspace Document (Data Management)", "description": "\r\n Creates a readable XML document of the geodatabase contents.\r\n  The XML Workspace Document is useful for sharing geodatabase schemas or copying geodatabase schemas from one type to another.", "example": {"title": "ExportXMLWorkspaceDocument example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ExportXMLWorkspaceDocument tool in immediate mode.", "code": "import arcpy arcpy.ExportXMLWorkspaceDocument_management ( \"c:/data/StJohns.gdb\" , \"c:/data/StJohns.xml\" , \"SCHEMA_ONLY\" , \"BINARY\" , \"METADATA\" )"}, "usage": ["\r\n The output can be created as an XML file or as a compressed ZIP file that contains the XML file. To create an XML file, give the output file name  a ", ".xml", " extension. To create a compressed ZIP file, give the output file name a ", ".zip", " or ", ".z", " extension.", "The output XML workspace document can be very large when you copy both the data and the schema. Generally, this alternative is not recommended for copying geodatabase data. Alternatives for making a geodatabase copy include the use of the ", "Copy", " tool or the ", "Clip", " tool to extract subsets of the data. ", "If the input is a geodatabase or a feature dataset, all data elements contained within that workspace are exported. If you only want to export a subset of data elements to an XML file, you will need to copy them to a new geodatabase to be exported.", "If you export a feature class in a network, topology, relationship class, or terrain, all the feature classes participating in the network, topology, relationship class, or terrain are also exported.", "The tool messages will include the list of data element names that were exported."], "parameters": [{"name": "in_data", "isInputFile": true, "isOptional": false, "description": " The input datasets to be exported and represented in an XML workspace document. The input data can be a geodatabase, feature dataset, feature class, table, raster, or raster catalog. ", "dataType": "Feature Class; Feature Dataset; Raster Dataset; Table; Workspace"}, {"name": "out_file", "isOutputFile": true, "isOptional": false, "description": " The XML workspace document file to be created. This can be an XML file ( .xml ) or a compressed ZIP file ( .zip or .z ). ", "dataType": "File"}, {"name": "export_type", "isOptional": true, "description": " Determines if the output XML workspace document will contain all of the data from the input (table and feature class records, including geometry) or only the schema. DATA \u2014 Export both the schema and the data. This is the default. SCHEMA_ONLY \u2014 Export the schema only.", "dataType": "String"}, {"name": "storage_type", "isOptional": true, "description": " Determines how feature geometry is stored when data is exported from a feature class. BINARY \u2014 Store geometry in a compressed base64 binary format. This binary format will produce a smaller XML workspace document. Use this option when the XML workspace document will be read by a custom program that uses ArcObjects. This is the default. NORMALIZED \u2014 The geometry will be stored in an uncompressed format, resulting in a larger file. Use this option when the XML workspace document will be read by a custom program that does not use ArcObjects.", "dataType": "String"}, {"name": "export_metadata", "isOptional": true, "description": "Determines if metadata will be exported. METADATA \u2014 If the input contains metadata, it will be exported. This is the default. NO_METADATA \u2014 Do not export metadata.", "dataType": "Boolean"}]},
{"syntax": "TraceGeometricNetwork_management (in_geometric_network, out_network_layer, in_flags, in_trace_task_type, {in_barriers}, {in_junction_weight}, {in_edge_along_digitized_weight}, {in_edge_against_digitized_weight}, {in_disable_from_trace}, {in_trace_ends}, {in_trace_indeterminate_flow}, {in_junction_weight_filter}, {in_junction_weight_range}, {in_junction_weight_range_not}, {in_edge_along_digitized_weight_filter}, {in_edge_against_digitized_weight_filter}, {in_edge_weight_range}, {in_edge_weight_range_not})", "name": "Trace Geometric Network (Data Management)", "description": "\r\n Solves the specified network analysis problem based on the flags, barriers, and specified weight properties.\r\n", "example": {"title": "TraceGeometricNetwork example 1 (stand-alone script)", "description": "The following stand-alone Python script demonstrates how to use the TraceGeometricNetwork function in \r\nPython scripting to run a Find Connected Trace on a geometric network with Flags and no Barriers.", "code": "# Import arcpy module import arcpy # Local variables: gnVersionFDS_Net = \"C:/GeometricNetworks/GeometricNetwork.gdb/gnVersionFDS_with_GN/gnVersionFDS_1_Net\" Flags = \"C:/GeometricNetworks/GeometricNetwork.gdb/gnVersionFDS_with_GN/Flags\" gnVersionFDS_1 = \"gnVersionFDS_1_Net\" # Process: Trace Geometric Network arcpy.gp.TraceGeometricNetwork ( gnVersionFDS_Net , gnVersionFDS_1 , Flags , \"FIND_CONNECTED\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"NO_TRACE_ENDS\" , \"\" , \"\" , \"\" , \"AS_IS\" , \"\" , \"\" , \"\" , \"AS_IS\" )"}, "usage": ["This tool does not work with network datasets (the networks used by the ArcGIS Network Analyst extension).", "This  tool performs a spatial search using the input flags and barriers to locate coincident features.  This is used, in the case of flags, to determine which features the trace should originate from and, in the case of barriers, to determine which features should block the trace.  Therefore, the input flags and barriers must be precisely located.  In ModelBuilder, if you are uncertain whether the flags and barriers are located correctly, you can use the ", "Snap", " tool in order to specify which features the flags and barriers should be coincident with.", "The output of this tool is a group layer containing one layer for each feature class contained within the input geometric network.  Any features returned from the trace will be selected within their respective layer.  When the input layers are present in the map, the output group layer may contain layers without a selection; these are empty layers from which no features were returned from the trace.  In ModelBuilder, you can use the ", "Select Data", " tool to extract specific layers from the group layer and the ", "Make_Feature_Layer", " tool to create a feature layer from the output of the ", "Select Data", " tool.  Use the ", "Get_Count", " tool to determine if any of the layers in the group layer are empty.   This is especially useful when publishing your model to ArcGIS Server, since  group layers are not supported as an output parameter type from a geoprocessing task.  Because they aren't supported as direct output, you need to use the ", "Select Data", " tool to create a single (non-group) layer for output.", "This tool cannot be run while editing a geometric network in a personal geodatabase.", "Unlike other datasets, such as topology or network datasets, geometric networks do not have an associated layer; therefore, when using this tool in ArcMap, the geometric network must be selected from disk.  There is no option to select the geometric network from a layer drop-down list as input."], "parameters": [{"name": "in_geometric_network", "isInputFile": true, "isOptional": false, "description": "The geometric network on which the trace will be performed. ", "dataType": "Geometric Network"}, {"name": "out_network_layer", "isOutputFile": true, "isOptional": false, "description": " The name of the group layer that will store the results of the trace as a selected set. ", "dataType": "Group Layer"}, {"name": "in_flags", "isInputFile": true, "isOptional": false, "description": " A point feature class representing a set of flags which serve as starting points for the tracing operation. For example, if you are performing an upstream trace, you use a flag to specify where the upstream trace will begin. Flags can be placed anywhere along edges or junctions, but junctions will be considered first when both a junction and edge are found at the location. ", "dataType": "Feature Layer"}, {"name": "in_trace_task_type", "isInputFile": true, "isOptional": false, "description": "Trace task to be performed on the specified geometric network. FIND_COMMON_ANCESTORS \u2014 Find the common features that are upstream of a set of points in your network. Requires flow direction to be set on the geometric network. FIND_CONNECTED \u2014 Find the features that are connected to a given point through your network. FIND_LOOPS \u2014 Find loops within the network that are defined by determining for each connected feature on which you placed a flag, the features that loop back on themselves (that is, can't be reached from more than one direction). FIND_DISCONNECTED \u2014 Find all of the features that are not connected to a given point through your network. FIND_PATH_UPSTREAM \u2014 Find an upstream path from a point in your network. The path found can be just one of a number of paths between the flags depending on whether or not your network contains loops. The flags you place on the network must be either all edge flags or all junction flags. Requires a weight to be defined for the trace and flow direction to be set on the geometric network. When a weight is not specified, the shortest path is determined by the least number of features between the two points. FIND_PATH \u2014 Find a path between two or more flags in the network. The path found can be just one of a number of paths between the flags depending on whether or not your network contains loops. The flags you place on the network must be either all edge flags or all junction flags. When a weight is not specified, the shortest path is determined by the least number of features between the two points. TRACE_DOWNSTREAM \u2014 Find all network features that lie downstream (with flow direction) of a given point in your network. Requires flow direction to be set on the geometric network. FIND_UPSTREAM_ACCUMULATION \u2014 Determine the total cost of all network features that lie upstream of a given point in your network. Requires a weight be defined for the trace and flow direction to be set on the geometric network. TRACE_UPSTREAM \u2014 Find all features that lie upstream (against flow direction) of a given point in your network. Requires flow direction to be set on the geometric network.", "dataType": "String"}, {"name": "in_barriers", "isInputFile": true, "isOptional": true, "description": " A point feature class representing a set of barriers defining places in the network past which traces cannot continue. If you are only interested in tracing on a particular part of your network, you can use barriers to isolate that part of the network. Barriers can be placed anywhere along edges or junctions, but junctions will be considered first when both a junction and edge are found at the location. The feature will be treated as disabled and will not be considered during the trace, unless you've set the Limit results to features stopping the trace parameter to TRACE_ENDS parameter to purposefully find features stopping the trace. ", "dataType": "Feature Layer"}, {"name": "in_junction_weight", "isInputFile": true, "isOptional": true, "description": "A junction weight that is used as a cost for traversing through any junction. The weight must already be defined for the given geometric network. This parameter is disabled or ignored when one of the following cost-independent trace task types is specified: FIND_COMMON_ANCESTORS FIND_CONNECTED FIND_LOOPS FIND_DISCONNECTED TRACE_DOWNSTREAM TRACE_UPSTREAM", "dataType": "String"}, {"name": "in_edge_along_digitized_weight", "isInputFile": true, "isOptional": true, "description": " An edge weight that is used as a cost for traversing through an edge along the digitized direction of that edge. The weight must already be defined for the given geometric network. This parameter is disabled or ignored when one of the following cost-independent trace task types is specified: FIND_COMMON_ANCESTORS FIND_CONNECTED FIND_LOOPS FIND_DISCONNECTED TRACE_DOWNSTREAM TRACE_UPSTREAM", "dataType": "String"}, {"name": "in_edge_against_digitized_weight", "isInputFile": true, "isOptional": true, "description": " An edge weight that is used as a cost for traversing through an edge against the digitized direction of that edge. The weight must already be defined for the given geometric network. This parameter is disabled or ignored when one of the following cost-independent trace task types is specified: FIND_COMMON_ANCESTORS FIND_CONNECTED FIND_LOOPS FIND_DISCONNECTED TRACE_DOWNSTREAM TRACE_UPSTREAM", "dataType": "String"}, {"name": "in_disable_from_trace", "isInputFile": true, "isOptional": false, "description": " List of feature classes that are disabled from participating in the trace. Specifying a feature class as disabled makes the trace operation treat all features in that feature class as either being disabled or as having a barrier placed on them. Use this option to exclude an entire feature class from consideration during a trace. For example, by disabling the switches layer in an electrical distribution network, setting the Limit results to features stopping the trace parameter to TRACE_ENDS and tracing from a given point in the network, you can find the switches that need to be thrown to isolate this point from the network; these will be the features at which the trace operation is stopped. ", "dataType": "String"}, {"name": "in_trace_ends", "isInputFile": true, "isOptional": true, "description": "Indicates whether the trace should include all features or only those stopping the trace. Use this option when you need to determine which features are stopping a trace. In order to be returned from the trace operation with this option, features must fall into one of the following categories: The feature is connected to only one other geometric network feature (dead ends). The feature is disabled (including those in disabled feature classes). The feature has a barrier placed on it. TRACE_ENDS \u2014 Include those features stopping the trace. NO_TRACE_ENDS \u2014 Include all features. This is the default.", "dataType": "Boolean"}, {"name": "in_trace_indeterminate_flow", "isInputFile": true, "isOptional": true, "description": "Indicates whether the trace should include all features or only those stopping the trace. Only honored when one of flow-dependent trace task type is set: TRACE_INDETERMINATE_FLOW \u2014 Trace features that have indeterminate or uninitialized flow direction. NO_TRACE_INDETERMINATE_FLOW \u2014 Do not trace features that have indeterminate or uninitialized flow direction. This is the default. FIND_PATH_UPSTREAM TRACE_DOWNSTREAM FIND_UPSTREAM_ACCUMULATION TRACE_UPSTREAM", "dataType": "Boolean"}, {"name": "in_junction_weight_filter", "isInputFile": true, "isOptional": true, "description": "The weight to use to create the junction weight filter which is used to filter junction features during the trace. ", "dataType": "String"}, {"name": "in_junction_weight_range", "isInputFile": true, "isOptional": true, "description": " Specifies valid or invalid ranges of weight values for network features that can be traced. It is disabled when a cost-independent trace task type is set. In order to create a weight filter, you must specify valid weight ranges for the features. A weight filter can be composed of a number of ranges. When you specify multiple weight ranges, you must delimit the ranges with commas. Low and high values in a range are separated by a hyphen. Ranges consisting of a single value do not contain a hyphen and are delimited with commas, for example, 0-2,3,6,7-10. ", "dataType": "String"}, {"name": "in_junction_weight_range_not", "isInputFile": true, "isOptional": true, "description": "Applies the logical NOT operator to the specified junction weight ranges. By default, the junction weight ranges that you enter specify junction features that can be traced. By checking this option, you indicate that junction features with weights in the ranges you entered cannot be traced. AS_IS \u2014 Weight ranges specify features that can be traced. This is the default. NOT \u2014 Weight ranges specify features that cannot be traced.", "dataType": "Boolean"}, {"name": "in_edge_along_digitized_weight_filter", "isInputFile": true, "isOptional": true, "description": "The weight to use to create the along edge weight filter, which is used to filter edge features during the trace. ", "dataType": "String"}, {"name": "in_edge_against_digitized_weight_filter", "isInputFile": true, "isOptional": true, "description": "The weight to use to create the against edge weight filter, which is used to filter edge features during the trace. ", "dataType": "String"}, {"name": "in_edge_weight_range", "isInputFile": true, "isOptional": true, "description": " Specifies valid or invalid ranges of weight values for network features that can be traced. It is disabled when a cost-independent trace task type is set. In order to create a weight filter, you must specify valid weight ranges for the features. A weight filter can be composed of a number of ranges. When you specify multiple weight ranges, you must delimit the ranges with commas. Low and high values in a range are separated by a hyphen. Ranges consisting of a single value do not contain a hyphen and are delimited with commas, for example, 0-2,3,6,7-10. ", "dataType": "String"}, {"name": "in_edge_weight_range_not", "isInputFile": true, "isOptional": true, "description": "Applies the logical NOT operator to the specified edge weight ranges. By default, the edge weight ranges that you enter specify edge features that can be traced. By checking this option, you indicate that edge features with weights in the ranges you entered cannot be traced. AS_IS \u2014 Weight ranges specify features that can be traced. This is the default. NOT \u2014 Weight ranges specify features that cannot be traced.", "dataType": "Boolean"}]},
{"syntax": "RemoveConnectivityRuleFromGeometricNetwork_management (in_geometric_network, in_connectivity_rules)", "name": "Remove Connectivity Rule From Geometric Network (Data Management)", "description": "\r\nRemoves a connectivity rule from the geometric network.\r\n", "example": {"title": null, "description": "The following stand-alone Python script demonstrates how to use the \r\nRemoveConnectivityRuleFromGeometricNetwork to remove an edge-junction connectivity rule \r\nfrom a geometric network.", "code": "# Import arcpy module import arcpy # Local variables: Water_Net = \"C: \\\\ testing \\\\ GeometricNetworks \\\\ Montgomery.gdb \\\\ Water \\\\ Water_Net\" # Process: Remove Connectivity Rule From Geometric Network arcpy.gp.RemoveConnectivityRuleFromGeometricNetwork ( Water_Net , \"Distribmains; Distribmains;Fittings;Tap\" )"}, "usage": ["\r\nThe feature classes specified must reside in the geometric network.", "\r\nRemoval of an edge-edge rule does not remove the corresponding edge-junction rules.  Conversely, removal of an edge-junction rule does not remove the corresponding edge-edge rule."], "parameters": [{"name": "in_geometric_network", "isInputFile": true, "isOptional": false, "description": " The geometric network from which the connectivity rule will be removed. ", "dataType": "Geometric Network"}, {"name": "in_connectivity_rules", "isInputFile": true, "isOptional": false, "description": " For Edge-Junction rules, the edge feature class with subtype and junction feature class with subtype. For Edge-Edge rules, the from edge feature class with subtype, the to edge feature class with subtype and junction with subtype. ", "dataType": "String"}]},
{"syntax": "ReconcileVersions_management (input_database, reconcile_mode, {target_version}, {edit_versions}, {acquire_locks}, {abort_if_conflicts}, {conflict_definition}, {conflict_resolution}, {with_post}, {with_delete}, {out_log})", "name": "Reconcile Versions (Data Management)", "description": "\r\nReconciles a version or multiple versions against a target version.\r\n", "example": {"title": "ReconcileVersions example (stand-alone script):", "description": "The following stand-alone script demonstrates how to use the ReconcileVersions tool to reconcile all versions owned by the user specified in the SDE  connection file.", "code": "# Name: ReconcileVersions.py # Description: reconciles all versions owned by a user with SDE.Default # Import system modules import arcpy , os from arcpy import env # set workspace workspace = 'Database Connections//bender@production.sde' # set the workspace environment env.workspace = workspace # Use a list comprehension to get a list of version names where the owner # is the current user and make sure sde.default is not selected. verList = [ ver.name for ver in arcpy.da.ListVersions () if ver.isOwner == True and ver.name.lower () != 'sde.default' ] arcpy.ReconcileVersions_management ( workspace , \"ALL_VERSIONS\" , \"SDE.Default\" , verList , \"LOCK_AQUIRED\" , \"NO_ABORT\" , \"BY_OBJECT\" , \"FAVOR_TARGET_VERSION\" , \"NO_POST\" , \"KEEP_VERSION\" , \"c:\\RecLog.txt\" ) print 'Reconciling Complete'"}, "usage": ["The reconcile process requires that you are the only user currently editing the version and the only user able to edit the version throughout the reconcile process until you save or post.\r\n  \r\n", "The reconcile process requires that you have full permissions to all the feature classes that have been modified in the version being edited.", "Versioning tools only work with Enterprise (ArcSDE) Geodatabases. File and Personal geodatabases don't support versioning.", "The geodatabase is designed to efficiently manage and support long transactions using versions.", "The reconcile process detects differences between the edit version and the target version and flags these differences as conflicts. If conflicts exist, they should be resolved."], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": "The enterprise geodatabase that contains the versions to be reconciled. The default is to use the workspace defined in the environment. ", "dataType": "Workspace"}, {"name": "reconcile_mode", "isOptional": false, "description": " Determines which versions will be reconciled when the tool is executed. ALL_VERSIONS \u2014 Reconciles edit versions with the target version. This is the default. BLOCKING_VERSIONS \u2014 Reconciles versions that are blocking the target version from compressing. This option uses the recommended reconcile order.", "dataType": "String"}, {"name": "target_version", "isOptional": true, "description": " Name of any version in the direct ancestry of the an edit version, such as the parent version or the default version. Typically contains edits from other versions that the user performing the reconcile would like to pull into their edit version. ", "dataType": "String"}, {"name": "edit_versions", "isOptional": false, "description": "Name of the edit version or versions to be reconciled with the selected target version. This can be an individual version name or a Python list of version names. ", "dataType": "String"}, {"name": "acquire_locks", "isOptional": true, "description": "Determines whether feature locks will be acquired. LOCK_ACQUIRED \u2014 Acquires locks during the reconcile process. This should be used when the intention is to post edits. It ensures that the target version is not modified in the time between the reconcile and post operations. This is the default. NO_LOCK_ACQUIRED \u2014 No locks are acquired during the reconcile process. This allows multiple users to reconcile in parallel. It should be used when the edit version will not be posted to the target version because there is a possibility that the target version could be modified in the time between the reconcile and post operations. ", "dataType": "Boolean"}, {"name": "abort_if_conflicts", "isOptional": true, "description": "Determines if the reconcile process should be aborted if conflicts are found between the target version and the edit version. NO_ABORT \u2014 Does not abort the reconcile if conflicts are found. This is the default. ABORT_CONFLICTS \u2014 Aborts the reconcile if conflicts are found. ", "dataType": "Boolean"}, {"name": "conflict_definition", "isOptional": true, "description": "Describes the conditions required for a conflict to occur: BY_OBJECT \u2014 Any changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default. BY_ATTRIBUTE \u2014 Only changes to the same attribute of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to different attributes will not be considered a conflict during reconcile. ", "dataType": "Boolean"}, {"name": "conflict_resolution", "isOptional": true, "description": "Describes the behavior if a conflict is detected: FAVOR_TARGET_VERSION \u2014 For all conflicts, resolves in favor of the target version. This is the default. FAVOR_EDIT_VERSION \u2014 For all conflicts, resolves in favor of the edit version. ", "dataType": "String"}, {"name": "with_post", "isOptional": true, "description": "Posts the current edit session to the reconciled target version. NO_POST \u2014 Current edit version will not be posted to the target version after the reconcile. This is the default. POST \u2014 Current edit version will be posted to the target version after the reconcile. ", "dataType": "Boolean"}, {"name": "with_delete", "isOptional": true, "description": " DELETE_VERSION \u2014 Current edit version that was reconciled will be deleted after being posted to the target version. NO_DELETE_VERSION \u2014 Current edit version that was reconciled will not be deleted. This is the default. ", "dataType": "Boolean"}, {"name": "out_log", "isOutputFile": true, "isOptional": true, "description": " Specify a name and location to where the log file will be written. The log file is an ASCII file containing the contents of the geoprocessing messages. ", "dataType": "File"}]},
{"syntax": "AddEdgeEdgeConnectivityRuleToGeometricNetwork_management (in_geometric_network, in_from_edge_feature_class, from_edge_subtype, in_to_edge_feature_class, to_edge_subtype, in_junction_subtypes, default_junction_subtype)", "name": "Add Edge-Edge Connectivity Rule To Geometric Network (Data Management)", "description": "\r\n Adds an edge-edge connectivity rule to a geometric network.\r\n", "example": {"title": "AddEdgeEdgeConnectivityRuleToGeometricNetwork example (stand-alone script)", "description": "The following stand-alone Python script demonstrates how to use the \r\nAddEdgeJunctionConnectivityRuleToGeometricNetwork in Python script to add an edge-edge\r\nconnectivity rule with three different types of junctions to a geometric network.", "code": "# Import arcpy module import arcpy # Local variables: Water_Net = \"C:/data/Montgomery.gdb/Water/Water_Net\" # Process: Add Edge-Edge Connectivity Rule To Geometric Network arcpy.AddEdgeEdgeConnectivityRuleToGeometricNetwork ( Water_Net , \"Distribmains\" , \"Distribmains\" , \"Distribmains\" , \"Distribmains\" , \"Fittings,Tap;Fittings,Tee;Fittings,Bend\" , \"Fittings,Tap\" )"}, "usage": ["\r\n The feature classes specified must reside in the geometric network.", " As a by-product of adding an edge-edge rule, edge-junction rules between the from and to edge feature classes and the junction feature class will be added to the geometric network, if they do not already exist.\r\n", " The default junction subtype must be one of the specified junction subtypes."], "parameters": [{"name": "in_geometric_network", "isInputFile": true, "isOptional": false, "description": " The geometric network to which the connectivity rule will be added. ", "dataType": "Geometric Network"}, {"name": "in_from_edge_feature_class", "isInputFile": true, "isOptional": false, "description": " The name of the from edge feature class. ", "dataType": "String"}, {"name": "from_edge_subtype", "isOptional": false, "description": " The subtype description for the from edge feature class. If subtypes do not exist on the feature class, use the feature class name. ", "dataType": "String"}, {"name": "in_to_edge_feature_class", "isInputFile": true, "isOptional": false, "description": " The name of the to edge feature class. ", "dataType": "String"}, {"name": "to_edge_subtype", "isOptional": false, "description": " The subtype description for the to edge feature class. If subtypes do not exist on the feature class, use the feature class name. ", "dataType": "String"}, {"name": "in_junction_subtypes", "isInputFile": true, "isOptional": false, "description": "The junction feature classes and subtypes through which these edge feature classes or subtypes will be permitted to connect. ", "dataType": "String"}, {"name": "default_junction_subtype", "isOptional": false, "description": " Junction Subtype that will serve as the default junction subtype for the edge-edge connectivity rule. ", "dataType": "String"}]},
{"syntax": "DownloadRasters_management (in_image_service, out_folder, {where_clause}, {selection_feature}, {clipping}, {convert_rasters}, {format}, {compression_method}, {compression_quality}, {MAINTAIN_FOLDER})", "name": "Download Rasters (Data Management)", "description": "\r\nAllows you to download the source  files of the selected rasters \r\nfrom an image service or mosaic dataset.", "example": {"title": "DownloadRasters example 1 (Python window)", "description": "This is a Python sample for the DownloadRasters tool.", "code": "import arcpy DownloadRasters_management ( \"http://srv/arcgis/services/Ext/MDpan/ImageServer?\" , \"c:/dload/\" , \"AcquisitionDate = date '1999-08-18'\" , \"\" , \"c:/workspace/clippingfeat.shp\" , \"TIFF\" , \"JPEG\" , \"75\" , \"MAINTAIN_FOLDER\" , \"CONVERT_AS_REQUIRED\" )"}, "usage": ["\r\nThe raster datasets you download are the source files, unless you convert them to another format.\r\nFormat conversion can be forced, or will only occur when required. Downloaded files are converted if clipping occurs or the source file cannot be downloaded as a raster.", "You can download selected rasters or LAS files from an image service or a mosaic dataset to a specified folder in the original file format.", "If a clipping extent is specified, the rasters that intersect the clip extent will be clipped and then converted to  a specified format.", "You can choose to download the data in the same folder structure as the source."], "parameters": [{"name": "in_image_service", "isInputFile": true, "isOptional": false, "description": " The image service or mosaic dataset from which the selected rasters (footprints) will be downloaded. ", "dataType": "Image Service; Mosaic Layer; Raster Layer; String"}, {"name": "out_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder to which the source files will be downloaded. ", "dataType": "Folder"}, {"name": "where_clause", "isOptional": true, "description": "Use SQL to define a query, or use the Query Builder to build a query that identifies the raster datasets to be downloaded. ", "dataType": "SQL Expression"}, {"name": "selection_feature", "isOptional": true, "description": " Specify the extent. You can specify the extent by specifying a feature as the minimum bounding rectangle, or by specifying the coordinates of the minimum bounding rectangle. Any rasters that intersect the extent will be downloaded. ", "dataType": "Extent"}, {"name": "clipping", "isOptional": true, "description": "Specify if you want to clip the downloaded images based on the geometry of a feature. Any raster that intersects the clipping geometry will be clipped and then downloaded. This is useful when your area of interest is not a rectangle. When downloaded images are clipped, you need to specify an output format for the clipped images. NO_CLIPPING \u2014 The files will be clipped based on the minimum bounding rectangle that has been specified. This is the default. CLIPPING \u2014 The files will be clipped based on the geometry of the selection_feature.", "dataType": "Boolean"}, {"name": "convert_rasters", "isOptional": true, "description": "Choose whether to always convert your rasters to the specified format, or to only convert when it is necessary. CONVERT_AS_REQUIRED \u2014 You will download the rasters in their source file format, unless conversion is required. This is the default. ALWAYS_CONVERT \u2014 You will download the rasters with the format that you have specified.", "dataType": "Boolean"}, {"name": "format", "isOptional": true, "description": " If a clipping extent was specified, then you need to choose the output format for the clipped rasters. TIFF \u2014 Tagged Image File Format. This is the default. BIL \u2014 ESRI band interleaved by line. BSQ \u2014 ESRI band sequential. BIP \u2014 ESRI band interleaved by pixel. BMP \u2014 Bitmap. ENVI \u2014 ENVI DAT file. IMAGINE Image \u2014 ERDAS IMAGINE. JPEG \u2014 Joint Photographics Experts Group. If chosen, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100. GIF \u2014 Graphic interchange format. JP2 \u2014 JPEG 2000. If chosen, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100. PNG \u2014 Portable Network Graphics.", "dataType": "String"}, {"name": "compression_method", "isOptional": true, "description": " Choose the compression method to use with the specified Output Format. NONE \u2014 No compression will occur. This is the default. JPEG \u2014 Lossy compression that uses the public JPEG compression algorithm. If you choose JPEG, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100. This compression can be used for JPEG files and TIFF files. LZW \u2014 Lossless compression that preserves all raster cell values. PACKBITS \u2014 PackBits compression for TIFF files. RLE \u2014 Run-length encoding for IMG files. CCITT_GROUP3 \u2014 Lossless compression for 1-bit data. CCITT_GROUP4 \u2014 Lossless compression for 1-bit data. CCITT_1D \u2014 Lossless compression for 1-bit data.", "dataType": "String"}, {"name": "compression_quality", "isOptional": true, "description": "Specify the compression quality when JPEG or JP2 compression is chosen. Valid values range from 1 to 100, where a higher number means better image quality but less compression. ", "dataType": "Long"}, {"name": "MAINTAIN_FOLDER", "isOptional": true, "description": "Choose whether to have a flat folder structure or to maintain the hierarchical folder structure used to store the source raster datasets. NO_MAINTAIN_FOLDER \u2014 All the files will be downloaded into the out_folder as a flat folder structure. This is the default. MAINTAIN_FOLDER \u2014 The files will be downloaded to the out_folder in the same hierarchical structure as the source data.", "dataType": "Boolean"}]},
{"syntax": "AnalyzeMosaicDataset_management (in_mosaic_dataset, {where_clause}, {checker_keywords})", "name": "Analyze Mosaic Dataset (Data Management)", "description": "Examines a mosaic dataset to determine solutions for known errors and other issues, and to detect methods for optimization. \r\n The errors, warnings, and messages can be examined when the mosaic dataset is open within ArcMap.", "example": {"title": "AnalyzeMosaicDataset example 1 (Python window)", "description": "This is a Python sample for the AnalyzeMosaicDataset tool.", "code": "import arcpy arcpy.AnalyzeMosaicDataset_management ( \" \\\\ cpu\\data \\a nalyze.gdb\\mosaicds\" , \"SensorName = 'Landsat-7-ETM+'\" , \"FOOTPRINT;FUNCTION;RASTER;PATHS;PYRAMIDS\" )"}, "usage": ["To examine the analysis results, open the mosaic dataset in ArcMap, right-click the mosaic dataset in the table of contents and click ", "Data ", ">", " View Analysis Results", ". The ", "Prepare", " window opens allowing you to view and interact with the errors, warnings, and messages. When you right-click on an error, warning, or message, a recommended action is listed. ", "The errors and warnings are categorized in the following way:", "A message reports statistical facts related to the mosaic dataset.", "In the results table, many of the errors and warnings can be fixed by right-clicking on the issue.", "Messages do not have a solution, since there is no issue at hand."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The mosaic dataset to be analyzed. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": "You can define a query to confine the analysis to specific rasters within the mosaic dataset. Using SQL you can define a query, or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "checker_keywords", "isOptional": false, "description": "Choose which parts of the mosaic dataset you want to analyze for known issues. FOOTPRINT \u2014 The checker analyzes the footprint geometry of each selected mosaic dataset item. This is on by default. FUNCTION \u2014 The checker analyzes the raster function stack of the raster associated with each selected mosaic dataset item. This is on by default. RASTER \u2014 The checker analyzes the actual raster dataset stored within each selected row. This is on by default. PATHS \u2014 The checker analyzes for broken paths. This is on by default. STALE \u2014 The checker analyzes each mosaic dataset item for staleness. PYRAMIDS \u2014 The checker analyzes the raster pyramids associated with each mosaic dataset item in the selected mosaic dataset. The pyramids will be tested for disconnected auxiliary files. The auxiliary files may be disconnected when they are stored in the raster proxy location. PERFORMANCE \u2014 The checker analyzes the entire mosaic dataset and each selected item for performance-related issues. Examples of performance-related issues include no compression during transmission or recommending item cache on items where there are a lot of raster functions involved.This checker remains off by default. ", "dataType": "String"}]},
{"syntax": "GenerateAttachmentMatchTable_management (in_dataset, in_folder, out_match_table, in_key_field, {in_file_filter}, {in_use_relative_paths})", "name": "Generate Attachment Match Table (Data Management)", "description": "\r\nArcGIS geoprocessing tool that creates a match table to be used with the  Add Attachments  and  Remove Attachment  tools. Learn more about working with the Attachments geoprocessing tools", "example": {"title": "GenerateAttachmentMatchTable example (Python window)", "description": "The following code snippet demonstrates how to use the GenerateAttachmentMatchTable in the Python window.", "code": "import arcpy arcpy.GenerateAttachmentMatchTable_management ( \"C:/data/parcels.gdb/parcels\" , \"C:/attachment_folder\" , \"C:/data/temp.gdb/matchtable\" , \"AttachmentKeyField\" , \"*.jpg; *.pdf\" , \"ABSOLUTE\" )"}, "usage": ["\r\nThis tool will go through each row for the input target dataset and compare the ", "Key Field", " in this dataset to the names of files in the ", "Input Folder", ". For each match that occurs, a record will be created in the output table that contains the ObjectID value from the ", "Input Dataset", " and the name of the matched file (or optionally the full path to that file). When used in the ", "Add_Attachments", " and ", "Remove_Attachments", " tools the MATCHID field is used as the key field to link the files on disk to records in the input dataset.", " If the Output Match Table  location is a folder, the output can be created as a dBASE table by specifying a name with the extension.dbf, or can be created as an INFO table by specifying a name with no extension. If the Output Location is a geodatabase, the match table  will be a geodatabase table (do not specify an extension)."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " Input dataset that contains records that will have files attached. ", "dataType": "Table View"}, {"name": "in_folder", "isInputFile": true, "isOptional": false, "description": " Folder that contains files to attach. ", "dataType": "Folder"}, {"name": "out_match_table", "isOutputFile": true, "isOptional": false, "description": " Table that will be generated which contains two columns: MATCHID and FILENAME. ", "dataType": "Table"}, {"name": "in_key_field", "isInputFile": true, "isOptional": false, "description": " The values in this field will match the names of the files in the input folder. The matching behavior will ignore file extensions, which allows multiple files with various file extensions to match with a single record in the input dataset. For example, if the input Key Field value is lot5986, a file on disk named lot5986.jpg would match with this record. ", "dataType": "Field"}, {"name": "in_file_filter", "isInputFile": true, "isOptional": true, "description": "This parameter is used to limit the files the tool considers for matching. If the file name does not meet the criteria in the file filter parameter it will not be processed and therefore will not show up in the output match table. Wild cards ( * ) can be used in this parameter for more flexible filtering options. Multiple semicolon-delimited filters can be used as well. For example, consider a directory that contains the following files: parcel.tif , parcel.doc , parcel.jpg , houses.jpg , and report.pdf . To limit the possible matches in this list to .jpg files, use *.jpg . To limit the possible matches in this list to .pdf and .doc files, use *.pdf; *.doc . To limit the possible matches in this list to files beginning with parcel, use parcel* . To limit the possible matches in this list to files that contain the text arc, use *arc* . ", "dataType": "String"}, {"name": "in_use_relative_paths", "isInputFile": true, "isOptional": true, "description": " Determines if the output match table field FILENAME will contain a full path to the dataset or only the file name. RELATIVE \u2014 The output FILENAME field will contain relative paths. This is the default. ABSOLUTE \u2014 The output FILENAME field will contain full paths to the data.", "dataType": "Boolean"}]},
{"syntax": "RemoveAttachments_management (in_dataset, in_join_field, in_match_table, in_match_join_field, {in_match_name_field})", "name": "Remove Attachments (Data Management)", "description": "\r\n Removes attachments from geodatabase feature class or table records. Since attachments are not actually stored in the input dataset, no changes will be made to that feature class or table, but rather to the related geodatabase table that stores the attachments and maintains linkage with the input dataset. A match table is used to identify which input records (or attribute groups of records) will have attachments removed.\r\n", "example": {"title": "RemoveAttachments example (Python window)", "description": "The following code snippet illustrates how to use the RemoveAttachments tool in the Python window.", "code": "import arcpy arcpy.RemoveAttachments_management ( r\"C:\\Data\\City.gdb\\Parcels\" , \"ParcelID\" , r\"C:\\Data\\matchtable.csv\" , \"ParcelID\" , \"Picture\" )"}, "usage": ["\r\nAn alternative to using this tool is to delete selected records from the ", "InputDataset__ATTACH", " table in the same geodatabase as the ", "Input Dataset", ", which stores attachments and maintains linkage to the ", "Input Dataset", ".\r\n"], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " Geodatabase table or feature class from which to remove attachments. Attachments are not removed directly from this table, but rather from the related attachment table that stores the attachments. The Input Dataset must be in a version 10 or later geodatabase, and the table must have attachments enabled. ", "dataType": "Table View"}, {"name": "in_join_field", "isInputFile": true, "isOptional": false, "description": " Field from the Input Dataset that has values which match the values in the Match Join Field . Records that have join field values that match between the Input Dataset and the Match Table will have attachments removed. This field can be an Object ID field or any other identifying attribute. ", "dataType": "Field"}, {"name": "in_match_table", "isInputFile": true, "isOptional": false, "description": " Table that identifies which input records will have attachments removed. ", "dataType": "Table View"}, {"name": "in_match_join_field", "isInputFile": true, "isOptional": false, "description": " Field from the match table that indicates which records in the Input Dataset will have specified attachments removed. This field can have values that match Input Dataset Object IDs or some other identifying attribute. ", "dataType": "Field"}, {"name": "in_match_name_field", "isInputFile": true, "isOptional": true, "description": " Field from the match table that has the names of attachments to remove from Input Dataset records. If no name field is specified, all attachments will be removed from each record specified in the Match Join Field . If a name field is specified, but a record has a null or empty value in the name field, all attachments will be removed from that record. This field's values should be the short names of the attachment to remove, not the full paths to the files used to make the original attachments. ", "dataType": "Field"}]},
{"syntax": "AlterMosaicDatasetSchema_management (in_mosaic_dataset, {side_tables}, {raster_type_names})", "name": "Alter Mosaic Dataset Schema (Data Management)", "description": "Prepares the schema of a mosaic dataset in ArcSDE by generating any tables or fields that may be required. This allows the nonowner of the mosaic dataset to make modifications, such as adding raster data. This tool prevents schema-locking issues that can arise when a mosaic dataset is stored in ArcSDE. The ArcSDE owner will run this tool to create any side tables and fields that may be needed by the user. The ArcSDE owner must also  grant the proper permissions to allow users to insert, update, or delete records.", "example": {"title": "AlterMosaicDatasetSchema example 1 (Python window)", "description": "This is a Python sample for the AlterMosaicDatasetSchema tool.", "code": "import arcpy arcpy.AlterMosaicDatasetSchema_management ( \" \\\\ serv \\f older\\myFGBD.gdb\\md_01\" , \"ANALYSIS;BOUNDARY;LEVELS;OVERVIEW\" , \"QuickBird;IKONOS;Match-AT\" )"}, "usage": ["This tool is only necessary when you are creating a mosaic dataset within ArcSDE and a nonowner will be editing the mosaic dataset.", "Use this tool to set up a mosaic dataset (in a file or ArcSDE geodatabase) that will be published as an image service allowing users to upload or edit the items. A user can only upload their data if the raster type is allowed by the mosaic dataset.", "Choose the operations  that are allowed for this mosaic dataset. This will create the necessary tables that will be required. If a nonowner tries to perform an operation that is not allowed, the operation will fail.", "Choose the raster types that are allowed for this mosaic dataset. This will create the necessary fields that will be required. If a nonowner uses the ", "Add Rasters To Mosaic Dataset", " and tries to use a ", "Raster type", " that  has not been specified by this tool, the ", "Add Rasters To Mosaic Dataset", " will fail.", "\r\nIf you run this tool without choosing any raster types, only the additional mosaic dataset tables will be created. These tables include overview, seamline, color correction, stereo, cell size levels, status, error, and permissions.\r\n"], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The input mosaic dataset schema to alter. ", "dataType": "Mosaic Layer"}, {"name": "side_tables", "isOptional": false, "description": "Choose all the operations that are allowable for this mosaic dataset. The proper tables will be created. ANALYSIS \u2014 Choose this option if a nonowner is allowed to run the Analyze Mosaic Dataset tool on the mosaic dataset. BOUNDARY \u2014 Choose this option if a nonowner is allowed to create or edit the boundary of the mosaic dataset. This is also required if a nonowner will add rasters outside of the existing boundary. COLOR_CORRECTION \u2014 Choose this option if a nonowner is allowed to color correct the mosaic dataset. LEVELS \u2014 Choose this option if a nonowner is allowed to calculate cell size ranges or create seamlines for the mosaic dataset. LOG \u2014 Choose this option if a nonowner is allowed to create a log table for the mosaic dataset. OVERVIEW \u2014 Choose this option if a nonowner is allowed to create overviews for the mosaic dataset. SEAMLINE \u2014 Choose this option if a nonowner is allowed to create seamlines for the mosaic dataset. STEREO \u2014 Choose this option if a nonowner is allowed to define stereo pairs for the mosaic dataset.", "dataType": "String"}, {"name": "raster_type_names", "isOptional": false, "description": " Choose all the raster types that are allowable for this mosaic dataset. The proper fields will be created. If you wish to use a custom raster type, then type the path of the custom raster type file. CADRG/ECRG \u2014 CIB \u2014 DTED \u2014 FORMOSAT-2 \u2014 GeoEye-1 \u2014 HRE \u2014 IKONOS \u2014 KOMPSAT-2 \u2014 LAS \u2014 Landsat 1-5 MSS \u2014 Landsat 4-5 TM \u2014 Landsat 7 ETM+ \u2014 NITF \u2014 Quickbird \u2014 RADARSAT-2 \u2014 RapidEye \u2014 Raster Process Definition \u2014 SPOT 5 \u2014 WorldView-1 \u2014 WorldView-2 \u2014", "dataType": "String"}]},
{"syntax": "EnableAttachments_management (in_dataset)", "name": "Enable Attachments (Data Management)", "description": "\r\n Enables attachments on a geodatabase feature class or table. Creates the necessary attachment relationship class and attachment table that will internally store attachment files.\r\n", "example": {"title": "EnableAttachments example (Python window)", "description": "The following code snippet illustrates how to use the EnableAttachments tool in the Python window.", "code": "import arcpy arcpy.EnableAttachments_management ( r\"C:\\Data\\City.gdb\\Parcels\" )"}, "usage": ["Attachments must first be enabled using this tool before they can be added using the ", "Add Attachments", " tool.", "\r\nIf the geodatabase feature class or table already has attachments enabled, a warning message will be delivered and no processing will take place.\r\n"], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " Geodatabase table or feature class for which attachments will be enabled. The input must be in a version 10 or later geodatabase. ", "dataType": "Table View"}]},
{"syntax": "DisableAttachments_management (in_dataset)", "name": "Disable Attachments (Data Management)", "description": "\r\n Disables attachments on a geodatabase feature class or table. Deletes the attachment relationship class and attachment table.\r\n", "example": {"title": "DisableAttachments example (Python window)", "description": "The following code snippet illustrates how to use the DisableAttachments tool in the Python window.", "code": "import arcpy arcpy.DisableAttachments_management ( r\"C:\\Data\\City.gdb\\Parcels\" )"}, "usage": ["\r\nThis tool permanently deletes all attachments internally stored in the geodatabase and associated with the ", "Input Dataset", ". If attachments are enabled after being disabled, no attachments that were previously associated with the feature class or table will be present.\r\n", "If the geodatabase feature class or table does not have attachments enabled, a warning message will be delivered and no processing will\r\ntake place.\r\n"], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " Geodatabase table or feature class for which attachments will be disabled. The input must be in a version 10 or later geodatabase. ", "dataType": "Table View"}]},
{"syntax": "MakeLasDatasetLayer_management (in_las_dataset, out_layer, {class_code}, {return_values}, {no_flag}, {synthetic}, {keypoint}, {withheld}, {surface_constraints})", "name": "Make LAS Dataset Layer (Data Management)", "description": "\r\nCreates a  LAS dataset layer that can apply  filters on LAS files and enable or disable surface constraints referenced by a LAS dataset.", "example": {"title": "MakeLasDatasetLayer example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.MakeLasDatasetLayer_management ( 'Baltimore.lasd' , 'Baltimore Layer' , 2 , 'LAST' , 'INCLUDE_UNFLAGGED' , 'EXCLUDE_SYNTHETIC' , 'INCLUDE_KEYPOINT' , 'EXCLUDE_WITHHELD' )"}, "usage": ["This tool creates  a temporary layer stored in memory that can be preserved as a layer file by using the ", "Save To Layer File", " tool.", "LAS points can be classified into a number of categories that describe the material encountered by the lidar return, such as  ground, building, or water. The American Society for Photogrammetry and Remote Sensing (ASPRS) defined the following class codes for LAS file versions  1.1, 1.2, and 1.3:", "Classification Value ", "Classification Type", "0", "Never Classified", "1", "Unassigned", "2", "Ground", "3", "Low Vegetation", "4", "Medium Vegetation", "5", "High Vegetation", "6", "Building", "7", "Noise", "8", "Model Key", "9", "Water", "10", "Reserved for ASPRS Definition", "11", "Reserved for ASPRS Definition", "12", "Overlap", "13\u201331", "Reserved for ASPRS Definition", "While the LAS 1.0 specifications provide class codes ranging  from 0 to 255, it does not have a standardized classification scheme. Any class codes used in 1.0 files would typically be defined by the data vendor and provided through auxiliary information.", "The layer can be used as a staging ground for generating input into other geoprocessing tools that accept a LAS dataset. For example, if a raster or TIN dataset is needed from first return bare earth lidar data, the filter options can be used to select the second class code and  first return value. The resulting LAS dataset layer could then be used in the ", "LAS Dataset To Raster", " tool or the ", "LAS Dataset To TIN", " tool to create the desired output.   "], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": " The name of the resulting LAS dataset layer. Text followed by a backslash or forward slash will be used to denote a group layer. ", "dataType": "Las Dataset Layer"}, {"name": "class_code", "isOptional": false, "description": "The classification codes to use as a query filter for LAS data points. Valid values range from 1 to 32. No filter is applied by default. ", "dataType": "Long"}, {"name": "return_values", "isOptional": false, "description": "Specifies the return values to be used for filtering the data points. When nothing is specified, all returns are used. Valid return options include any number from 1 to 5 and the following keywords: LAST_RETURN \u2014 The last return available for each lidar pulse. FIRST_OF_MANY \u2014 The first return available for each lidar pulse with multiple returns. LAST_OF_MANY \u2014 The last return available for each lidar pulse with multiple returns. SINGLE_RETURN \u2014 The data point from a lidar pulse that only has one return.", "dataType": "String"}, {"name": "no_flag", "isOptional": true, "description": " Indicates whether data points that do not have any classification flags assigned should be enabled for display and analysis. INCLUDE_UNFLAGGED \u2014 Unflagged points will be displayed. This is the default. EXCLUDE_UNFLAGGED \u2014 Unflagged points will not be displayed.", "dataType": "Boolean"}, {"name": "synthetic", "isOptional": true, "description": " Indicates whether data points flagged as synthetic, or points that originated from a data source other than lidar, should be enabled for display and analysis.. INCLUDE_SYNTHETIC \u2014 Synthetic points will be displayed. This is the default. EXCLUDE_SYNTHETIC \u2014 Synthetic points will not be displayed.", "dataType": "Boolean"}, {"name": "keypoint", "isOptional": true, "description": " Indicates whether data points flagged as model key-points, or significant measurements that should not be thinned away, should be enabled for display and analysis.. INCLUDE_KEYPOINT \u2014 Model key-points will be displayed. This is the default. EXCLUDE_KEYPOINT \u2014 Model key-points will not be displayed.", "dataType": "Boolean"}, {"name": "withheld", "isOptional": true, "description": " Indicates whether data points flagged as withheld, which typically represent unwanted noise measurements, should be enabled for display and analysis. EXCLUDE_WITHHELD \u2014 Withheld points will not be displayed. This is the default. INCLUDE_WITHHELD \u2014 Withheld points will be displayed.", "dataType": "Boolean"}, {"name": "surface_constraints", "isOptional": false, "description": "The name of the surface constraint features that will be enabled in the layer. All constraints are enabled by default. ", "dataType": "String"}]},
{"syntax": "ErasePoint_edit (in_features, remove_features, {operation_type})", "name": "Erase Point (Editing)", "description": "Deletes points from the input that are either inside or outside the  Remove Features , depending on the  Operation Type .", "example": {"title": "ErasePoint Example 1 (stand-alone script)", "description": null, "code": "# Name: ErasePoint_Example.py # Description: Erase points inside polygon features # Requirements:  # Author: ESRI import arcpy from arcpy import env env.workspace = \"C:/data\" inFeatures = \"wells.shp\" removeFeatures = \"land.shp\" operationType = \"INSIDE\" try : arcpy.ErasePoint_edit ( inFeatures , removeFeatures , operationType ) except Exception , e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"Line  %i \" % tb.tb_lineno print e.message"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "To delete points inside or on the boundary of the ", "Remove Features", ", use operation type INSIDE. To delete points outside the ", "Remove Features", ", use operation type OUTSIDE.", "For multipoint input features, only points inside or outside the ", "Remove Features", " will be deleted, depending on the ", "Operation Type", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features. ", "dataType": "Feature layer"}, {"name": "remove_features", "isOptional": false, "description": "Input features inside or outside the Remove Features will be deleted, depending on the Operation Type parameter. ", "dataType": "Feature Layer"}, {"name": "operation_type", "isOptional": true, "description": "Determines if points INSIDE or OUTSIDE the Remove Features will be deleted. INSIDE \u2014 Input point features inside or on the boundary of the Remove Features will be deleted. OUTSIDE \u2014 Input point features outside the Remove Features will be deleted.", "dataType": "String"}]},
{"syntax": "TrimLine_edit (in_features, {dangle_length}, {delete_shorts})", "name": "Trim Line (Editing)", "description": " Removes portions of a line that extend a specified distance past a line intersection (dangles). Any line that does not touch another line at both endpoints can be trimmed, but only the portion of the line that extends past the intersection by the specified distance will be removed. Tool use is intended for quality control tasks such as cleaning up topology errors in features that were digitized without having set proper snapping environments.", "example": {"title": "Trim Line Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Trim Line tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.TrimLine_edit ( \"majorrds.shp\" , \"15 Feet\" , \"DELETE_SHORT\" )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", " Any line that does not touch another line at both endpoints can be trimmed. There are two types of lines that meet this condition:", "If no ", "Dangle Length", " is specified, all dangling lines (line segments that do not touch another line at both endpoints), regardless of length, will be trimmed back to the point of intersection.", "If the ", "Delete Short Features", " option is used, any free-standing or stand-alone features with a total length less than or equal to the specified Dangle Length will be deleted. If this option is not used, only dangling line segments will be trimmed.", "If a feature layer or feature class with a selection is used as the input, only the selected features will be trimmed. All features within the layer or feature class will be used to assess if a dangle exists and should be deleted, but only the selected features will be modified.", "This tool will not change any feature attributes, except the geometry (length) fields in a personal, file, or ArcSDE geodatabase feature class."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The line input features to be trimmed. ", "dataType": "Feature Layer"}, {"name": "dangle_length", "isOptional": true, "description": "Line segments that are shorter than the specified Dangle Length and do not touch another line at both endpoints (dangles) will be trimmed. If no Dangle Length is specified, all dangling lines (line segments that do not touch another line at both endpoints), regardless of length, will be trimmed back to the point of intersection. ", "dataType": "Linear Unit"}, {"name": "delete_shorts", "isOptional": true, "description": "Controls whether line segments which are less than the dangle length and are free-standing will be deleted. DELETE_SHORT \u2014 Delete short free-standing features. This is the default. KEEP_SHORT \u2014 Do not delete short free-standing features.", "dataType": "Boolean"}]},
{"syntax": "Snap_edit (in_features, snap_environment)", "name": "Snap (Editing)", "description": "Moves points or vertices to coincide exactly with the vertices, edges, or end points of other features. Snapping rules can be specified to control whether the input vertices are snapped to the nearest vertex, edge, or endpoint within a specified distance.", "example": {"title": "Snap Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Snap tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.Snap_edit ( \"climate.shp\" , [[ \"Habitat_Analysis.gdb/vegtype\" , \"VERTEX\" , \"30 Feet\" ], [ \"Habitat_Analysis.gdb/vegtype\" , \"EDGE\" , \"20 Feet\" ]])"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "The ", "Snap Environment", " parameter allows for the vertices of the input features to be snapped to the vertices, edges, and end points of multiple layers or feature classes. When multiple snapping rules are given, they are prioritized as follows: from top to bottom in the tool dialog or from left to right in scripting.", "The input features' vertices will be snapped to the nearest vertex, edge, or end point within the specified distance.", "In the ", "Snap Environment", " parameter, multiple snap rules can be designated using the same layer or feature class with a different type (END | VERTEX | EDGE).", "If a layer or feature class with a selection is used as the input, only vertices of the selected features will be snapped.", "When snapping features in one feature class to features in the same feature class, the feature with the lower Object or Feature ID will ", "usually", " be snapped to the feature with the higher Object ID. For example, if points OBJECTID 1 and OBJECTID 2 are within the snapping distance, the point with OBJECTID 1 will be snapped to the location of the point with OBJECTID 2 (and not vice versa). Use the Sort tool to rearrange features so this snapping behavior can be controlled.", "When the tool is used in ArcMap, the Snap Environment of the application is defaulted into the ", "Snap Environment", " parameter, where it can be modified if desired.", "One use case for this tool is to rectify the differences in shared or common boundaries between two datasets by snapping the vertices in one boundary to the vertices, edges, or end points of the other. If the input features do not have enough vertices to match the exact curvature of the other boundary, vertices can be added to the input features using the ", "Densify", " tool to allow for an added level of detail."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features whose vertices will be snapped to the vertices, edges, or end points of other features. The input features can be points, multipoints, lines, or polygons. ", "dataType": "Feature Layer"}, {"name": "snap_environment", "isOptional": false, "description": "Snapping Environment Components: Snapping Environment Type Options: In the Snap Environment parameter, if no unit is entered with the Distance (i.e., '10' instead of '10 Meters'), the linear or angular unit from the input feature's coordinate system will be used as default. If the input features have a projected coordinate system, the linear unit will be used. Features -- Features that the input features' vertices will be snapped to. These features can be points, multipoints, lines, or polygons. Type -- Type of feature part that the input features' vertices can be snapped to (END | VERTEX | EDGE). Distance -- Distance within which the input features' vertices will be snapped to the nearest vertex, edge, or end point. END \u2014 Input feature vertices will be snapped to feature ends. VERTEX \u2014 Input feature vertices will be snapped to feature vertices. EDGE \u2014 Input feature vertices will be snapped to feature edges.", "dataType": "Value Table"}]},
{"syntax": "Generalize_edit (in_features, {tolerance})", "name": "Generalize (Editing)", "description": "Simplifies the input features using the Douglas-Peucker simplification algorithm with a specified maximum offset tolerance. The output features will contain a subset of the original input vertices. ", "example": {"title": "Generalize Example (Python Window)", "description": "The following Python window script demonstrates how to use the Generalize function in immediate mode:", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\\data.gdb\" arcpy.Generalize_edit ( \"zones\" , \"10 Feet\" )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "This tool uses the same algorithm as the ", "Simplify Line", " tool's ", "point remove method", ". The  ", "Simplify Line", " tool provides more parameters and creates a new output, whereas this tool modifies the input feature class.", "This tool generalizes features record by record. Sections of lines and polygon boundaries which were coincident between features may not be conincident after the tool is run.", "B\u00e9zier curve, circular arc, elliptic arc segments will be converted to a set of straight line segments. ", "This tool does not delete records or features.  If the tolerance is larger than a polygon, the polygon will be reduced to three vertices."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The polygon or line features to be generalized. ", "dataType": "Feature Layer"}, {"name": "tolerance", "isOptional": true, "description": "The tolerance sets the maximum allowable offset, which will determine the degree of simplification. This value limits the distance the output geometry can differ from the input geometry. You can specify a preferred unit of measurement. The default is the feature unit. ", "dataType": "Linear unit"}]},
{"syntax": "FlipLine_edit (in_features)", "name": "Flip Line (Editing)", "description": "Reverses the from-to direction of line features. You can view the orientation of line features by symbolizing line features with arrowheads. ", "example": {"title": "FlipLine Example(stand-alone script)", "description": "This example shows how to use Python stand-alone script to do in-place editing.", "code": "# Name: Flipline_Example.py # Description: Flip line features # Requirements:  # Author: ESRI import arcpy from arcpy import env env.workspace = \"C:/data\" inFeatures = \"harvestable.shp\" try : arcpy.FlipLine_edit ( inFeatures ) except Exception , e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"Line  %i \" % tb.tb_lineno print e.message"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "Direction-dependent attributes, such as address ranges, are not flipped----only the geometry is flipped. For example, suppose a line feature has an LF-ADD attribute ('left from address') of 100 and an LT-ADD ('left-to address') of 198. These values will not change if the line is flipped. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. This must be Polyline. ", "dataType": "Feature Layer"}]},
{"syntax": "ExtendLine_edit (in_features, {length}, {extend_to})", "name": "Extend Line (Editing)", "description": "This tool extends line segments to the first intersecting feature within a specified distance. If no intersecting feature is within the specified distance, the line segment will not be extended. Tool use is intended for quality control tasks such as cleaning up topology errors in features that were digitized without having set proper snapping environments.", "example": {"title": "Extend Line Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Extend Line tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.ExtendLine_edit ( \"majorrds.shp\" , \"15 Feet\" , \"EXTENSION\" )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "If no ", "Extend Length", " is specified, segments will be extended to the first intersecting feature.", "If the ", "Extend to Extensions", " option is used, line segments can be extended to existing line features as well as other extended line segments within the specified extend length, forming intersections between two extensions.", "A feature can be extended to itself, if it is the first intersecting feature.", "If a feature layer or feature class with a selection is used as the input, only the selected features will be extended. All features within the layer or feature class will be used to assess if an extension can be performed, but only the selected features will be modified.", "This tool will not change any feature attributes, except the managed geometry (length) fields in a personal, file, or SDE geodatabase feature class."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The line input features to be extended. ", "dataType": "Feature Layer"}, {"name": "length", "isOptional": true, "description": "The maximum distance a line segment can be extended to an intersecting feature. ", "dataType": "Linear Unit"}, {"name": "extend_to", "isOptional": true, "description": "Controls whether line segments can be extended to other extended line segments within the specified extend length. EXTENSION \u2014 Line segments can be extended to other extended line segments as well as existing line features. This is the default. FEATURE \u2014 Line segments can only be extended to existing line features. ", "dataType": "Boolean"}]},
{"syntax": "Densify_edit (in_features, {densification_method}, {distance}, {max_deviation}, {max_angle})", "name": "Densify (Editing)", "description": "Adds vertices along line or polygon features. Also replaces curve segments (Bezier, circular arcs, and elliptical arcs) with line segments.", "example": {"title": "Densify example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Densify function in immediate mode.", "code": "import arcpy arcpy.Densify_edit ( \"C:/data.gdb/lines\" , \"ANGLE\" , \"\" , \"\" , \"0.75\" )"}, "usage": ["This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data changes.", "Straight line segments are densified by the ", "Distance", " parameter.  Curve segments are simplified through densification by either the ", "Distance", " ,", " Maximum Deflection Angle", ", or", " Maximum Offset Deviation", " parameter. ", "Densification is done segment by segment.", "Only one densification method can be selected each time ", "Densify", " is executed.", "The ", "Spatial Reference", " of the data is very important to the result generated by this tool. Data should be densified in an appropriate coordinate system to maintain the correct shape of the features.", "   For each vertex of the original feature, including the start and end point, there will be a coincident vertex in the resulting feature.  ", "When densifying by ", "Maximum Offset Deviation", ", if the input geometry contains circular arcs, then an upper limit on the offset will be enforced such that the angle between two consecutive line segments in the output cannot exceed ten degrees.  This angle can be exceeded if you densify by the ", "Maximum Deflection Angle", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The polygon or line feature class to be densified. ", "dataType": "Feature Layer"}, {"name": "densification_method", "isOptional": true, "description": "The method selected to handle feature densification. DISTANCE \u2014 The tool will apply the Distance method to curves the same as it does to straight lines. This is the default. OFFSET \u2014 The tool will apply the Maximum Offset Deviation parameter to curves. ANGLE \u2014 The tool will apply the Maximum Deflection Angle parameter to curves. ", "dataType": "String"}, {"name": "distance", "isOptional": true, "description": " The maximum linear distance between vertices. This distance will always be applied to line segments and to simplify curves. The default value is a function of the data's xy tolerance. ", "dataType": "Linear unit"}, {"name": "max_deviation", "isOptional": true, "description": "The maximum distance the output segment can be from the original. This parameter only affects curves. The default value is a function of the data's xy tolerance. ", "dataType": "Linear unit"}, {"name": "max_angle", "isOptional": true, "description": "The maximum angle that the output geometry can be from the input geometry. The valid range is from 0 to 90. The default value is 10. This parameter only affects curves. ", "dataType": "Double"}]},
{"syntax": "RegisterRaster_management (in_raster, register_mode, {reference_raster}, {Input_link_file}, {transformation_type}, {Output_CPT_link_file})", "name": "Register Raster (Data Management)", "description": "\r\nPerforms a geographic transformation to an existing raster dataset  by automatically registering it to a reference image. This is similar to the  Auto Register  button on the  Georeferencing  toolbar.  In order to \r\nautomatically register the image, the input raster and the reference raster must be in a relatively close geographic area. You may need to create a link file with  a few links in order to get your input raster into the same map space. ", "example": {"title": "RegisterRaster example 1 (Python window)", "description": "This is a Python sample for the RegisterRaster tool.", "code": "import arcpy arcpy.RegisterRaster_management ( \" \\\\ cpu\\data \\n onref.tif\" , \"REGISTER\" , \" \\\\ cpu\\data\\yesref.tif\" , \" \\\\ cpu\\data\\links.txt\" \"POLYORDER1\" , \"#\" )"}, "usage": ["\r\nThe input raster will have its georeferencing information updated. \r\n", "The ", "REMOVE", " keyword allows you to remove any geographic transformation that has been applied using this tool."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": " The input raster dataset. After the tool runs, the input raster will have its geographic transformation updated. ", "dataType": "Raster Layer"}, {"name": "register_mode", "isOptional": false, "description": " Choose the registration mode. You can either register the raster with a transformation, or you can reset the transformation. REGISTER \u2014 The tool will apply a geometric transformation on the input raster. This is the default. RESET \u2014 The tool will remove the geometric transformation previously added by this tool. CREATE_LINKS \u2014 The tool will create a link file with the automatically generated links.", "dataType": "String"}, {"name": "reference_raster", "isOptional": true, "description": " Choose the reference raster that the input will be matched to. ", "dataType": "Raster Layer"}, {"name": "Input_link_file", "isInputFile": true, "isOptional": true, "description": " The link file that will be used to place the input raster in the same map space as the reference raster. Each row in the input link file can one of the following set of values, each delimited by a TAB: <Optional ID> <From X> <From Y> <To X> <To Y> <From X> <From Y> <To X> <To Y> <Residual X> <Residual Y> <Residual>", "dataType": "Text File"}, {"name": "transformation_type", "isOptional": true, "description": "The geometric transformation type. POLYORDER0 \u2014 A zero-order polynomial is used to shift your data. This is commonly used when your data is already georeferenced, but a small shift will better line up your data. Only one link is required to perform a zero-order polynomial shift. POLYORDER1 \u2014 A first-order polynomial (affine) fits a flat plane to the input points. This is the default. POLYORDER2 \u2014 A second-order polynomial fits a somewhat more complicated surface to the input points. POLYORDER3 \u2014 A third-order polynomial fits a more complicated surface to the input points. ADJUST \u2014 A transformation that optimizes for both global and local accuracy. It accomplishes this by first performing a polynomial transformation, then adjusting the control points locally, to better match the target control points, using a triangulated irregular network (TIN) interpolation technique. SPLINE \u2014 A transformation that exactly transforms the source control points to the target control points. This means that the control points will be accurate, but the raster pixels that are between the control points are not. PROJECTIVE \u2014 A transformation that can warp lines so that they remain straight. In doing so, lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products.", "dataType": "String"}, {"name": "Output_CPT_link_file", "isOutputFile": true, "isOptional": true, "description": " The output link file that contains the registration links created by this tool. The output link table can be used in the Warp From File tool. Each row in the output link file has the following values, each delimited by a TAB: <From X> <From Y> <To X> <To Y> <Residual X> <Residual Y> <Residual> ", "dataType": "Text File"}]},
{"syntax": "ComputeAdjustments_management (in_mosaic_dataset, control_points_features, output_solution_table, {adjustment_type}, {transformation_type})", "name": "Compute Adjustments (Data Management)", "description": "\r\nThis tool is used to compute the adjustments\r\nto the mosaic dataset.", "example": {"title": null, "description": null, "code": ""}, "usage": ["\r\nUse the output control points from the \r\nCompute Tie Points tool as the input control points for this tool.", "The output solution table from this tool will be used in the Apply Adjustment tool."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The input mosaic dataset that will be adjusted. ", "dataType": "Mosaic Layer; Mosaic Dataset"}, {"name": "control_points_features", "isOptional": false, "description": " The control points that will be used to compute the adjustments. You can use your own ground control points, or you can use the output control points from the Compute Tie Points tool. ", "dataType": "Feature Class; Feature Layer"}, {"name": "output_solution_table", "isOutputFile": true, "isOptional": false, "description": " The output solution table that will contain the adjustments. ", "dataType": "Feature Class"}, {"name": "adjustment_type", "isOptional": true, "description": " Choose which type of adjustment you want to perform on the mosaic dataset. LINK \u2014 This will calculate a shift between the mosaic dataset items. BUNDLE ADJUSTMENT \u2014 This will calculate a bundle block adjustment.", "dataType": "String"}, {"name": "transformation_type", "isOptional": true, "description": " Choose which type of transformation will be used when adjusting the mosaic dataset. POLYORDER0 \u2014 A zero-order polynomial is used to shift your data. This is commonly used when your data is already georeferenced, but a small shift will better line up your data. Only one link is required to perform a zero-order polynomial shift. POLYORDER1 \u2014 A first-order polynomial (affine) fits a flat plane to the input points. This is the default. POLYORDER2 \u2014 A second-order polynomial fits a somewhat more complicated surface to the input points. POLYORDER3 \u2014 A third-order polynomial fits a more complicated surface to the input points. ", "dataType": "String"}]},
{"syntax": "ComputeTiePoints_management (in_mosaic_dataset, control_points_features, {in_aoi_dataset}, {out_image_feature_points})", "name": "Compute Tie Points (Data Management)", "description": "\r\nThis tool computes the tie points between the mosaic dataset items.\r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["\r\nOnce you compute the tie points, you can use the control point table in the Compute Adjustment tool.\r\n"], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The input mosaic dataset that will be used to create tie points. ", "dataType": "Mosaic Layer; Mosaic Dataset"}, {"name": "control_points_features", "isOptional": false, "description": " The output control point table. This table will contain the tie points that were created by this tool. ", "dataType": "Feature Class"}, {"name": "in_aoi_dataset", "isInputFile": true, "isOptional": true, "description": " Input an area of interest that you only want to create tie points within. If you leave this empty, then the entire mosaic dataset will be used as the area of interest. ", "dataType": "Feature Class; Feature Layer"}, {"name": "out_image_feature_points", "isOutputFile": true, "isOptional": true, "description": " The output image feature points table. This will be saved as a polygon feature class. ", "dataType": "Feature Class"}]},
{"syntax": "ApplyAdjustments_management (in_mosaic_dataset, adjustment_mode, {input_solution_table})", "name": "Apply Adjustments (Data Management)", "description": "\r\nApply the geographic adjustments\r\nto the mosaic dataset.", "example": {"title": null, "description": null, "code": ""}, "usage": ["\r\n\r\n"], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The input mosaic dataset that will have the adjustments applied to it. ", "dataType": "Mosaic Layer; Mosaic Dataset"}, {"name": "adjustment_mode", "isOptional": false, "description": " Choose whether you want to adjust the mosaic dataset using the solution table or if you want to reset the mosaic dataset back without any adjustments applied. ADJUST \u2014 Adjust the mosaic dataset using the input solution table. RESET \u2014 Reset the mosaic dataset so there are no adjustments applied to it.", "dataType": "String"}, {"name": "input_solution_table", "isInputFile": true, "isOptional": true, "description": " If you are adjusting your mosaic dataset, then you must specify an input solution table. ", "dataType": "Feature Class; Feature Layer"}]},
{"syntax": "CreateMapTilePackage_management (in_map, service_type, output_file, format_type, level_of_detail, {service_file}, {summary}, {tags})", "name": "Create Map Tile Package (Data Management)", "description": "\r\nGenerates tiles from a map document and packages the tiles to create a single compressed  .tpk  file.\r\n ", "example": {"title": "CreateMapTilePackage example 1 (Python window)", "description": " The following Python script demonstrates how to use the CreateMapTilePackage tool from the Python window.", "code": "import arcpy arcpy.env.workspace = \"C:/TilePackageExample\" arcpy.CreateMapTilePackage_management ( 'Example.mxd' , \"ONLINE\" , 'Example.tpk' , \"PNG8\" , \"10\" )"}, "usage": ["\r\n The input map document must have description and tags in order for the tool to execute. To add description and tags, choose ", "File ", ">", " Map Document Properties", " from the main menu and enter description and tags.\r\n", "By choosing PNG for the ", "Tiling Format", " parameter, the tool will automatically select the correct format (PNG8, PB24, or PNG32) based on the specified ", "Level of Display", ".", "To unpack a map tile package, either drag the ", ".tpk", " file into ArcMap or right-click on the ", ".tpk", " in the ", "Catalog", " window file and select ", "Unpack", ".  Alternatively, you can use the ", "Extract_Package", " tool and specify an output folder.  ", "By default,   ", "Unpack", " will always extract the package into your user profile under: ", "To change the default location of where your packages will be unpacked, open ", "ArcMap Options", " from the ", "Customize", " menu.  From the ", "Sharing", " tab find the ", "Packaging", " section and check ", "Use user specified location", " and browse  to the new folder location. "], "parameters": [{"name": "in_map", "isInputFile": true, "isOptional": false, "description": "The map document from which tiles will be generated and packaged. ", "dataType": "ArcMap Document"}, {"name": "service_type", "isOptional": false, "description": "Determines whether the tiling scheme will be generated from an existing map service or if map tiles will be generated for ArcGIS Online, Bing maps, and Google Maps. EXISTING \u2014 Tiling scheme from an existing map service will be used. You must specify a map service in the service_file parameter.Choose this option if your organization has created a tiling scheme for an existing service on your server and you want to match it. Matching tiling schemes ensures that your tiles will overlay correctly in your ArcGIS Runtime Application.If you choose this option, your source map document should use the same coordinate system as the map whose tiling scheme you are importing. ONLINE \u2014 The ArcGIS Online/Bing Maps/Google Maps tiling scheme is used. This is the default.The ArcGIS Online/Bing Maps/Google Maps tiling scheme allows you to overlay your cache tiles with tiles from these online mapping services. ArcGIS for Desktop includes this tiling scheme as a built-in option when loading a tiling scheme. When you choose this tiling scheme, the data frame of your source map document must use the WGS 1984 Web Mercator (Auxiliary Sphere) projected coordinate system.The ArcGIS Online/Bing Maps/Google Maps tiling scheme is required if you'll be overlaying your package with ArcGIS Online, Bing Maps, or Google Maps. One advantage of the ArcGIS Online/Bing Maps/Google Maps tiling scheme is that it is widely known in the web mapping world, so your tiles will match those of other organizations that have used this tiling scheme. Even if you don't plan to overlay any of these well-known map services, you may choose the tiling scheme for its interoperability potential.The ArcGIS Online/Bing Maps/Google Maps tiling scheme may contain scales that would be zoomed in too far to be of use to your map. Packaging for large scales can take up much time and disk storage space. For example, the largest scale in the tiling scheme is about 1:1,000. Packaging the entire continental United States at this scale can take weeks and require hundreds of gigabytes of storage. If you aren't prepared to package at this scale level, you should remove this scale level when you create the tile package.", "dataType": "Boolean"}, {"name": "output_file", "isOutputFile": true, "isOptional": false, "description": "The output map tile package. ", "dataType": "File"}, {"name": "format_type", "isOptional": false, "description": "Specifies the format of the generated tiles. PNG \u2014 Use PNG to automatically select the correct format (PNG8, PB24, or PNG32) based on the specified Level of Detail . This is the default. PNG8 \u2014 Use PNG 8 for overlay services that need to have a transparent background, such as roads and boundaries. PNG 8 creates tiles of very small size on disk with no loss of information. Do not use PNG 8 if your map contains more than 256 colors. Imagery, hillshades, gradient fills, transparency, and antialiasing can easily push your map over 256 colors. Even symbols such as highway shields may have subtle antialiasing around the edges that unexpectedly adds colors to your map. PNG24 \u2014 You can use PNG 24 for overlay services, such as roads and boundaries, that have more than 256 colors (if fewer than 256 colors, use PNG 8). PNG32 \u2014 Use PNG 32 for overlay services, such as roads and boundaries, that have more than 256 colors. PNG 32 is an especially good choice for overlay services that have antialiasing enabled on lines or text. PNG 32 creates larger tiles on disk than PNG 24, but the tiles are fully supported in all browsers. JPEG \u2014 Use this format for basemap services that have large color variation and do not need to have a transparent background. For example, raster imagery and very detailed vector basemaps tend to work well with JPEG. JPEG is a lossy image format. It attempts to selectively remove data without affecting the appearance of the image. This can cause very small tile sizes on disk, but if your map contains vector linework or labels, it may produce too much noise or blurry area around the lines. If this is the case, you can attempt to raise the compression value from the default of 75. A higher value, such as 90, may balance an acceptable quality of linework with the small tile size benefit of the JPEG.It's up to you to decide what image quality you consider acceptable. If you are willing to accept a minor amount of noise in the images, you may save large amounts of disk space by choosing JPEG. The smaller tile size also means the application can download the tiles faster. MIXED \u2014 A mixed package uses JPEG in the center of the package with PNG 32 on the edge of the package. Use the mixed mode when you want to cleanly overlay raster packages on other layers.When a mixed package is created, PNG 32 tiles are created anywhere that transparency is detected (in other words, anywhere that the data frame background is visible). The rest of the tiles are built using JPEG. This keeps the average file size down while providing you with a clean overlay on top of other packages. If you do not use the mixed mode package in this scenario, you will see a nontransparent collar around the periphery of your image where it overlaps the other package.", "dataType": "String"}, {"name": "level_of_detail", "isOptional": false, "description": " Specify the number of scale levels at which tiles will be generated for the package. Possible values are 1 through 20. ", "dataType": "Long"}, {"name": "service_file", "isOptional": true, "description": " Specifies the name of the map service or the XML files to use for the tiling scheme. This parameter is required only when the service_type parameter is EXISTING. ", "dataType": "MapServer; File"}, {"name": "summary", "isOptional": true, "description": " Adds summary information to the properties of the package. ", "dataType": "String"}, {"name": "tags", "isOptional": true, "description": " Adds tag information to the properties of the package. Multiple tags can be added separated by a comma or semicolon. ", "dataType": "String"}]},
{"syntax": "CreateRole_management (input_database, role, {grant_revoke}, {user_name})", "name": "Create Role (Data Management)", "description": "\r\nThe Create Role tool creates a database role and lets you add users to or remove users from the role. \r\n", "example": {"title": "CreateRole example 1", "description": "Creates a database role, editors, in a geodatabase in Oracle.", "code": "CreateRole_management ( \"C:\\Documents and Settings \\a dministrator\\Application Data\\ESRI\\ArcCatalog\\gdb_oracle.sde\" , \"editors\" , \"\" , \"\" )"}, "usage": ["\r\nThis tool can only be used with Oracle, Microsoft SQL Server, or PostgreSQL."], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " Specify the connection file to a database or enterprise geodatabase. You must connect as a database administrator user. ", "dataType": "Workspace"}, {"name": "role", "isOptional": false, "description": " Type the name of the database role you want to create. If the role already exists, type the name of the role for which you want to add users or remove users. ", "dataType": "String"}, {"name": "grant_revoke", "isOptional": true, "description": "Specify whether to grant the role to a user or list of users or remove a user or list of users from the role. GRANT \u2014 Grants the role to the specified user or users, thereby making them a member of the role REVOKE \u2014 Revokes the role from the specified user or users, thereby removing them from the role", "dataType": "String"}, {"name": "user_name", "isOptional": true, "description": "Type the name of the user for which you want to change role membership. To specify multiple users, type the user names separated by commas (no spaces). ", "dataType": "String"}]},
{"syntax": "CreateUnRegisteredFeatureclass_management (out_path, out_name, {geometry_type}, {template}, {has_m}, {has_z}, {spatial_reference}, {config_keyword})", "name": "Create Unregistered Feature Class (Data Management)", "description": "\r\nThis tool applies to ArcSDE geodatabases only. It creates an empty, unregistered feature class which is a requirement to create and/or publish to  ArcGIS Spatial Data Server .\r\n ", "example": {"title": null, "description": "CreateUnRegisteredFeatureClass example (Python window)", "code": "arcpy.CreateUnRegisteredFeatureclass_management ( \"Database Servers \\\\ SQLEXP.gds \\\\ FirstDB (VERSION:dbo.DEFAULT)\" , \"New_FC\" , \"POINT\" , \"\" , \"DISABLED\" , \"DISABLED\" , \"\" , \"\" )"}, "usage": ["\r\nThe \r\n", "Feature Class Location", " (", "ArcSDE", " geodatabase) must\r\nalready exist.", "This tool creates only simple feature classes such\r\nas point, multipoint, polygon, and polyline. Custom feature classes\r\nsuch as annotation, dimensions, and relationship class are created\r\nin the ", "Catalog", " window or in ", "ArcCatalog", ".  ", "An empty feature class created by this tool will have either a field named ", "OBJECTID", " of type integer or will contain the same field names and types of any selected input template feature class. ", "This tool can only be run as a foreground process.", "Learn more about foreground versus background processing"], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": "The ArcSDE geodatabase in which the output feature class will be created. ", "dataType": "Workspace;Feature Dataset"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": " The name of the feature class to be created. ", "dataType": "String"}, {"name": "geometry_type", "isOptional": true, "description": " The geometry type of the feature class. Only relevant where the dimensionality information is stored in the types metadata, such as Postgres, postgis, and ORACLE SDO. Point \u2014 Multipoint \u2014 Polyline \u2014 Polygon \u2014", "dataType": "String"}, {"name": "template", "isOptional": false, "description": "An existing feature class or layer to use as a template to define the attribute schema of the output feature class. ", "dataType": "Feature Layer"}, {"name": "has_m", "isOptional": true, "description": " Determines if the feature class contains linear measurement values (m values). DISABLED \u2014 The output feature class will not have m values. This is the default. SAME_AS_TEMPLATE \u2014 The output feature class will have m values if the template has m values. ENABLED \u2014 The output feature class will have m values.", "dataType": "String"}, {"name": "has_z", "isOptional": true, "description": " Determines if the feature class contains elevation values (z values). DISABLED \u2014 The output feature class will not have z values. This is the default. SAME_AS_TEMPLATE \u2014 The output feature class will have z values if the template has z values. ENABLED \u2014 The output feature class will have z values.", "dataType": "String"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output feature dataset. You can specify the spatial reference in several ways: By entering the path to a .prj file, such as C:/workspace/watershed.prj . By referencing a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland . By defining a spatial reference object prior to using this tool, such as sr = arcpy.SpatialReference(\"C:/data/Africa/Carthage.prj\") , which you then use as the spatial reference parameter. ", "dataType": "Spatial Reference"}, {"name": "config_keyword", "isOptional": true, "description": " Determines the storage parameters of the database table. ", "dataType": "String"}]},
{"syntax": "CreateUnRegisteredTable_management (out_path, out_name, {template}, {config_keyword})", "name": "Create Unregistered Table (Data Management)", "description": "\r\nThis tool applies to enterprise geodatabases only.  Creates an empty unregistered  ArcSDE file to be used with  ArcGIS Spatial Data Server .\r\n", "example": {"title": null, "description": "\r\nCreateUnRegisteredTable (Python window)", "code": "arcpy.CreateUnRegisteredTable_management ( \"Database Servers \\\\ SQLEXP.gds \\\\ FIRSTDB (VERSION:dbo.DEFAULT)\" , \"New_Table\" , \"\" , \"\" )"}, "usage": ["The  empty table created by this tool will have either a field named ", "OBJECTID", " of type integer or will contain the same field names and types of any selected input ", "Template Table Name", ". "], "parameters": [{"name": "out_path", "isOutputFile": true, "isOptional": false, "description": " The enterprise database in which the output table will be created. ", "dataType": "Workspace"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": " The name of the table to be created. ", "dataType": "String"}, {"name": "template", "isOptional": false, "description": " A table or list of tables whose fields and attribute schema are used to define the fields in the output table. ", "dataType": "Table View"}, {"name": "config_keyword", "isOptional": true, "description": " Determines the storage parameters of the table in an ArcSDE geodatabase. Learn more about configuration keywords for enterprise geodatabases ", "dataType": "String"}]},
{"syntax": "CreateDatabaseUser_management (input_database, {user_authentication_type}, user_name, {user_password}, {role}, {tablespace_name})", "name": "Create Database User (Data Management)", "description": "\r\nThe  Create Database User  tool creates a database user with privileges sufficient to create data in the database.", "example": {"title": "CreateUser example 1", "description": "Creates a database user in Oracle and creates a default tablespace for the user", "code": "#Import arcpy module import arcpy CreateDatabaseUser_management ( \"C:\\Documents and Settings\\user1\\Application Data\\ESRI\\ArcCatalog\\oracledb1.sde\" , \"DB\" , \"map\" , \"Pam987\" , \"sdetbs\" )"}, "usage": ["\r\n This tool can be used only with Oracle, Microsoft SQL Server, or PostgreSQL. (Not supported with SQL Azure.)", "For Oracle and SQL Server, if an operating system login already exists, the ", "Create Database User", " tool can add that login as a user to the specified database.", "Users created in the database have the following privileges granted to them:", "DBMS", "Privileges", "Oracle", "CREATE SESSION", "CREATE SEQUENCE", "CREATE TRIGGER", "CREATE VIEW", "CREATE TABLE", "SELECT ON DBA_ROLES", "PostgreSQL", "USAGE on the sde schema if the user is created in a geodatabase or a database that has the ST_Geometry type installed", "SELECT, INSERT, UPDATE, DELETE on the public.geometry_columns table and SELECT on the public.spatial_ref_systable if the PostGIS geometry type is installed", "SQL Server", "CREATE TABLE", "CREATE PROCEDURE", "CREATE VIEW", "VIEW DEFINITION", "If the login does not exist in the SQL Server instance or PostgreSQL database cluster, the ", "Create Database User", " tool adds the login, creates a user in the database specified for the Input Database, and creates a schema for the user in the database. The specified database is set as the user's default database in SQL Server. ", "If the login already exists in the SQL Server instance, the ", "Create Database User", " tool adds the user to the database you specify for the Input Database and creates a matching schema. The user's default database is not changed in SQL Server.", "If the login already exists in the PostgreSQL database cluster, the ", "Create Database User", " tool creates a matching schema in the database you specify for the Input Database.", "You cannot create a user named sde with this tool. The sde user is a geodatabase administrator user and requires more privileges than the ", "Create Database User", " tool grants."], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " Specify the connection file to a database or enterprise geodatabase in Oracle, PostgreSQL, or SQL Server. Be sure the connection file connects to the database as a database administrator user. ", "dataType": "Workspace"}, {"name": "user_authentication_type", "isOptional": true, "description": "Use this only if an operating system login exists for which you want to create a database user. Only enabled for SQL Server and Oracle databases. DB \u2014 Create a database-authenticated user. This is the default. If your DBMS is not configured to allow database authentication, do not use this option. OSA \u2014 Create an operating system-authenticated user. The corresponding login must already exist. If your DBMS is not configured to allow operating system authentication, do not use this option.", "dataType": "Boolean"}, {"name": "user_name", "isOptional": false, "description": " Type a name for the new database user. If you chose to create a database user for an operating system login, the user name must match the login name. ", "dataType": "String"}, {"name": "user_password", "isOptional": true, "description": " Type a password for the new user. The password policy of the underlying database will be enforced. If you chose to create a database user for an operating system login, no input is required. ", "dataType": "Encrypted String"}, {"name": "role", "isOptional": true, "description": " If you want to add the new user to an existing database role, type the name of the role. ", "dataType": "String"}, {"name": "tablespace_name", "isOptional": true, "description": " When creating a user in an Oracle database, type the name of the tablespace to be used as the default tablespace for the user. You can specify a preconfigured tablespace, or, if the tablespace does not already exist, it will be created in the Oracle default storage location with its size set to 400 MB. If no tablespace is specified, the user's default tablespace will be set to the Oracle default tablespace. ", "dataType": "String"}]},
{"syntax": "RecoverFileGDB_management (input_file_gdb, output_location, out_name)", "name": "Recover File Geodatabase (Data Management)", "description": "\r\nRecovers data from a file geodatabase that has become corrupt. ", "example": {"title": "RecoverFileGDB example 1 (Python window)", "description": "The following Python window script demonstrates how to use the RecoverFileGDB tool in immediate mode.", "code": "arcpy.RecoverFileGDB_management ( 'C:/fgdb/Whistler.gdb' , 'C:/recoveredData' , 'recoveredWhistler.gdb' )"}, "usage": ["The ", "Recover File Geodatabase", " tool can only recover simple feature classes and tables.  Complex data and relationships will not be recovered.\r\n"], "parameters": [{"name": "input_file_gdb", "isInputFile": true, "isOptional": false, "description": " Input corrupt file geodatabase. ", "dataType": "Workspace"}, {"name": "output_location", "isOutputFile": true, "isOptional": false, "description": "Output folder location for the recovered file geodatabase. ", "dataType": "Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": " Name for the output file geodatabase. ", "dataType": "String"}]},
{"syntax": "RepairMosaicDatasetPaths_management (in_mosaic_dataset, paths_list, {where_clause})", "name": "Repair Mosaic Dataset Paths (Data Management)", "description": "\r\nRepairs broken file paths within a mosaic dataset.\r\n", "example": {"title": "\r\nRepairMosaicDatasetPaths example 1 (Python window)", "description": "This is a Python sample for RepairMosaicDatasetPaths.", "code": "import arcpy arcpy.RepairMosaicDatasetPaths_management ( \"C:/Workspace/repairmd.gdb/md\" , \" \\\\\\\\ server1 \\\\ md \\\\ fgdb.gdb \\\\ md c: \\\\ storage \\\\ md \\\\ mdgdb.gdb \\\\ md\" , \"#\" )"}, "usage": ["You need to know the file path location in order to change it. You can use the ", "Export Mosaic Dataset Paths", " tool to retrieve the original path names.", "You can type an asterisk (*) as the original path if you wish to change all your paths."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The mosaic dataset to be repaired. ", "dataType": "Mosaic Layer"}, {"name": "paths_list", "isOptional": false, "description": " A list paths to remap, including the current path stored in the mosaic dataset and the path to which it will be changed. ", "dataType": "Value Table"}, {"name": "where_clause", "isOptional": true, "description": " Using SQL, you can define a query or use the Query Builder to build a query. ", "dataType": "SQL Expression"}]},
{"syntax": "ExportMosaicDatasetPaths_management (in_mosaic_dataset, out_table, {where_clause}, {export_mode}, {types_of_paths})", "name": "Export Mosaic Dataset Paths (Data Management)", "description": "\r\nCreates a table listing the paths to the mosaic dataset items. The table can display all the file paths or just the ones that are broken.", "example": {"title": "ExportMosaicDatasetPaths example 1 (Python window)", "description": "This is a Python sample for the ExportMosaicDatasetPaths tool.", "code": "import arcpy arcpy.ExportMosaicDatasetPaths_management ( \"C:/Workspace/exportmd.gdb/md\" , \"C:/workspace/brokenpaths.dbf\" , \"#\" , \"BROKEN\" , \"RASTER\" )"}, "usage": ["\r\nThe output of this tool is a table, either in a geodatabase or a .DBF file.\r\n"], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": " The input mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": " The output table. This table can be created as a geodatabase table or a DBF file. The output table will have a field that lists the Source OID. This is the OID of the row in the original raster catalog table. ", "dataType": "Table"}, {"name": "where_clause", "isOptional": true, "description": "Using SQL, you can define a query or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "export_mode", "isOptional": true, "description": "Choose what paths to output to the table. You can choose to output all the file paths or just the ones that are broken. ALL \u2014 This option will export all the paths to the table. This is the default. BROKEN \u2014 This option will export only the broken paths to the table. ", "dataType": "String"}, {"name": "types_of_paths", "isOptional": false, "description": "Choose which types of paths to export. RASTER \u2014 The paths to the source raster dataset will be exported. ITEM_CACHE \u2014 The paths to the raster item cache will be exported.", "dataType": "String"}]},
{"syntax": "BuildMosaicDatasetItemCache_management (in_mosaic_dataset, {where_clause}, {define_cache}, {generate_cache}, {item_cache_folder}, {compression_method}, {compression_quality}, {max_allowed_rows}, {max_allowed_columns}, {request_size_type}, {request_size})", "name": "Build Mosaic Dataset Item Cache (Data Management)", "description": "\r\nInserts the Cached Raster function into the function chain for items within a mosaic dataset.\r\n", "example": {"title": "BuildMosaicDatasetItemCache example 1 (Python window)", "description": "This is a Python sample for BuildMosaicDatasetItemCache.", "code": "import arcpy arcpy.BuildMosaicDatasetItemCache_management ( \"C:/Workspace/itemcache.gdb/md\" , \"#\" , \"DEFINE_CACHE\" , \"NO_GENERATE_CACHE\" , \"C:/workspace/itemcache\" , \"LOSSY\" , \"80\" , \"#\" , \"#\" )"}, "usage": ["The ", "Cached Raster function", " is inserted on the top of every function chain; therefore, it's the last function implemented in the chain.", "If you do not use the ", "Generate Cache", " option on this tool to generate the cache, then you can use the \r\n", "Synchronize Mosaic Dataset", " tool to generate the cache.", "The cache is not moved with the mosaic dataset when it is shared (published) to the server. If you will be building the cache for a mosaic dataset that will be published as an image service you may want to run this tool on the mosaic dataset after it has been shared to the server. Also, or in addition, make sure the path to the cache is accessible by the server. If you build the cache prior to publishing the mosiac dataset, you can move the cache to the server and update the cache path stored in the mosiac dataset."], "parameters": [{"name": "in_mosaic_dataset", "isInputFile": true, "isOptional": false, "description": "The path and name of the mosaic dataset. ", "dataType": "Mosaic Layer"}, {"name": "where_clause", "isOptional": true, "description": " Using SQL, you can define a query or use the Query Builder to build a query. ", "dataType": "SQL Expression"}, {"name": "define_cache", "isOptional": true, "description": " Choose to define the mosaic dataset cache. A Cached Raster function will be inserted to the selected items. If an item already has a Cached Raster function, it will not add another one. DEFINE_CACHE \u2014 The Cached Raster function will be added to the selected items. If an item already has this function, it will not add another one. This is the default. NO_DEFINE_CACHE \u2014 No raster cache will be defined.", "dataType": "Boolean"}, {"name": "generate_cache", "isOptional": true, "description": " Choose to generate the cache files based on the properties defined in the Cached Raster function, such as the location and the compression of the cache. GENERATE_CACHE \u2014 Cache will be generated. This is the default. NO_GENERATE_CACHE \u2014 Cache will not be generated.", "dataType": "Boolean"}, {"name": "item_cache_folder", "isOptional": true, "description": " The location where the cached dataset will be stored. By default, the cache is generated and stored in a folder next to where the mosaic dataset resides. This folder has the same name as the geodatabase, with a .cache extension. However, if the mosaic dataset is created in an ArcSDE geodatabase, the cache will be created within that geodatabase. Once an item cache is created, regenerating an item cache to a different location is not possible by specifying a different Cache Path and rerunning this tool. It will still generate the item cache in the location where it was generated the first time. However, you can remove this function and insert a new one with the new path or use the Repair Mosaic Dataset tool to modify the cache path, then run this tool to generate the item cache in a different location. ", "dataType": "Workspace"}, {"name": "compression_method", "isOptional": true, "description": " The type of compression, if any, that will be used when generating the cache. Lossless \u2014 Lossless compression retains the values of each pixel when generating the cache dataset. Lossy \u2014 Lossy compression does not retain the exact values of each pixel when generating the cached dataset. When using this method, a compression quality can also be set. None \u2014 No compression will be used when generating the cached dataset.", "dataType": "String"}, {"name": "compression_quality", "isOptional": true, "description": " The compression quality to use when lossy compression method is used. The compression quality value is between 1 and 100 percent. ", "dataType": "Long"}, {"name": "max_allowed_rows", "isOptional": true, "description": " Limits the size of the cache dataset by number of pixels, in rows. The cache raster will be not be generated if the dimension is beyond this limit. ", "dataType": "Long"}, {"name": "max_allowed_columns", "isOptional": true, "description": " Limits the size of the cache dataset by number of pixels, in columns. The cache raster will be not be generated if the dimension is beyond this limit. ", "dataType": "Long"}, {"name": "request_size_type", "isOptional": true, "description": " Choose whether to specify a pixel size factor or a pixel size, while resampling your item cache. PIXEL_SIZE_FACTOR \u2014 Allows you to choose a scaling factor to resample the cached raster. This is the default. If you do not wish to resample the cache, then choose PIXEL_SIZE_FACTOR with a factor of 1. PIXEL_SIZE \u2014 Allows you to specify the pixel size at which to create the cached raster.", "dataType": "String"}, {"name": "request_size", "isOptional": true, "description": "Specify the request size. The value that you type will be based on the request_size_type option. If you chose PIXEL_SIZE_FACTOR, then your value will be a scaling factor. If you chose PIXEL_SIZE, then your value will be the resampled pixel size. ", "dataType": "Double"}]},
{"syntax": "UpgradeDataset_management (in_dataset)", "name": "Upgrade Dataset (Data Management)", "description": "\r\nUpgrades the schema of  a mosaic dataset, network dataset, or parcel fabric to the current ArcGIS release. Upgrading the dataset allows the dataset to make use of  new functionality available in the current software release.", "example": {"title": "UpgradeDataset example 1 (Python window)", "description": "The following Python window example demonstrates how to use the UpgradeDataset tool in immediate mode.", "code": "import arcpy arcpy.UpgradeDataset_management ( \"Database Connections/city_data.sde/MontanaMD\" )"}, "usage": ["Before a dataset can be upgraded the geodatabase must be first upgraded to the current release using the ", "Upgrade_Geodatabase", " tool."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": " Dataset that will be upgraded to the current ArcGIS client release. ", "dataType": "Mosaic Layer; Network Dataset Layer; Parcel Fabric Layer"}]},
{"syntax": "CreateDatabaseView_management (input_database, view_name, view_definition)", "name": "Create Database View (Data Management)", "description": "\r\nCreates a view in an enterprise database based on an SQL expression.", "example": {"title": "CreateDatabaseView example (Python window)", "description": "The following Python window script demonstrates how to use the CreateDatabaseView tool in immediate mode.", "code": "import arcpy arcpy.CreateDatabaseView_management ( \"Database Connections/city_data.sde\" , \"trees\" , \"select objectid, owner, parcel from inventory where type = trees\" )"}, "usage": ["\r\nThe SQL expression used to define the view will be validated by the database upon execution of the tool. Therefore, valid syntax for the view definition will be determined by the underlying database being used.\r\nIf the syntax is incorrect, an error message will be returned.", "This tool is supported for enterprise databases and enterprise geodatabases (ArcSDE). File and personal geodatabases are not supported."], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " The database that contains the tables or feature classes used to construct the view. This database is also where the view will be created. ", "dataType": "Workspace"}, {"name": "view_name", "isOptional": false, "description": " The name of view that will be created in the database. ", "dataType": "String"}, {"name": "view_definition", "isOptional": false, "description": " An SQL statement used to construct the view. ", "dataType": "String"}]},
{"syntax": "PackageLocator_management (In_locator, output_file, {copy_arcsde_locator}, {additional_files}, {summary}, {tags})", "name": "Package Locator (Data Management)", "description": "\r\nPackage a locator or composite locator  to create a single compressed  .gcpk  file. Learn more about sharing your locator as a locator package", "example": {"title": "PackageLocator example 1 (Python window)", "description": "The following Python script demonstrates how to use the PackageLocator tool from within the Python window.", "code": "import arcpy arcpy.env.workspace = \"C:/MyData/Locators\" arcpy.PackageLocator_geocoding ( 'Atlanta_composite' , 'Altanta_composite.gcpk' , \"COPY_ARCSDE\" , \"#\" , \"Summary of package\" , \"tag1; tag2; tag3\" )"}, "usage": ["This tool is located in the Geocoding  toolbox.  For convenience, a copy of this tool also resides in the Data Management toolbox Package toolset.", "The input locator must have a description in order for the tool to execute.  To add  summary and tags, click the ", "Description", " tab of a locator in ArcCatalog, and click   the ", "Edit", " button ", " to enter the information on the ", "Item Description", " window.", "A warning is issued when this tool encounters an invalid locator. The invalid locator will not be packaged. ", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is checked", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is unchecked", "The locator package file (", ".gcpk", ") can be shared with other users or can be loaded to your ArcGIS online account. ", "Learn more about sharing a package", "To unpack a locator package, drag the ", ".gcpk", " file into ", "ArcMap", " or right-click on the ", ".gcpk", " file and click ", "Unpack", ".   ", "   ", "Unpack", " will extract a package into your user profile under: ", "Alternatively, you can use the ", "Extract Package", " tool and specify an output folder.", "Learn more about unpacking a locator package"], "parameters": [{"name": "In_locator", "isInputFile": true, "isOptional": false, "description": "The locator or composite locator that will be packaged. ", "dataType": "Input address locator"}, {"name": "output_file", "isOutputFile": true, "isOptional": false, "description": " The name and location of the output locator package ( .gcpk ). ", "dataType": "File"}, {"name": "copy_arcsde_locator", "isOptional": true, "description": "Specifies whether participating locators will be copied or their connection information will be preserved in the composite locator. This option only applies to composite locators. COPY_ARCSDE \u2014 All participating locators, including locators in ArcSDE, will be copied to the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Connection information of the participating locators that are stored in ArcSDE will be preserved in the composite locator. ", "dataType": "Boolean"}, {"name": "additional_files", "isOptional": true, "description": "Adds additional files to a package. Additional files, such as .doc , .txt , .pdf , and so on, are used to provide more information about the contents and purpose of the package. ", "dataType": "File"}, {"name": "summary", "isOptional": true, "description": "Adds Summary information to the properties of the package. ", "dataType": "String"}, {"name": "tags", "isOptional": true, "description": "Adds Tag information to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. ", "dataType": "String"}]},
{"syntax": "ConsolidateLocator_management (in_locator, output_folder, {copy_arcsde_locator})", "name": "Consolidate Locator (Data Management)", "description": "\r\nConsolidate a locator or composite locator  by copying all locators into a single folder.", "example": {"title": "ConsolidateLocator example 1 (Python window)", "description": "The following Python script demonstrates how to use the ConsolidateLocator tool from the Python window:", "code": "import arcpy arcpy.env.workspace = \"C:/MyData/Locators\" arcpy.ConsolidateLocator_Geocoding ( 'Atlanta_composite' , 'Consolidate_folder' , \"COPY_ARCSDE\" )"}, "usage": ["This tool is located in the Geocoding  toolbox.  For convenience, a copy of this tool also resides in the Data Management toolbox Package toolset.", "A warning is issued when this tool encounters an invalid locator. The invalid locator will not be packaged. ", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is checked", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is unchecked"], "parameters": [{"name": "in_locator", "isInputFile": true, "isOptional": false, "description": " The input locator or composite locator that will be consolidated. ", "dataType": "Locator"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder that will contain the locator or composite locator with its participating locators. ", "dataType": "Folder"}, {"name": "copy_arcsde_locator", "isOptional": true, "description": "Specifies whether participating locators will be copied or their connection information will be preserved in the composite locator. This option only applies to composite locators. COPY_ARCSDE \u2014 All participating locators, including locators in ArcSDE, will be copied to the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Connection information of the participating locators that are stored in ArcSDE will be preserved in the composite locator. ", "dataType": "Boolean"}]},
{"syntax": "MakeQueryLayer_management (input_database, out_layer_name, query, {oid_fields}, {shape_type}, {srid}, {spatial_reference})", "name": "Make Query Layer (Data Management)", "description": "Creates a  query layer  from a DBMS table based on an input SQL select statement.", "example": {"title": "MakeQueryLayer example 1 (Python window)", "description": "The following Python window script demonstrates how to use the MakeQueryLayer tool in immediate mode.", "code": "import arcpy sr = arcpy.SpatialReference ( \"WGS 1984 UTM Zone 12N\" ) arcpy.MakeQueryLayer_management ( \"Database Connections/moab.sde\" , \"Slickrock\" , \"select * from moabtrails where name = 'slickrock'\" , \"OBJECTID\" , \"POLYLINE\" , \"32611\" , sr )"}, "usage": ["\r\nQuery layers will only work with enterprise databases. File or personal geodatabases are not valid input workspaces for this tool.\r\n", "If the result of the SQL query entered returns a spatial column, the output will be a feature layer. If the SQL query does not return a spatial column, the output will be a stand-alone table.", "The connection files necessary for this tool can be created using the ", "Create Database Connection", " tool. ", "It will then be the up to the user to change any of these desired values before executing the tool.", "For geographic data, each record in the result returned from the SQL statement should have an associated SRID (spatial reference identifier). The SRID value is used by the database to determine the spatial reference for the data. The specific functional differences for the SRID will vary between each DBMS platform. Some DBMS platforms support multiple SRID values within the same table; ArcGIS will only support one value. This tool provides the ability to choose the SRID value or will default to the SRID from the first record in the result set. "], "parameters": [{"name": "input_database", "isInputFile": true, "isOptional": false, "description": " The database connection file that contains the data to be queried. ", "dataType": "Workspace"}, {"name": "out_layer_name", "isOutputFile": true, "isOptional": false, "description": " Output name for the feature layer or table view to be created. ", "dataType": "String"}, {"name": "query", "isOptional": false, "description": " SQL statement defining the select query to be issued to the database. ", "dataType": "String"}, {"name": "oid_fields", "isOptional": false, "description": " One or more fields from the SELECT statement SELECT list that can be used to generate a dynamic, unique row identifier. ", "dataType": "String"}, {"name": "shape_type", "isOptional": true, "description": " The shape type for the query layer. Only those records from the result set of the query that match the specified shape type will be used in the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before executing the tool if it is not the desired output shape type. This parameter is ignored if the result set of the query does not return a geometry field. POINT \u2014 The output query layer will use point geometry. MULTIPOINT \u2014 The output query layer will use multipoint geometry. POLYGON \u2014 The output query layer will use polygon geometry. POLYLINE \u2014 The output query layer will use polyline geometry.", "dataType": "String"}, {"name": "srid", "isOptional": true, "description": " Sets the SRID (spatial reference identifier) value for queries that return geometry. Only those records from the result set of the query that match the specified SRID value will be used in the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before executing the tool if it is not the desired output SRID value. This parameter is ignored if the result set of the query does not return a geometry field. ", "dataType": "String"}, {"name": "spatial_reference", "isOptional": true, "description": " Sets the coordinate system that will be used by the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before executing the tool if it is not the desired output coordinate system. This parameter is ignored if the result set of the query does not return a geometry field. ", "dataType": "Spatial Reference"}]},
{"syntax": "LasPointStatsAsRaster_management (in_las_dataset, out_raster, {method}, {sampling_type}, {sampling_value})", "name": "LAS Point Statistics As Raster (Data Management)", "description": "Creates a raster whose cell values reflect statistical information about measurements from LAS files referenced by a LAS dataset.", "example": {"title": "LasPointStatsAsRaster example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.LasPointStatsAsRaster_3d ( \"test.lasd\" , \"lidar_intensity.img\" , \"INTENSITY_RANGE\" , \"CELLSIZE\" , 15 )"}, "usage": ["Consider using PREDOMINANT_LAST_RETURN for the ", "Method", " parameter to identify locations with higher return values that could indicate the presence of vegetation.", "Consider using Z_RANGE for the ", "Method", " parameter to determine locations with potential outliers.", "The LAS dataset layer can be used to filter LAS points by class code or return values. The layer can be created by using the ", "Make LAS Dataset Layer", " tool, or by loading the LAS dataset in ArcMap or ArcScene and specifying the desired class codes and return values through the layer properties dialog box."], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The location and name of the output raster. When storing a raster dataset in a geodatabase or in a folder such as an Esri Grid, no file extension should be added to the name of the raster dataset. A file extension can be provided to define the raster's format when storing it in a folder: If the raster is stored as a TIFF file or in a geodatabase, its raster compression type and quality can be specified using geoprocessing environment settings. .bil\u2014Esri BIL .bip\u2014Esri BIP .bsq\u2014Esri BSQ .dat\u2014ENVI DAT .img\u2014ERDAS IMAGINE .png\u2014PNG .tif\u2014TIFF", "dataType": "Raster Dataset"}, {"name": "method", "isOptional": true, "description": " The type of statistics collected about the LAS points in each cell of the output raster. PULSE_COUNT \u2014 The number of last return points. POINT_COUNT \u2014 The number of points from all returns. PREDOMINANT_LAST_RETURN \u2014 The most frequent last return value. PREDOMINANT_CLASS \u2014 The most frequent class code. INTENSITY_RANGE \u2014 The range of intensity values. Z_RANGE \u2014 The range of elevation values.", "dataType": "String"}, {"name": "sampling_type", "isOptional": true, "description": " Specifies the method used for interpreting the Sampling Value to define the resolution of the output raster. OBSERVATIONS \u2014 Defines the number of cells that divide the lengthiest side of the LAS dataset extent. CELLSIZE \u2014 Defines the cell size of the output raster. This is the default.", "dataType": "String"}, {"name": "sampling_value", "isOptional": true, "description": " Specifies the value used in conjunction with the Sampling Type to define the resolution of the output raster. ", "dataType": "Double"}]},
{"syntax": "GaussianGeostatisticalSimulations_ga (in_geostat_layer, number_of_realizations, output_workspace, output_simulation_prefix, {in_conditioning_features}, {conditioning_field}, {conditioning_measurement_error_field}, {cell_size}, {in_bounding_dataset}, {save_simulated_rasters}, {quantile}, {threshold}, {in_stats_polygons}, {raster_stat_type})", "name": "Gaussian Geostatistical Simulations (Geostatisical Analyst)", "description": "Performs a conditional or unconditional geostatistical simulation based on a Simple Kriging model. \r\n Learn more about Gaussian Geostatistical Simulations \r\n", "example": {"title": "GaussianGeostatisticalSimulations example 1 (Python window)", "description": "Perform an unconditional simulation.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.GaussianGeostatisticalSimulations_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"10\" , \"C:/gapyexamples/output\" , \"ggs\" , \"\" , \"\" , \"2000\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"MEAN\" )"}, "usage": ["The input geostatistical layer  must be the result of performing Simple Kriging on a dataset. Geostatistical layers resulting from other types of kriging cannot be used with this tool.", "Additionally:", "To generate conditional realizations, the conditioning data should be the same as the data that was used to construct the Simple Kriging model that the simulation will be based on. However, other datasets can be used to condition the realizations.", "Output generated by this tool can be identified as follows:", "Use different prefixes to identify output from different simulation runs. If you use the same prefix, all previous results starting with that prefix will be erased before the new results are created. Alternatively, the output from different simulation runs can be stored in separate folders.", "If selected, the polygon output feature class will contain summary statistics of the values simulated within each polygon. To learn more about these summary statistics, refer to ", "How Gaussian Geostatistical Simulations work", ".", "Polygons representing areas of interest must be fully contained within the simulated raster's extent. If any portion of a polygon is covered by NoData values in the simulated rasters, the polygon attribute table will contain invalid results. In this case, the CELL_COUNT field will show the number of cells within the polygon that have simulated values, and the number will be expressed as a negative value.", "The Seed value (set in the Environment variables\u2014see ", "Random Number Generator", ") specifies the sequence of random numbers used in a simulation. By default, the Seed value is set to 0, so that each simulation will use a new sequence of random numbers. If the Seed is set to a value greater than 0, the simulation results will be replicated until the Seed value is changed. Setting the Seed to a value other than 0 may be useful when the results of a simulation study need to be replicated.", "If you have opted to save the simulated rasters, only the first two will be added to the table of contents in ArcMap. You can, however, browse to the output workspace and add the rest.", "The Environment setting for handling Coincident Points (under Geostatistical Analyst Settings) does not affect the results of unconditional or conditional simulations. Input datasets with coincident points are managed in the Geostatistical Wizard when the Simple Kriging layer (that will be used as input for the simulation) is built.", "For conditional simulations, points of the conditioning dataset that fall inside the same cell will be averaged, and the realizations will be conditioned to honor that average value. If the output cell size is large, many points will fall inside each cell and will be averaged, and the realizations will be conditioned to honor these (relatively) few average values.", "If bounding features are supplied, any features or rasters supplied in the Mask environment will be ignored.", "Current software limitations are as follows:", "An error of ", "Not enough memory to execute requested operation", " might indicate that the cell size requested will produce an output raster that is too large.", "For data formats that support Null values, such as file and personal geodatabase feature classes, a Null value will be used to indicate that a prediction could not be made for that location or that the value showed be ignored when used as input. For data formats that do not support Null values, such as shapefiles, the value of  -1.7976931348623158e+308 is used (this is the negative of the C++ defined constant DBL_MAX)  to indicate that a prediction could not be made for that location."], "parameters": [{"name": "in_geostat_layer", "isInputFile": true, "isOptional": false, "description": "Input a geostatistical layer resulting from a Simple Kriging model. ", "dataType": "Geostatistical Layer"}, {"name": "number_of_realizations", "isOptional": false, "description": "The number of simulations to perform. ", "dataType": "Long"}, {"name": "output_workspace", "isOutputFile": true, "isOptional": false, "description": "Stores all the simulation results. ", "dataType": "Workspace"}, {"name": "output_simulation_prefix", "isOutputFile": true, "isOptional": false, "description": "A 1- to 3-character alphanumeric prefix that is automatically added to the output dataset names. ", "dataType": "String"}, {"name": "in_conditioning_features", "isInputFile": true, "isOptional": true, "description": "The features used to condition the realizations. If left blank, unconditional realizations are produced. ", "dataType": "Feature Layer"}, {"name": "conditioning_field", "isOptional": true, "description": "The field used to condition the realizations. If left blank, unconditional realizations are produced. ", "dataType": "Field"}, {"name": "conditioning_measurement_error_field", "isOptional": true, "description": "Specifies a constant measurement error for all input data in the input semivariogram model. Use this field if the measurement error values are not the same at each sampling location. The input's unit of measurement is applied. Leave this blank if there are no measurement error values. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "in_bounding_dataset", "isInputFile": true, "isOptional": true, "description": "Limits the analysis to these features' bounding polygon. If point features are entered, then a convex hull polygon is automatically created. Realizations are then performed within that polygon. If bounding features are supplied, any features or rasters supplied in the Mask environment will be ignored. ", "dataType": "Feature Layer"}, {"name": "save_simulated_rasters", "isOptional": true, "description": "Determines whether the simulated rasters are saved to disk or not. ", "dataType": "Boolean"}, {"name": "quantile", "isOptional": true, "description": "The quantile value for which the output raster will be generated. ", "dataType": "Double"}, {"name": "threshold", "isOptional": true, "description": "The threshold value for which the output raster will be generated, as the percentage of the number of times the set threshold was exceeded, on a cell-by-cell basis. ", "dataType": "Double"}, {"name": "in_stats_polygons", "isInputFile": true, "isOptional": true, "description": "These polygons represent areas of interest for which summary statistics are calculated. ", "dataType": "Feature Layer"}, {"name": "raster_stat_type", "isOptional": false, "description": "The simulated rasters are post-processed on a cell by cell basis and each selected statistics type is calculated and reported in an output raster. MIN \u2014 Calculates the minimum (smallest value) MAX \u2014 Calculates the maximum (largest value) MEAN \u2014 Calculates the mean (average) STDDEV \u2014 Calculates the standard deviation QUARTILE1 \u2014 Calculates the 25th quantile MEDIAN \u2014 Calculates the median QUARTILE3 \u2014 Calculates the 75th quantile QUANTILE \u2014 Calculates a user specified quantile (0 < Q < 1) P_THRSHLD \u2014 Calculates the percentage of the simulations where the cell value exceeds a user-specified threshold value", "dataType": "String"}]},
{"syntax": "ExtractValuesToTable_ga (in_features, in_rasters, out_table, {out_raster_names_table}, {add_warning_field})", "name": "Extract Values To Table (Geostatisical Analyst)", "description": "Extracts cell values from a set of rasters to a table, based on a point or polygon feature class.  \r\n\r\n", "example": {"title": "ExtractValuesToTable interactive window example", "description": "Extract the cell values from a raster, based on a point feature class, to a table.", "code": "import arcpy from arcpy import env env.workspace = \"C:/gapyexamples/data\" arcpy.ExtractValuesToTable_ga ( \"C:/gapyexamples/data/ca_ozone_pts.shp\" , \"C:/gapyexamples/data/inraster\" , \"C:/gapyexamples/output/outEVFR.dbf\" , \"\" , \"\" )"}, "usage": ["This tool is primarily designed to analyze the output from ", "Gaussian Geostatistical Simulations", ".", "All the rasters must have the same spatial reference and cell size.", " If a point feature class is used, the output table has a record for each point and each raster that has data. Polygonal data will be treated as point data, the cell center of the input rasters will determine the number of points and will also be used to decide whether the \"cell\" is contained within the polygon or not.", "Values outside the raster's extent will be ignored.", "Multipart feature classes are not supported.", "In the case of coincident points, the last one encountered will be used and the rest will be ignored. In the same vane, if overlapping polygons are encountered only one of the intersected features will be processed.", "The ", "Sample", " tool can also be used, however, the output table needs to be transposed if the values are to be graphed."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The points or polygon features to be created. ", "dataType": "Feature Layer"}, {"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": "The rasters must all have the same extent, coordinate system, and cell size. ", "dataType": "Raster Layer; Mosaic Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output table contains a record for each point and each raster that has data. If polygon features are input, they are converted to points which coincide with the raster cell centers. ", "dataType": "Table"}, {"name": "out_raster_names_table", "isOutputFile": true, "isOptional": true, "description": "Saves the names of the Input rasters to disc. ", "dataType": "Table"}, {"name": "add_warning_field", "isOptional": true, "description": "Records if input features are partially or completely covered by the Input rasters. TRUE \u2014 Warning field is added to the output table and populated with a P when a feature is partially covered by raster values. FALSE \u2014 Warning field is not added to the output table. ", "dataType": "Boolean"}]},
{"syntax": "DensifySamplingNetwork_ga (in_geostat_layer, number_output_points, out_feature_class, {selection_criteria}, {threshold}, {in_weight_raster}, {in_candidate_point_features}, {inhibition_distance})", "name": "Densify Sampling Network (Geostatisical Analyst)", "description": "Uses  inter alia , the Standard Error of Prediction surface on a predefined geostatistical kriging layer to determine where new locations are required or which can be removed.", "example": {"title": "DensifySamplingNetwork example 1 (Python window)", "description": "Densify a sampling network based on a predefined geostatistical kriging layer.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.DensifySamplingNetwork_ga ( \"C:/gapyexamples/data/Kriging.lyr\" , 2 , \"C:/gapyexamples/output/outDSN\" )"}, "usage": ["The input geostatistical layer must be a kriging layer.", "The selection criteria are based upon;", "The case might arise where only a single new location is generated when more were requested. This happens when the new location keeps on being selected based on the selection criteria. This can be prevented by specifying a value for the ", "Inhibition distance", " parameter.  ", "To decide which locations have the least influence on the prediction surface you may use the feature class that was used to create the kriging layer for the ", "Input candidate point features", " parameter."], "parameters": [{"name": "in_geostat_layer", "isInputFile": true, "isOptional": false, "description": "Input a geostatistical layer resulting from a Kriging model. ", "dataType": "Geostatistical Layer"}, {"name": "number_output_points", "isOptional": false, "description": "Specify how many sample locations to generate. ", "dataType": "Long"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The name of the output feature class. ", "dataType": "Feature Class"}, {"name": "selection_criteria", "isOptional": true, "description": "Methods to densify a sampling network. ", "dataType": "String"}, {"name": "threshold", "isOptional": true, "description": "The threshold value used to densify the sampling network, applicable only when STDERR_THRESHOLD or QUARTILE_THRESHOLD selection criteria are used. ", "dataType": "Double"}, {"name": "in_weight_raster", "isInputFile": true, "isOptional": true, "description": "A raster used to determine which locations to weight for preference. ", "dataType": "Raster Layer"}, {"name": "in_candidate_point_features", "isInputFile": true, "isOptional": true, "description": "Sample locations to pick from. ", "dataType": "Feature Layer"}, {"name": "inhibition_distance", "isOptional": true, "description": "Used to prevent any samples being placed within this distance from each other. ", "dataType": "Linear unit"}]},
{"syntax": "CreateSpatiallyBalancedPoints_ga (in_probability_raster, number_output_points, out_feature_class)", "name": "Create Spatially Balanced Points (Geostatisical Analyst)", "description": " Generates a set of sample points based on inclusion probabilities resulting in a spatially balanced, i.e. maximized, and thus more efficient sample design.", "example": {"title": "CreateSpatiallyBalancedPoints example 1 (Python window)", "description": "Create a set of spatially balanced points based on an input inclusion probability raster.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.CreateSpatiallyBalancedPoints_ga ( \"ca_prob\" , \"10\" , \"C:/gapyexamples/output/csbp\" )"}, "usage": [], "parameters": [{"name": "in_probability_raster", "isInputFile": true, "isOptional": false, "description": "This raster defines the inclusion probabilities for each location in the area of interest. The location values range from 0 (low inclusion probability) to 1 (high inclusion probability). ", "dataType": "Raster Layer; Mosaic Layer"}, {"name": "number_output_points", "isOptional": false, "description": "Specify how many sample locations to generate. ", "dataType": "Long"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class contains the selected sample locations and their inclusion probabilities. ", "dataType": "Feature Class"}]},
{"syntax": "RadialBasisFunctions_ga (in_features, z_field, {out_ga_layer}, {out_raster}, {cell_size}, {search_neighborhood}, {radial_basis_functions}, {small_scale_parameter})", "name": "Radial Basis Functions (Geostatisical Analyst)", "description": "Uses one of five basis functions to process each measured sample value, thus creating an exact interpolation surface.", "example": {"title": "RadialBasisFunctions example 1 (Python window)", "description": "Interpolate point features onto a rectangular raster.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.RadialBasisFunctions_ga ( \"ca_ozone_pts\" , \"OZONE\" , \"outRBF\" , \"C:/gapyexamples/output/rbfout\" , \"2000\" , arcpy.SearchNeighborhoodStandard ( 300000 , 300000 , 0 , 15 , 10 , \"ONE_SECTOR\" ), \"THIN_PLATE_SPLINE\" , \"\" )"}, "usage": ["The smooth search neighborhood is only available for the Inverse multiquadric function.", "For all methods except the Inverse multiquadric function, the higher the parameter value, the smoother the surface. The opposite is true for the Inverse multiquadric function."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "Field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input features contain z-values or m-values. ", "dataType": "Field"}, {"name": "out_ga_layer", "isOutputFile": true, "isOptional": true, "description": "The geostatistical layer produced. This layer is required output only if no output raster is requested. ", "dataType": "Geostatistical Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster. This raster is required output only if no output geostatistical layer is requested. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "search_neighborhood", "isOptional": true, "description": "Defines which surrounding points will be used to control the output. Standard is the default. This is a Search Neighborhood class ( SearchNeighborhoodStandard , SearchNeighborhoodSmooth ), SearchNeighborhoodStandardCircular and SearchNeighborhoodSmoothCircular . Standard Smooth StandardCircular SmoothCircular Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one. Radius\u2014The length of the radius of the search circle. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Radius\u2014The length of the radius of the search circle. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one.", "dataType": "Geostatistical Search Neighborhood"}, {"name": "radial_basis_functions", "isOptional": true, "description": "Available Radial basis functions. THIN_PLATE_SPLINE \u2014 Thin-plate spline SPLINE_WITH_TENSION \u2014 Spline with tension COMPLETELY_REGULARIZED_SPLINE \u2014 Completely regularized spline MULTIQUADRIC_FUNCTION \u2014 Multiquadric function INVERSE_MULTIQUADRIC_ FUNCTION \u2014 Inverse multiquadric function ", "dataType": "String"}, {"name": "small_scale_parameter", "isOptional": true, "description": "Used to calculate the weights assigned to the points located in the moving window. Each of the radial basis functions has a parameter that controls the degree of small-scale variation of the surface. The (optimal) parameter is determined by finding the value that minimizes the root mean square prediction error (RMSPE). ", "dataType": "Double"}]},
{"syntax": "GAMovingWindowKriging_ga (in_ga_model_source, in_datasets, in_locations, neighbors_max, out_featureclass, {cell_size}, {out_surface_grid})", "name": "Moving Window Kriging (Geostatisical Analyst)", "description": "Recalculates the Range, Nugget, and Partial Sill semivariogram parameters based on a smaller neighborhood, moving through all location points. How Moving Window Kriging works \r\n", "example": {"title": "MovingWindowKriging example 1 (Python window)", "description": "Predict values at select point locations.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.GAMovingWindowKriging_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"C:/gapyexamples/data/ca_ozone_pts.shp OZONE\" , \"C:/gapyexamples/data/obs_pts.shp\" , \"10\" , \"C:/gapyexamples/output/outMWK\" , \"\" , \"\" )"}, "usage": ["The geostatistical model source is either a geostatistical layer or a geostatistical model (XML).", "The input dataset must have more than 10 points. However, it is recommended that it be used with large datasets that have nonstationary trends.", "For data formats that support Null values, such as file and personal geodatabase feature classes, a Null value will be used to indicate that a prediction could not be made for that location or that the value showed be ignored when used as input. For data formats that do not support Null values, such as shapefiles, the value of  -1.7976931348623158e+308 is used (this is the negative of the C++ defined constant DBL_MAX)  to indicate that a prediction could not be made for that location."], "parameters": [{"name": "in_ga_model_source", "isInputFile": true, "isOptional": false, "description": "The geostatistical model source to be analyzed. ", "dataType": "File; Geostatistical Layer"}, {"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": "Input datasets displays two pieces: Dataset\u2014The path and the name of the input data. Field\u2014The name of the required field.", "dataType": "Geostatistical Value Table"}, {"name": "in_locations", "isInputFile": true, "isOptional": false, "description": "Point locations where predictions will be performed. ", "dataType": "Feature Layer"}, {"name": "neighbors_max", "isOptional": false, "description": "Number of neighbors to use in the moving window. ", "dataType": "Long"}, {"name": "out_featureclass", "isOutputFile": true, "isOptional": false, "description": "Feature class storing the results. ", "dataType": "Feature Class"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "out_surface_grid", "isOutputFile": true, "isOptional": true, "description": "The prediction values in the output feature class are interpolated onto a raster using the Local polynomial interpolation method . ", "dataType": "Raster Dataset"}]},
{"syntax": "LocalPolynomialInterpolation_ga (in_features, z_field, {out_ga_layer}, {out_raster}, {cell_size}, {power}, {search_neighborhood}, {kernel_function}, {use_condition_number}, {bandwidth}, {condition_number}, {weight_field}, {output_type})", "name": "Local Polynomial Interpolation (Geostatisical Analyst)", "description": "Fits the specified order (zero, first, second, third, and so on) polynomial, each within specified overlapping neighborhoods, to produce an output surface.", "example": {"title": "LocalPolynomialInterpolation example 1 (Python window)", "description": "Interpolate point features onto a rectangular raster.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.LocalPolynomialInterpolation_ga ( \"ca_ozone_pts\" , \"OZONE\" , \"outLPI\" , \"C:/gapyexamples/output/lpiout\" , \"2000\" , \"2\" , arcpy.SearchNeighborhoodSmooth ( 300000 , 300000 , 0 , 0.5 ), \"QUARTIC\" , \"\" , \"\" , \"\" , \"\" , \"PREDICTION\" )"}, "usage": [" Use ", "Local Polynomial Interpolation", " when your dataset exhibits short-range variation.", "Global Polynomial Interpolation", " is useful for creating smooth surfaces and identifying long-range trends in the dataset. However, in earth sciences the variable of interest usually has short-range variation in addition to long-range trend. When the dataset exhibits short-range variation, Local Polynomial Interpolation maps can capture the short-range variation. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "Field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input features contain z-values or m-values. ", "dataType": "Field"}, {"name": "out_ga_layer", "isOutputFile": true, "isOptional": true, "description": "The geostatistical layer produced. This layer is required output only if no output raster is requested. ", "dataType": "Geostatistical Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster. This raster is required output only if no output geostatistical layer is requested. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "power", "isOptional": true, "description": " The order of the polynomial. ", "dataType": "Long"}, {"name": "search_neighborhood", "isOptional": true, "description": "Defines which surrounding points will be used to control the output. Standard is the default. This is a Search Neighborhood class ( SearchNeighborhoodStandard , SearchNeighborhoodSmooth ), SearchNeighborhoodStandardCircular and SearchNeighborhoodSmoothCircular . Standard Smooth StandardCircular SmoothCircular Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one. Radius\u2014The length of the radius of the search circle. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Radius\u2014The length of the radius of the search circle. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one.", "dataType": "Geostatistical Search Neighborhood"}, {"name": "kernel_function", "isOptional": true, "description": " The kernel function used in the simulation. QUARTIC \u2014 Fourth-order polynomial function. EXPONENTIAL \u2014 The function grows or decays proportionally. GAUSSIAN \u2014 Bell-shaped function that falls off quickly towards plus/minus infinity. EPANECHNIKOV \u2014 A discontinuous parabolic function. POLYNOMIAL5 \u2014 Fifth-order polynomial function. CONSTANT \u2014 An indicator function.", "dataType": "String"}, {"name": "use_condition_number", "isOptional": true, "description": "Option to control the creation of prediction and prediction standard errors where the predictions are unstable. This option is only available for polynomials of order 1, 2 and 3. NO_USE_CONDITION_NUMBER \u2014 Prediction and prediction standard errors can be created where the predictions are unstable. This is the default. USE_CONDITION_NUMBER \u2014 Prediction and prediction standard errors will not be created where the predictions are unstable.", "dataType": "Boolean"}, {"name": "bandwidth", "isOptional": true, "description": " Used to specify the maximum distance at which data points are used for prediction. With increasing bandwidth, prediction bias increases and prediction variance decreases. ", "dataType": "Double"}, {"name": "condition_number", "isOptional": true, "description": " Every invertible square matrix has a condition number that indicates how inaccurate the solution to the linear equations can be with a small change in the matrix coefficients (it can be due to imprecise data). If the condition number is large, a small change in the matrix coefficients results in a large change in the solution vector. ", "dataType": "Double"}, {"name": "weight_field", "isOptional": true, "description": "Used to emphasize an observation. The larger the weight, the more impact it has on the prediction. For coincident observations, assign the largest weight to the most reliable measurement. ", "dataType": "Field"}, {"name": "output_type", "isOutputFile": true, "isOptional": true, "description": " Surface type to store the interpolation results. PREDICTION \u2014 Prediction surfaces are produced from the interpolated values. PREDICTION_STANDARD_ERROR \u2014 Standard Error surfaces are produced from the standard errors of the interpolated values. CONDITION_NUMBER \u2014 The Spatial condition number surface indicates variation in the numerical model stability and provides additional information on the prediction uncertainty as the prediction standard error surface is created, assuming that the model is correct, which implies that there is no reason for model instability. ", "dataType": "String"}]},
{"syntax": "KernelInterpolationWithBarriers_ga (in_features, z_field, {out_ga_layer}, {out_raster}, {cell_size}, {in_barrier_features}, {kernel_function}, {bandwidth}, {power}, {ridge}, {output_type})", "name": "Kernel Interpolation With Barriers (Geostatisical Analyst)", "description": "A moving window predictor that uses the shortest distance between points so that points on either side of the line barriers are connected.", "example": {"title": "KernelInterpolationWithBarriers (Python window)", "description": "Interpolate point features onto a rectangular raster using a barrier feature class.", "code": "import arcpy arcpy.env.workspace = \"C:/gapysamples/data\" arcpy.KernelInterpolationWithBarriers_ga ( \"ca_ozone_pts\" , \"OZONE\" , \"outKIWB\" , \"C:/gapyexamples/output/kiwbout\" , \"2000\" , \"ca_outline\" , \"QUARTIC\" , \"\" , \"\" , \"50\" , \"PREDICTION\" )"}, "usage": ["The Absolute feature barrier employs a \u201cnon Euclidean\u201d distance approach rather than a \u201cline of sight\u201d approach. The \u201cline of sight\u201d approach requires that a straight line between the measured location and the location where the prediction is required do not intersect the barrier feature. If the distance around the barrier is within the searching neighborhood specifications, then it will be considered in this \u201cnon Euclidean\u201d distance approach. ", "The processing time is dependent on the complexity of the barrier feature classes geometry. Tools in the ", "Generalization toolset", " can be used to create a new feature class by smoothing or deleting some of these features."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "Field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input features contain z-values or m-values. ", "dataType": "Field"}, {"name": "out_ga_layer", "isOutputFile": true, "isOptional": true, "description": "The geostatistical layer produced. This layer is required output only if no output raster is requested. ", "dataType": "Geostatistical Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster. This raster is required output only if no output geostatistical layer is requested. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "in_barrier_features", "isInputFile": true, "isOptional": true, "description": "Absolute barrier features using non-Euclidean distances rather than line-of-sight distances. ", "dataType": "Feature Layer"}, {"name": "kernel_function", "isOptional": true, "description": " The kernel function used in the simulation. QUARTIC \u2014 Fourth-order polynomial function. EXPONENTIAL \u2014 The function grows or decays proportionally. GAUSSIAN \u2014 Bell-shaped function that falls off quickly towards plus/minus infinity. EPANECHNIKOV \u2014 A discontinuous parabolic function. POLYNOMIAL5 \u2014 Fifth-order polynomial function. CONSTANT \u2014 An indicator function.", "dataType": "String"}, {"name": "bandwidth", "isOptional": true, "description": " Used to specify the maximum distance at which data points are used for prediction. With increasing bandwidth, prediction bias increases and prediction variance decreases. ", "dataType": "Double"}, {"name": "power", "isOptional": true, "description": " Sets the order of the polynomial. ", "dataType": "Long"}, {"name": "ridge", "isOptional": true, "description": " Used for the numerical stabilization of the solution of the system of linear equations. It does not influence predictions in the case of regularly distributed data without barriers. Predictions for areas in which the data is located near the feature barrier or isolated by the barriers can be unstable and tend to require relatively large ridge parameter values. ", "dataType": "Double"}, {"name": "output_type", "isOutputFile": true, "isOptional": true, "description": " Surface type to store the interpolation results. PREDICTION \u2014 Prediction surfaces are produced from the interpolated values. PREDICTION_STANDARD_ERROR \u2014 Standard Error surfaces are produced from the standard errors of the interpolated values.", "dataType": "String"}]},
{"syntax": "IDW_ga (in_features, z_field, {out_ga_layer}, {out_raster}, {cell_size}, {power}, {search_neighborhood}, {weight_field})", "name": "IDW (Geostatisical Analyst)", "description": " Uses the measured values surrounding the prediction location  to predict a value for any unsampled location, based on the assumption that things that are close to one another are more alike than those that are farther apart.", "example": {"title": "IDW (Python window)", "description": "Interpolate a series of point features onto a raster.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.IDW_ga ( \"ca_ozone_pts\" , \"OZONE\" , \"outIDW\" , \"C:/gapyexamples/output/idwout\" , \"2000\" , \"2\" , arcpy.SearchNeighborhoodStandard ( 300000 , 300000 , 0 , 15 , 10 , \"ONE_SECTOR\" ), \"\" )"}, "usage": ["The predicted value is limited to the range of the values used to interpolate. Because IDW is a weighted distance average, the average cannot be greater than the highest or less than the lowest input. Therefore, it cannot create ridges or valleys if these extremes have not already been sampled. ", "IDW can produce \"bulls eyes\" around data locations.", "There are no assumptions required of the input data.", "This method is well suited to be used with very large input datasets."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "Field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input features contain z-values or m-values. ", "dataType": "Field"}, {"name": "out_ga_layer", "isOutputFile": true, "isOptional": true, "description": "The geostatistical layer produced. This layer is required output only if no output raster is requested. ", "dataType": "Geostatistical Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster. This raster is required output only if no output geostatistical layer is requested. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "power", "isOptional": true, "description": "The exponent of distance that controls the significance of surrounding points on the interpolated value. A higher power results in less influence from distant points. ", "dataType": "Double"}, {"name": "search_neighborhood", "isOptional": true, "description": "Defines which surrounding points will be used to control the output. Standard is the default. This is a Search Neighborhood class ( SearchNeighborhoodStandard , SearchNeighborhoodSmooth ), SearchNeighborhoodStandardCircular and SearchNeighborhoodSmoothCircular . Standard Smooth StandardCircular SmoothCircular Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one. Radius\u2014The length of the radius of the search circle. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Radius\u2014The length of the radius of the search circle. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one.", "dataType": "Geostatistical Search Neighborhood"}, {"name": "weight_field", "isOptional": true, "description": "Used to emphasize an observation. The larger the weight, the more impact it has on the prediction. For coincident observations, assign the largest weight to the most reliable measurement. ", "dataType": "Field"}]},
{"syntax": "GlobalPolynomialInterpolation_ga (in_features, z_field, {out_ga_layer}, {out_raster}, {cell_size}, {power}, {weight_field})", "name": "Global Polynomial Interpolation (Geostatisical Analyst)", "description": "Fits a smooth surface that is defined by a mathematical function (a polynomial) to the input sample points.", "example": {"title": "GlobalPolynomialInterpolation example 1 (Python window)", "description": "Interpolate point features onto a rectangular raster.", "code": "import arcpy arcpy.env.workspace = \"C:/gapysamples/data\" arcpy.GlobalPolynomialInterpolation_ga ( \"ca_ozone_pts\" , \"OZONE\" , \"outGPI\" , \"C:/gapyexamples/output/gpiout\" , \"2000\" , \"2\" , \"\" )"}, "usage": ["The result from this tool is a smooth surface that represents gradual trends in the surface over the area of interest. ", " The ", "Local Polynomial interpolation tool", " should be used when short-range variation exists in the data. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "Field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input features contain z-values or m-values. ", "dataType": "Field"}, {"name": "out_ga_layer", "isOutputFile": true, "isOptional": true, "description": "The geostatistical layer produced. This layer is required output only if no output raster is requested. ", "dataType": "Geostatistical Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster. This raster is required output only if no output geostatistical layer is requested. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "power", "isOptional": true, "description": " The order of the polynomial. ", "dataType": "Long"}, {"name": "weight_field", "isOptional": true, "description": "Used to emphasize an observation. The larger the weight, the more impact it has on the prediction. For coincident observations, assign the largest weight to the most reliable measurement. ", "dataType": "Field"}]},
{"syntax": "DiffusionInterpolationWithBarriers_ga (in_features, z_field, {out_ga_layer}, {out_raster}, {cell_size}, {in_barrier_features}, {bandwidth}, {number_iterations}, {weight_field}, {in_additive_barrier_raster}, {in_cumulative_barrier_raster}, {in_flow_barrier_raster})", "name": "Diffusion Interpolation With Barriers (Geostatisical Analyst)", "description": "Uses a kernel that is based upon the heat equation and allows one to use a combination of raster and feature datasets to act as a barrier.", "example": {"title": "DiffusionInterpolationWithBarriers example 1 (Python window)", "description": "Interpolate point features that are constrained by a barrier onto a rectangular raster.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.DiffusionInterpolationWithBarriers_ga ( \"ca_ozone_pts\" , \"OZONE\" , \"outDIWB\" , \"C:/gapyexamples/output/diwbout\" , \"2000\" , \"ca_outline\" , \"\" , \"10\" , \"\" , \"\" , \"\" , \"\" )"}, "usage": ["The Absolute feature barrier employs a \u201cnon Euclidean\u201d distance approach rather than a \u201cline of sight\u201d approach. The \u201cline of sight\u201d approach requires that a straight line between the measured location and the location where the prediction is required do not intersect the barrier feature. If the distance around the barrier is within the searching neighborhood specifications, then it will be considered in this \u201cnon Euclidean\u201d distance approach. ", "The processing time is dependent on the complexity of the barrier feature classes geometry. Tools in the ", "Generalization toolset", " can be used to create a new feature class by smoothing or deleting some of these features.", "For the ", "Input additive barrier raster", " parameter, the values must be greater than or equal to 1. A value of 1 implies that there is no barrier. ", "The ", "Input cummulative barrier raster", " parameter should have values that are of the same order as that of the x,y coordinates. If neighboring cells have the same values then it implies that there is no barrier at that location.", "Input flow barrier raster", " should have values that are of the same order as that of the x,y coordinates. If the neighboring cells have the same values then it implies that there is no barrier at that location. Also, if you go from a high to a low value then it implies that there is no barrier.", "A value of Nodata, in the raster barriers, has the same effect as that of an absolute barrier."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "Field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input features contain z-values or m-values. ", "dataType": "Field"}, {"name": "out_ga_layer", "isOutputFile": true, "isOptional": true, "description": "The geostatistical layer produced. This layer is required output only if no output raster is requested. ", "dataType": "Geostatistical Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster. This raster is required output only if no output geostatistical layer is requested. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "in_barrier_features", "isInputFile": true, "isOptional": true, "description": "Absolute barrier features using non-Euclidean distances rather than line-of-sight distances. ", "dataType": "Feature Layer"}, {"name": "bandwidth", "isOptional": true, "description": " Used to specify the maximum distance at which data points are used for prediction. With increasing bandwidth, prediction bias increases and prediction variance decreases. ", "dataType": "Double"}, {"name": "number_iterations", "isOptional": true, "description": " The iteration count controls the accuracy of the numerical solution because the model solves the diffusion equation numerically. The larger this number, the more accurate the predictions, yet the longer the processing time. The more complex the barrier's geometry and the larger the bandwidth, the more iterations are required for accurate predictions. ", "dataType": "Long"}, {"name": "weight_field", "isOptional": true, "description": "Used to emphasize an observation. The larger the weight, the more impact it has on the prediction. For coincident observations, assign the largest weight to the most reliable measurement. ", "dataType": "Field"}, {"name": "in_additive_barrier_raster", "isInputFile": true, "isOptional": true, "description": "The travel distance from one raster cell to the next based on this formula: (average cost value in the neighboring cells) x (distance between cell centers). ", "dataType": "Raster Layer"}, {"name": "in_cumulative_barrier_raster", "isInputFile": true, "isOptional": true, "description": "The travel distance from one raster cell to the next based on this formula: (difference between cost values in the neighboring cells) + (distance between cell centers). ", "dataType": "Raster Layer"}, {"name": "in_flow_barrier_raster", "isInputFile": true, "isOptional": true, "description": "A flow barrier is used when interpolating data with preferential direction of data variation, based on this formula: Indicator (cost values in the to neighboring cell > cost values in the from neighboring cell) * (cost values in the to neighboring cell - cost values in the from neighboring cell) + (distance between cell centers), where indicator(true) = 1 and indicator(false) = 0. ", "dataType": "Raster Layer"}]},
{"syntax": "ConsolidateLocator_geocoding (in_locator, output_folder, {copy_arcsde_locator})", "name": "Consolidate Locator (Geocoding)", "description": "\r\nConsolidate a locator or composite locator  by copying all locators into a single folder.", "example": {"title": "ConsolidateLocator example 1 (Python window)", "description": "The following Python script demonstrates how to use the ConsolidateLocator tool from the Python window:", "code": "import arcpy arcpy.env.workspace = \"C:/MyData/Locators\" arcpy.ConsolidateLocator_Geocoding ( 'Atlanta_composite' , 'Consolidate_folder' , \"COPY_ARCSDE\" )"}, "usage": ["This tool is located in the Geocoding  toolbox.  For convenience, a copy of this tool also resides in the Data Management toolbox Package toolset.", "A warning is issued when this tool encounters an invalid locator. The invalid locator will not be packaged. ", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is checked", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is unchecked"], "parameters": [{"name": "in_locator", "isInputFile": true, "isOptional": false, "description": " The input locator or composite locator that will be consolidated. ", "dataType": "Locator"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " The output folder that will contain the locator or composite locator with its participating locators. ", "dataType": "Folder"}, {"name": "copy_arcsde_locator", "isOptional": true, "description": "Specifies whether participating locators will be copied or their connection information will be preserved in the composite locator. This option only applies to composite locators. COPY_ARCSDE \u2014 All participating locators, including locators in ArcSDE, will be copied to the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Connection information of the participating locators that are stored in ArcSDE will be preserved in the composite locator. ", "dataType": "Boolean"}]},
{"syntax": "PackageLocator_geocoding (In_locator, output_file, {copy_arcsde_locator}, {additional_files}, {summary}, {tags})", "name": "Package Locator (Geocoding)", "description": "\r\nPackage a locator or composite locator  to create a single compressed  .gcpk  file. Learn more about sharing your locator as a locator package", "example": {"title": "PackageLocator example 1 (Python window)", "description": "The following Python script demonstrates how to use the PackageLocator tool from within the Python window.", "code": "import arcpy arcpy.env.workspace = \"C:/MyData/Locators\" arcpy.PackageLocator_geocoding ( 'Atlanta_composite' , 'Altanta_composite.gcpk' , \"COPY_ARCSDE\" , \"#\" , \"Summary of package\" , \"tag1; tag2; tag3\" )"}, "usage": ["This tool is located in the Geocoding  toolbox.  For convenience, a copy of this tool also resides in the Data Management toolbox Package toolset.", "The input locator must have a description in order for the tool to execute.  To add  summary and tags, click the ", "Description", " tab of a locator in ArcCatalog, and click   the ", "Edit", " button ", " to enter the information on the ", "Item Description", " window.", "A warning is issued when this tool encounters an invalid locator. The invalid locator will not be packaged. ", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is checked", "If the  locator to be consolidated or packaged is a composite locator and the ", "Composite locator only: copy participating locators in ArcSDE database instead of referencing them", " option is unchecked", "The locator package file (", ".gcpk", ") can be shared with other users or can be loaded to your ArcGIS online account. ", "Learn more about sharing a package", "To unpack a locator package, drag the ", ".gcpk", " file into ", "ArcMap", " or right-click on the ", ".gcpk", " file and click ", "Unpack", ".   ", "   ", "Unpack", " will extract a package into your user profile under: ", "Alternatively, you can use the ", "Extract Package", " tool and specify an output folder.", "Learn more about unpacking a locator package"], "parameters": [{"name": "In_locator", "isInputFile": true, "isOptional": false, "description": "The locator or composite locator that will be packaged. ", "dataType": "Input address locator"}, {"name": "output_file", "isOutputFile": true, "isOptional": false, "description": " The name and location of the output locator package ( .gcpk ). ", "dataType": "File"}, {"name": "copy_arcsde_locator", "isOptional": true, "description": "Specifies whether participating locators will be copied or their connection information will be preserved in the composite locator. This option only applies to composite locators. COPY_ARCSDE \u2014 All participating locators, including locators in ArcSDE, will be copied to the consolidated folder or package. This is the default. PRESERVE_ARCSDE \u2014 Connection information of the participating locators that are stored in ArcSDE will be preserved in the composite locator. ", "dataType": "Boolean"}, {"name": "additional_files", "isOptional": true, "description": "Adds additional files to a package. Additional files, such as .doc , .txt , .pdf , and so on, are used to provide more information about the contents and purpose of the package. ", "dataType": "File"}, {"name": "summary", "isOptional": true, "description": "Adds Summary information to the properties of the package. ", "dataType": "String"}, {"name": "tags", "isOptional": true, "description": "Adds Tag information to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. ", "dataType": "String"}]},
{"syntax": "StandardizeAddresses_geocoding (in_address_data, in_input_address_fields, in_address_locator_style, in_output_address_fields, out_address_data, {in_relationship_type})", "name": "Standardize Addresses (Geocoding)", "description": "Standardizes the address information in a table or feature class.  Addresses are often presented in different forms that may contain various abbreviations of words, such as \"W\" for \"WEST\" or  \"RD\" for \"ROAD\".  Based on an address style you select,   the address can be broken into multiple parts, such as  House Number, Prefix Direction, Prefix Type, Street Name and Street Type. Each part will contain a  piece of address information  and the standardized value, such as \"1ST\" instead of \"FIRST\" as Street Name, \"AVE\" instead of \"AVENUE\" as Street Type.    The address style specifies the  components of an address and determines how the components are ordered and standardized. The input address you want to standardize  can be stored in a single field. If the address information  has already been split into multiple fields in the input feature class or table, this tool can concatenate the fields on the fly and standardize the information. ", "example": {"title": "StandardizeAddresses Example (Python Window)", "description": "The following Python window script demonstrates how to use the StandardizeAddresses function in immediate mode. ", "code": "import arcpy env.workspace = \"C:/ArcTutor/geocoding/atlanta.gdb\" # Set local variables: input_feature_class = \"streets\" address_fields = \"ID;FULL_STREET_NAME\" locator_style = \"US Address-Dual Ranges\" standardized_fields = \"PreDir;PreType;StreetName;SufType;SufDir\" standardized_feature_class = \"StandardizedStreet\" arcpy.StandardizeAddresses_geocoding ( input_feature_class , address_fields , locator_style , standardized_fields , standardized_feature_class , \"Static\" )"}, "usage": ["The input address data can be a table or feature class that contains address attributes that can be standardized based on an address locator style.", "The input address you want to standardize  can be stored in a single field, such as the ", "Address", " field in a customer address table. You can then select the field as the ", "Input Address Field", " in the tool.", "If the address information  has already been broken into multiple fields in the input feature class or table which you can find   in common reference street or point address data, the standardization process will concatenate the fields on the fly and standardize the information.  You specify the fields in the order that  forms the complete address.", "When using an address locator style that handles street addresses, you may want to specify a house number field or any numeric field as the ", "Input Address Field", " along with other address attributes so that the concatenated address can be standardized correctly.  For example, consider the input string \"17 Mile Road\", \"Mile\" may be interpreted as a street name   instead of \"17 Mile\"  when no house number is given.  The confusion can be avoided if the input address is  \"101 17 Mile Road\". The ", "ObjectID", " field is a numeric field that you can use for this purpose, as illustrated above.", "The standardized result can be saved in one of the following two output options:"], "parameters": [{"name": "in_address_data", "isInputFile": true, "isOptional": false, "description": "The table or feature class containing address information that you want to standardize. ", "dataType": "Table View"}, {"name": "in_input_address_fields", "isInputFile": true, "isOptional": false, "description": "The set of fields in the input table or feature class that, when concatenated, forms the address to be standardized. ", "dataType": "Field"}, {"name": "in_address_locator_style", "isInputFile": true, "isOptional": false, "description": "The address locator style to use to standardize the address information in the input table or feature class. ", "dataType": "Address Locator Style"}, {"name": "in_output_address_fields", "isInputFile": true, "isOptional": false, "description": "The set of standardized address fields to include in the output table or feature class. ", "dataType": "Field"}, {"name": "out_address_data", "isOutputFile": true, "isOptional": false, "description": "The output table or feature class to create containing the standardized address fields. ", "dataType": "Table;Feature Class"}, {"name": "in_relationship_type", "isInputFile": true, "isOptional": true, "description": "Indicates whether to create a static or dynamic output dataset. The option only works if both the input and output datasets are stored in the same geodatabase workspace. This option is only supported if you have ArcGIS for Desktop Standard or Advanced licences. An error message saying \"Standardize addresses failed\" will be displayed if you do not have the proper license. Static \u2014 Creates an output table or feature class that contains a copy of the rows or features in the input table and the standardized address fields. This is the default option. Dynamic \u2014 Creates a table containing the standardized address fields and a relationship class that joins to the input table or feature class. Note: The option only works if both the input and output datasets are stored in the same geodatabase workspace. License: This option is only supported if you have ArcGIS for Desktop Standard or Advanced licences. An error message saying \"Standardize addresses failed\" will be displayed if you do not have the proper license.", "dataType": "Boolean"}]},
{"syntax": "ReverseGeocode_geocoding (in_features, in_address_locator, out_feature_class, {address_type}, {search_distance})", "name": "Reverse Geocode (Geocoding)", "description": " Creates addresses from point locations in a feature class. The reverse geocoding process searches for the nearest address or intersection for the point location based on the specified search distance.", "example": {"title": "ReverseGeocode Example (Python Window)", "description": "The following Python window script demonstrates how to use the ReverseGeocode function in immediate mode. ", "code": "# Import system modules import arcpy from arcpy import env env.workspace = \"C:/data/locations.gdb\" # Set local variables: input_feature_class = \"customers\" address_locator = \"e:/StreetMap/data/Street_Addresses_US\" result_feature_class = \"customers_with_address\" arcpy.ReverseGeocode_geocoding ( input_feature_class , address_locator , result_feature_class , \"ADDRESS\" , \"100 Meters\" )"}, "usage": [" The input feature class must contain point shapes with valid XY coordinates. Addresses will not be returned on points with null coordinates. ", "The output feature class will contain the same number of records as in the input feature class. Additional fields containing the result addresses   are added to the feature class. The names  of the fields are prefixed with \"REV_\".  If an address cannot be found, the fields will contain empty values.", "If the ", "spatial reference", " of the input feature class is different from the address locator, the address locator will transform the coordinates on the fly and try to find the match. The output feature class will be saved in the same spatial reference as the input feature class. Changing the spatial reference of the output feature class is possible by setting a different ", "output coordinate system", " in the tool's ", "environment settings", ". ", "If a point in the input feature class fails to return an address,   it means there are no features in the address locator that can be associated with the input point. Here are a few common causes for the unmatched points:", "You may increase the search distance so that   the chance to find the nearest address is higher, or use a different address locator that  contains more features or covers a bigger area to match the input points. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "A point feature class or layer from which addresses are returned based on the features' point location. ", "dataType": "Feature Class"}, {"name": "in_address_locator", "isInputFile": true, "isOptional": false, "description": " The address locator to use to reverse geocode the input feature class. ", "dataType": "Address Locator"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output feature class. ", "dataType": "Feature Class"}, {"name": "address_type", "isOptional": true, "description": " Indicates whether to return addresses for the points as street addresses or intersection addresses if the address locator supports intersection matching. ADDRESS \u2014 Returns street addresses or in the format defined by the input address locator. This is the default option. INTERSECTION \u2014 Returns intersection addresses. This option is available if the address locator supports matching intersection addresses.", "dataType": "String"}, {"name": "search_distance", "isOptional": true, "description": " The distance used to search for the nearest address or intersection for the point location. ", "dataType": "Linear unit"}]},
{"syntax": "RematchAddresses_geocoding (in_geocoded_feature_class, in_where_clause)", "name": "Rematch Addresses (Geocoding)", "description": "Rematches addresses in a geocoded feature class.", "example": {"title": "RematchAddresses Example (Python Window)", "description": "The following Python window script demonstrates how to use the RematchAddress function in immediate mode. ", "code": "# Rematch unmatched addresses in a geocoded feature class. # Import system modules import arcpy from arcpy import env env.workspace = \"C:/ArcTutor/Geocoding/atlanta.gdb\" # Set local variables: where_clause = \"Status\" = 'U' geocoded_feature_class = \"geocode_result\" arcpy.GeocodeAddresses_geocoding ( geocoded_feature_class , where_clause )"}, "usage": ["The input feature class has to be a feature class that was created by the ", "Geocode Addresses", " tool or a process in which a table of addresses was geocoded."], "parameters": [{"name": "in_geocoded_feature_class", "isInputFile": true, "isOptional": false, "description": "The geocoded feature class you want to rematch. ", "dataType": "Feature Class"}, {"name": "in_where_clause", "isInputFile": true, "isOptional": false, "description": "An SQL expression used to select a subset of features. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, or shapefiles, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD]. For more information on SQL syntax and how it differs between data sources, see SQL Reference . ", "dataType": "SQL Expression"}]},
{"syntax": "RebuildAddressLocator_geocoding (in_address_locator)", "name": "Rebuild Address Locator (Geocoding)", "description": "Rebuilds an address locator to update the locator with the current reference data. Since an address locator contains a snapshot of the reference data when it was created, it will not geocode addresses against the updated data  when the geometry and attributes of the reference data are changed.  To geocode addresses against the current version of the\r\nreference data, the address locator must be rebuilt if you want to\r\nupdate the changes in the locator.\r\n Learn more about updating your reference data", "example": {"title": "RebuildAddressLocator Example (Python Window)", "description": "The following Python window script demonstrates how to use the RebuildAddressLocator function in immediate mode. ", "code": "# Import system modules import arcpy from arcpy import env env.workspace = \"C:/ArcTutor/Geocoding/atlanta.gdb\" # Set local variables: address_locator = \"Atlanta_AddressLocator\" arcpy.RebuildAddressLocator_geocoding ( address_locator )"}, "usage": ["You must have write privileges to the address locator to use this tool.", "In order to rebuild an address locator, all the referenced feature classes and tables need to exist. Rebuilding the locator will fail If you have moved your reference data to another location since the last time the locator was created.   You may ", "repair the address locator", " by updating the locations of the reference data  in ", "ArcGIS for Desktop", ".    As an alternative, you can re-create an address locator using the ", "Create Address Locator", " tool. ", "Address locators based on a versioned geodatabase will be rebuilt with the same version used when the locator was created.", "Composite address locators cannot be rebuilt using this tool. The best practice for managing composite locators is to create a composite locator using the ", "Create Composite Address Locator", " tool in a geoprocessing model. In ModelBuilder, you can also chain the tool with the ", "Create Address Locator", " tool  to use  its outputs as the inputs to the composite locator and manage the entire process of creating or updating a composite address locator.", "Address locators created prior to ArcGIS version 9.2 cannot be rebuilt."], "parameters": [{"name": "in_address_locator", "isInputFile": true, "isOptional": false, "description": "The address locator to rebuild. ", "dataType": "Address Locator"}]},
{"syntax": "GeocodeAddresses_geocoding (in_table, address_locator, in_address_fields, out_feature_class, {out_relationship_type})", "name": "Geocode Addresses (Geocoding)", "description": "Geocodes a table of addresses.  This process requires a table that stores the addresses you want to geocode and an address locator or a composite address locator. This  tool matches the addresses against the address locator and saves the result for each input record  in a new point feature class.  ", "example": {"title": "GeocodeAddresses Example (Python Window)", "description": "The following Python window script demonstrates how to use the GeocodeAddress function in immediate mode. ", "code": "# Import system modules import arcpy from arcpy import env env.workspace = \"C:/ArcTutor/Geocoding/atlanta.gdb\" # Set local variables: address_table = \"customers\" address_locator = \"Atlanta_AddressLocator\" geocode_result = \"geocode_result\" arcpy.GeocodeAddresses_geocoding ( address_table , address_locator , \"Address Address VISIBLE NONE;City CITY VISIBLE NONE;State State VISIBLE NONE;Zip Zip VISIBLE NONE\" , geocode_result , STATIC )"}, "usage": ["The output feature class is saved in the same ", "spatial reference", " as the address locator. Changing the spatial reference of the output feature class is possible by setting a different ", "output coordinate system", " in the tool's ", "environment settings", ". ", "The output feature class, by default, stores a copy of input address and additional information such as score, status, and matched address of each record.  The addresses can be rematched either using the ", "Rematch Addresses", "       tool or the ", "Interactive Rematch", " dialog box in ArcMap. Editing addresses in the input address table will not change the result in the output feature class once the matching process finishes and the feature class is created. ", "Set the ", "Dynamic Output Feature Class", " parameter to true (checked) if you want the matching result in the output feature class to be  updated automatically when the input address table is updated. A relationship class is created for the input table and the output feature class. When an address in the input table is changed in an editing session in ArcMap, the address will be geocoded again immediately and the related record in the output feature class   will be updated with the new geocoding result. The automatic update is also supported for adding a new record or deleting an existing record in the input table.  ", "This option is only available if the input address table and output feature class are in the same geodatabase workspace. ", "This option is disabled if you are using an ", "ArcGIS for Desktop Basic", " license, as relationship classes cannot be generated with a ", "Basic", " licence."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table of addresses to geocode. ", "dataType": "Table View"}, {"name": "address_locator", "isOptional": false, "description": "The address locator to use to geocode the table of addresses. ", "dataType": "Address Locator"}, {"name": "in_address_fields", "isInputFile": true, "isOptional": false, "description": "The mapping of address fields used by the address locator to fields in the input table of addresses. Each field mapping in this parameter is in the format <input address field> <table field name> where <input address field> is the name of the input address field used by the address locator, and <table field name> is the name of the corresponding field in the table of addresses. If you choose not to map an optional input address field used by the address locator to a field in the input table of addresses, specify that there is no mapping by using \"<None>\" in place of a field name. To determine the alias name for a reference data field used by a locator style, open the Create Address Locator tool and choose the locator style. The name that appears in the Field Name column of the Field Map section is the field's alias name. ", "dataType": "Field Info"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output geocoded feature class or shapefile. ", "dataType": "Feature Class"}, {"name": "out_relationship_type", "isOutputFile": true, "isOptional": true, "description": "Indicates whether to create a static copy of the address table inside the geocoded feature class or to create a dynamically updated geocoded feature class. This option is only supported if you have ArcGIS for Desktop Standard or Advanced licences. An error message saying \"Geocode addresses failed\" will be displayed if you do not have the proper license. STATIC \u2014 Creates a static copy of the fields input address table in the output feature class. This is the default. DYNAMIC \u2014 Creates a relationship class between the input address table and output feature class so that edits to the addresses in the input address table are automatically updated in the output feature class. This option is supported only if the input address table and output feature class are in the same geodatabase workspace. License: This option is only supported if you have ArcGIS for Desktop Standard or Advanced licences. An error message saying \"Geocode addresses failed\" will be displayed if you do not have the proper license.", "dataType": "Boolean"}]},
{"syntax": "CreateCompositeAddressLocator_geocoding (in_address_locators, in_field_map, {in_selection_criteria}, out_composite_address_locator)", "name": "Create Composite Address Locator (Geocoding)", "description": "Creates a composite address locator. A composite address locator consists of two or more individual address locators that allow addresses to be matched against the multiple address locators.  \r\n Learn more about composite address locators \r\n", "example": {"title": "CreateCompositeAddressLocator Example (Python Window)", "description": "The following Python window script demonstrates how to use the CreateCompositeAddressLocator function in immediate mode. ", "code": "# Example 1: # Create a composite address locator using the StreetMap US Streets and Tutorial Atlanta locators. # Import system modules import arcpy from arcpy import env env.workspace = \"C:/ArcTutor/Geocoding/atlanta.gdb\" # Set local variables: US_Streets_locator = \"C:/dm_stmap_dvd/streetmap_na/data/Street_Addresses_US\" Atlanta_locator = Atlanta Atlanta_Composite = US_Atlanta_Composite arcpy.CreateCompositeAddressLocator_geocoding ( \"Atlanta_locator Atlanta;US_Streets_locator US_Streets\" , \"Address 'Street or Intersection' true true false 100 Text 0 0 ,First,#,Atlanta_locator,Address,0,0,US_Streets_locator,Street,0,0;City 'City or Placename' true true false 40 Text 0 0 ,First,#,Atlanta_locator,City,0,0,US_Streets_locator,City,0,0;State 'State' true true false 20 Text 0 0 ,First,#,Atlanta_locator,State,0,0,US_Streets_locator,State,0,0;Zip 'Zipcode' true true false 10 Text 0 0 ,First,#,Atlanta_locator,Zip,0,0,US_Streets_locator,ZIP,0,0\" , \"Atlanta ' \\\" City \\\"  = 'Atlanta'';US_Streets #\" , Atlanta_Composite )"}, "usage": ["Prior to creating composite address locators, use the ", "Create Address Locator", " tool to create participating address locators.", "Spatial reference for a composite address locator is required. The spatial reference of the first participating address locator is used unless you specify a different ", "output coordinate system", " in the tool's ", "environment settings", "."], "parameters": [{"name": "in_address_locators", "isInputFile": true, "isOptional": false, "description": "The order of the participating address locators determines how candidates are searched and an address is matched. When you geocode a single address, the address will be matched against all participating address locators unless the locator is specified with a selection criterion. All the found candidates will be displayed based on the order of the listed participating address locators. If you geocode a table of addresses, addresses are matched automatically to the first best candidate found from the first participating address locators. If the address fails to match, it will fall back to the subsequent locator in the list. A reference name for each participating address locator is required. This is the name of the address locator referred to by the composite address locator. The name should contain no space or special symbols. The maximum length of the name is 14 characters. ", "dataType": "Value Table"}, {"name": "in_field_map", "isInputFile": true, "isOptional": false, "description": "The mapping of input fields used by each participating address locator to the input fields of the composite address locator. ", "dataType": "Field Mappings"}, {"name": "in_selection_criteria", "isInputFile": true, "isOptional": false, "description": "Selection criteria for each participating address locator. Only one selection criterion is supported for each participating address locator. Using selection criteria will disqualify participating address locators that do not meet the criteria on a particular address so that the geocoding process will be more efficient. Refer to the topic Creating a composite address locator to learn more about the use of selection criteria in the geocoding process. ", "dataType": "Value Table"}, {"name": "out_composite_address_locator", "isOutputFile": true, "isOptional": false, "description": "The composite address locator to create. ", "dataType": "Address Locator"}]},
{"syntax": "CreateAddressLocator_geocoding (in_address_locator_style, in_reference_data, in_field_map, out_address_locator, {config_keyword})", "name": "Create Address Locator (Geocoding)", "description": "Creates an address locator. The address locator can be used to find a location of an address, geocode a table of addresses, or  get the address of a point location.  Learn more about common geocoding tasks", "example": {"title": "CreateAddressLocator Example (Python Window)", "description": "The following Python window script demonstrates how to use the CreateAddressLocator function in immediate mode. ", "code": "#   Create a street address locator using a street centerline feature class  #   in a file geodatabase as reference data. #   The new address locator will be created in the same file geodatabase. # Import system modules import arcpy from arcpy import env env.workspace = \"C:/ArcTutor/Geocoding/atlanta.gdb\" arcpy.CreateAddressLocator_geocoding ( \"US Address - Dual Ranges\" , \"streets Primary\" , \"'Feature ID' FeatureID VISIBLE NONE;'*From Left' L_F_ADD VISIBLE NONE;'*To Left' L_T_ADD VISIBLE NONE;'*From Right' R_F_ADD VISIBLE NONE;'*To Right' R_T_ADD VISIBLE NONE;'Prefix Direction' PREFIX VISIBLE NONE;'Prefix Type' PRE_TYPE VISIBLE NONE;'*Street Name' NAME VISIBLE NONE;'Suffix Type' TYPE VISIBLE NONE;'Suffix Direction' SUFFIX VISIBLE NONE;'Left City or Place' CITYL VISIBLE NONE;'Right City or Place' CITYR VISIBLE NONE;'Left Zipcode' ZIPL VISIBLE NONE;'Right Zipcode' ZIPR VISIBLE NONE;'Left State' State_Abbr VISIBLE NONE;'Right State' State_Abbr VISIBLE NONE\" , Atlanta_AddressLocator , \"\" )"}, "usage": ["Address locators can be created in the same workspace as the reference data or other workspace in a geodatabase or file folder you specify.", "The role of a reference dataset defines the role that it plays as reference data for the address locator. The address locator styles provided with ArcGIS use the following values to describe the roles of reference datasets:", "Custom locator styles or locator styles provided by third parties may define different roles for reference data feature classes and tables. Refer to their documentation for information on the roles that they define for reference datasets.", "Composite address locators cannot be created using this tool. Use the ", "Create Composite Address Locator", " tool to create a composite addres locator."], "parameters": [{"name": "in_address_locator_style", "isInputFile": true, "isOptional": false, "description": "The address locator style on which to base the new address locator. ", "dataType": "Address Locator Style"}, {"name": "in_reference_data", "isInputFile": true, "isOptional": false, "description": "The reference data feature classes and tables to be used by the address locator, along with their roles. Custom locator styles or locator styles provided by third parties may define a different set of roles for reference datasets. Primary table\u2014Defines the primary reference dataset feature class for a locator, such as a street centerline feature class. This is a required table. Alternate Name table\u2014Defines an alternate street name table that contains alternate names for the street or point features. The table is required to have a JoinID that can be used to join to the primary table. This table is optional. Alias table\u2014Defines a place name alias table that contains place names and actual addresses for the names. User can find the location using either the place name such as Field Museum or the address 1400 S Lakeshore Drive Chicago, IL 60605 . This table is optional.", "dataType": "Value Table"}, {"name": "in_field_map", "isInputFile": true, "isOptional": false, "description": "The mapping of reference data fields used by the address locator style to fields in the reference datasets. Each field mapping in this parameter is in the format: where <locator field alias> is the alias name for the reference data field used by the address locator, and <dataset field name> is the name of the field in the reference dataset. Fields with an asterisk (\" * \") next to their names are required by the address locator style. VISIBLE\u2014Field is visible; NONE\u2014the geometry is a copy of the original value. If you choose not to map an optional reference data field used by the address locator style to a field in a reference dataset, specify that there is no mapping by using \" <None> \" in place of a field name. To determine the alias name for a reference data field used by a locator style, open the Create Address Locator tool and choose the locator style. The name that appears in the Field Name column of the Field Map control is the field's alias name. ", "dataType": "Field Info"}, {"name": "out_address_locator", "isOutputFile": true, "isOptional": false, "description": "The address locator to create. ", "dataType": "Address Locator"}, {"name": "config_keyword", "isOptional": true, "description": "The configuration keyword that determines the storage parameters of the table in a Relational Database Management System (RDBMS)\u2014ArcSDE and file geodatabases only. ", "dataType": "String"}]},
{"syntax": "IterateRowSelection_mb (in_table, {fields}, {skip_nulls})", "name": "Iterate Row Selection (ModelBuilder)", "description": " Iterates over rows in a table.  \r\n Learn how Iterate Row Selection works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", " ", "Iterate Row Selection ", " calls ", "Make Table View", " to select a table record and creates two outputs: Selected Rows and Value. The output Selected Rows is a Table View and can be used as inputs to other geoprocessing tools that accept a Table View as input in ModelBuilder.", "More than one group by field can be used for the selection. If one or more group by fields are selected, the number of iterations is determined by the number of unique combinations of the group by fields. For each iteration, the selection of the output feature layer is determined by the number of records that match the given combination of the group by fields. If more than one group by field is chosen, then the values are concatenated <field value1>_<field value2> in the output inline variable name. See an example below where two\r\ngroup fields: call type and the crime period are used. The output ", "Value", " is Vandalism_Morning and so on for\r\nthe other values.\r\n", "If no group by field is chosen, the output value is the group by ObjectID field, and the selection is one record per selection.", "The tool has two outputs: Output Selected Rows and group Value of the ObjectID field for selected features, which could be used as ", "in-line variable", " ", "%Value%", " in other tools.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " Input table containing records to iterate through. ", "dataType": "Table View"}, {"name": "fields", "isOptional": false, "description": " Input field used to group the records for selection. Any number of input fields can be defined, resulting in a selection based on a unique combination of the fields. Define the null value for the field. By default, null values are included in the selection unless specified or the option to skip all null values is checked. The default values are \"\" for strings and \"0\" for numbers. ", "dataType": "Value Table"}, {"name": "skip_nulls", "isOptional": true, "description": " Determines if null values in the grouping fields are skipped during selection. Checked\u2014Skip through all the null values in the grouping fields during selection. Unchecked\u2014Set as default. Include all the null values in the grouping fields during selection.", "dataType": "Boolean"}]},
{"syntax": "IterateRasters_mb (in_workspace, {wildcard}, {raster_format}, {recursive})", "name": "Iterate Rasters (ModelBuilder)", "description": " Iterates over rasters in a Workspace or a Raster Catalog. \r\n Learn how Iterate Rasters works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "Rasters in a workspace or Raster Catalog can be restricted to iterate over raster types, such as ASC, BIL, BIP, BMP, BSQ, CIT, COT, DT0, DT1, DT2, ERS, GIF, GIS, GRID , HDF, IMG, JP2, JPG, LAN, LGG, NTF, OVR, PNG, RAW, RPF, SID, and TIF.", "The tool has two outputs: Output Raster and Name, which could be used as ", "in-line variable", " ", "%Name%", " in other tools.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": " Workspace or a Raster Catalog containing the rasters to iterate through. ", "dataType": "Workspace; Raster Catalog"}, {"name": "wildcard", "isOptional": true, "description": " Combination of * and characters that help to limit the results. The asterisk is the same as saying ALL. If no wildcard is specified, all inputs will be returned. For example, it can be used to restrict Iteration over input names starting with a certain character or word (e.g., A* or Ari* or Land* ,and so on). ", "dataType": "String"}, {"name": "raster_format", "isOptional": true, "description": " Choose the Raster Format, such as ASC, BIL, BIP, BMP, BSP, BSQ, CIT, COT, DT0, DT1, DT2, ERS, GIF, GIS, GRID, HDF, IMG, JP2, JPG, LAN, LGG, NTF, PNG, RAW, RPF, SID, and TIF, or type in other extensions. ", "dataType": "String"}, {"name": "recursive", "isOptional": true, "description": " Determines if subfolders of the main folder will be iterated through recursively. Checked\u2014Will iterate through all subfolders. Unchecked\u2014Set as Default. Will not iterate through all subfolders.", "dataType": "Boolean"}]},
{"syntax": "IterateFiles_mb (in_folder, {wildcard}, {extension}, {recursive})", "name": "Iterate Files (ModelBuilder)", "description": " Iterates over files in a folder.  \r\n Learn how Iterate Files works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", " The ", "File Extension", " option allows you to iterate over files with a particular extension.  For example, if you only want files with the extension of ", ".log", " to be iterated, set the ", "File Extension", " parameter to ", "log", ".  ", " If the file appears as a dataset in the ", "Catalog", " window, then ", "Iterate Files", " will skip the file. The most common examples are shape files, .dbf files, raster files (.jpg, .tif, .img, .png, and .bmp), CAD files (.dxf and .dwg), map files (.mxd), and personal geodatabases (.mdb). ", "The tool has two outputs: Output File and Name which could be used as an ", "in-line variable", " (for example, ", "%Name%", ") in other tools.", "Specify the file extension in the optional parameter for extensions, such as .txt, .pdf, .zip, and so on.  For example, if the tool iterates over text files and the output is used in tools, such as ", "ASCII To Raster", " tool, specify the extension .txt in the optional file extension parameter. ", "Most tools honor the output file format from the ", "Iterate Files", " tool except tools such as ", "Copy", ", which requires you to define the file extension in the output name. For example, If you are using the ", "Iterate Files", " tool to iterate ", ".zip", " files for use as input to the ", "Copy", " tool, the output name must contain the extension ", "Name.zip", " or ", "%Name%.zip", ".  ", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_folder", "isInputFile": true, "isOptional": false, "description": " Folder in which the input files are located. ", "dataType": "Folder"}, {"name": "wildcard", "isOptional": true, "description": " Combination of * and characters that help to limit the results. The asterisk is the same as saying ALL. If no wildcard is specified, all inputs will be returned. For example, it can be used to restrict Iteration over input names starting with a certain character or word (e.g., A* or Ari* or Land* ,and so on). ", "dataType": "String"}, {"name": "extension", "isOptional": true, "description": "The file extension, such as TXT, ZIP, and so on. Only files with the extension will be iterated. Do not use a period before the file extension. ", "dataType": "String"}, {"name": "recursive", "isOptional": true, "description": " Determines if all subfolders of the main folder will be recursively iterated through. Checked\u2014Will iterate through all subfolders. Unchecked\u2014Will not iterate through all subfolders.", "dataType": "Boolean"}]},
{"syntax": "IterateFieldValues_mb (in_table, field, {data_type}, {unique_values}, {skip_nulls}, {null_value})", "name": "Iterate Field Values (ModelBuilder)", "description": " Iterates over each value in a field. \r\n Learn how Iterate Field Values works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "The output of the tool is a variable named ", "Value", " and contains the value of the field.  This variable can be used in for ", "in-line variable substitution", " (", "%Value%", ").", " The ", "Data Type", " parameter specifies the data type of the output variable.  The default data type is string, but depending on how the output will be used in the model, different data types may be specified. For example, if your field contains the path to a feature class, you can set the ", "Data Type", " to Feature Class and use the output variable as input to a tool that accepts a feature class.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " Input table that will be iterated over. ", "dataType": "Table View"}, {"name": "field", "isOptional": false, "description": " Input field for iteration. ", "dataType": "Field"}, {"name": "data_type", "isOptional": true, "description": "The data type of the output value. The default data type is String, but depending on how the output will be used in the model, different data types may be specified. For example, if your field contains the path to a feature class, you can set the Data Type to Feature Class and use the output variable as input to a tool that accepts a feature class. ", "dataType": "String"}, {"name": "unique_values", "isOptional": true, "description": " Determines if iteration values will be based on unique values. Checked\u2014The iteration values will be based on the unique value of the specified field. Unchecked\u2014The iteration will run for each record in the input table.", "dataType": "Boolean"}, {"name": "skip_nulls", "isOptional": true, "description": " Determines if null values in the field will be skipped. Checked\u2014Will skip through all the null values in the field during selection. Unchecked\u2014Will include all the null values in the field during selection.", "dataType": "Boolean"}, {"name": "null_value", "isOptional": true, "description": " Specify the Null Value to skip, such as -9999, Null, -1. The default values are \"\" for strings and \"0\" for numbers. ", "dataType": "String"}]},
{"syntax": "IterateFeatureSelection_mb (in_features, {fields}, {skip_nulls})", "name": "Iterate Feature Selection (ModelBuilder)", "description": " Iterates over features in a feature class.  \r\n Learn how Iterate Feature Selection works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "Iterate Feature Selection", " calls ", "Make Feature Layer", " to make a new selection and creates two outputs: Selected Features and Value. The output Selected Features is a feature layer and can be used in other geoprocessing tools that accept a feature layer in ModelBuilder.", "Complex feature classes, such as annotation and dimensions, are not supported by this tool.", "More than one group by field can be used for the selection. If one or more group by fields are selected, the number of iterations is determined by the number of unique combinations of the group by fields. For each iteration, the selection of the output feature layer is determined by the number of records that match the given combination of the group by fields. If more than one group by field are chosen, then the values are concatenated <field value1>_<field value2> in the output inline variable name. See an example below where two group fields: Name of the hurricane and the Category of the hurricane are used. The output Value is 'Katrina_H5' and so on for the other values.", "If no group by field is chosen, the output value is the group by ObjectID field, and the selection is one record per selection. If the table does not have an ObjectID (OID) field, such as an Excel table, then the value is blank.", "The tool has two outputs: Output Selected Features and Group Value of the field for selected features, which could be used as ", "in-line variable", " ", "%Value%", " in other tools.", "The temporary feature layer can be saved as a layer file using the ", "Save To Layer File", " tool or saved as a new feature class using the ", "Copy Features", "  tool.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " Input feature class or layer containing features to iterate. ", "dataType": "Feature Layer"}, {"name": "fields", "isOptional": false, "description": " Input field or fields used to group the features for selection. Any number of input fields can be defined, resulting in a selection based on a unique combination of the fields. If no field is specified, the OID is used to iterate over features. Define the null value for the field. By default, null values are included in the selection unless specified or the option to skip all null values is checked. The default values are \"\" for strings and \"0\" for numbers. ", "dataType": "Value Table"}, {"name": "skip_nulls", "isOptional": true, "description": " Determines if null values in the grouping field or fields are skipped during selection. Checked\u2014Skip through all the null values in the grouping fields during selection. Unchecked\u2014Set as default. Include all the null values in the grouping fields during selection.", "dataType": "Boolean"}]},
{"syntax": "IterateFeatureClasses_mb (in_workspace, {wildcard}, {feature_type}, {recursive})", "name": "Iterate Feature Classes (ModelBuilder)", "description": " Iterates over feature classes in a Workspace or Feature Dataset. \r\n Learn how Iterate Feature Classes works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", " When this tool is used in a model, the first feature class is used as a template for the output. This allows for selection of fields in subsequent tools.", "You can iterate over feature classes of any geometry type, such as Annotation, Arc, Dimension, Edge, Junction, Label, Line, Node, Point, Polygon, Region, Route, or TIC.", "The tool has two outputs: Output Feature Class and Name, which could be used as ", "in-line variable", " (e.g. ", "%Name%", ") in other tools.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": " Workspace or feature dataset which stores the feature classes to iterate. If you define a geodatabase as your input workspace only the feature classes directly under the geodatabase will be iterated over (standalone feature classes). To iterate over all feature classes within a dataset located in the input geodatabase check the recursive option. ", "dataType": "Workspace; Feature Dataset"}, {"name": "wildcard", "isOptional": true, "description": " Combination of * and characters that help to limit the results. The asterisk is the same as saying ALL. If no wildcard is specified, all inputs will be returned. For example, it can be used to restrict Iteration over input names starting with a certain character or word (e.g., A* or Ari* or Land* ,and so on). ", "dataType": "String"}, {"name": "feature_type", "isOptional": true, "description": " The feature type to be used as a filter. Only features of the specified type will be output. Not specifying a Feature Type means that all features will be output. ANNOTATION \u2014 Only annotation feature classes will be the output. ARC \u2014 Only arc features classes will be the output. DIMENSION \u2014 Only dimension feature classes will be the output. EDGE \u2014 Only edge feature classes will be the output. JUNCTION \u2014 Only junction feature classes will be the output. LABEL \u2014 Only label features classes will be the output. LINE \u2014 Only line feature classes will be the output. NODE \u2014 Only node features (from a coverage) will be the output. POINT \u2014 Only point feature classes will be the output. POLYGON \u2014 Only polygon feature classes will be the output. REGION \u2014 Only region features (from a coverage) will be the output. ROUTE \u2014 Only route features (from a coverage) will be the output. TIC \u2014 Only tic features (from a coverage) will be the output.", "dataType": "String"}, {"name": "recursive", "isOptional": true, "description": "Determines if the iterator will iterate through all sub-folders in the main workspace. Checked\u2014Will iterate through all subfolders. Unchecked\u2014Will not iterate through all subfolders.", "dataType": "Boolean"}]},
{"syntax": "IterateDatasets_mb (in_workspace, {wildcard}, {dataset_type}, {recursive})", "name": "Iterate Datasets (ModelBuilder)", "description": " Iterates over datasets in a Workspace or Feature Dataset. \r\n Learn how Iterate Datasets works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "Iterates over datasets of the following type: CAD, Coverage, Feature Dataset, Geometric Network, Network, Raster, Raster Catalog, Terrain, TIN, and Topology.", "The tool has two outputs: Output Dataset and Name which could be used as ", "in-line variable", " (e.g. ", "%Name%", ") in other tools. ", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": " Workspace folder or a Feature Dataset which stores the dataset to iterate. ", "dataType": "Workspace; Feature Dataset"}, {"name": "wildcard", "isOptional": true, "description": " Combination of * and characters that help to limit the results. The asterisk is the same as saying ALL. If no wildcard is specified, all inputs will be returned. For example, it can be used to restrict Iteration over input names starting with a certain character or word (e.g., A* or Ari* or Land* ,and so on). ", "dataType": "String"}, {"name": "dataset_type", "isOptional": true, "description": "The Dataset Type, such as CAD, Cover, Feature, GeometricNetwork, Network, Raster, RasterCatalog, Terrain, TIN, and Topology. ", "dataType": "String"}, {"name": "recursive", "isOptional": true, "description": " Determines if subfolders in the input workspace will be iterated through recursively. Checked\u2014Will iterate through all subfolders. Unchecked\u2014Will not iterate through all subfolders.", "dataType": "Boolean"}]},
{"syntax": "For_mb (from, to, increment)", "name": "For (ModelBuilder)", "description": " Iterates over a starting and ending value by a given value. It works exactly like  For  in any scripting/programming language, executing through a set number of items. \r\n Learn how For works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "The tool increments the starting value until the maximum specified limit is reached.  For example, If the ", "From Value", " is 0 and ", "To Value", " is 10 to be incremented ", "By Value", " 3, the iteration will increment until value 9.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model.", " The ", "For", " tool replaces ", "Iterate By Count", " or ", "Iterate By Variable", " that previously existed in the ", "Iteration Options", " of Model Properties. "], "parameters": [{"name": "from", "isOptional": false, "description": " Value to start the iteration from. ", "dataType": "Long"}, {"name": "to", "isOptional": false, "description": "Value to run the iteration to. ", "dataType": "Long"}, {"name": "increment", "isOptional": false, "description": " Value to increment by. ", "dataType": "Long"}]},
{"syntax": "Stop_mb (in_values, {condition})", "name": "Stop (ModelBuilder)", "description": "For the set of input values, iteration will continue if all the inputs are true and stop if any one of the inputs is false. It is functionally similar to the  While  iterator but is useful to stop a model if there is one  While iterator  in a model and no additional iterators can be added. \r\n Learn how Stop works in ModelBuilder", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting."], "parameters": [{"name": "in_values", "isInputFile": true, "isOptional": false, "description": " Input values to check whether to stop the model iteration. ", "dataType": "Multiple Value"}, {"name": "condition", "isOptional": true, "description": " Choose True of False to set the condition. True \u2014 Iteration will run until all the input values are True. This is the default. False \u2014 Iteration will run until all in input values are False.", "dataType": "String"}]},
{"syntax": "SelectData_mb (in_dataelement, {out_dataelement})", "name": "Select Data (ModelBuilder)", "description": " The  Select Data  tool selects data in a parent data element such as a folder, geodatabase, feature dataset, or coverage. The tool allows access to the data stored inside a parent container, such as feature classes or tables inside a geodatabase. \r\n Learn how Select Data works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "Selecting the child from the parent using this tool enables you to continue processing after performing a task where the output data is a container, such as a feature dataset, and the next tool in the model requires a feature class.", "The tool's output always includes the full path to the child dataset.", "When building models where the input to ", "Select Data", " does not exist, the name of the child data element may need to be typed in."], "parameters": [{"name": "in_dataelement", "isInputFile": true, "isOptional": false, "description": "The input data element can be a folder, geodatabase, feature dataset, or coverage. ", "dataType": "Data Element; Composite Layer"}, {"name": "out_dataelement", "isOutputFile": true, "isOptional": true, "description": "The child data element is contained by the input data element. Once the input data element is specified, the child data element control contains a drop-down list of the data elements contained in the input data element. For example, if the input is a feature dataset, all the feature classes within the feature dataset are included in the drop-down list. A single element is selected from this list. ", "dataType": "String"}]},
{"syntax": "ParsePath_mb (in_data_element, {parse_type})", "name": "Parse Path (ModelBuilder)", "description": " The  Parse Path  tool parses the input into its file, path, name, or extension. The output can be used as  in-line variables  in the output name of other tools. \r\n Learn how Parse Path works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "Parsing results are controlled by the ", "Parse Type", " parameter.  Example: If the input to the ", "Parse Path", " tool is ", "C:\\ToolData\\InputFC.shp", ", then", "Parse Type", "Result", "FILE", "PATH", "NAME", "EXTENSION", "The same functionality can be accessed in scripting with the Python ", "os", " module. For example if you pass an input variable:", "Input =  \"C:\\ToolData\\InputFC.shp\"", ", then", "import os", " os.path.basename(Input)", "import os", " os.path.dirname(Input)", "import os", " ", "os.path.basename(Input).rstrip(os.path.splitext(Input)[1])", "import os", " os.path.splitext(Input)[1].lstrip(\".\")", "The output of Parse Path is a string and cannot be connected directly as an input to the tools such as ", "Create Feature Class", " in parameters like Feature Class Location which requires a workspace data type as input. Use %Value% inline variable substitution in such cases as shown below:"], "parameters": [{"name": "in_data_element", "isInputFile": true, "isOptional": false, "description": " Input values that you want to parse. ", "dataType": "Any value"}, {"name": "parse_type", "isOptional": true, "description": " Choose a parse type from File, Path, Name, or Extension. Given the input value of C:\\ToolData\\InputFC.shp : FILE \u2014 Output will be the file. Example: InputFC.shp PATH \u2014 Output will be the file path. Example: C:\\ToolData NAME \u2014 Output will be the file name. Example: InputFC EXTENSION \u2014 Output will be the file extension. Example: shp", "dataType": "String"}]},
{"syntax": "MergeBranch_mb ({in_values})", "name": "Merge Branch (ModelBuilder)", "description": "The  Merge Branch  tool merges two or more logical branches into a single output.  Branching in a model is accomplished by creating a script tool that implements the necessary if-then-else logic. It is often the case when branching that you need to merge two branches into a single process. What this means is that if you test an input against a condition (examples: whether the data exists on the disk, whether the cell size is greater than 30 meters, whether the field value is 1), it will create two outputs: True, if the condition is true, and False, if the condition is false. If the condition is True, you want to run some processes and if the condition is False, you want different processes to run, as illustrated below.   At any point, only one of the branches will run depending on the condition and the input. The  Merge Branch  tool is used in such cases where it is not possible to say which branch will run and produce results. The output of both branches becomes the input for the  Merge Branch  tool. The tool looks at the inputs and passes the last output of a branch that has-been-run to the next tool. The  Merge Branch  tool allows any number of inputs and uses the multivalue parameter control.", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "The tool examines the list of input variables and returns the first variable that is in the has-been-run state.", "Merge Branch", " accepts any data type in its list of values. The output data type is Any Value, which is a generic data type. This means you can connect the output of ", "Merge Branch", " to any parameter of any tool. When the connected tool is run, it expects the contents of an Any Value variable to be of the correct data type; it is up to you to make sure that the contents are correct for the tool's parameter.", "\r\n", "All the tools in ArcGIS are empty (without color) when added in a model except ", "Merge Branch", " and ", "Collect Values", ". Unlike other system tools, ", "Merge Branch", " is always a ready-to-run state (colored in). This is because the input to ", "Merge Branch", " is a multiple value data type, and an empty multiple value is considered a valid input."], "parameters": [{"name": "in_values", "isInputFile": true, "isOptional": false, "description": "List of values from different branches. The first ready-to-run state value in the list will be the output of the tool. ", "dataType": "Multiple Value"}]},
{"syntax": "GetFieldValue_mb (in_table, field, {data_type}, {null_value})", "name": "Get Field Value (ModelBuilder)", "description": " The  Get Field Value  tool gets the value of the first row of a table for the specified field.  \r\n Learn how Get Field Value works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "This tool differs from the ", "Iterate Field Value", " tool as only the first row of the input table is read, and the value of the specified field in this row becomes the output."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": " Input table to get the value from. ", "dataType": "Table View"}, {"name": "field", "isOptional": false, "description": " Input field to get the value from. The value of the first record will be output. ", "dataType": "Field"}, {"name": "data_type", "isOptional": true, "description": " The data type of the output. ", "dataType": "String"}, {"name": "null_value", "isOptional": true, "description": "The value to use for null values. The default is 0 for numbers and blank (\"\") for strings. ", "dataType": "String"}]},
{"syntax": "CollectValues_mb (in_value)", "name": "Collect Values (ModelBuilder)", "description": " The  Collect Values  tool is designed to collect output values of an iterator or to convert a list of multivalues into a single input. The output of  Collect Values  can be used as input to tools like  Merge ,  Append ,  Mosaic , and  Cell Statistics .  \r\n Learn how Collect Values works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "\r\n", "All the tools in ArcGIS are empty (without color) when added in a model except ", "Collect Values", " and ", "Merge Branch", ". Unlike other system tools, ", "Collect Values", " is always in a ready-to-run state (colored in). This is because the input to ", "Collect Values", " is a multiple value data type, and an empty multiple value is considered a valid input. ", "Tool outputs with the ", "Add To Display", "  option checked are added to the display in ", "ArcMap", " using the variable name. If you are using an iterator in the model and want to  add the outputs of all the iterations to display in ", "ArcMap", " with the actual unique output name instead of the name of the variable,  connect the output to be displayed to ", "the Collect Value", " tool, then right-click the output of ", "Collect Value", ", and check the ", "Add To Display", " option. If the model will be run from the model tool dialog box, then make the output of ", "Collect Value", " a ", "model parameter", ", since only the output model parameters are added to display. "], "parameters": [{"name": "in_value", "isInputFile": true, "isOptional": false, "description": " Input values to be collected. ", "dataType": "Multiple Value"}]},
{"syntax": "CalculateValue_mb (expression, {code_block}, {data_type})", "name": "Calculate Value (ModelBuilder)", "description": " Calculate Value tool returns a value based on a specified Python expression.", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "The ", "Data Type", " parameter is used in ModelBuilder to help chain the output of the ", "Calculate Value", " tool with other tools. For example, if you use the ", "Calculate Value", " tool to calculate a distance for use as input to the ", "Buffer Distance", " parameter of the ", "Buffer", " tool, specify Linear Unit for the ", "Data Type", " parameter.", "Variables created in ModelBuilder can be used by this tool, but variables desired for use in the expression parameter cannot be connected to the ", "Calculate Value", " tool. To use them in the expression, enclose the variable name in percent signs (", "%", "). For example, if you want to divide a variable named 'Input' by 100, your expression would be ", "%Input%/100", ". ", "Note: in the previous expression, if Input = 123, the expression will return 1. To get decimal places, add decimals to the values in the expression. For example: ", "%Input%/100.00", " will return 1.23.  The illustration below shows another example of using variables in the expression.", "In-line variable of type string should be enclosed within quotes (", "\"%string variable%\"", ") in an expression. In-line variables of type numbers (double, long) do not require quotes (", "%double%", ").  ", "Expressions can be created in a standard Python format ONLY. Other scripting languages are not supported.", "The ", "Calculate Value", " tool can evaluates simple mathematical expressions. For example:", "The ", "Calculate Value", " tool allows the use of the Python math module to perform more complex mathematical operations. The math module is accessed by preceding the desired function with \"math\". For example:", "Constants are also supported through the math module. For example:", "The ", "arcgis.rand()", " function is supported. The ", "arcgis.rand()", " function has been created for ArcGIS tools and should not be confused with the Python ", "Rand()", " function. Examples of using the ", "arcgis.rand()", " are as follows:", "The expression ", "arcgis.rand", " must be entered in lowercase characters.", "Generally, you will type the expressions in the ", "Expression", " parameter. More complicated expressions, such as multiline calculations or logical operations (if, then), will require the use of the ", "Code Block", " parameter. The ", "Code Block", " parameter cannot be used on its own; it must be used in conjunction with the ", "Expression", " parameter. ", "Variables defined in the ", "Code Block", " parameter can be referenced from the expression.", "Functions can be defined in the ", "Code Block", " parameter and called from the expression. In the example below, the function returns a wind direction string based on a random input value. In Python, functions are defined using the ", "def", " keyword followed by the name of the function and the function's input parameters. In this case, the function is ", "getWind", " and has one parameter, ", "wind", ". Values are returned from a function using the ", "return", " keyword.", "You can pass variable through the Expression parameter and use if-else logic with inline variables in the code block as shown  below.  The code block checks to see if the ", "Input Cell Size", " variable is empty then returns a value based on the condition.", "Python methods can be used directly in the Expression parameter of the tool. For example, if  you have an input value with a decimal (field value of the input table in this case) and want to use the value in the output name of another tool through ", "inline variable substitution", ", the decimal can be replaced using Python method ", "replace", " in the ", "Calculate Value", " tool expression.", "Python modules can be called and methods such as replace combined or stacked in the code block parameter. In the example below the  ", "time", " module is imported in the code block which returns the current date and time such as ", "Fri Mar 19 2010 09:42:39", ". This returned value is used as name in ", "Create Folder", " tool to name the folder. Since the name of the folder cannot contain spaces or punctuation marks, the ", "replace", " method in Python is used by stacking the method for each element that needs to be replaced. The resulting name of the folder in this example is  ", "FriMar192010094239", ".", "If you are calculating a value in the model and want to use the calculated value with tools such as  ", "Buffer", " that require a buffer distance value as well as a linear unit you have to:", "You can  use the output of ", "Calculate Value", " tool directly in any ", "Spatial Analyst", " tools which accept a raster or a constant value such as ", "Plus", ",", "Greater Than", ", and ", "Less Than", " (these tools are found  in the ", "Spatial Analyst", " toolbox/", "Math toolset", ").  To use the output of ", "Calculate Value", ", change the output data type to Formulated Raster. This output data type format  is a raster surface whose cell values are represented by a formula or constant.", "In Python, part of syntax is proper indentation. Indentation level (two spaces or four spaces) does not matter as long as it is consistent throughout the code block.", "You cannot access model variables from the code block. Such variables must be passed to the code block from the expression. This can be achieved by creating a definition in the ", "Code Block", " and referencing the definition in the ", "Expression", " box.", "  When writing Python scripts, use standard Python statements instead of the ", "Calculate Value", " tool."], "parameters": [{"name": "expression", "isOptional": false, "description": "The Python expression to be evaluated. ", "dataType": "SQL Expression"}, {"name": "code_block", "isOptional": true, "description": "Additional Python code. Code in the code block can be referenced in the Expression parameter. ", "dataType": "String"}, {"name": "data_type", "isOptional": true, "description": "The data type of the output returned from the Python expression. This parameter should be used in ModelBuilder to help chain Calculate Value with other tools. ", "dataType": "String"}]},
{"syntax": "TransformRouteEvents_lr (in_table, in_event_properties, in_routes, route_id_field, target_routes, target_route_id_field, out_table, out_event_properties, cluster_tolerance, {in_fields})", "name": "Transform Route Events (Linear Referencing)", "description": "This tool transforms the measures of events from one route reference to another and writes them to a new event table.", "example": {"title": "TransformRouteEvents example (Python window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.TransformRouteEvents_lr ( \"pavement.dbf\" , \"route1 LINE begin_mp end_mp\" , \"hwy.shp\" , \"route1\" , \"hwy_new.shp\" , \"route1\" , \"trans_out1.dbf\" , \"route1 LINE fmp tmp\" , \"0.1 meters\" )"}, "usage": ["Transforming events allows you to use the events from one route reference with another route reference having different route identifiers and/or measures.", "Any whole or partial event that intersects a target route is written to the new event table.", "The best results will be achieved when the source routes and the target routes closely overlay.", "Using a large cluster tolerance to overcome discrepancies between the source and target routes can produce unexpected results.", "The output event type (POINT or LINE) must match the input event type.", "Use the ", "Make Table View", " tool prior to this tool to effectively reduce the number of events that will be processed.", "The output table can be displayed in ArcMap using the ", "Make Route Event Layer", " tool or using the ", "Display Route Events command in ArcMap", "."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input event table. ", "dataType": "Table View"}, {"name": "in_event_properties", "isInputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events in the input event table. Route Identifier Field\u2014The field containing values that indicate along which route each event is. This field can be numeric or character. Event Type\u2014The type of events in the input event table (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a from-measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "in_routes", "isInputFile": true, "isOptional": false, "description": "The input route features. ", "dataType": "Feature Layer"}, {"name": "route_id_field", "isOptional": false, "description": "The field containing values that uniquely identify each input route. ", "dataType": "Field"}, {"name": "target_routes", "isOptional": false, "description": "The route features to which the input events will be transformed. ", "dataType": "Feature Layer"}, {"name": "target_route_id_field", "isOptional": false, "description": "The field containing values that uniquely identify each target route. ", "dataType": "Field"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table to be created. ", "dataType": "Table"}, {"name": "out_event_properties", "isOutputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events that will be written to the output event table. Route Identifier Field\u2014The field that will contain values that indicate along which route each event is. Event Type\u2014The type of events the output event table will contain (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a single measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field that will contain measure values. Required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field that will contain measure values. Required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "cluster_tolerance", "isOptional": false, "description": "The maximum tolerated distance between the input events and the target routes. ", "dataType": "Linear Unit"}, {"name": "in_fields", "isInputFile": true, "isOptional": true, "description": "Specifies whether the output event table will contain route location fields plus all the attributes from the input events. FIELDS \u2014 The output event table will contain route location fields plus all the attributes from the input events. This is the default. NO_FIELDS \u2014 The output event table will only contain route location fields plus the ObjectID field from the input events. ", "dataType": "Boolean"}]},
{"syntax": "OverlayRouteEvents_lr (in_table, in_event_properties, overlay_table, overlay_event_properties, overlay_type, out_table, out_event_properties, {zero_length_events}, {in_fields}, {build_index})", "name": "Overlay Route Events (Linear Referencing)", "description": "Overlays two event tables to create an output event table that represents the union or intersection of the input.", "example": {"title": "OverlayRouteEvents Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.OverlayRouteEvents_lr ( \"accident.dbf\" , \"rkey POINT mile\" , \"pavecond.dbf\" , \"rkey LINE fmp tmp\" , \"INTERSECT\" , \"accpav\" , \"rkey POINT mile\" )"}, "usage": ["Line-on-line, line-on-point, point-on-line, and point-on-point event overlays can be performed.", "The input and overlay events should be based on the same route reference.", "The input tables can be any type of table that ArcGIS supports. The output table can be a dBASE file or a geodatabase table.", "The output table can be displayed in ArcMap using the ", "Make Route Event Layer", " tool or using the ", "Display Route Events command in ArcMap", ".", "If both the input and overlay event properties are type POINT, the output event properties must also be defined as type POINT.", "If both the input and overlay event properties are type LINE, the output event properties must also be defined as type LINE.", "If either the input or overlay event properties are type POINT, the output event properties must be defined as type POINT when an INTERSECT overlay is performed. The output event properties must be defined as type LINE when a UNION overlay is performed.", "If both the input and overlay event properties are type POINT, only the points that have the exact same measure value are considered to intersect. There is no search tolerance.", "An attribute index on the route identifier field speeds up the ", "dynamic segmentation", " process. If you will be using the ", "Output Event Table", " for ", "dynamic segmentation", ", it is recommended that you choose to have an attribute index created.", "Use the ", "Make Table View", " tool prior to this tool to effectively reduce the number of events that will be processed.", "If either the input or overlay events do not have an ObjectID field, use the ", "Make Query Table", " tool prior to this tool to add a virtual ObjectID field."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input event table. ", "dataType": "Table View"}, {"name": "in_event_properties", "isInputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events in the input event table. Route Identifier Field\u2014The field containing values that indicate along which route each event is. This field can be numeric or character. Event Type\u2014The type of events in the input event table (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a from-measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "overlay_table", "isOptional": false, "description": "The overlay event table. ", "dataType": "Table View"}, {"name": "overlay_event_properties", "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events in the overlay event table. Route Identifier Field\u2014The field containing values that indicate which route each event is along. This field can be numeric or character. Event Type\u2014The type of events in the overlay event table (POINT or LINE). From-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is POINT or LINE. Note when the Event Type is POINT the label for this parameter becomes \"Measure Field\". To-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is LINE. POINT\u2014Point events occur at a precise location along a route. Only a from-measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified.", "dataType": "Route Measure Event Properties"}, {"name": "overlay_type", "isOptional": false, "description": "The type of overlay to be performed. INTERSECT \u2014 Writes only overlapping events to the output event table. This is the default. UNION \u2014 Writes all events to the output table. Linear events are split at their intersections. ", "dataType": "String"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table to be created. ", "dataType": "Table"}, {"name": "out_event_properties", "isOutputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events that will be written to the output event table. Route Identifier Field\u2014The field that will contain values that indicate along which route each event is. Event Type\u2014The type of events the output event table will contain (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a single measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field that will contain measure values. Required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field that will contain measure values. Required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "zero_length_events", "isOptional": true, "description": "Specifies whether to keep zero length line events in the output table. This parameter is only valid when the output event type is LINE. ZERO \u2014 Keeps zero length line events. This is the default. NO_ZERO \u2014 Does not keep zero length line events. ", "dataType": "Boolean"}, {"name": "in_fields", "isInputFile": true, "isOptional": true, "description": "Specifies whether all the fields from the input and overlay event tables will be written to the output event table. FIELDS \u2014 Includes all the fields from the input tables in the output table. This is the default. NO_FIELDS \u2014 Does not include all the fields from the input tables in the output table. Only the ObjectID and the route location fields are kept. ", "dataType": "Boolean"}, {"name": "build_index", "isOptional": true, "description": "Specifies whether an attribute index will be created for the route identifier field that is written to the output event table. INDEX \u2014 Creates an attribute index. This is the default. NO_INDEX \u2014 Does not create an attribute index. ", "dataType": "Boolean"}]},
{"syntax": "MakeRouteEventLayer_lr (in_routes, route_id_field, in_table, in_event_properties, out_layer, {offset_field}, {add_error_field}, {add_angle_field}, {angle_type}, {complement_angle}, {offset_direction}, {point_event_type})", "name": "Make Route Event Layer (Linear Referencing)", "description": "Creates a temporary feature layer using routes and route events. When the temporary layer is used (displayed on a map, or used by another geoprocessing tool),  dynamic segmentation  is performed.", "example": {"title": "MakeRouteEventLayer Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.MakeRouteEventLayer_lr ( \"route_hwy.shp\" , \"rkey\" , \"accident.dbf\" , \"rkey POINT mile\" , \"accident_events\" , \"#\" , \"ERROR_FIELD\" , \"ANGLE_FIELD\" )"}, "usage": ["The input table can be any type of table ArcGIS supports.", "Use ", "Make Feature Layer", " on the routes and/or ", "Make Table View", " on the events prior to this tool to reduce the number of routes and events that will be processed.", "Not all types of tables have an ObjectID field. When such tables are used by this tool the resulting layer will not be selectable and cannot be used effectively by certain geoprocessing operations. Consider using the", "Make Query Table", " tool prior to this tool to add a virtual ObjectID field.", "Temporary layers are stored in memory and can be used as input to other geoprocessing functions in your current ArcCatalog or ArcMap session.", "In ArcMap, temporary feature layers can be displayed if you have indicated that you want to add results of geoprocessing operations to the display under ", "Geoprocessing ", ">", " Geoprocessing Options... ", ">", " Add results of geoprocessing operations to the display", ".", "Once you exit ArcCatalog or ArcMap, temporary feature layers are removed from memory. To persist a temporary layer on disk, use the ", "Save To Layer File (Management)", " or ", "Copy Features (Management)", " tools."], "parameters": [{"name": "in_routes", "isInputFile": true, "isOptional": false, "description": "The route features upon which events will be located. ", "dataType": "Feature Layer"}, {"name": "route_id_field", "isOptional": false, "description": "The field containing values that uniquely identify each route. ", "dataType": "Field"}, {"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The table whose rows will be located along routes. ", "dataType": "Table View"}, {"name": "in_event_properties", "isInputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events in the input event table. Route Identifier Field\u2014The field containing values that indicate along which route each event is. This field can be numeric or character. Event Type\u2014The type of events in the input event table (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a from-measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": "The layer to be created. This layer is stored in memory, so a path is not necessary. ", "dataType": "Feature Layer"}, {"name": "offset_field", "isOptional": true, "description": "The field containing values used to offset events from their underlying route. This field must be numeric. ", "dataType": "Field"}, {"name": "add_error_field", "isOptional": true, "description": "Specifies whether a field named LOC_ERROR will be added to the temporary layer that is created. NO_ERROR_FIELD \u2014 Do not add a field to store locating errors. This is the default. ERROR_FIELD \u2014 Add a field to store locating errors. ", "dataType": "Boolean"}, {"name": "add_angle_field", "isOptional": true, "description": "Specifies whether a field named LOC_ANGLE will be added to the temporary layer that is created. This parameter is only valid when the event type is POINT. NO_ANGLE_FIELD \u2014 Do not add a field to store locating angles. This is the default. ANGLE_FIELD \u2014 Add a field to store locating angles. ", "dataType": "Boolean"}, {"name": "angle_type", "isOptional": true, "description": "Specifies the type of locating angle that will be calculated. This parameter is only valid if ANGLE_FIELD has been specified. NORMAL \u2014 The normal (perpendicular) angle will be calculated. This is the default. TANGENT \u2014 The tangent angle will be calculated. ", "dataType": "String"}, {"name": "complement_angle", "isOptional": true, "description": "Specifies whether the complement of the locating angle will be calculated. This parameter is only valid if ANGLE_FIELD has been specified. ANGLE \u2014 Do not write the complement of the angle. Write only the calculated angle. This is the default. COMPLEMENT \u2014 Write the complement of the angle. ", "dataType": "Boolean"}, {"name": "offset_direction", "isOptional": true, "description": "Specifies the side on which the route events with a positive offset are displayed. This parameter is only valid if an offset field has been specified. LEFT \u2014 Events with a positive offset will be displayed to the left of the route. The side of the route is determined by the measures and not necessarily the digitized direction. This is the default. RIGHT \u2014 Events with a positive offset will be displayed to the right of the route. The side of the route is determined by the digitized direction. ", "dataType": "Boolean"}, {"name": "point_event_type", "isOptional": true, "description": "Specifies whether point events will be treated as point features or multipoint features. POINT \u2014 Point events will be treated as point features. This is the default. MULTIPOINT \u2014 Point events will be treated as multipoint features. ", "dataType": "Boolean"}]},
{"syntax": "LocateFeaturesAlongRoutes_lr (in_features, in_routes, route_id_field, radius_or_tolerance, out_table, out_event_properties, {route_locations}, {distance_field}, {zero_length_events}, {in_fields}, {m_direction_offsetting})", "name": "Locate Features Along Routes (Linear Referencing)", "description": "Computes the intersection of input features (point, line, or polygon) and route features and writes the route and measure information to a new event table.", "example": {"title": "LocateFeaturesAlongRoutes example 1 (Python window)", "description": "The following Python script demonstrates how to use the LocateFeaturesAlongRoutes function in the Python window", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.LocateFeaturesAlongRoutes_lr ( \"rail_segments.shp\" , \"rail_routes.shp\" , \"rkey\" , \"0.5 Feet\" , \"locate_lines\" , \"rkey LINE fmp tmp\" )"}, "usage": ["The output table can be a dBASE file or a geodatabase table.", "The event type must be POINT when the ", "Input Features", " are points and must be LINE when the input features are lines or polygons.", "The best results will be achieved when the input features and the target routes closely overlay.", "Using a large search radius or cluster tolerance to overcome discrepancies between the input features and target routes can produce unexpected results.", "To reduce the number of input features that will be processed by this tool, you can input layers that have selections. See ", "Using Layers and Table Views", " for more information.", "The output table can be displayed in ArcMap using the ", "Make Route Event Layer", " tool or using the ", "Display Route Events command in ArcMap", "."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point, line, or polygon features. ", "dataType": "Feature Layer"}, {"name": "in_routes", "isInputFile": true, "isOptional": false, "description": "The routes with which the input features will be intersected. ", "dataType": "Feature Layer"}, {"name": "route_id_field", "isOptional": false, "description": "The field containing values that uniquely identify each route. This field can be numeric or character. ", "dataType": "Field"}, {"name": "radius_or_tolerance", "isOptional": false, "description": "If the input features are points, the search radius is a numeric value defining how far around each point a search will be done to find a target route. If the input features are lines, the search tolerance is really a cluster tolerance, which is a numeric value representing the maximum tolerated distance between the input lines and the target routes. If the input features are polygons, this parameter is ignored and no search radius is used. ", "dataType": "Linear unit"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table to be created. ", "dataType": "Table"}, {"name": "out_event_properties", "isOutputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events that will be written to the output event table. Route Identifier Field\u2014The field that will contain values that indicate along which route each event is. Event Type\u2014The type of events the output event table will contain (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a single measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field that will contain measure values. Required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field that will contain measure values. Required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "route_locations", "isOptional": true, "description": "When locating points along routes, it is possible that more than one route may be within the search radius of any given point. This parameter is ignored when locating lines or polygons along routes. FIRST \u2014 Only the closest route location will be written to the output event table. This is the default. ALL \u2014 Every route location (within the search radius) will be written to the output event table. ", "dataType": "Boolean"}, {"name": "distance_field", "isOptional": true, "description": "Specifies whether a field named DISTANCE will be added to the output event table. The values in this field are in the units of the specified search radius. This parameter is ignored when locating lines or polygons along routes. DISTANCE \u2014 A field containing the point-to-route distance will be added to the output event table. This is the default. NO_DISTANCE \u2014 A field containing the point-to-route distance will not be added to the output event table. ", "dataType": "Boolean"}, {"name": "zero_length_events", "isOptional": true, "description": "When locating polygons along routes, it is possible that events can be created where the from-measure is equal to the to-measure. This parameter is ignored when locating points or lines along routes. ZERO \u2014 Zero length line events will be written to the output event table. This is the default. NO_ZERO \u2014 Zero length line events will not be written to the output event table. ", "dataType": "Boolean"}, {"name": "in_fields", "isInputFile": true, "isOptional": true, "description": "Specifies whether the output event table will contain route location fields plus all the attributes from the input features. FIELDS \u2014 The output event table will contain route location fields plus all the attributes from the input features. This is the default. NO_FIELDS \u2014 The output event table will only contain route location fields plus the ObjectID field from the input features. ", "dataType": "Boolean"}, {"name": "m_direction_offsetting", "isOptional": true, "description": " Specifies whether the offset distance calculated should be based on the M direction or the digitized direction. Distances are included in the output event table if the distance_field parameter value DISTANCE is specified. M_DIRECTION \u2014 The distance values in the output event table will be calculated based on the routes' M direction. Input features to the left of the M Direction of the route will be assigned a positive offset (+), and features to the right of the M Direction will be assigned a negative offset value (-). This is the default. NO_M_DIRECTION \u2014 The distance values in the output event table will be calculated based on the routes' digitized direction. Input features to the left of the digitized direction of the route will be assigned a negative (-), offset and features to the right of the digitized direction will be assigned a positive offset value (+).", "dataType": "Boolean"}]},
{"syntax": "DissolveRouteEvents_lr (in_events, in_event_properties, dissolve_field, out_table, out_event_properties, {dissolve_type}, {build_index})", "name": "Dissolve Route Events (Linear Referencing)", "description": "Removes redundant information from event tables or separates event tables having more than one descriptive attribute into individual tables.", "example": {"title": "DissolveRouteEvents Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.DissolveRouteEvents_lr ( \"pavecond.dbf\" , \"rkey LINE fmp tmp\" , \"lanes\" , \"pave_dissolve1.dbf\" , \"rkey LINE fmp tmp\" )"}, "usage": ["The input table can be any type of table that ArcGIS supports. The output table can be a dBASE file or a geodatabase table.", "If the input events do not have an ObjectID field, use ", "Make Query Table", " prior to using this tool  to add a virtual ObjectID field.", "An attribute index on the route identifier field speeds up the ", "dynamic segmentation", " process. If you will be using the ", "Output Event Table", " for ", "dynamic segmentation", ", it is recommended that you choose to have an attribute index created.", "The output table can be displayed in ArcMap using the ", "Make Route Event Layer", " tool or using the ", "Display Route Events command in ArcMap", "."], "parameters": [{"name": "in_events", "isInputFile": true, "isOptional": false, "description": "The table whose rows will be aggregated. ", "dataType": "Table View"}, {"name": "in_event_properties", "isInputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events in the input event table. Route Identifier Field\u2014The field containing values that indicate along which route each event is. This field can be numeric or character. Event Type\u2014The type of events in the input event table (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a from-measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field containing measure values. This field must be numeric and is required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "dissolve_field", "isOptional": false, "description": "The field(s)used to aggregate rows. ", "dataType": "Field"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The table to be created. ", "dataType": "Table"}, {"name": "out_event_properties", "isOutputFile": true, "isOptional": false, "description": "Parameter consisting of the route location fields and the type of events that will be written to the output event table. Route Identifier Field\u2014The field that will contain values that indicate along which route each event is. Event Type\u2014The type of events the output event table will contain (POINT or LINE). POINT\u2014Point events occur at a precise location along a route. Only a single measure field must be specified. LINE\u2014Line events define a portion of a route. Both from- and to-measure fields must be specified. From-Measure Field\u2014A field that will contain measure values. Required when the event type is POINT or LINE. Note when the Event Type is POINT, the label for this parameter becomes Measure Field. To-Measure Field\u2014A field that will contain measure values. Required when the event type is LINE.", "dataType": "Route Measure Event Properties"}, {"name": "dissolve_type", "isOptional": true, "description": "Specifies whether the input events will be concatenated or dissolved. DISSOLVE \u2014 Events will be aggregated wherever there is measure overlap. This is the default. CONCATENATE \u2014 Events will be aggregated where the to-measure of one event matches the from-measure of the next event. This option is applicable only for line events. ", "dataType": "Boolean"}, {"name": "build_index", "isOptional": true, "description": "Specifies whether an attribute index will be created for the route identifier field that is written to the output event table. INDEX \u2014 Creates an attribute index. This is the default. NO_INDEX \u2014 Does not create an attribute index. ", "dataType": "Boolean"}]},
{"syntax": "CreateRoutes_lr (in_line_features, route_id_field, out_feature_class, measure_source, {from_measure_field}, {to_measure_field}, {coordinate_priority}, {measure_factor}, {measure_offset}, {ignore_gaps}, {build_index})", "name": "Create Routes (Linear Referencing)", "description": "Creates routes from existing lines. The input line features that share a common identifier are merged to create a single route.", "example": {"title": "CreateRoutes Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CreateRoutes function in the Python window.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Data\" arcpy.CreateRoutes_lr ( base_roads.shp , \"route1\" , \"newRoutes\" , \"LENGTH\" , \"#\" , \"#\" , \"LOWER_LEFT\" , 0.00018939394 )"}, "usage": ["The unique values in the ", "Route Identifier Field", " are written to ", "Output Route Feature Class", ".", "Use the ", "Make Feature Layer", " or ", "Make Query Table", " tools to effectively reduce the number of lines that will be used to create routes.", "If the ", "Output Route Feature Class", " will be written to a geodatabase, an appropriate ", "M Tolerance", ", ", "M Resolution", ", and ", "M Domain", " should be set.", "Use a ", "Measure Factor", " to convert between route measure units. For example, to convert from feet to miles, use a factor of 0.00018939394.", "Use a ", "Measure Offset", " in applications where the start measure of each route needs to be a value other than 0.", "The ", "Ignore spatial gaps", " parameter is not used when the TWO_FIELDS ", "Measure Source", " option has been specified. This is because measure values are dictated by the ", "From-Measure Field", " and ", "To-Measure Field", " values.", "When the LENGTH or ONE_FIELD ", "Measure Source", " option is used, the ", "Coordinate Priority", " is determined by placing the minimum bounding rectangle around the input features that will be merged to create one route.", "When the TWO_FIELDS ", "Measure Source", " option is used, it is not necessary to specify a coordinate priority because measure direction is implied by the values in the ", "From-Measure Field", " and the ", "To-Measure Field", ".", "An attribute index on the route identifier field speeds up the ", "dynamic segmentation", " process. If you will be using the ", "Output Route Feature Class", " for ", "dynamic segmentation", ", it is recommended that you choose to have an attribute index created.", "If any features are rejected by the Create Routes tool, a text file is created in the temporary file path to store information about those features. For example, ", "C:\\Documents and Settings\\patrickb\\Local Settings\\Temp\\Create_Output0.txt", " (where ", "Create_Output", " is the name of the output route feature class).", "The ", "outputMFlag", " environment setting will be ignored. The ", "Output Route Feature Class", " will have M (measure) values."], "parameters": [{"name": "in_line_features", "isInputFile": true, "isOptional": false, "description": "The features from which routes will be created. ", "dataType": "Feature Layer"}, {"name": "route_id_field", "isOptional": false, "description": "The field containing values that uniquely identify each route. ", "dataType": "Field"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to be created. It can be a shapefile or a geodatabase feature class. ", "dataType": "Feature Class"}, {"name": "measure_source", "isOptional": false, "description": "Specifies how route measures will be obtained. LENGTH \u2014 The geometric length of the input features will be used to accumulate the measures. This is the default. ONE_FIELD \u2014 The value stored in a single field will be used to accumulate the measures. TWO_FIELDS \u2014 The values stored in both from- and to-measure fields will be used to set the measures. ", "dataType": "String"}, {"name": "from_measure_field", "isOptional": true, "description": "A field containing measure values. This field must be numeric and is required when the measure source is ONE_FIELD or TWO_FIELDS. ", "dataType": "Field"}, {"name": "to_measure_field", "isOptional": true, "description": "A field containing measure values. This field must be numeric and is required when the measure source is TWO_FIELDS. ", "dataType": "Field"}, {"name": "coordinate_priority", "isOptional": true, "description": "The position from which measures will be accumulated for each output route. This parameter is ignored when the measure source is TWO_FIELDS. UPPER_LEFT \u2014 Measures will be accumulated from the point closest to the minimum bounding rectangle's upper left corner. This is the default. LOWER_LEFT \u2014 Measures will be accumulated from the point closest to the minimum bounding rectangle's lower left corner. UPPER_RIGHT \u2014 Measures will be accumulated from the point closest to the minimum bounding rectangle's upper right corner. LOWER_RIGHT \u2014 Measures will be accumulated from the point closest to the minimum bounding rectangle's lower right corner. ", "dataType": "String"}, {"name": "measure_factor", "isOptional": true, "description": "A value multiplied by the measure length of each input line before they are merged to create route measures. The default is 1. ", "dataType": "Double"}, {"name": "measure_offset", "isOptional": true, "description": "A value added to the route measures after the input lines have been merged to create a route. The default is 0. ", "dataType": "Double"}, {"name": "ignore_gaps", "isOptional": true, "description": "Specifies whether spatial gaps will be ignored when calculating the measures on disjointed routes. This parameter is applicable when the measure source is LENGTH or ONE_FIELD. IGNORE \u2014 Spatial gaps will be ignored. Measure values will be continuous for disjointed routes. This is the default. NO_IGNORE \u2014 Do not ignore spatial gaps. The measure values on disjointed routes will have gaps. The gap distance is calculated using the straight-line distance between the endpoints of the disjointed parts. ", "dataType": "Boolean"}, {"name": "build_index", "isOptional": true, "description": "Specifies whether an attribute index will be created for the route identifier field that is written to the output route feature class. INDEX \u2014 Create an attribute index. This is the default. NO_INDEX \u2014 Do not create an attribute index. ", "dataType": "Boolean"}]},
{"syntax": "CalibrateRoutes_lr (in_route_features, route_id_field, in_point_features, point_id_field, measure_field, out_feature_class, {calibrate_method}, {search_radius}, {interpolate_between}, {extrapolate_before}, {extrapolate_after}, {ignore_gaps}, {keep_all_routes}, {build_index})", "name": "Calibrate Routes (Linear Referencing)", "description": "Recalculates  route  measures using points.", "example": {"title": "CalibrateRoutes Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CalibrateRoutes function in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CalibrateRoutes_lr ( \"hwy.shp\" , \"RID\" , \"cal_pts.shp\" , \"RID\" , \"MEASURE\" , \"C:/output/hwy_new.shp\" , \"DISTANCE\" , \"5.0 Feet\" , \"BETWEEN\" , \"BEFORE\" , \"AFTER\" , \"#\" , \"NO_KEEP\" )"}, "usage": ["Either whole or partial routes can be calibrated. You can choose to interpolate between the input points, extrapolate before the input points, extrapolate after the input points, or use any combination of these three methods.", "Use ", "Make Feature Layer", " or ", "Make Query Table", " to effectively reduce the routes that will be calibrated.", "If the ", "Output Route Feature Class", " will be written to a geodatabase, an appropriate ", "M Tolerance", ", ", "M Resolution", ", and ", "M Domain", " should be set.", "The Output Route Feature Class will include all the fields from the Input Route Features.", "The ", "outputMFlag", " environment setting will be ignored. The Output Route Feature Class will have ", "M (measure) values", ".", "A search radius of infinity cannot be specified.", "An attribute index on the route identifier field speeds up the ", "dynamic segmentation", " process. If you will be using the ", "Output Route Feature Class", " for ", "dynamic segmentation", ", it is recommended that you choose to have an attribute index created.", "If any features are rejected by the Calibrate Routes process, a text file is created in the temporary file path to store information about those features. For example, C:\\Documents and Settings\\patrickb\\Local Settings\\Temp\\Calibrate_Output0.txt (where 'Calibrate_Output' is the name of the Output Route Feature Class)."], "parameters": [{"name": "in_route_features", "isInputFile": true, "isOptional": false, "description": "The route features to be calibrated. ", "dataType": "Feature Layer"}, {"name": "route_id_field", "isOptional": false, "description": "The field containing values that uniquely identify each route. This field can be numeric or character. ", "dataType": "Field"}, {"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The point features used to calibrate the routes. ", "dataType": "Feature Layer"}, {"name": "point_id_field", "isOptional": false, "description": "The field that identifies the route on which each calibration point is located. The values in this field match those in the route identifier field. This field can be numeric or character. ", "dataType": "Field"}, {"name": "measure_field", "isOptional": false, "description": "The field containing the measure value for each calibration point. This field must be numeric. ", "dataType": "Field"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class to be created. It can be a shapefile or a geodatabase feature class. ", "dataType": "Feature Class"}, {"name": "calibrate_method", "isOptional": true, "description": "Specifies how route measures will be recalculated. DISTANCE \u2014 Measures will be recalculated using the shortest path distance between the calibration points. This is the default. MEASURES \u2014 Measures will be recalculated using the pre-existing measure distance between the calibration points. ", "dataType": "String"}, {"name": "search_radius", "isOptional": true, "description": "Limits how far a calibration point can be from a route by specifying the distance and its unit of measure. If the units of measure are Unknown, the same units as the coordinate system of the route feature class will be used. ", "dataType": "Linear unit"}, {"name": "interpolate_between", "isOptional": true, "description": "Specifies whether measure values will be interpolated between the calibration points. BETWEEN \u2014 Interpolate between the calibration points. This is the default. NO_BETWEEN \u2014 Do not interpolate between the calibration points. ", "dataType": "Boolean"}, {"name": "extrapolate_before", "isOptional": true, "description": "Specifies whether measure values will be extrapolated before the calibration points. BEFORE \u2014 Extrapolate before the calibration points. This is the default. NO_BEFORE \u2014 Do not extrapolate before the calibration points. ", "dataType": "Boolean"}, {"name": "extrapolate_after", "isOptional": true, "description": "Specifies whether measure values will be extrapolated after the calibration points. AFTER \u2014 Extrapolate after the calibration points. This is the default. NO_AFTER \u2014 Do not extrapolate after the calibration points. ", "dataType": "Boolean"}, {"name": "ignore_gaps", "isOptional": true, "description": "Specifies whether spatial gaps will be ignored when recalculating the measures on disjointed routes. IGNORE \u2014 Spatial gaps will be ignored. Measure values will be continuous for disjointed routes. This is the default. NO_IGNORE \u2014 Do not ignore spatial gaps. The measure values on disjointed routes will have gaps. The gap distance is calculated using the straight-line distance between the endpoints of the disjointed parts. ", "dataType": "Boolean"}, {"name": "keep_all_routes", "isOptional": true, "description": "Specifies whether route features that do not have any calibration points will be excluded from the output feature class. KEEP \u2014 Keep all route features in the output feature class. This is the default. NO_KEEP \u2014 Do not keep all route features in the output feature class. Features that have no calibration points will be excluded. ", "dataType": "Boolean"}, {"name": "build_index", "isOptional": true, "description": "Specifies whether an attribute index will be created for the route identifier field that is written to the Output Route Feature Class. INDEX \u2014 Create an attribute index. This is the default. NO_INDEX \u2014 Do not create an attribute index. ", "dataType": "Boolean"}]},
{"syntax": "EmpiricalBayesianKriging_ga (in_features, z_field, {out_ga_layer}, {out_raster}, {cell_size}, {transformation_type}, {max_local_points}, {overlap_factor}, {number_semivariograms}, {search_neighborhood}, {output_type}, {quantile_value}, {threshold_type}, {probability_threshold})", "name": "Empirical Bayesian Kriging (Geostatisical Analyst)", "description": "Empirical Bayesian Kriging is an interpolation method that accounts for the error in estimating the underlying semivariogram through repeated simulations. What is Empirical Bayesian Kriging?", "example": {"title": "EmpiricalBayesianKriging example 1 (Python window)", "description": "Interpolate a series of point features onto a raster.", "code": "import arcpy arcpy.EmpiricalBayesianKriging_ga ( \"ca_ozone_pts\" , \"OZONE\" , \"outEBK\" , \"C:/gapyexamples/output/ebkout\" , 10000 , \"NONE\" , 50 , 0.5 , 100 , arcpy.SearchNeighborhoodStandard ( 300000 , 300000 , 0 , 15 , 10 , \"ONE_SECTOR\" ), \"PREDICTION\" , \"\" , \"\" , \"\" )"}, "usage": ["\r\nThis kriging method can handle moderately ", "nonstationary", " input data.\r\n", "Only ", "Standard Circular", " and ", "Smooth Circular", " ", "Search neighborhoods", " are allowed for this interpolation method. ", "A ", "Smooth Circular", " ", "Search neighborhood", " will substantially increase the execution time.", "The larger the ", "Maximum number of points in each local model", "  and ", "Local model overlap factor", " values, the longer the execution time.  Applying a ", "Data transformation", " will also significantly increase execution time."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "Field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input features contain z-values or m-values. ", "dataType": "Field"}, {"name": "out_ga_layer", "isOutputFile": true, "isOptional": true, "description": "The geostatistical layer produced. This layer is required output only if no output raster is requested. ", "dataType": "Geostatistical Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster. This raster is required output only if no output geostatistical layer is requested. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "transformation_type", "isOptional": true, "description": "Type of transformation to be applied to the input data. NONE \u2014 Do not apply any transformation. This is the default. EMPIRICAL \u2014 Multiplicative Skewing transformation with Empirical base function. LOGEMPIRICAL \u2014 Multiplicative Skewing transformation with Log Empirical base function. All data values must be positive.", "dataType": "String"}, {"name": "max_local_points", "isOptional": true, "description": " The input data will automatically be divided into groups that do not have more than this number of points. ", "dataType": "Long"}, {"name": "overlap_factor", "isOptional": true, "description": "A factor representing the degree of overlap between local models (also called subsets). Each input point can fall into several subsets, and the overlap factor specifies the average number of subsets that each point will fall into. A high value of the overlap factor makes the output surface smoother, but it also increases processing time. Typical values vary between 0.01 and 5. ", "dataType": "Double"}, {"name": "number_semivariograms", "isOptional": true, "description": "The number of simulated semivariograms. ", "dataType": "Long"}, {"name": "search_neighborhood", "isOptional": true, "description": "Defines which surrounding points will be used to control the output. Standard is the default. This is a Search Neighborhood class ( SearchNeighborhoodStandard , SearchNeighborhoodSmooth ), SearchNeighborhoodStandardCircular and SearchNeighborhoodSmoothCircular . Standard Smooth StandardCircular SmoothCircular Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Major semiaxis\u2014The major semiaxis value of the searching neighborhood. Minor semiaxis\u2014The minor semiaxis value of the searching neighborhood. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one. Radius\u2014The length of the radius of the search circle. Angle\u2014The angle of rotation for the axis (circle) or semimajor axis (ellipse) of the moving window. Maximum neighbors\u2014The maximum number of neighbors that will be used to estimate the value at the unknown location. Minimum neighbors\u2014The minimum number of neighbors that will be used to estimate the value at the unknown location. Sector type\u2014The geometry of the neighborhood. One sector\u2014Single ellipse. Four sectors\u2014Ellipse divided into four sectors. Four sectors shifted\u2014Ellipse divided into four sectors and shifted 45 degrees. Eight sectors\u2014Ellipse divided into eight sectors. Radius\u2014The length of the radius of the search circle. Smoothing factor\u2014The Smooth Interpolation option creates an outer ellipse and an inner ellipse at a distance equal to the Major Semiaxis multiplied by the Smoothing factor. The points that fall outside the smallest ellipse but inside the largest ellipse are weighted using a sigmoidal function with a value between zero and one.", "dataType": "Geostatistical Search Neighborhood"}, {"name": "output_type", "isOutputFile": true, "isOptional": true, "description": " Surface type to store the interpolation results. PREDICTION \u2014 Prediction surfaces are produced from the interpolated values. PREDICTION_STANDARD_ERROR \u2014 Standard Error surfaces are produced from the standard errors of the interpolated values. PROBABILITY \u2014 Probability surface of values exceeding or not exceeding a certain threshold. QUANTILE \u2014 Quantile surface depicting the chance that a prediction is above a certain value.", "dataType": "String"}, {"name": "quantile_value", "isOptional": true, "description": "The quantile value for which the output raster will be generated. ", "dataType": "Double"}, {"name": "threshold_type", "isOptional": true, "description": "Determines whether the probability values exceed the threshold value or not. EXCEED \u2014 Probability values exceed the threshold. This is the default. NOT_ EXCEED \u2014 Probability values will not exceed the threshold.", "dataType": "String"}, {"name": "probability_threshold", "isOptional": true, "description": "The probability threshold value. If left empty, the median of the input data will be used. ", "dataType": "Double"}]},
{"syntax": "ArealInterpolationLayerToPolygons_ga (in_areal_interpolation_layer, in_polygon_features, out_feature_class, {append_all_fields})", "name": "Areal Interpolation Layer To Polygons (Geostatisical Analyst)", "description": " Reaggregates the predictions of an Areal Interpolation layer to a new set of polygons.  ", "example": {"title": "ArealInterpolationLayerToPolygons example 1 (Python window)", "description": "Aggregate areal interpolation predictions to a new set of polygons.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.ArealInterpolationLayerToPolygons_ga ( \"AI_layer\" , \"new_polys\" , \"pred_new_polys\" , \"ALL\" )"}, "usage": ["This tool is used to reaggregate polygonal data.  After an Areal Interpolation layer is created in the ", "Geostatistical Wizard", ", this tool aggregates the predictions to a new set of polygons.", "This tool can be used in the workflow to downscale or upscale polygonal data, such as predicting population in census blocks from population counts in postal codes.", "The input geostatistical layer must be the result of performing Areal Interpolation on a dataset.  Geostatistical layers resulting from other interpolation techniques cannot be used with  this tool.", "The fields in the output feature class can include the following (where applicable):"], "parameters": [{"name": "in_areal_interpolation_layer", "isInputFile": true, "isOptional": false, "description": "Input geostatistical layer resulting from an Areal Interpolation model. ", "dataType": "Geostatistical Layer"}, {"name": "in_polygon_features", "isInputFile": true, "isOptional": false, "description": "The polygons where predictions and standard errors will be aggregated. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing the aggregated predictions and standard errors for the new polygons. ", "dataType": "Feature Class"}, {"name": "append_all_fields", "isOptional": true, "description": " Determines whether all fields will be copied from the input features to the output feature class. ALL \u2014 All fields from the input features will be copied to the output feature class. This is the default. FID_ONLY \u2014 Only FID will be copied, and it will be named Source_ID on the output feature class. ", "dataType": "Boolean"}]},
{"syntax": "GASetModelParameter_ga (in_ga_model_source, model_param_xpath, in_param_value, out_ga_model)", "name": "Set Model Parameter (Geostatisical Analyst)", "description": "Sets parameter value(s) in an existing geostatistical model source.", "example": {"title": "SetModelParameter example 1 (Python window)", "description": "Change the value of a parameter in a geostatistical model source.", "code": "import arcpy from arcpy import env env.workspace = \"C:/gapyexamples/data\" newParam = arcpy.GASetModelParameter_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"/model[@name = 'Kriging']/model[@name = 'Variogram']/value[@name = 'Nugget']\" , \"1\" , \"C:/gapyexamples/output/outModel.xml\" ) print newParam"}, "usage": ["This tool is generally used in a model or in scripting.", "The geostatistical model source is either a geostatistical layer or a geostatistical model (XML).", "The examples below can be used in the ", "Parameter XML Path", " to specify which parameter will be set to a new value.", "The code snippet below sets multiple parameters via a single call to the tool."], "parameters": [{"name": "in_ga_model_source", "isInputFile": true, "isOptional": false, "description": "The geostatistical model source to be analyzed. ", "dataType": "File; Geostatistical Layer"}, {"name": "model_param_xpath", "isOptional": false, "description": "XML path to the required model parameter. ", "dataType": "String"}, {"name": "in_param_value", "isInputFile": true, "isOptional": false, "description": "Value for the parameter defined by the XML path. ", "dataType": "String"}, {"name": "out_ga_model", "isOutputFile": true, "isOptional": false, "description": "Geostatistical model created with the parameter value defined in the XML path. ", "dataType": "File"}]},
{"syntax": "GAGetModelParameter_ga (in_ga_model_source, model_param_xpath, out_string)", "name": "Get Model Parameter (Geostatisical Analyst)", "description": "Gets model parameter value from an existing geostatistical model source.", "example": {"title": "GetModelParameter example 1 (Python window)", "description": "Extract parameter values from a geostatistical model source.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" outParam = arcpy.GAGetModelParameter_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"/model[@name = 'Kriging']/model[@name = 'Variogram']/value[@name = 'Nugget']\" ) print outParam"}, "usage": ["This tool is generally used in a model or in scripting.", "The geostatistical model source is either a geostatistical layer or a geostatistical model (XML).", "See ", "Set Model Parameter", " for additional information.", "An example", "will return \"out_param_value\" = 0.345;1127.14"], "parameters": [{"name": "in_ga_model_source", "isInputFile": true, "isOptional": false, "description": "The geostatistical model source to be analyzed. ", "dataType": "File; Geostatistical Layer"}, {"name": "model_param_xpath", "isOptional": false, "description": "XML path to the required model parameter. ", "dataType": "String"}, {"name": "out_string", "isOutputFile": true, "isOptional": false, "description": "Requested model parameter. ", "dataType": "String"}]},
{"syntax": "GALayerToPoints_ga (in_geostat_layer, in_locations, {z_field}, out_feature_class, {append_all_fields})", "name": "GA Layer To Points (Geostatisical Analyst)", "description": "Exports a geostatistical layer to points. The tool can also be used to predict values at unmeasured locations or to validate predictions made at measured locations.", "example": {"title": "GALayerToPoints (Python window)", "description": "Export a geostatistical layer to a point feature class.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.GALayerToPoints_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"C:/gapyexamples/data/obs_pts.shp\" , \"\" , \"C:/gapyexamples/output/krig_pts\" )"}, "usage": ["For data formats that support Null values, such as file and personal geodatabase feature classes, a Null value will be used to indicate that a prediction could not be made for that location or that the value showed be ignored when used as input. For data formats that do not support Null values, such as shapefiles, the value of  -1.7976931348623158e+308 is used (this is the negative of the C++ defined constant DBL_MAX)  to indicate that a prediction could not be made for that location.", "The fields in the output feature class can include the following (where applicable):"], "parameters": [{"name": "in_geostat_layer", "isInputFile": true, "isOptional": false, "description": "The geostatistical layer to be analyzed. ", "dataType": "Geostatistical Layer"}, {"name": "in_locations", "isInputFile": true, "isOptional": false, "description": "Point locations where predictions or validations will be performed. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": true, "description": "If this field is left blank, predictions are made at the location points. If a field is selected, predictions are made at the location points and compared to their Z_value_field values, then a validation analysis is performed. ", "dataType": "Field"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class containing either the predictions or the predictions and the validation results. ", "dataType": "Feature Class"}, {"name": "append_all_fields", "isOptional": true, "description": " Determines whether all fields will be copied from the input features to the output feature class. ALL \u2014 All fields from the input features will be copied to the output feature class. This is the default. FID_ONLY \u2014 Only FID will be copied, and it will be named Source_ID on the output feature class. ", "dataType": "Boolean"}]},
{"syntax": "GALayerToGrid_ga (in_geostat_layer, out_surface_grid, {cell_size}, {points_per_block_horz}, {points_per_block_vert})", "name": "GA Layer To Grid (Geostatisical Analyst)", "description": "Exports a Geostatistical layer to a raster.", "example": {"title": "GALayerToGrid example 1 (Python window)", "description": "Convert a geostatistical layer into a raster.", "code": "import arcpy from arcpy import env arcpy.GALayerToGrid_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"C:/gapyexamples/output/krig_grid\" , \"2000\" , \"1\" , \"1\" )"}, "usage": ["The output raster will be created at the cell size specified in the ", "Output cell size", " parameter.", "Select the number of predictions for each cell in the horizontal and vertical directions for block interpolation."], "parameters": [{"name": "in_geostat_layer", "isInputFile": true, "isOptional": false, "description": "The geostatistical layer to be analyzed. ", "dataType": "Geostatistical Layer"}, {"name": "out_surface_grid", "isOutputFile": true, "isOptional": false, "description": "The raster to be created. ", "dataType": "Raster Dataset"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This value can be explicitly set under Raster Analysis from the Environment Settings. If not set, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "points_per_block_horz", "isOptional": true, "description": "The number of predictions for each cell in the horizontal direction for block interpolation. ", "dataType": "Long"}, {"name": "points_per_block_vert", "isOptional": true, "description": "The number of predictions for each cell in the vertical direction for block interpolation. ", "dataType": "Long"}]},
{"syntax": "GALayerToContour_ga (in_geostat_layer, contour_type, out_feature_class, {contour_quality})", "name": "GA Layer To Contour (Geostatisical Analyst)", "description": "Creates a feature class of coutours from a geostatiscal analysis layer.  The output feature class can be either a line feature class of contour lines or a polygon feature class of filled contours.", "example": {"title": "GALayerToContour interactive window example", "description": "Export a geostatistical layer to a contour feature class.", "code": "import arcpy from arcpy import env env.workspace = \"C:/gapyexamples/data\" arcpy.GALayerToContour_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"Contour\" , \"C:/gapyexamples/output/krig_cont\" , \"Presentation\" )"}, "usage": ["For data formats that support Null values, such as file and personal geodatabase feature classes, a Null value will be used to indicate that a prediction could not be made for that location or that the value showed be ignored when used as input. For data formats that do not support Null values, such as shapefiles, the value of  -1.7976931348623158e+308 is used (this is the negative of the C++ defined constant DBL_MAX)  to indicate that a prediction could not be made for that location."], "parameters": [{"name": "in_geostat_layer", "isInputFile": true, "isOptional": false, "description": "The geostatistical layer to be analyzed. ", "dataType": "Geostatistical Layer"}, {"name": "contour_type", "isOptional": false, "description": "Type of contour to represent the geostatistical layer. CONTOUR \u2014 The contour or isoline representation of the geostatistical layer. Displays the lines in either draft or presentation quality. FILLED CONTOUR \u2014 The polygon representation of the geostatistical layer. It assumes for the graphical display that the values between contour lines are the same for all locations within the polygon. Displays the lines in either draft or presentation quality. ", "dataType": "String"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class will either be a polyline or a polygon, depending on the selected Contour type. ", "dataType": "Feature Class"}, {"name": "contour_quality", "isOptional": true, "description": "Determines the smoothness of contour line representation. DRAFT \u2014 The default Draft quality presents a generalized version of isolines for faster display. PRESENTATION \u2014 The Presentation option ensures more detailed isolines for the output feature class. ", "dataType": "String"}]},
{"syntax": "GACreateGeostatisticalLayer_ga (in_ga_model_source, in_datasets, out_layer)", "name": "Create Geostatistical Layer (Geostatisical Analyst)", "description": "Creates a new geostatistical layer. An  existing geostatistical layer or geostatistical model is required to populate the initial values for the new layer. The input to this tool can be created using the  Geostatistical Wizard .", "example": {"title": "CreateGeostatisticalLayer example 1 (Python window)", "description": "Use an existing geostatistical layer to create a new geostatistical layer.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.GACreateGeostatisticalLayer_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"ca_ozone_pts.shp X=Shape Y=Shape F1=OZONE\" , \"outCGL\" )"}, "usage": ["Geostatistical model source is either a geostatistical layer or a geostatistical model (XML).", "The input to this tool is a geostatistical model source which can be either a geostatistical layer or an XML file. The complete name of the dataset that is used to create the geostatistical layer is stored within the layer. However, the XML file only contains the model parameters and not the dataset information.", "If the geostatistical model source uses a ", "normal score transformation", ", the parameters of the transformation will be recalculated for the input datasets.", "Layers can be input to the ", "Input dataset(s)", " parameter.  If you specify a layer, the ", "selected features", " in the layer will be used to create the Geostatistical Layer.  If you specify a path to a dataset on disk, all features in the dataset will be used.", "All the Geostatistical Analyst geoprocessing tools are functional in ArcGlobe or ArcScene. However, a geostatistical layer can only be displayed using ArcMap or ArcCatalog and it is therefore recommended that geostatistical analysis rather be performed using these products.  ", "For data formats that support Null values, such as file and personal geodatabase feature classes, a Null value will be used to indicate that a prediction could not be made for that location or that the value showed be ignored when used as input. For data formats that do not support Null values, such as shapefiles, the value of  -1.7976931348623158e+308 is used (this is the negative of the C++ defined constant DBL_MAX)  to indicate that a prediction could not be made for that location."], "parameters": [{"name": "in_ga_model_source", "isInputFile": true, "isOptional": false, "description": "The geostatistical model source to be analyzed. ", "dataType": "File; Geostatistical Layer"}, {"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": "A semicolon delimited string of elements. Each element contains: The path to a dataset or the name of a layer in the current table of contents, followed by a space. A list of fields, each field name separated by a space, unless the dataset is a raster.", "dataType": "Geostatistical Value Table"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": "The geostatistical layer produced by the tool. ", "dataType": "Geostatistical Layer"}]},
{"syntax": "GACalculateZValue_ga (in_geostat_layer, point_coord)", "name": "Calculate Z-value (Geostatisical Analyst)", "description": "Uses the interpolation model in a geostatistical layer to predict a value at a single location.", "example": {"title": "GACalculateZValue (Python window)", "description": "Predict a value at a location using a kriging geostatistical layer.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" outCZV = arcpy.GACalculateZValue_ga ( \"C:/gapyexamples/data/Kriging.lyr\" , \"-2000000 -50000\" ) print outCZV"}, "usage": ["This tool is generally used in a model or in scripting."], "parameters": [{"name": "in_geostat_layer", "isInputFile": true, "isOptional": false, "description": "The geostatistical layer to be analyzed. ", "dataType": "Geostatistical Layer"}, {"name": "point_coord", "isOptional": false, "description": "The x,y coordinate of the point for which the Z-value will be calculated. ", "dataType": "Point"}]},
{"syntax": "SubsetFeatures_ga (in_features, out_training_feature_class, {out_test_feature_class}, {size_of_training_dataset}, {subset_size_units})", "name": "Subset Features (Geostatisical Analyst)", "description": "Divides the original dataset into two parts: one part to be used to model the spatial structure and produce a surface, the other to be used to compare and validate the output surface.    \r\n Learn more about Subset Features \r\n", "example": {"title": "SubsetFeatures example 1 (Python window)", "description": "Randomly split the features into two feature classes.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.SubsetFeatures_ga ( \"ca_ozone_pts\" , \"C:/gapyexamples/output/training\" , \"\" , \"\" , \"PERCENTAGE_OF_INPUT\" )"}, "usage": ["If multipart features are used as input, the output will be a subset of multipart features and not individual features.", "If you want the random sequence used to create the subsets to be repeatable, you need to specify a nonzero seed value in the ", "Random_number_generator", " environment variable. ", "The test feature class is often used in validation of a model created using the training feature class."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "Points, lines, polygon features or table from which to create a subset. ", "dataType": "Table View"}, {"name": "out_training_feature_class", "isOutputFile": true, "isOptional": false, "description": "The subset of training features to be created. ", "dataType": "Feature Class; Table"}, {"name": "out_test_feature_class", "isOutputFile": true, "isOptional": true, "description": "The subset of test features to be created. ", "dataType": "Feature Class; Table"}, {"name": "size_of_training_dataset", "isOptional": true, "description": "The size of the output training feature class, entered either as a percentage of the input features or as an absolute number of features. ", "dataType": "Double"}, {"name": "subset_size_units", "isOptional": true, "description": "Type of subset size. PERCENTAGE_OF_INPUT \u2014 The percentage of the input features that will be in the training dataset. ABSOLUTE_VALUE \u2014 The number of features that will be in the training dataset.", "dataType": "Boolean"}]},
{"syntax": "GASemivariogramSensitivity_ga (in_ga_model_source, in_datasets, in_locations, {nugget_span_percents}, {nugget_calc_times}, {partialsill_span_percents}, {partialsill_calc_times}, {range_span_percents}, {range_calc_times}, {minrange_span_percents}, {minrange_calc_times}, out_table)", "name": "Semivariogram Sensitivity (Geostatisical Analyst)", "description": "Performs a sensitivity analysis with varying Nugget, Partial Sill, and Range values. \r\n\r\n How Semivariogram Sensitivity works", "example": {"title": "SemivariogramSensitivity example 1 (Python window)", "description": "Performs a sensitivity analysis by varying the Nugget, Partial Sill, and Range values.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" arcpy.GASemivariogramSensitivity_ga ( \"C:/gapyexamples/data/kriging.lyr\" , \"C:/gapyexamples/data/ca_ozone_pts.shp OZONE\" , \"C:/gapyexamples/data/obs_pts.shp\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"\" , \"C:/gapyexamples/output/outtabSS\" )"}, "usage": ["The geostatistical model source is either a geostatistical layer or a geostatistical model (XML).", "Set the environment variable ", "Seed", " equal to a nonzero value if the random sequence should be repeatable.", "For data formats that support Null values, such as file and personal geodatabase feature classes, a Null value will be used to indicate that a prediction could not be made for that location or that the value showed be ignored when used as input. For data formats that do not support Null values, such as shapefiles, the value of  -1.7976931348623158e+308 is used (this is the negative of the C++ defined constant DBL_MAX)  to indicate that a prediction could not be made for that location."], "parameters": [{"name": "in_ga_model_source", "isInputFile": true, "isOptional": false, "description": "The geostatistical model source to be analyzed. ", "dataType": "File; Geostatistical Layer"}, {"name": "in_datasets", "isInputFile": true, "isOptional": false, "description": "Input datasets displays two pieces: Dataset\u2014The path and the name of the input data. Field\u2014The name of the required field.", "dataType": "Geostatistical Value Table"}, {"name": "in_locations", "isInputFile": true, "isOptional": false, "description": "Point locations where the sensitivity analysis is performed. ", "dataType": "Feature Layer"}, {"name": "nugget_span_percents", "isOptional": true, "description": "The percentage subtracted and added to the Nugget parameter to create a range for subsequent random Nugget parameter selection. ", "dataType": "Double"}, {"name": "nugget_calc_times", "isOptional": true, "description": "Number of random Nugget values drawn from the Nugget span. ", "dataType": "Long"}, {"name": "partialsill_span_percents", "isOptional": true, "description": "Percentage subtracted from and added to the Partial Sill parameter to create a range for random Partial Sill selection. ", "dataType": "Double"}, {"name": "partialsill_calc_times", "isOptional": true, "description": "Number of random Partial Sill values drawn from the Partial Sill span. ", "dataType": "Long"}, {"name": "range_span_percents", "isOptional": true, "description": "Percentage subtracted and added to the Major Range parameter to create a range for a random Major Range selection. ", "dataType": "Double"}, {"name": "range_calc_times", "isOptional": true, "description": "Number of random Major Range values drawn from the Major Range span. ", "dataType": "Long"}, {"name": "minrange_span_percents", "isOptional": true, "description": "Percentage subtracted and added to the Minor Range parameter to create a range for random Minor Range selection. ", "dataType": "Double"}, {"name": "minrange_calc_times", "isOptional": true, "description": "Number of random Minor Range values drawn from the Minor Range span. If Anisotropy has been set in <in_geostat_layer>, a value is required. ", "dataType": "Long"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "Table storing the sensitivity results. ", "dataType": "Table"}]},
{"syntax": "GANeighborhoodSelection_ga (in_dataset, out_layer, point_coord, neighbors_max, neighbors_min, minor_semiaxis, major_semiaxis, angle, {shape_type})", "name": "Neighborhood Selection (Geostatisical Analyst)", "description": "Creates a layer of points based on a user-defined neighborhood. \r\n\r\n The input point feature class contains all the red and blue points. You want to create a selection of points around the red star based on a circular neighborhood. The output layer contains the selection set of 20 (blue) points.", "example": {"title": "NeighborhoodSelection example 1 (Python window)", "description": "Create a feature layer based on some neighborhood criteria.", "code": "import arcpy from arcpy import env env.workspace = \"C:/gapyexamples/data\" arcpy.GANeighborhoodSelection_ga ( \"ca_ozone_pts\" , \"outNS\" , \"-2000000 -50000\" , \"20\" , \"5\" , \"200000\" , \"200000\" , \"0\" , \"One sector\" )"}, "usage": ["This tool can be used in conjunction with the ", "interpolation", " tools when different ", "searching_neighborhoods", " are examined."], "parameters": [{"name": "in_dataset", "isInputFile": true, "isOptional": false, "description": "Points used to create a neighborhood selection. ", "dataType": "Feature Layer"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": "Layer to store the neighborhood selection. ", "dataType": "Feature Layer"}, {"name": "point_coord", "isOptional": false, "description": "The neighborhood center's x,y coordinate. ", "dataType": "Point"}, {"name": "neighbors_max", "isOptional": false, "description": "The number of points to use in each sector. If a sector has the required number of points, all points in that sector are used. ", "dataType": "Long"}, {"name": "neighbors_min", "isOptional": false, "description": "The minimum number of points to use in each sector. If the minimum number of required points are not available in any given sector, the nearest available point outside the sector will be selected. ", "dataType": "Long"}, {"name": "minor_semiaxis", "isOptional": false, "description": "Size of the minor semiaxis of the search neighborhood. ", "dataType": "Double"}, {"name": "major_semiaxis", "isOptional": false, "description": "Size of the major semiaxis of the search neighborhood. ", "dataType": "Double"}, {"name": "angle", "isOptional": false, "description": "The angle of rotation of the neighborhood axis. ", "dataType": "Double"}, {"name": "shape_type", "isOptional": true, "description": "The geometry of the neighborhood. ONE SECTOR \u2014 Single ellipse FOUR SECTORS \u2014 Ellipse divided into four sectors FOUR SECTOR SHIFTED \u2014 Ellipse divided into four sectors and shifted 45 degrees EIGHT SECTORS \u2014 Ellipse divided into eight sectors ", "dataType": "String"}]},
{"syntax": "CrossValidation_ga (in_geostat_layer, {out_point_feature_class})", "name": "Cross Validation (Geostatisical Analyst)", "description": "Removes one data location and then predicts the associated data using the data at the rest of the locations.   The primary use for this tool is to compare the predicted value to the observed value in order to obtain useful information about some of your model parameters.", "example": {"title": "CrossValidation example 1 (Python window)", "description": "Perform cross validation on an input geostatistical layer.", "code": "import arcpy arcpy.env.workspace = \"C:/gapyexamples/data\" cvResult = arcpy.CrossValidation_ga ( \"C:/gapyexamples/data/kriging.lyr\" ) print \"Root Mean Square error = \" + str ( cvResult.rootMeanSquare )"}, "usage": ["When using this tool in Python,  the ", "result", " object contains both a feature class and a   ", "CrossValidationResult", ", which has the following properties;", "The fields in the optional output feature class are described in ", "GA Layer To Points", "."], "parameters": [{"name": "in_geostat_layer", "isInputFile": true, "isOptional": false, "description": "The geostatistical layer to be analyzed. ", "dataType": "Geostatistical Layer"}, {"name": "out_point_feature_class", "isOutputFile": true, "isOptional": true, "description": "Stores the cross-validation statistics at each location in the geostatistical layer. ", "dataType": "Feature Class"}]},
{"syntax": "CopyTraversedSourceFeatures_na (input_network_analysis_layer, output_location, edge_feature_class_name, junction_feature_class_name, turn_table_name)", "name": "Copy Traversed Source Features (Network Analyst)", "description": "\r\n Creates two feature classes and a table, which together contain information about the edges, junctions, and turns that are traversed while solving a network analysis layer.   Learn about the output from Copy Traversed Source Features", "example": {"title": "CopyTraversedSourceFeatures example 1 (Python Window)", "description": "The following Python window script demonstrates how to use the CopyTraversedSourceFeatures tool to write the traversed edges, junctions, and\r\n turns from a Route network analysis layer to feature classes and table in an in-memory workspace.", "code": "import arcpy arcpy.na.CopyTraversedSourceFeatures ( \"Route\" , \"in_memory\" , \"TraversedEdges\" , \"TraversedJunctions\" , \"TraversedTurns\" )"}, "usage": ["\r\nThe tool solves the input network analysis layer if it isn't already solved. The analysis layer is re-solved if any changes have been made to the inputs since the last solve.", "Traversed source features can be generated for the following network analysis layers:", "Traversed source features cannot be generated for the following layers:", "The output junctions feature class not only includes points that represent traversed network junctions, it also includes points that represent the following:", "Learn about the output from Copy Traversed Source Features", "The coordinate system for the output feature classes can be controlled by specifying the ", "Output Coordinate System", " environment setting or by specifying a feature dataset in a geodatabase as the value for the Output Location parameter. If the Output Coordinate System environment setting is not specified or the Output Location parameter is not a feature dataset, the output feature classes have the same coordinate system as the input network analysis layer. "], "parameters": [{"name": "input_network_analysis_layer", "isInputFile": true, "isOptional": false, "description": " The network analysis layer from which traversed source features will be copied. If the network analysis layer does not have a valid result, the layer will be solved to produce one. ", "dataType": "Network Analyst Layer"}, {"name": "output_location", "isOutputFile": true, "isOptional": false, "description": " The workspace where the output table and two feature classes will be saved. ", "dataType": "Workspace; Feature Dataset"}, {"name": "edge_feature_class_name", "isOptional": false, "description": " The name of the feature class that will contain information about the traversed edge source features. If the solved network analysis layer doesn't traverse any edge features, an empty feature class is created. ", "dataType": "String"}, {"name": "junction_feature_class_name", "isOptional": false, "description": " The name of the feature class that will contain information about the traversed junction source features, including system junctions and relevant points from the input network analysis layer. If the solved network analysis layer doesn't traverse any junctions, an empty feature class is created. ", "dataType": "String"}, {"name": "turn_table_name", "isOptional": false, "description": " The name of the table that will contain information about the traversed global turns and turn features that scale cost for the underlying edges. If the solved network analysis layer doesn't traverse any turns, an empty table is created. Since restricted turns are never traversed, they are never included in the output. ", "dataType": "String"}]},
{"syntax": "UpdateTrafficData_na (provider, user_name, password, regions, traffic_data_output_folder, expected_update_interval, prediction_cutoff, compress_data, {maximum_file_age})", "name": "Update Traffic Data (Network Analyst)", "description": "Downloads live traffic data from a web service and stores it in a  dynamic traffic format (DTF) file, which is a file that network datasets can read for live-traffic analysis and display.", "example": {"title": "UpdateTrafficData example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Update Traffic Data tool.", "code": "import arcpy arcpy.na.UpdateTrafficData ( \"NAVTEQ North America\" , \"myUserName\" , \"myPassword\" , [ \"New England\" , \"New York/Northern NJ/Connecticut\" ], \"d:/data/dtfs\" , 15 , 120 , \"NO_COMPRESS\" , 720 )"}, "usage": ["\r\nYou need to have an account with one of the data providers listed in the Provider parameter before this tool can download traffic data.\r\n", "By scheduling this tool to run at regular intervals or strategic times, you can ensure the latest traffic data is available. One common method of scheduling is to use Windows Task Scheduler on a Python script that calls this tool."], "parameters": [{"name": "provider", "isOptional": false, "description": " Choose the name of your traffic data provider. The tool supports downloading traffic data from the following providers: NAVTEQ : TomTom : INRIX : NAVTEQ : North America Europe South America Oceania Middle East & Africa India TomTom : North America Europe INRIX : California", "dataType": "String"}, {"name": "user_name", "isOptional": false, "description": " The user name authorized by the data provider to download traffic data. The tool fails to execute if the user name cannot be authenticated by the data provider. If the Provider parameter is TomTom North America or TomTom Europe, use \"APIKEY\" as the parameter value. ", "dataType": "String"}, {"name": "password", "isOptional": false, "description": " The password provided by the data provider to download traffic data. The tool fails to execute if the password cannot be authenticated by the data provider. ", "dataType": "Encrypted String"}, {"name": "regions", "isOptional": false, "description": "Enter the regions for which you want to download traffic data. To download all available regions, type \"#\" . ", "dataType": "String"}, {"name": "traffic_data_output_folder", "isOptional": false, "description": " The folder in which the DTF file will be created. If the folder is empty, the tool creates a TrafficIndex.xml file along with the DTF file. For subsequent tool runs, the tool updates TrafficIndex.xml and creates a DTF file. If you download data from multiple data providers, a unique folder should be specified for each data provider. ", "dataType": "Folder"}, {"name": "expected_update_interval", "isOptional": false, "description": " The time interval in minutes after which the downloaded traffic data is no longer up-to-date, and the data provider makes available refreshed data. After this interval has elapsed, it is recommended that you rerun the tool and download the latest data. ", "dataType": "Long"}, {"name": "prediction_cutoff", "isOptional": false, "description": " The time interval (in minutes) for which the predictive traffic data is processed by the tool. Data providers may supply predictive data for the next day or other time period. This time-span value is used to limit the amount of predictive traffic data that is processed by the tool to speed up tool execution. ", "dataType": "Long"}, {"name": "compress_data", "isOptional": false, "description": " COMPRESS \u2014 Downloads the TrafficIndex.xml and DTF files, then creates copies of the DTF files in a zipped folder. Use this option if network datasets will connect to the live traffic data via a geoprocessing service; transferring the zipped folder of DTF files to clients is quicker than transferring the DTF files themselves. NO_COMPRESS \u2014 Downloads the TrafficIndex.xml and DTF files without creating copies of them in a zipped folder. Use this option if network datasets will connect to the live traffic data via a folder connection. This is the default value. ", "dataType": "Boolean"}, {"name": "maximum_file_age", "isOptional": true, "description": " The time interval (in minutes) for which the traffic files (that is, the DTF files) will be kept in the traffic data output folder. This parameter facilitates deleting traffic files that are no longer required. When the tool is rerun, any traffic data files that are older than the maximum file age are deleted automatically. The default value is 720 minutes (12 hours). ", "dataType": "Long"}]},
{"syntax": "UpdateTrafficIncidents_na (provider, user_name, password, regions, incidents_feature_class_location, incidents_feature_class_name, {time_zone_boundaries}, {time_zone_id_field})", "name": "Update Traffic Incidents (Network Analyst)", "description": "Creates a point feature class containing live traffic-incident data from a web service.\r\nTraffic incidents include events such as accidents and road construction.", "example": {"title": "UpdateTrafficIncidents example 1 (Python window)", "description": "The following Python window script demonstrates how to use the Update Traffic Incidents tool.", "code": "import arcpy arcpy.na.UpdateTrafficIncidents ( \"NAVTEQ North America\" , \"myUserName\" , \"myPassword\" , [ \"New England\" , \"New York/Northern NJ/Connecticut\" ], \"C:/Data/Traffic.gdb\" , \"Traffic_Incidents\" , \"C:/data/TimeZones.gdb/NATimeZones\" , \"MSTIMEZONE\" )"}, "usage": ["\r\nYou need to have an account with one of the data providers listed in the ", "Provider", " parameter before this tool can download traffic data.\r\n", "\r\nIf you run this tool multiple times using the same output workspace and feature class name, all prior features are deleted before creating new ones."], "parameters": [{"name": "provider", "isOptional": false, "description": " Choose the name of your traffic-incident data provider. The tool supports downloading traffic incidents from the following providers: NAVTEQ : TomTom : NAVTEQ : North America Europe South America Middle East & Africa TomTom : North America Europe", "dataType": "String"}, {"name": "user_name", "isOptional": false, "description": " The user name authorized by the data provider to download incidents. The tool fails to execute if the user name cannot be authenticated by the data provider. If the Provider parameter is TomTom North America or TomTom Europe, use \"APIKEY\" as the parameter value. ", "dataType": "String"}, {"name": "password", "isOptional": false, "description": " The password provided by the data provider to download traffic-incident data. The tool fails to execute if the password cannot be authenticated by the data provider. ", "dataType": "Encrypted String"}, {"name": "regions", "isOptional": false, "description": "Enter the regions for which you want to download traffic-incident data. To download all available regions, type \"#\" . ", "dataType": "String"}, {"name": "incidents_feature_class_location", "isOptional": false, "description": " The ArcSDE, file, or personal geodatabase in which the output feature class will be created. This workspace must already exist. ", "dataType": "Workspace; Feature Dataset"}, {"name": "incidents_feature_class_name", "isOptional": false, "description": " The name of the feature class to be created. If the tool has been run before and the feature class already exists, the tool will delete existing features and create new ones based on the most recent incident data. ", "dataType": "String"}, {"name": "time_zone_boundaries", "isOptional": true, "description": "The polygon feature class whose features delineate time zones. By providing this feature class, incidents occurring within time zone boundaries can be reported in local time, not only Coordinated Universal Time. If you don't provide a time zone boundaries feature class, incident start and end times can be reported in Coordinated Universal Time (UTC) only; all local time fields are assigned null values. Occasionally, certain incidents from traffic feeds such as weather events have null geometries. In this situation, the local time fields are assigned null values even if the time zone boundaries feature class is provided. Setting the Time Zone ID Field property is required if you provide a time-zone-boundaries feature class. ", "dataType": "Feature Layer"}, {"name": "time_zone_id_field", "isOptional": true, "description": " The text field, from the time-zone-boundaries feature class, that contains Windows time zone identifiers. The values in this field correspond to time zone keys in the Windows registry. You can follow similar steps to those outlined in the topic Adding time zones to a network dataset to find proper time zone names for the polygons in your input feature class. ", "dataType": "Field"}]},
{"syntax": "SolveVehicleRoutingProblem_na (orders, depots, routes, breaks, time_units, distance_units, network_dataset, output_workspace_location, output_unassigned_stops_name, output_stops_name, output_routes_name, output_directions_name, {default_date}, {uturn_policy}, {time_window_factor}, {spatially_cluster_routes}, {route_zones}, {route_renewals}, {order_pairs}, {excess_transit_factor}, {point_barriers}, {line_barriers}, {polygon_barriers}, {time_attribute}, {distance_attribute}, {use_hierarchy_in_analysis}, {restrictions}, {attribute_parameter_values}, {maximum_snap_tolerance}, {exclude_restricted_portions_of_the_network}, {feature_locator_where_clause}, {populate_route_lines}, {route_line_simplification_tolerance}, {populate_directions}, {directions_language}, {directions_style_name}, {save_output_layer}, {service_capabilities})", "name": "Solve Vehicle Routing Problem (Network Analyst)", "description": "\r\nCreates a vehicle routing problem (VRP) network analysis layer, sets the analysis properties, and solves the analysis, which is ideal for setting up a VRP web service. A vehicle routing problem analysis layer finds the best routes for a fleet of vehicles.", "example": {"title": "SolveVehicleRoutingProblem example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy orders = arcpy.FeatureSet () orders.load ( \"Stores\" ) depots = arcpy.FeatureSet () depots.load ( \"DistributionCenter\" ) routes = arcpy.RecordSet () routes.load ( \"RoutesTable\" ) arcpy.na.SolveVehicleRoutingProblem ( orders , depots , routes , \"\" , \"Minutes\" , \"Miles\" , \"Streets_ND\" )"}, "usage": ["The tool dialog box groups the various optional parameters into the following six categories to make it easier for you to manage them:", "Learn about the output from Solve Vehicle Routing Problem"], "parameters": [{"name": "orders", "isOptional": false, "description": "The orders that the routes of the VRP analysis should visit. An order can represent a delivery (for example, furniture delivery), a pickup (such as an airport shuttle bus picking up a passenger), or some type of service or inspection (a tree trimming job or building inspection, for instance). The orders feature set has an associated attribute table. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the order. The name must be unique. If the name is left null, a name is automatically generated at solve time. ServiceTime: This property specifies how much time will be spent at the network location when the route visits it; that is, it stores the impedance value for the network location. A zero or null value indicates the network location requires no service time. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). TimeWindowStart1: The beginning time of the first time window for the network location. This field can contain a null value; a null value indicates no beginning time. A time window only states when a vehicle can arrive at an order; it doesn't state when the service time must be completed. To account for service time and leave before the time window is over, subtract ServiceTime from the TimeWindowEnd1 field. The time window fields can contain a time-only value or a date and time value. If a time field such as TimeWindowStart1 has a time-only value (for example, 8:00 AM), the date is assumed to be the date specified by the Default Date property of the analysis layer. Using date and time values (for example, 7/11/2010 8:00 AM) allows you to set time windows that span multiple days. If you are using traffic data , the time-of-day fields for the network location always reference the same time zone as the edge on which the network location is located. TimeWindowEnd1: The ending time of the first window for the network location. This field can contain a null value; a null value indicates no ending time. TimeWindowStart2: The beginning time of the second time window for the network location. This field can contain a null value; a null value indicates that there is no second time window. If the first time window is null, as specified by the TimeWindowStart1 and TimeWindowEnd1 fields, the second time window must also be null. If both time windows are nonnull, they can't overlap. Also, the second time window must occur after the first. TimeWindowEnd2: The ending time of the second time window for the network location. This field can contain a null value. When TimeWindowStart2 and TimeWindowEnd2 are both null, there is no second time window. When TimeWindowStart2 is not null but TimeWindowEnd2 is null, there is a second time window that has a starting time but no ending time. This is valid. MaxViolationTime1: A time window is considered violated if the arrival time occurs after the time window has ended. This field specifies the maximum allowable violation time for the first time window of the order. It can contain a zero value but can't contain negative values. A zero value indicates that a time window violation at the first time window of the order is unacceptable; that is, the first time window is hard. On the other hand, a null value indicates that there is no limit on the allowable violation time. A nonzero value specifies the maximum amount of lateness; for example, a route can arrive at an order up to 30 minutes beyond the end of its first time window. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). Time window violations can be tracked and weighted by the solver. Because of this, you can direct the VRP solver to take one of three approaches: By assigning an importance level for the Time Window Violation Importance parameter ( time_window_factor for Python), you are essentially choosing one of these three approaches. In any case, however, the solver will return an error if the value set for MaxViolationTime1 is surpassed. MaxViolationTime2: The maximum allowable violation time for the second time window of the order. This field is analogous to the MaxViolationTime1 field. DeliveryQuantities: The size of the delivery. You can specify size in any dimension you want, such as weight, volume, or quantity. You can even specify multiple dimensions, for example, weight and volume. Enter delivery quantities without indicating units. For example, if a 300-pound object needs to be delivered to an order, enter 300 . You will need to remember that the value is in pounds. If you are tracking multiple dimensions, separate the numeric values with a space. For instance, if you are recording the weight and volume of a delivery that weighs 2,000 pounds and has a volume of 100 cubic feet, enter 2000 100 . Again, you need to remember the units\u2014in this case, pounds and cubic feet. You also need to remember the sequence the values and their corresponding units are entered in. Make sure that Capacities for Routes and DeliveryQuantities and PickupQuantities for Orders are specified in the same manner; that is, the values need to be in the same units, and if you are using multiple dimensions, the dimensions need to be listed in the same sequence for all parameters. So if you specify weight in pounds, followed by volume in cubic feet for DeliveryQuantities, the capacity of your routes and the pickup quantities of your orders need to be specified the same way: weight in pounds, then volume in cubic feet. If you mix units or change the sequence, you will get unwanted results without receiving any warning messages. An empty string or null value is equivalent to all dimensions being zero. If the string has an insufficient number of values in relation to the capacity count, or dimensions being tracked, the remaining values are treated as zeros. Delivery quantities can't be negative. PickupQuantities: The size of the pickup. You can specify size in any dimension you want, such as weight, volume, or quantity. You can even specify multiple dimensions, for example, weight and volume. You cannot, however, use negative values. This field is analogous to the DeliveryQuantities field of Orders. Revenue: The income generated if the order is included in a solution. This field can contain a null value\u2014a null value indicates zero revenue\u2014but it can't have a negative value. Revenue is included in optimizing the objective function value but is not part of the solution's operating cost. That is, the TotalCost field in the route class never includes revenue in its output; however, revenue weights the relative importance of servicing orders. SpecialtyNames: A space-separated string containing the names of the specialties required by the order. A null value indicates that the order doesn't require specialties. The spelling of any specialties listed in the Orders and Routes classes must match exactly so that the VRP solver can link them together. To illustrate what specialties are and how they work, assume a lawn care and tree trimming company has a portion of its orders that requires a bucket truck to trim tall trees. The company would enter BucketTruck in the SpecialtyNames field for these orders to indicate their special need. SpecialtyNames would be left as null for the other orders. Similarly, the company would also enter BucketTruck in the SpecialtyNames field of routes that are driven by trucks with hydraulic booms. It would leave the field null for the other routes. At solve time, the VRP solver assigns orders without special needs to any route, but it only assigns orders that need bucket trucks to routes that have them. AssignmentRule: This field specifies the rule for assigning the order to a route. It is constrained by a domain of values, which are listed below (their coded values are shown in parentheses). With this setting, only the relative sequence is maintained, not the absolute sequence. To illustrate what this means, imagine there are two orders: A and B. They have sequence values of 2 and 3, respectively. If you set their AssignmentRule field values to Preserve route and relative sequence, A's and B's actual sequence values may change after solving because other orders, breaks, and depot visits could still be sequenced before, between, or after A and B. However, B cannot be sequenced before A. This field can't contain a null value. CurbApproach: The CurbApproach property specifies the direction a vehicle may arrive at and depart from the network location. There are four choices (their coded values are shown in parentheses): RouteName: The name of the route to which the order is assigned. As an input field, this field is used to preassign an order to a specific route. It can contain a null value, indicating that the order is not preassigned to any route, and the solver determines the best possible route assignment for the order. If this is set to null, the sequence field must also be set to null. After a solve operation, if the order is routed, the RouteName field contains the name of the route that the order is assigned to. Sequence: This indicates the sequence of the order on its assigned route. As an input field, this field is used to specify the relative sequence for an order on the route. This field can contain a null value specifying that the order can be placed anywhere along the route. A null value can only occur together with a null RouteName value. The input sequence values are positive and unique for each route (shared across renewal depot visits, orders, and breaks), but do not need to start from 1 or be contiguous. After a solve operation, the Sequence field contains the sequence value of the order on its assigned route. Output sequence values for a route are shared across depot visits, orders, and breaks; start from 1 (at the starting depot); and are consecutive. So the smallest possible output sequence value for a routed order is 2, since a route always begins at a depot. A time window only states when a vehicle can arrive at an order; it doesn't state when the service time must be completed. To account for service time and leave before the time window is over, subtract ServiceTime from the TimeWindowEnd1 field. The time window fields can contain a time-only value or a date and time value. If a time field such as TimeWindowStart1 has a time-only value (for example, 8:00 AM), the date is assumed to be the date specified by the Default Date property of the analysis layer. Using date and time values (for example, 7/11/2010 8:00 AM) allows you to set time windows that span multiple days. If you are using traffic data , the time-of-day fields for the network location always reference the same time zone as the edge on which the network location is located. Minimize the overall violation time, regardless of the increase in travel cost for the fleet. Find a solution that balances overall violation time and travel cost. Ignore the overall violation time; instead, minimize the travel cost for the fleet. Exclude (0)\u2014The order is to be excluded from the subsequent solve operation. Preserve route and relative sequence (1)\u2014The solver must always assign the order to the preassigned route and at the preassigned relative sequence during the solve operation. If this assignment rule can't be followed, it results in an order violation. With this setting, only the relative sequence is maintained, not the absolute sequence. To illustrate what this means, imagine there are two orders: A and B. They have sequence values of 2 and 3, respectively. If you set their AssignmentRule field values to Preserve route and relative sequence, A's and B's actual sequence values may change after solving because other orders, breaks, and depot visits could still be sequenced before, between, or after A and B. However, B cannot be sequenced before A. Preserve route (2)\u2014The solver must always assign the order to the preassigned route during the solve operation. A valid sequence must also be set even though the sequence may or may not be preserved. If the order can't be assigned to the specified route, it results in an order violation. Override (3)\u2014The solver tries to preserve the route and sequence preassignment for the order during the solve operation. However, a new route and/or sequence for the order may be assigned if it helps minimize the overall value of the objective function. This is the default value. Either side of vehicle (0)\u2014The vehicle can approach and depart the network location in either direction. U-turns are allowed. You should choose this setting if your vehicle can make a U-turn at the stop or if it can pull into a driveway or parking lot and turn around. Right side of vehicle (1)\u2014When the vehicle approaches and departs the network location, the curb must be on the right side of the vehicle. A U-turn is prohibited. Left side of vehicle (2)\u2014When the vehicle approaches and departs the network location, the curb must be on the left side of the vehicle. A U-turn is prohibited. No U-Turn (3)\u2014When the vehicle approaches the network location, the curb can be on either side of the vehicle; however, the vehicle must depart without turning around.", "dataType": "Feature Set"}, {"name": "depots", "isOptional": false, "description": "A depot is a location that a vehicle departs from at the beginning of its workday and returns to at the end of the workday. Vehicles are loaded (for deliveries) or unloaded (for pickups) at depots at the start of the route. In some cases, a depot can also act as a renewal location whereby the vehicle can unload or reload and continue performing deliveries and pickups. A depot has open and close times, as specified by a hard time-window. Vehicles can't arrive at a depot outside of this time window. The depots feature set has an associated attribute table. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the depot. The StartDepotName and EndDepotName fields of the Routes record set reference the names you specify here. It is also referenced by the Route Renewals record set, when used. Depot names are case insensitive and have to be nonempty and unique. TimeWindowStart1: The beginning time of the first time window for the network location. This field can contain a null value; a null value indicates no beginning time. Time window fields can contain a time-only value or a date and time value. If a time field has a time-only value (for example, 8:00 AM), the date is assumed to be the date specified by the Default Date parameter of the analysis layer. Using date and time values (for example, 7/11/2010 8:00 AM) allows you to set time windows that span multiple days. If you are using traffic data , the time-of-day fields for the network location always reference the same time zone as the edge on which the network location is located. TimeWindowEnd1: The ending time of the first window for the network location. This field can contain a null value; a null value indicates no ending time. TimeWindowStart2: The beginning time of the second time window for the network location. This field can contain a null value; a null value indicates that there is no second time window. If the first time window is null, as specified by the TimeWindowStart1 and TimeWindowEnd1 fields, the second time window must also be null. If both time windows are nonnull, they can't overlap. Also, the second time window must occur after the first. TimeWindowEnd2: The ending time of the second time window for the network location. This field can contain a null value. When TimeWindowStart2 and TimeWindowEnd2 are both null, there is no second time window. When TimeWindowStart2 is not null but TimeWindowEnd2 is null, there is a second time window that has a starting time but no ending time. This is valid. CurbApproach: The CurbApproach property specifies the direction a vehicle may arrive at and depart from the network location. There are four choices (their coded values are shown in parentheses): Bearing: The direction in which a point is moving. The units are degrees and are measured in a clockwise fashion from true north. This field is used in conjunction with the BearingTol field. Bearing data is usually sent automatically from a mobile device that is equipped with a GPS receiver. Try to include bearing data if you are loading an order that is moving, such as a pedestrian or a vehicle. Using this field tends to prevent adding locations to the wrong edges, which can occur when a vehicle is near an intersection or an overpass, for example. Bearing also helps Network Analyst determine which side of the street the point is on. For more information, see Bearing and BearingTol . BearingTol: The bearing tolerance value creates a range of acceptable bearing values when locating moving points on an edge using the Bearing field. If the value from the Bearing field is within the range of acceptable values that are generated from the bearing tolerance on an edge, the point can be added as a network location there; otherwise, the closest point on the next-nearest edge is evaluated. The units are in degrees, and the default value is 30. Values must be greater than zero and less than 180. A value of 30 means that when Network Analyst attempts to add a network location on an edge, a range of acceptable bearing values is generated 15\u00ba to either side of the edge (left and right) and in both digitized directions of the edge. For more information, see Bearing and BearingTol . NavLatency: This field is only used in the solve process if Bearing and BearingTol also have values; however, entering a NavLatency value is optional, even when values are present in Bearing and BearingTol. NavLatency indicates how much time is expected to elapse from the moment GPS information is sent from a moving vehicle to a server and the moment the processed route is received by the vehicle's navigation device. The time units of NavLatency are the same as the units of the cost attribute specified by the parameter Time Attribute. Time window fields can contain a time-only value or a date and time value. If a time field has a time-only value (for example, 8:00 AM), the date is assumed to be the date specified by the Default Date parameter of the analysis layer. Using date and time values (for example, 7/11/2010 8:00 AM) allows you to set time windows that span multiple days. If you are using traffic data , the time-of-day fields for the network location always reference the same time zone as the edge on which the network location is located. Either side of vehicle (0)\u2014The vehicle can approach and depart the network location in either direction. U-turns are allowed. You should choose this setting if your vehicle can make a U-turn at the stop or if it can pull into a driveway or parking lot and turn around. Right side of vehicle (1)\u2014When the vehicle approaches and departs the network location, the curb must be on the right side of the vehicle. A U-turn is prohibited. Left side of vehicle (2)\u2014When the vehicle approaches and departs the network location, the curb must be on the left side of the vehicle. A U-turn is prohibited. No U-Turn (3)\u2014When the vehicle approaches the network location, the curb can be on either side of the vehicle; however, the vehicle must depart without turning around.", "dataType": "Feature Set"}, {"name": "routes", "isOptional": false, "description": "The routes that are available for the given vehicle routing problem. A route specifies vehicle and driver characteristics; after solving, it also represents the path between depots and orders. A route can have start and end depot service times, a fixed or flexible starting time, time-based operating costs, distance-based operating costs, multiple capacities, various constraints on a driver's workday, and so on. The routes record set has several attributes. The fields in the attribute table are listed below and described. Name: The name of the route. The name must be unique. Network Analyst generates a unique name at solve time if the field value is null. Therefore, entering a value is optional in most cases. However, you must enter a name if your analysis includes breaks, route renewals, route zones, or orders that are preassigned to a route because the route name is used as a foreign key in these cases. Note that route names are case insensitive. StartDepotName: The name of the starting depot for the route. This field is a foreign key to the Name field in Depots. If the StartDepotName value is null, the route will begin from the first order assigned. Omitting the start depot is useful when the vehicle's starting location is unknown or irrelevant to your problem. However, when StartDepotName is null, EndDepotName cannot also be null. If the route is making deliveries and StartDepotName is null, it is assumed the cargo is loaded on the vehicle at a virtual depot before the route begins. For a route that has no renewal visits, its delivery orders (those with nonzero DeliveryQuantities values in the Orders class) are loaded at the start depot or virtual depot. For a route that has renewal visits, only the delivery orders before the first renewal visit are loaded at the start depot or virtual depot. EndDepotName: The name of the ending depot for the route. This field is a foreign key to the Name field in the Depots class. StartDepotServiceTime: The service time at the starting depot. This can be used to model the time spent for loading the vehicle. This field can contain a null value; a null value indicates zero service time. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). The service times at the start and end depots are fixed values (given by the StartDepotServiceTime and EndDepotServiceTime field values) and do not take into account the actual load for a route. For example, the time taken to load a vehicle at the starting depot may depend on the size of the orders. As such, the depot service times could be given values corresponding to a full truckload or an average truckload, or you could make your own time estimate. EndDepotServiceTime: The service time at the ending depot. This can be used to model the time spent for unloading the vehicle. This field can contain a null value; a null value indicates zero service time. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). The service times at the start and end depots are fixed values (given by the StartDepotServiceTime and EndDepotServiceTime field values) and do not take into account the actual load for a route. For example, the time taken to load a vehicle at the starting depot may depend on the size of the orders. As such, the depot service times could be given values corresponding to a full truckload or an average truckload, or you could make your own time estimate. EarliestStartTime: The earliest allowable starting time for the route. This is used by the solver in conjunction with the time window of the starting depot for determining feasible route start times. This field can't contain null values and has a default time-only value of 8:00 AM; the default value is interpreted as 8:00 a.m. on the date given by the Default Date parameter (default_date for Python). When using network datasets with traffic data across multiple time zones, the time zone for EarliestStartTime is the same as the time zone of the edge or junction on which the starting depot is located. LatestStartTime: The latest allowable starting time for the route. This field can't contain null values and has a default time-only value of 10:00 AM; the default value is interpreted as 10:00 a.m. on the date given by the Default Date property of the analysis layer. When using network datasets with traffic data across multiple time zones, the time zone for LatestStartTime is the same as the time zone of the edge or junction on which the starting depot is located. ArriveDepartDelay: This field stores the amount of travel time needed to accelerate the vehicle to normal travel speeds, decelerate it to a stop, and move it off and on the network (for example, in and out of parking). By including an ArriveDepartDelay value, the VRP solver is deterred from sending many routes to service physically coincident orders. The cost for this property is incurred between visits to noncoincident orders, depots, and route renewals. For example, when a route starts from a depot and visits the first order, the total arrive/depart delay is added to the travel time. The same is true when traveling from the first order to the second order. If the second and third orders are coincident, the ArriveDepartDelay value is not added between them since the vehicle doesn't need to move. If the route travels to a route renewal, the value is added to the travel time again. Although a vehicle needs to slow down and stop for a break and accelerate afterwards, the VRP solver cannot add the ArriveDepartDelay value for breaks. This means that if a route leaves an order, stops for a break, and continues to the next order, the arrive/depart delay is added only once, not twice. To illustrate, assume there are five coincident orders in a high-rise building, and they are serviced by three different routes. This means three arrive/depart delays would be incurred; that is, three drivers would need to separately find parking places and enter the same building. However, if the orders could be serviced by just one route instead, only one driver would need to park and enter the building\u2014only one arrive/depart delay would be incurred. Since the VRP solver tries to minimize cost, it will try to limit the arrive/depart delays and thus choose the single-route option. (Note that multiple routes may need to be sent when other constraints\u2014such as specialties, time windows, or capacities\u2014require it.) The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). Capacities: The maximum capacity of the vehicle. You can specify capacity in any dimension you want, such as weight, volume, or quantity. You can even specify multiple dimensions, for example, weight and volume. Enter capacities without indicating units. For example, assume your vehicle can carry a maximum of 40,000 pounds; you would enter 40000 . You need to remember for future reference that the value is in pounds. If you are tracking multiple dimensions, separate the numeric values with a space. For instance, if you are recording both weight and volume and your vehicle can carry a maximum weight of 40,000 pounds and a maximum volume of 2,000 cubic feet, Capacities should be entered as 40000 2000 . Again, you need to remember the units. You also need to remember the sequence the values and their corresponding units are entered in (pounds followed by cubic feet in this case). Remembering the units and the unit sequence is important for a couple of reasons: one, so you can reinterpret the information later; two, so you can properly enter values for the DeliveryQuantities and PickupQuantities fields for Orders. To elaborate on the second point, note that the VRP solver simultaneously refers to Capacities, DeliveryQuantities, and PickupQuantities to make sure that a route doesn't become overloaded. Since units can't be entered in the field, Network Analyst can't make unit conversions, so you need to enter the values for the three fields using the same units and the same unit sequence to ensure the values are correctly interpreted. If you mix units or change the sequence in any of the three fields, you will get unwanted results without receiving any warning messages. Thus, it is a good idea to set up a unit and unit-sequence standard beforehand and continually refer to it whenever entering values for these three fields. An empty string or null value is equivalent to all values being zero. Capacity values can't be negative. If the Capacities string has an insufficient number of values in relation to the DeliveryQuantities or PickupQuantities fields for orders, the remaining values are treated as zero. The VRP solver only performs a simple Boolean test to determine whether capacities are exceeded. If a route's capacity value is greater than or equal to the total quantity being carried, the VRP solver will assume the cargo fits in the vehicle. This could be incorrect, depending on the actual shape of the cargo and the vehicle. For example, the VRP solver allows you to fit a 1,000-cubic-foot sphere into a 1,000-cubic-foot truck that is 8 feet wide. In reality, however, since the sphere is 12.6 feet in diameter, it won't fit in the 8-foot wide truck. FixedCost: A fixed monetary cost that is incurred only if the route is used in a solution (that is, it has orders assigned to it). This field can contain null values; a null value indicates zero fixed cost. This cost is part of the total route operating cost. CostPerUnitTime: The monetary cost incurred\u2014per unit of work time\u2014for the total route duration, including travel times as well as service times and wait times at orders, depots, and breaks. This field can't contain a null value and has a default value of 1.0. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). CostPerUnitDistance: The monetary cost incurred\u2014per unit of distance traveled\u2014for the route length (total travel distance). This field can contain null values; a null value indicates zero cost. The unit for this field value is specified by the Distance Field Units parameter (distance_units for Python). OvertimeStartTime: The duration of regular work time before overtime computation begins. This field can contain null values; a null value indicates that overtime does not apply. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). For example, if the driver is to be paid overtime pay when the total route duration extends beyond eight hours, OvertimeStartTime is specified as 480 (8 hours * 60 minutes/hour), given the Time Field Units parameter is set to Minutes. CostPerUnitOvertime: The monetary cost incurred per time unit of overtime work. This field can contain null values; a null value indicates that the CostPerUnitOvertime value is the same as the CostPerUnitTime value. MaxOrderCount: The maximum allowable number of orders on the route. This field can't contain null values and has a default value of 30. MaxTotalTime: The maximum allowable route duration. The route duration includes travel times as well as service and wait times at orders, depots, and breaks. This field can contain null values; a null value indicates that there is no constraint on the route duration. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). MaxTotalTravelTime: The maximum allowable travel time for the route. The travel time includes only the time spent driving on the network and does not include service or wait times. This field can contain null values; a null value indicates there is no constraint on the maximum allowable travel time. This field value can't be larger than the MaxTotalTime field value. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). MaxTotalDistance: The maximum allowable travel distance for the route. The unit for this field value is specified by the Distance Field Units parameter (distance_units for Python). This field can contain null values; a null value indicates that there is no constraint on the maximum allowable travel distance. SpecialtyNames: A space-separated string containing the names of the specialties supported by the route. A null value indicates that the route does not support any specialties. This field is a foreign key to the SpecialtyNames field in the orders class. To illustrate what specialties are and how they work, assume a lawn care and tree trimming company has a portion of its orders that requires a bucket truck to trim tall trees. The company would enter BucketTruck in the SpecialtyNames field for these orders to indicate their special need. SpecialtyNames would be left as null for the other orders. Similarly, the company would also enter BucketTruck in the SpecialtyNames field of routes that are driven by trucks with hydraulic booms. It would leave the field null for the other routes. At solve time, the VRP solver assigns orders without special needs to any route, but it only assigns orders that need bucket trucks to routes that have them. AssignmentRule: This specifies whether the route can be used or not when solving the problem. This field is constrained by a domain of values, and the possible values are the following: Include\u2014 The route is included in the solve operation. This is the default value. Exclude\u2014 The route is excluded from the solve operation.", "dataType": "Record Set"}, {"name": "breaks", "isOptional": false, "description": "The rest periods, or breaks, for the routes in a given vehicle routing problem. A break is associated with exactly one route, and it can be taken after completing an order, while en route to an order, or prior to servicing an order. It has a start time and a duration, for which the driver may or may not be paid. There are three options for establishing when a break begins: using a time window, a maximum travel time, or a maximum work time. The breaks record set has associated attributes. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. RouteName: The name of the route that the break applies to. Although a break is assigned to exactly one route, many breaks can be assigned to the same route. This field is a foreign key to the Name field in the Routes class and can't have a null value. Precedence: Precedence values sequence the breaks of a given route. Breaks with a precedence value of 1 occur before those with a value of 2, and so on. All breaks must have a precedence value, regardless of whether they are time-window, maximum-travel-time, or maximum-work-time breaks. ServiceTime: The duration of the break. This field can contain null values; a null value indicates no service time. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). TimeWindowStart: The starting time of the break's time window. If this field is null and TimeWindowEnd has a valid time-of-day value, the break is allowed to start anytime before the TimeWindowEnd value. If this field has a value, MaxTravelTimeBetweenBreaks and MaxCumulWorkTime must be null; moreover, all other breaks in the analysis layer must have null values for MaxTravelTimeBetweenBreaks and MaxCumulWorkTime. An error will occur at solve time if a route has multiple breaks with overlapping time windows. The time window fields in breaks can contain a time-only value or a date and time value. If a time field, such as TimeWindowStart, has a time-only value (for example, 12:00 PM), the date is assumed to be the date specified by the Default Date parameter ( default_date for Python). Using date and time values (for example, 7/11/2012 12:00 PM) allows you to specify time windows that span two or more days. This is especially beneficial when a break should be taken sometime before and after midnight. When you use network datasets with traffic data across multiple time zones, the time zone for TimeWindowStart and TimeWindowEnd is assumed to be the same as the time zone of the edge or junction on which the starting depot is located. TimeWindowEnd: The ending time of the break's time window. If this field is null and TimeWindowStart has a valid time-of-day value, the break is allowed to start anytime after the TimeWindowStart value. If this field has a value, MaxTravelTimeBetweenBreaks and MaxCumulWorkTime must be null; moreover, all other breaks in the analysis layer must have null values for MaxTravelTimeBetweenBreaks and MaxCumulWorkTime. MaxViolationTime: This field specifies the maximum allowable violation time for a time-window break. A time window is considered violated if the arrival time falls outside the time range. A zero value indicates the time window cannot be violated; that is, the time window is hard. A nonzero value specifies the maximum amount of lateness; for example, the break can begin up to 30 minutes beyond the end of its time window, but the lateness is penalized as per the Time Window Violation Importance parameter ( time_window_factor for Python). This property can be null; a null value with TimeWindowStart and TimeWindowEnd values indicates that there is no limit on the allowable violation time. If MaxTravelTimeBetweenBreaks or MaxCumulWorkTime has a value, MaxViolationTime must be null. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). MaxTravelTimeBetweenBreaks: The maximum amount of travel time that can be accumulated before the break is taken. The travel time is accumulated either from the end of the previous break or, if a break has not yet been taken, from the start of the route. If this is the route's final break, MaxTravelTimeBetweenBreaks also indicates the maximum travel time that can be accumulated from the final break to the end depot. This field is designed to limit how long a person can drive until a break is required. For instance, if the Time Field Units parameter ( time_units for Python) of the analysis is set to Minutes, and MaxTravelTimeBetweenBreaks has a value of 120, the driver will get a break after two hours of driving. To assign a second break after two more hours of driving, the second break's MaxTravelTimeBetweenBreaks property should be 120. If this field has a value, TimeWindowStart, TimeWindowEnd, MaxViolationTime, and MaxCumulWorkTime must be null for an analysis to solve successfully. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). MaxCumulWorkTime: The maximum amount of work time that can be accumulated before the break is taken. Work time is always accumulated from the beginning of the route. Work time is the sum of travel time and service times at orders, depots, and breaks. Note, however, that this excludes wait time, which is the time a route (or driver) spends waiting at an order or depot for a time window to begin. This field is designed to limit how long a person can work until a break is required. For instance, if the Time Field Units property ( time_units for Python) is set to Minutes, MaxCumulWorkTime has a value of 120, and ServiceTime has a value of 15, the driver will get a 15-minute break after two hours of work. Continuing with the last example, assume a second break is needed after three more hours of work. To specify this break, you would enter 315 (five hours and 15 minutes) as the second break's MaxCumulWorkTime value. This number includes the MaxCumulWorkTime and ServiceTime values of the preceding break, along with the three additional hours of work time before granting the second break. To avoid taking maximum-work-time breaks prematurely, remember that they accumulate work time from the beginning of the route and that work time includes the service time at previously visited depots, orders, and breaks. If this field has a value, TimeWindowStart, TimeWindowEnd, MaxViolationTime, and MaxTravelTimeBetweenBreaks must be null for an analysis to solve successfully. The unit for this field value is specified by the Time Field Units parameter ( time_units for Python). IsPaid: A Boolean value indicating whether the break is paid or unpaid. A True value indicates that the time spent at the break is included in the route cost computation and overtime determination. A False value indicates otherwise. The default value is True. Sequence: As an input field, this indicates the sequence of the break on its route. This field can contain null values. The input sequence values are positive and unique for each route (shared across renewal depot visits, orders, and breaks) but need not start from 1 or be contiguous. The solver modifies the sequence field. After solving, this field contains the sequence value of the break on its route. Output sequence values for a route are shared across depot visits, orders, and breaks; start from 1 (at the starting depot); and are consecutive. ", "dataType": "Record Set"}, {"name": "time_units", "isOptional": false, "description": " The time units for all time-based field values in the analysis. Many features and records in a VRP analysis have fields for storing time values, such as ServiceTime for Orders and CostPerUnitTime for Routes. To minimize data entry requirements, these field values don't include units. Instead, all distance-based field values must be entered in the same units, and this parameter is used to specify the units of those values. Note that output time-based fields use the same units specified by this parameter. This time unit doesn't need to match the time unit of the network Time Attribute parameter ( time_attribute ). Seconds Minutes Hours Days", "dataType": "String"}, {"name": "distance_units", "isOptional": false, "description": " The distance units for all distance-based field values in the analysis. Many features and records in a VRP analysis have fields for storing distance values, such as MaxTotalDistance and CostPerUnitDistance for Routes. To minimize data entry requirements, these field values don't include units. Instead, all distance-based field values must be entered in the same units, and this parameter is used to specify the units of those values. Note that output distance-based fields use the same units specified by this parameter. This distance unit doesn't need to match the distance unit of the network Distance Attribute ( distance attribute ). Miles Kilometers Feet Yards Meters NauticalMiles", "dataType": "String"}, {"name": "network_dataset", "isOptional": false, "description": " The network dataset on which the vehicle routing problem analysis will be performed. The network dataset must have a time-based cost attribute since the VRP solver minimizes time. ", "dataType": "Network Dataset Layer"}, {"name": "output_workspace_location", "isOutputFile": true, "isOptional": false, "description": " The ArcSDE geodatabase, file geodatabase, or in-memory workspace in which the output feature classes will be created. This workspace must already exist. The default output workspace is in memory. ", "dataType": "Workspace"}, {"name": "output_unassigned_stops_name", "isOutputFile": true, "isOptional": false, "description": " The name of the output feature class that will contain any unreachable depots or unassigned orders. ", "dataType": "String"}, {"name": "output_stops_name", "isOutputFile": true, "isOptional": false, "description": " The name of the feature class that will contain the stops visited by routes. This feature class includes stops at depots, orders, and breaks. ", "dataType": "String"}, {"name": "output_routes_name", "isOutputFile": true, "isOptional": false, "description": " The name of the feature class that will contain the routes of the analysis. ", "dataType": "String"}, {"name": "output_directions_name", "isOutputFile": true, "isOptional": false, "description": " The name of the feature class that will contain the directions for the routes. ", "dataType": "String"}, {"name": "default_date", "isOptional": true, "description": " The default date for time field values that specify a time of day without including a date. ", "dataType": "Date"}, {"name": "uturn_policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "time_window_factor", "isOptional": true, "description": " Rates the importance of honoring time windows. There are three options, which are listed and described below. Low \u2014 Places more importance on minimizing drive times and less on arriving at stops on time. You may want to use this setting if you have a growing backlog of service requests. For the purpose of servicing more orders in a day and reducing the backlog, you can choose Low even though customers might be inconvenienced with your late arrivals. Medium \u2014 This is the default value. Balances the importance of minimizing drive times and arriving within time windows. High \u2014 Places more importance on arriving at stops on time than on minimizing drive times. Organizations that make time-critical deliveries or that are very concerned with customer service would choose High.", "dataType": "String"}, {"name": "spatially_cluster_routes", "isOptional": true, "description": " CLUSTER \u2014 Dynamic seed points are created for all routes automatically and the orders assigned to an individual route are spatially clustered. Clustering orders tends to keep routes in smaller areas and reduce how often different route lines intersect one another; yet, clustering also tends to increase overall travel times. NO_CLUSTER \u2014 Dynamic seed points aren't created. Choose this option if route zones are specified.", "dataType": "Boolean"}, {"name": "route_zones", "isOptional": true, "description": "Delineates work territories for given routes. A route zone is a polygon feature and is used to constrain routes to servicing only those orders that fall within or near the specified area. Here are some examples of when route zones may be useful: The route zones feature set has an associated attribute table. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. RouteName: The name of the route to which this zone applies. A route zone can have a maximum of one associated route. This field can't contain null values, and it is a foreign key to the Name field in the Routes feature layer. IsHardZone: A Boolean value indicating a hard or soft route zone. A True value indicates that the route zone is hard; that is, an order that falls outside the route zone polygon can't be assigned to the route. The default value is True (1). A False value (0) indicates that such orders can still be assigned, but the cost of servicing the order is weighted by a function that is based on the Euclidean distance from the route zone. Basically, this means that as the straight-line distance from the soft zone to the order increases, the likelihood of the order being assigned to the route decreases. Some of your employees don't have the required permits to perform work in certain states or communities. You can create a hard route zone so they only visit orders in areas where they meet the requirements. One of your vehicles breaks down frequently so you want to minimize response time by having it only visit orders that are close to your maintenance garage. You can create a soft or hard route zone to keep the vehicle nearby. ", "dataType": "Feature Set"}, {"name": "route_renewals", "isOptional": true, "description": "Specifies the intermediate depots that routes can visit to reload or unload the cargo they are delivering or picking up. Specifically, a route renewal links a route to a depot. The relationship indicates the route can renew (reload or unload while en route) at the associated depot. Route renewals can be used to model scenarios in which a vehicle picks up a full load of deliveries at the starting depot, services the orders, returns to the depot to renew its load of deliveries, and continues servicing more orders. For example, in propane gas delivery, the vehicle may make several deliveries until its tank is nearly or completely depleted, visit a refueling point, and make more deliveries. Here are a few rules and options to consider when also working with route seed points: The route renewals record set has associated attributes. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. DepotName: The name of the depot where this renewal takes place. This field can't contain a null value and is a foreign key to the Name field in the Depots feature layer. RouteName: The name of the route that this renewal applies to. This field can't contain a null value and is a foreign key to the Name field in the Routes feature layer. ServiceTime: The service time for the renewal. This field can contain a null value; a null value indicates zero service time. The unit for this field value is specified by the Time Field Units property of the analysis layer. The time taken to load a vehicle at a renewal depot may depend on the size of the vehicle and how full or empty the vehicle is. However, the service time for a route renewal is a fixed value and does not take into account the actual load. As such, the renewal service time should be given a value corresponding to a full truckload, an average truckload, or another time estimate of your choice. The reload/unload point, or renewal location, can be different from the start or end depot. Each route can have one or many predetermined renewal locations. A renewal location may be used more than once by a single route. In some cases where there may be several potential renewal locations for a route, the closest available renewal location is chosen by the solver.", "dataType": "Record Set"}, {"name": "order_pairs", "isOptional": true, "description": "Pairs pickup and delivery orders so they are serviced by the same route. Sometimes it is required that the pickup and delivery for orders be paired. For example, a courier company might need to have a route pick up a high-priority package from one order and deliver it to another without returning to a depot, or sorting station, to minimize delivery time. These related orders can be assigned to the same route with the appropriate sequence by using order pairs. Moreover, restrictions on how long the package can stay in the vehicle can also be assigned; for example, the package might be a blood sample that has to be transported from the doctor's office to the lab within two hours. The order pairs record set has associated attributes. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. FirstOrderName: The name of the first order of the pair. This field is a foreign key to the Name field in the Orders feature layer. SecondOrderName: The name of the second order of the pair. This field is a foreign key to the Name field in the Orders feature layer. The first order in the pair must be a pickup order; that is, the value for its DeliveryQuantities field is null. The second order in the pair must be a delivery order; that is, the value for its PickupQuantities field is null. The quantity that is picked up at the first order must agree with the quantity that is delivered at the second order. As a special case, both orders may have zero quantities for scenarios where capacities are not used. The order quantities are not loaded or unloaded at depots. MaxTransitTime: The maximum transit time for the pair. The transit time is the duration from the departure time of the first order to the arrival time at the second order. This constraint limits the time-on-vehicle, or ride time, between the two orders. When a vehicle is carrying people or perishable goods, the ride time is typically shorter than that of a vehicle carrying packages or nonperishable goods. This field can contain null values; a null value indicates that there is no constraint on the ride time. The unit for this field value is specified by the Time Field Units property of the analysis layer. Excess transit time (measured with respect to the direct travel time between order pairs) can be tracked and weighted by the solver. Because of this, you can direct the VRP solver to take one of three approaches: (1) minimize the overall excess transit time, regardless of the increase in travel cost for the fleet; (2) find a solution that balances overall violation time and travel cost; and (3) ignore the overall excess transit time and, instead, minimize the travel cost for the fleet. By assigning an importance level for the Excess Transit Time Importance parameter ( excess_transit_factor for Python), you are in effect choosing one of these three approaches. Regardless of the importance level, the solver will always return an error if the MaxTransitTime value is surpassed. ", "dataType": "Record Set"}, {"name": "excess_transit_factor", "isOptional": true, "description": "Rates the importance of reducing excess transit time of order pairs. Excess transit time is the amount of time exceeding the time required to travel directly between the paired orders. Excess time can be caused by driver breaks or travel to intermediate orders and depots. Listed below are the three values you can choose from. Low \u2014 The solver tries to find a solution that minimizes overall solution cost, regardless of excess transit time. This setting is commonly used with courier services. Since couriers transport packages as opposed to people, they don't need to worry about ride time. Using Low allows the couriers to service paired orders in the proper sequence and minimize the overall solution cost. Medium \u2014 This is the default setting. The solver looks for a balance between reducing excess transit time and reducing the overall solution cost. High \u2014 The solver tries to find a solution with the least excess transit time between paired orders at the expense of increasing the overall travel costs. It makes sense to use this setting if you are transporting people between paired orders and you want to shorten their ride time. This is characteristic of taxi services.", "dataType": "String"}, {"name": "point_barriers", "isOptional": true, "description": " Specifies point barriers, which are split into two types: restriction and added cost point barriers. They temporarily restrict traversal across or add impedance to points on the network. The point barriers are defined by a feature set, and the attribute values you specify for the point features determine whether they are restriction or added cost barriers. The fields in the attribute table are listed and described below. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the barrier. BarrierType: Specifies whether the barrier restricts travel completely or adds cost when traveling through it. There are two options: Additional_Time: If BarrierType is set to added cost, the value of the Additional_Time field indicates how much time is added to a route when the route passes through the barrier. The unit for this field value is specified by the Time Field Units property of the analysis layer. Additional_Distance: If BarrierType is set to added cost, the value of the Additional_Distance field indicates how much impedance is added to a route when the route passes through the barrier. The unit for this field value is specified by the Distance Field Units parameter. Restriction (0)\u2014Prohibits traversing through the barrier. This is the default value. Added Cost (2)\u2014Traversing through the barrier increases the network cost by the amount specified in the Additional_Time and AdditionalDistance fields.", "dataType": "Feature Set"}, {"name": "line_barriers", "isOptional": true, "description": " Specifies line barriers, which temporarily restrict traversal across them. The line barriers are defined by a feature set. The fields in the attribute table are listed and described below. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the barrier. ", "dataType": "Feature Set"}, {"name": "polygon_barriers", "isOptional": true, "description": " Specifies polygon barriers, which are split into two types: restriction and scaled cost polygon barriers. They temporarily restrict traversal or scale impedance on the parts of the network they cover. The polygon barriers are defined by a feature set, and the attribute values you specify for the polygon features determine whether they are restriction or scaled cost barriers. The fields in the attribute table are listed and described below. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the barrier. BarrierType: Specifies whether the barrier restricts travel completely or scales the cost of traveling through it. There are two options: Scaled_Time: The time-based impedance values of the edges underlying the barrier are multiplied by the value set in this field. This field is only relevant when the barrier is a scaled cost barrier. Scaled_Distance: The distance-based impedance values of the edges underlying the barrier are multiplied by the value set in this field. This field is only relevant when the barrier is a scaled cost barrier. Restriction (0)\u2014Prohibits traversing through any part of the barrier. This is the default value. Scaled Cost (1)\u2014Scales the impedance of underlying edges by multiplying them by the value of the Attr_[Impedance] property. If edges are partially covered by the barrier, the impedance is apportioned and multiplied.", "dataType": "Feature Set"}, {"name": "time_attribute", "isOptional": true, "description": "Defines the network cost attribute to use when determining the travel time of network elements. ", "dataType": "String"}, {"name": "distance_attribute", "isOptional": true, "description": "Defines the network cost attribute to use when determining the distance of network elements. ", "dataType": "String"}, {"name": "use_hierarchy_in_analysis", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields an exact route for the network dataset.", "dataType": "Boolean"}, {"name": "restrictions", "isOptional": true, "description": " Indicates which network restriction attributes are respected during solve time. ", "dataType": "String"}, {"name": "attribute_parameter_values", "isOptional": true, "description": "Specifies the parameter values for network attributes that have parameters. The record set has two columns that work together to uniquely identify parameters and another column that specifies the parameter value. The attribute parameter values record set has associated attributes. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. AttributeName: The name of the network attribute whose attribute parameter is set by the table row. ParameterName: The name of the attribute parameter whose value is set by the table row. (Object type parameters cannot be updated using this tool.) ParameterValue: The value you want for the attribute parameter. If a value is not specified, the attribute parameter is set to null. ", "dataType": "Record Set"}, {"name": "maximum_snap_tolerance", "isOptional": true, "description": "The maximum snap tolerance is the furthest distance that Network Analyst searches when locating or relocating a point onto the network. The search looks for suitable edges or junctions and snaps the point to the nearest one. If a suitable location isn't found within the maximum snap tolerance, the object is marked as unlocated. ", "dataType": "Linear unit"}, {"name": "exclude_restricted_portions_of_the_network", "isOptional": true, "description": " EXCLUDE \u2014 Specifies that the network locations are only placed on traversable portions of the network. This prevents placing network locations on elements that you can't reach due to restrictions or barriers. Before adding your network locations using this option, make sure that you have already added all the restriction barriers to the input network analysis layer to get expected results. This parameter is not applicable when adding barrier objects. In such cases, use \"#\" as the parameter value. INCLUDE \u2014 Specifies that the network locations are placed on all the elements of the network. The network locations that are added with this option may be unreachable during the solve process if they are placed on restricted elements.", "dataType": "Boolean"}, {"name": "feature_locator_where_clause", "isOptional": false, "description": "An SQL expression used to select a subset of source features that limits which network elements orders and depots can be located on. For example, to ensure orders and depots are not located on limited-access highways, write an SQL expression that excludes those source features. (Note that the other network analysis objects, such as barriers, ignore the feature locator WHERE clause when loading.) For more information on SQL syntax and how it differs between data sources, see SQL reference for query expressions used in ArcGIS . ", "dataType": "Value Table"}, {"name": "populate_route_lines", "isOptional": true, "description": " NO_ROUTE_LINES \u2014 No shape is generated for the output routes. You won't be able to generate driving directions if route lines aren't created. ROUTE_LINES \u2014 The output routes will have the exact shape of the underlying network sources.", "dataType": "Boolean"}, {"name": "route_line_simplification_tolerance", "isOptional": true, "description": " Specify by how much you want to simplify the route geometry. Simplification maintains critical points on a route, such as turns at intersections, to define the essential shape of the route and removes other points. The simplification distance you specify is the maximum allowable offset that the simplified line can deviate from from the original line. Simplifying a line reduces the number of vertices and tends to reduce drawing times. ", "dataType": "Linear unit"}, {"name": "populate_directions", "isOptional": true, "description": " DIRECTIONS \u2014 The feature class specified in the Output Directions Name parameter is populated with turn-by-turn instructions for each route. The network dataset must support driving directions; otherwise, an error occurs when solving with directions. NO_DIRECTIONS \u2014 Directions aren't generated.", "dataType": "Boolean"}, {"name": "directions_language", "isOptional": true, "description": "Choose a language to generate driving directions in. The languages that are available in the drop-down list depend on what ArcGIS language packs are installed on your computer. Note that if you are going to publish this tool as part of a service on a separate server, the ArcGIS language pack that corresponds with the language you choose must be installed on that server for the tool to function properly. Also, if a language pack isn't installed on your computer, the language won't appear in the drop-down list; however, you can type a language code instead. ", "dataType": "String"}, {"name": "directions_style_name", "isOptional": true, "description": " The name of the formatting style of directions. ", "dataType": "String"}, {"name": "save_output_layer", "isOptional": true, "description": "Choose whether the output includes a network analysis layer of the results. In either case, stand-alone tables and feature classes are returned. However, a server administrator may want to choose to output a network analysis layer as well so that the setup and results of the tool can be debugged using the Network Analyst controls in the ArcGIS for Desktop environment. This can make the debugging process much easier. In ArcGIS for Desktop , the default output location for the network analysis layer is in the scratch workspace, at the same level as the scratch geodatabase. That is, it is stored as a sibling of the scratch geodatabase. The output network analysis layer is stored as an LYR file whose name starts with _ags_gpna and is followed by an alphanumeric GUID. In either case, stand-alone tables and feature classes are returned. However, a server administrator may want to choose to output a network analysis layer as well so that the setup and results of the tool can be debugged using the Network Analyst controls in the ArcGIS for Desktop environment. This can make the debugging process much easier. In ArcGIS for Desktop , the default output location for the network analysis layer is in the scratch workspace, at the same level as the scratch geodatabase. That is, it is stored as a sibling of the scratch geodatabase. The output network analysis layer is stored as an LYR file whose name starts with _ags_gpna and is followed by an alphanumeric GUID. NO_SAVE_OUTPUT_LAYER \u2014 A network analysis layer isn't included in the output. SAVE_OUTPUT_LAYER \u2014 The output includes a network analysis layer of the results. ", "dataType": "Boolean"}, {"name": "service_capabilities", "isOptional": false, "description": " This property helps you govern the maximum amount of computer processing that occurs when running this tool as a geoprocessing service. You might want to do this for one of two reasons: one, to avoid letting your server solve problems that require more resources or processing time than you want to allow; two, to create multiple services with different VRP capabilities to support a business model. For example, if you have a tiered-service business model, you might want to provide a free VRP service that supports a maximum of five routes per solve and another service that is fee-based and supports more than five routes per solve. Along with limiting the maximum number of routes, you can limit how many orders or point barriers can be added to the analysis. Another way to govern problem sizes is by setting a maximum number of features\u2014usually street features\u2014that line or polygon barriers can intersect. The last method is to force a hierarchical solve, even if the user chooses not to use a hierarchy, when orders are geographically dispersed beyond a given straight-line distance. MAXIMUM POINT BARRIERS \u2014 The maximum number of point barriers allowed. An error is returned if this limit is exceeded. A null value indicates there is no limit. MAXIMUM FEATURE INTERSECTING LINE BARRIERS \u2014 The maximum number of source features that can be intersected by all line barriers in the analysis. An error is returned if this limit is exceeded. A null value indicates there is no limit. MAXIMUM FEATURES INTERSECTING POLYGON BARRIERS \u2014 The maximum number of source features that can be intersected by all polygon barriers in the analysis. An error is returned if this limit is exceeded. A null value indicates there is no limit. MAXIMUM ORDERS \u2014 The maximum number of orders allowed in the analysis. An error is returned if this limit is exceeded. A null value indicates there is no limit. MAXIMUM ROUTES \u2014 The maximum number of routes allowed in the analysis. An error is returned if this limit is exceeded. A null value indicates there is no limit. FORCE HIERARCHY BEYOND DISTANCE \u2014 The maximum straight-line distance between orders before the vehicle routing problem is solved using the network's hierarchy. The units for this value are the same as those specified in the Distance Field Units parameter.If the network doesn't have a hierarchy attribute, this constraint is ignored. If Use Hierarchy in Analysis is checked, hierarchy is always used. If the Use Hierarchy in Analysis parameter is unchecked and this constraint has a null value, hierarchy is not forced.", "dataType": "Value Table"}]},
{"syntax": "GenerateServiceAreas_na (Facilities, Break_Values, Break_Units, Network_Dataset, Service_Areas, {Travel_Direction}, {Time_of_Day}, {UTurn_Policy}, {Point_Barriers}, {Line_Barriers}, {Polygon_Barriers}, {Time_Attribute}, {Time_Attribute_Units}, {Distance_Attribute}, {Distance_Attribute_Units}, {Use_Hierarchy_in_Analysis}, {Restrictions}, {Attribute_Parameter_Values}, {Maximum_Snap_Tolerance}, {Exclude_Restricted_Portions_of_the_Network}, {Feature_Locator_WHERE_Clause}, {Polygons_for_Multiple_Facilities}, {Polygon_Overlap_Type}, {Detailed_Polygons}, {Polygon_Trim_Distance}, {Polygon_Simplification_Tolerance}, {Maximum_Facilities}, {Maximum_Number_of_Breaks}, {Maximum_Features_Affected_by_Point_Barriers}, {Maximum_Features_Affected_by_Line_Barriers}, {Maximum_Features_Affected_by_Polygon_Barriers}, {Maximum_Break_Time_Value}, {Maximum_Break_Distance_Value}, {Force_Hierarchy_beyond_Break_Time_Value}, {Force_Hierarchy_beyond_Break_Distance_Value})", "name": "Generate Service Areas (Network Analyst)", "description": "Creates a service area network analysis layer, sets the analysis properties, and solves the analysis. This tool is ideal for setting up a service area geoprocessing service on the web. A network service area is a region that encompasses all streets\r\nthat can be accessed within a given distance or travel time from one or more facilities.  Service areas are commonly used to visualize and measure accessibility. For example, a three-minute drive-time polygon around a grocery store can determine which residents are able to reach the store within three minutes and are thus more likely to shop there.", "example": {"title": "GenerateServiceAreas example 1 (Python window)", "description": "Execute the tool using the required parameters from the Python window.", "code": "import arcpy facilities = arcpy.FeatureSet () facilities.load ( \"FireStations\" ) arcpy.na.GenerateServiceAreas ( facilities , \"1 2 3\" , \"Minutes\" , \"Streets_ND\" , \"in_memory \\\\ FireStationServiceAreas\" )"}, "usage": ["The tool dialog box groups the various optional parameters into the following six categories to make it easier for you to manage them:", "Learn about the output from Generate Service Areas"], "parameters": [{"name": "Facilities", "isOptional": false, "description": " The facilities around which service areas are generated. The facilities feature set has three attributes: ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the facility. If the name is empty, blank, or null, a name is automatically generated at solve time. ", "dataType": "Feature Set"}, {"name": "Break_Values", "isOptional": false, "description": " Specifies the size and number of service area polygons to generate for each facility. The units are determined by the Break Units value. When the Generate Service Areas tool runs, a noteworthy interaction occurs among the following parameters: Break Values , Break Units , and either Time Attribute or Distance Attribute . Together, Break Values and Break Units define how far or how long the service area should extend around the facility or facilities. The Time Attribute and Distance Attribute parameters each define one network cost attribute. Only one of these two cost attributes is used, however, and the one that the solver chooses to use corresponds with the Break Units value. That is, when you specify a time-based Break Unit value, like seconds or minutes, the tool solves using the cost attribute defined in the Time Attribute parameter. When you specify a distance-based Break Unit value, like kilometers or miles, it uses the cost attribute defined in the Distance Attribute parameter. Multiple polygon breaks can be set to create concentric service areas per facility. For instance, to find 2-, 3-, and 5-mile service areas for each facility, enter \"2 3 5\", separating the values with a space. Set Break Units to Miles, and ensure that you have chosen a distance-based network attribute for the Distance Attribute parameter. ", "dataType": "String"}, {"name": "Break_Units", "isOptional": false, "description": "The units for the Break Values parameter. The Generate Service Areas tool chooses whether to use the network cost attribute specified in the Time Attribute or Distance Attribute parameter depending on whether the units you specify here are time or distance based. The tool performs the necessary units conversion when the Break Units value differs from the units of the corresponding time or distance cost attribute. Minutes Hours Days Seconds Miles Kilometers Meters Feet NauticalMiles Yards", "dataType": "String"}, {"name": "Network_Dataset", "isOptional": false, "description": " The network dataset on which the service area analysis will be performed. The network dataset needs at least one time-based and one distance-based cost attribute. ", "dataType": "Network Dataset Layer"}, {"name": "Service_Areas", "isOptional": false, "description": "The output workspace and name of the output features. This workspace must already exist. The default output workspace is in_memory. ", "dataType": "Feature Class"}, {"name": "Travel_Direction", "isOptional": true, "description": "Choose whether the direction of travel used to generate the service area polygons is toward or away from the facilities. The direction of travel may change the shape of the polygons because impedances on opposite sides of a street may differ, or one-way streets may exist. The direction you should choose depends on the nature of your service area analysis. The service area for a pizza delivery store, for example, should be created away from the facility, whereas the service area of a hospital should be created toward the facility since the critical travel time for a patient is traveling to the hospital. TRAVEL_FROM \u2014 The service area is generated in the direction away from the facilities. TRAVEL_TO \u2014 The service area is created in the direction towards the facilities.", "dataType": "String"}, {"name": "Time_of_Day", "isOptional": true, "description": " The time to depart from or arrive at the facilities. The interpretation of this value depends on whether travel is toward or away from the facilities. Your network dataset must include traffic data for this parameter to have any effect. Repeatedly solving the same analysis, but using different Time of Day values, allows you to see how a facility's reach changes over time. For instance, the five-minute service area around a fire station may start out large in the early morning, diminish during the morning rush hour, grow in the late morning, and so on, throughout the day. It represents the departure time if Travel Direction is set to TRAVEL_FROM. It represents the arrival time if Travel Direction is set to TRAVEL_TO.", "dataType": "Date"}, {"name": "UTurn_Policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "Point_Barriers", "isOptional": true, "description": " Specifies point barriers, which are split into two types: restriction and added cost point barriers. They temporarily restrict traversal across or add impedance to points on the network. The point barriers are defined by a feature set, and the attribute values you specify for the point features determine whether they are restriction or added cost barriers. The fields in the attribute table are listed and described below. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the barrier. BarrierType: Specifies whether the barrier restricts travel completely or adds cost when traveling through it. There are two options: Use the value 0 for Restriction and 2 for Added Cost. AdditionalCost: AdditionalCost indicates how much impedance is added when a service area passes through the barrier. The unit for this field value is the same as the units specified for Break Units . Restriction (0)\u2014Prohibits traversing through the barrier. This is the default value. Added Cost (2)\u2014Traversing through the barrier increases the network cost by the amount specified in the Additional_Time and AdditionalDistance fields.", "dataType": "Feature Set"}, {"name": "Line_Barriers", "isOptional": true, "description": " Specifies line barriers, which temporarily restrict traversal across them. The line barriers are defined by a feature set. The fields in the attribute table are listed and described below. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the barrier. ", "dataType": "Feature Set"}, {"name": "Polygon_Barriers", "isOptional": true, "description": " Specifies polygon barriers, which are split into two types: restriction and scaled cost polygon barriers. They temporarily restrict traversal or scale impedance on the parts of the network they cover. The polygon barriers are defined by a feature set, and the attribute values you specify for the polygon features determine whether they are restriction or scaled cost barriers. The fields in the attribute table are listed and described below. ObjectID: The system-managed ID field. Shape: The geometry field indicating the geographic location of the network analysis object. Name: The name of the barrier. BarrierType: Specifies whether the barrier restricts travel completely or scales the cost of traveling through it. There are two options: Use the value 0 for Restriction and 1 for Scaled Cost. ScaledCostFactor: ScaledCostFactor indicates how much the impedance is multiplied by when a service area passes through the barrier. Restriction (0)\u2014Prohibits traversing through any part of the barrier. This is the default value. Scaled Cost (1)\u2014Scales the impedance of underlying edges by multiplying them by the value of the ScaledCostFactor property. If edges are partially covered by the barrier, the impedance is apportioned and multiplied.", "dataType": "Feature Set"}, {"name": "Time_Attribute", "isOptional": true, "description": "Defines the network cost attribute to use when the Break Units value is a time unit. The tool performs the necessary time-unit conversion when the Break Units value differs from the units of the cost attribute defined here. In other words, the time units of the breaks and the network cost attribute don't need to be the same. ", "dataType": "String"}, {"name": "Time_Attribute_Units", "isOptional": true, "description": "The units of the network cost attribute specified by the Time Attribute parameter. This is merely an informational parameter that cannot be changed without directly editing the network dataset. It is also unnecessary to change since the unit conversions between break value units and the cost attribute are handled for you. ", "dataType": "String"}, {"name": "Distance_Attribute", "isOptional": true, "description": "Defines the network cost attribute to use when the Break Units value is a distance unit. The tool performs the necessary distance-unit conversion when the Break Units value differs from the units of the cost attribute defined here. In other words, the distance units of the breaks and the network cost attribute don't need to be the same. ", "dataType": "String"}, {"name": "Distance_Attribute_Units", "isOptional": true, "description": "The units of the network cost attribute specified by the Distance Attribute parameter. This is merely an informational parameter that cannot be changed without directly editing the network dataset. It is also unnecessary to change since the unit conversions between break value units and the cost attribute are handled for you. ", "dataType": "String"}, {"name": "Use_Hierarchy_in_Analysis", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields a service area measured along all edges of the network dataset regardless of hierarchy level.", "dataType": "Boolean"}, {"name": "Restrictions", "isOptional": true, "description": " Indicates which network restriction attributes are respected during solve time. ", "dataType": "String"}, {"name": "Attribute_Parameter_Values", "isOptional": true, "description": "Specifies the parameter values for network attributes that have parameters. The record set has two columns that work together to uniquely identify parameters and another column that specifies the parameter value. The attribute parameter values record set has associated attributes. The fields in the attribute table are listed below and described. ObjectID: The system-managed ID field. AttributeName: The name of the network attribute whose attribute parameter is set by the table row. ParameterName: The name of the attribute parameter whose value is set by the table row. (Object type parameters cannot be updated using this tool.) ParameterValue: The value you want for the attribute parameter. If a value is not specified, the attribute parameter is set to null. ", "dataType": "Record Set"}, {"name": "Maximum_Snap_Tolerance", "isOptional": true, "description": "The maximum snap tolerance is the furthest distance that Network Analyst searches when locating or relocating a point onto the network. The search looks for suitable edges or junctions and snaps the point to the nearest one. If a suitable location isn't found within the maximum snap tolerance, the object is marked as unlocated. ", "dataType": "Linear unit"}, {"name": "Exclude_Restricted_Portions_of_the_Network", "isOptional": true, "description": " EXCLUDE \u2014 Facilities are only located on traversable portions of the network. This prevents locating them on elements that can't be reached during the solve process due to restrictions or barriers. Bear in mind that facilities may be located farther from their intended location than if this option were left unchecked. INCLUDE \u2014 Facilities can be located on any of the elements of the network; however, the facilities that are located on restricted elements cannot be used during the solve process.", "dataType": "Boolean"}, {"name": "Feature_Locator_WHERE_Clause", "isOptional": true, "description": "An SQL expression used to select a subset of source features that limits which network elements facilities can be located on. The syntax for this parameter consists of two parts: the first is the source feature class name (followed by a space), and the second is the SQL expression. To write an SQL expression for two or more source feature classes, separate them with a semicolon. To ensure facilities are not located on limited-access highways, for example, write an SQL expression like the following to exclude those source features: \"Streets\" \"FUNC_CLASS not in('1', '2')\" . Note that barriers ignore the feature locator WHERE clause when loading. ", "dataType": "String"}, {"name": "Polygons_for_Multiple_Facilities", "isOptional": true, "description": "Choose how service area polygons are generated when multiple facilities are present in the analysis. NO_MERGE \u2014 Creates individual polygons for each facility. The polygons can overlap each other. NO_OVERLAP \u2014 Creates individual polygons such that a polygon from one facility cannot overlap polygons from other facilities; furthermore, any portion of the network can only be covered by the service area of the nearest facility. MERGE \u2014 Creates and joins the polygons of different facilities that have the same break value. ", "dataType": "String"}, {"name": "Polygon_Overlap_Type", "isOptional": true, "description": "Specifies the option to create concentric service area polygons as disks or rings. This option is applicable only when multiple break values are specified for the facilities. RINGS \u2014 Does not include the area of the smaller breaks. This creates polygons going between consecutive breaks. Use this option if you want to find the area from one break to another. For instance, If you create 5- and 10-minute service areas, then the 10-minute service area polygon will exclude the area under the 5-minute service area polygon. DISKS \u2014 Creates polygons going from the facility to the break. For instance, If you create 5- and 10-minute service areas, then the 10-minute service area polygon will include the area under the 5-minute service area polygon.", "dataType": "String"}, {"name": "Detailed_Polygons", "isOptional": true, "description": "Specifies the option to create detailed or generalized polygons. SIMPLE_POLYS \u2014 Creates generalized polygons, which are generated quickly and are fairly accurate. This is the default. DETAILED_POLYS \u2014 Creates detailed polygons, which accurately model the service area lines and may contain islands of unreached areas. This option is much slower than generating generalized polygons. This option isn't supported when using hierarchy.", "dataType": "Boolean"}, {"name": "Polygon_Trim_Distance", "isOptional": true, "description": " Specifies the distance within which the service area polygons are trimmed. This is useful when your data is very sparse and you don't want the service area to cover large areas where there are no features. No value or a value of 0 for this parameter specifies that the service area polygons should not be trimmed. The parameter value is ignored when using hierarchy. ", "dataType": "Linear unit"}, {"name": "Polygon_Simplification_Tolerance", "isOptional": true, "description": " Specify by how much you want to simplify the polygon geometry. Simplification maintains critical points of a polygon to define its essential shape and removes other points. The simplification distance you specify is the maximum allowable offset from the original polygon the simplified polygon can deviate from. Simplifying a polygon reduces the number of vertices and tends to reduce drawing times. ", "dataType": "Linear unit"}, {"name": "Maximum_Facilities", "isOptional": true, "description": " Limits how many facilities can be added to the service area analysis. This parameter helps you govern the amount of processing that occurs when solving. For example, you could assign a low value to this parameter for a free version of the service you are creating and use a higher value for a paid-subscription version of the service. A null value indicates there is no limit. ", "dataType": "Long"}, {"name": "Maximum_Number_of_Breaks", "isOptional": true, "description": " Limits how many breaks can be added to the service area analysis. This parameter helps you govern the amount of processing that occurs when solving. For example, you could assign a low value to this parameter for a free version of the service you are creating and use a higher value for a paid-subscription version of the service. A null value indicates there is no limit. ", "dataType": "Long"}, {"name": "Maximum_Features_Affected_by_Point_Barriers", "isOptional": true, "description": " Limits how many features can be affected by point barriers. This parameter helps you govern the amount of processing that occurs when solving. For example, you could assign a low value to this parameter for a free version of the service you are creating and use a higher value for a paid-subscription version of the service. A null value indicates there is no limit. ", "dataType": "Long"}, {"name": "Maximum_Features_Affected_by_Line_Barriers", "isOptional": true, "description": " Limits how many features can be affected by line barriers. This parameter helps you govern the amount of processing that occurs when solving. For example, you could assign a low value to this parameter for a free version of the service you are creating and use a higher value for a paid-subscription version of the service. A null value indicates there is no limit. ", "dataType": "Long"}, {"name": "Maximum_Features_Affected_by_Polygon_Barriers", "isOptional": true, "description": " Limits how many features can be affected by polygon barriers. This parameter helps you govern the amount of processing that occurs when solving. For example, you could assign a low value to this parameter for a free version of the service you are creating and use a higher value for a paid-subscription version of the service. A null value indicates there is no limit. ", "dataType": "Long"}, {"name": "Maximum_Break_Time_Value", "isOptional": true, "description": "Limits how large the value of the Break Value parameter can be when solving time-based service areas. This parameter helps you govern the amount of processing that occurs when solving. For example, you could assign a low value to this parameter for a free version of the service you are creating and use a higher value for a paid-subscription version of the service. A null value indicates there is no limit. ", "dataType": "Double"}, {"name": "Maximum_Break_Distance_Value", "isOptional": true, "description": "Limits how large the value of the Break Value parameter can be when solving distance-based service areas. This parameter helps you govern the amount of processing that occurs when solving. For example, you could assign a low value to this parameter for a free version of the service you are creating and use a higher value for a paid-subscription version of the service. A null value indicates there is no limit. ", "dataType": "Double"}, {"name": "Force_Hierarchy_beyond_Break_Time_Value", "isOptional": true, "description": " Specifies the break value after which the solver will force hierarchy even if hierarchy was not enabled when solving time-based service areas. Solving service areas for high break values while using the network's hierarchy tends to incur much less processing than solving the same service areas without using the hierarchy. This parameter helps you govern the amount of processing that occurs when solving. A null value indicates that the hierarchy will never be enforced and the value of the Use Hierarchy in Analysis parameter will always be honored. If the input network dataset does not support hierarchy, specifying a value for this parameter will result in an error. A null value should be used in this case. ", "dataType": "Double"}, {"name": "Force_Hierarchy_beyond_Break_Distance_Value", "isOptional": true, "description": " Specifies the break value after which the solver will force hierarchy even if hierarchy was not enabled when solving distance-based service areas. Solving service areas for high break values while using the network's hierarchy tends to incur much less processing than solving the same service areas without using the hierarchy. This parameter helps you govern the amount of processing that occurs when solving. A null value indicates that the hierarchy will never be enforced and the value of the Use Hierarchy in Analysis parameter will always be honored. If the input network dataset does not support hierarchy, specifying a value for this parameter will result in an error. A null value should be used in this case. ", "dataType": "Double"}]},
{"syntax": "UpdateByGeometry_na (in_turn_features)", "name": "Update by Geometry (Network Analyst)", "description": "Updates all the edge references in the turn feature class using the geometry of the turn features. This tool is useful when the IDs listed for the turn can no longer find the edges participating in the turn due to edits to the underlying edges.", "example": {"title": "UpdateByGeometry example 1 (Python window)", "description": "Execute the tool using all parameters. ", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.UpdateByGeometry ( \"Transportation/RestrictedTurns\" )"}, "usage": ["The tool updates the Edge#FID field values on the turn feature class based on the coincidence between the turn features and the edge features from the network sources.", "Errors encountered when updating the turn features are reported in an error file written to the directory defined by the TEMP system variable. The full path name to the error file is reported as a warning message."], "parameters": [{"name": "in_turn_features", "isInputFile": true, "isOptional": false, "description": "The turn feature class to be updated. ", "dataType": "Feature Layer"}]},
{"syntax": "UpdateByAlternateIDFields_na (in_network_dataset, alternate_ID_field_name)", "name": "Update by Alternate ID Fields (Network Analyst)", "description": " Updates all the edge references in turn feature classes using an alternate ID field. This tool should be used after making edits to the input line features that are referenced by the turn features to synchronize the turn features based on the alternate ID fields. ", "example": {"title": "UpdateByAlternateIDFields example 1 (Python window)", "description": "Execute the tool using all parameters. ", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.UpdateByAlternateIDFields ( \"Transportation/Streets_ND\" , \"ID\" )"}, "usage": ["This tool updates the Edge#FID field values on the turn feature class based on the alternate IDs stored on each turn feature. If the turn feature classes do not reference the edges based on alternate IDs, use the ", "Populate Alternate ID Fields", " tool first to create and populate alternate ID fields. "], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset whose turn feature classes are being updated by their alternate ID fields. ", "dataType": "Network Dataset Layer"}, {"name": "alternate_ID_field_name", "isOptional": false, "description": "The name of the alternate ID field on the edge feature sources of the network dataset. All edge feature sources referenced by turns must have the same name for the alternate ID field. ", "dataType": "String"}]},
{"syntax": "TurnTableToTurnFeatureClass_na (in_turn_table, reference_line_features, out_feature_class_name, {reference_nodes_table}, {maximum_edges}, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})", "name": "Turn Table To Turn Feature Class (Network Analyst)", "description": "Converts an ArcView turn table or  ArcInfo Workstation  coverage turn table to an ArcGIS turn feature class.", "example": {"title": "TurnTableToTurnFeatureClass example 1 (Python window)", "description": "Convert an ", "code": "import arcpy arcpy.na.TurnTableToTurnFeatureClass ( \"C:/data/delayturns.trn\" , \"C:/data/SoCal.gdb/Transportation/Streets\" , \"DelayTurns\" )"}, "usage": ["The turn feature class to be created is placed in the same workspace as the reference line feature class.", "The coordinates in the output turn feature class will have elevation (Z) values if the reference line feature class supports Z values."], "parameters": [{"name": "in_turn_table", "isInputFile": true, "isOptional": false, "description": "The .dbf or INFO turn table from which the new turn feature class is being created. INFO tables do not support mixed case path names on Linux and Solaris. ", "dataType": "Table View"}, {"name": "reference_line_features", "isOptional": false, "description": "The line feature class to which the input turn table refers. The feature class must be a source in a network dataset. ", "dataType": "Feature Class"}, {"name": "out_feature_class_name", "isOutputFile": true, "isOptional": false, "description": "The name of the new turn feature class to create. ", "dataType": "String"}, {"name": "reference_nodes_table", "isOptional": true, "description": "The nodes.dbf table in the .nws folder containing the original ArcView GIS network in which the input turn table participated. This parameter is ignored if the input turn table is an INFO table. If the input turn table is a .dbf table and this parameter is omitted, then U-turns and turns that traverse between edges connected to each other at both ends will not be created in the output turn feature class. Errors will be reported in an error file written to the directory defined by the TEMP system variable. The full path name to the error file is reported as a warning message. ", "dataType": "dBASE Table"}, {"name": "maximum_edges", "isOptional": true, "description": "The maximum number of edges per turn in the new turn feature class. The default value is 5. The maximum value is 20. ", "dataType": "Long"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the configuration keyword that determines the storage parameters of the output turn feature class. This parameter is used only if the output turn feature class is created in an ArcSDE geodatabase. ", "dataType": "String"}, {"name": "spatial_grid_1", "isOptional": true, "description": "The Spatial Grid 1, 2, and 3 parameters apply only to file geodatabases and certain ArcSDE geodatabase feature classes. If you are unfamiliar with setting grid sizes, leave these options as 0,0,0 and ArcGIS will compute optimal sizes for you. For more information about this parameter, refer to the Add Spatial Index tool documentation. ", "dataType": "Double"}, {"name": "spatial_grid_2", "isOptional": true, "description": " Cell size of the second spatial grid. Leave the size at 0 if you only want one grid. Otherwise, set the size to at least three times larger than Spatial Grid 1. ", "dataType": "Double"}, {"name": "spatial_grid_3", "isOptional": true, "description": " Cell size of the third spatial grid. Leave the size at 0 if you only want two grids. Otherwise, set the size to at least three times larger than Spatial Grid 2. ", "dataType": "Double"}]},
{"syntax": "PopulateAlternateIDFields_na (in_network_dataset, alternate_ID_field_name)", "name": "Populate Alternate ID Fields (Network Analyst)", "description": "Creates and populates additional fields on the turn feature classes that reference the edges by alternate IDs. The alternate IDs allow for another set of IDs that can help maintain the integrity of the turn features in case the source edges are being edited.", "example": {"title": "PopulateAlternateID Fields example 1 (Python window)", "description": "Execute the tool using all parameters. ", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.PopulateAlternateIDFields ( \"Transportation/Streets_ND\" , \"ID\" )"}, "usage": ["The tool creates and populates new fields called AltID<n>, where n is the maximum number of edges per turn. For example, for two-edge turn feature classes, the tool creates and populates fields called AltID1 and AltID2.", "Shapefiles do not have a persistent unique identifier, unlike the geodatabase's ObjectID field. Alternate IDs can avoid the ID shift problem commonly encountered when working with turn feature classes in a shapefile workspace. ", "If the edge feature sources do not have an alternate ID field, for example, a unique identifier provided by a data vendor, then you must create and populate such a field so that it can used by this tool to reference the turn features."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset whose turn feature classes are to receive alternate ID fields. The fields will be created on all of the turn feature classes added as a turn source to the network dataset. ", "dataType": "Network Dataset Layer"}, {"name": "alternate_ID_field_name", "isOptional": false, "description": "The name of the alternate ID field on the edge feature sources of the network dataset. All edge feature sources referenced by turns must have the same name for the alternate ID field. ", "dataType": "String"}]},
{"syntax": "IncreaseMaximumEdges_na (in_turn_features, maximum_edges)", "name": "Increase Maximum Edges (Network Analyst)", "description": "Increases the maximum number of edges per turn in a turn feature class.", "example": {"title": "IncreaseMaximumEdges example 1 (Python window)", "description": "Execute the tool using all parameters. ", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.IncreaseMaximumEdges ( \"Transportation/RestrictedTurns\" , 8 )"}, "usage": ["Once the maximum number of edges is increased, it cannot be decreased later. So only increase by the needed amount.", "Increasing the number of edges by one adds three additional fields to the turn feature class. Take care not to exceed the maximum number of fields allowed by the database being used. For example, a personal geodatabase is limited to 255 fields."], "parameters": [{"name": "in_turn_features", "isInputFile": true, "isOptional": false, "description": "The turn feature class that is having its maximum number of edges raised. ", "dataType": "Feature Layer"}, {"name": "maximum_edges", "isOptional": false, "description": "The new maximum number of edges in the input turn feature class. The value must be at least one higher than the existing maximum number of edges and cannot be greater than 30. ", "dataType": "Long"}]},
{"syntax": "CreateTurnFeatureClass_na (out_location, out_feature_class_name, {maximum_edges}, {in_network_dataset}, {in_template_feature_class}, {spatial_reference}, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3}, {has_z})", "name": "Create Turn Feature Class (Network Analyst)", "description": "Creates a new turn feature class to store turn features that model turning movements in a network dataset.", "example": {"title": "CreateTurnFeatureClass example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.CreateTurnFeatureClass ( \"Transportation\" , \"DelayTurns\" )"}, "usage": ["You can add a turn feature class as a turn sources to your network only if your network dataset supports turns. If you want to add turns to a network that does not support turns, you will have to create a new network dataset that supports turns.", "After creating the turn feature class, the turn features can be created in ArcMap using the commands to create linear features on the ", "Editor", " toolbar."], "parameters": [{"name": "out_location", "isOutputFile": true, "isOptional": false, "description": "The ArcSDE, file, or personal geodatabase, or the folder in which the output turn feature class will be created. This workspace must already exist. ", "dataType": "Workspace;Feature Dataset"}, {"name": "out_feature_class_name", "isOutputFile": true, "isOptional": false, "description": "The name of the turn feature class to be created. ", "dataType": "String"}, {"name": "maximum_edges", "isOptional": true, "description": "The maximum number of edges that turns in the new turn feature class can model. The default value is 5. The maximum value is 30. ", "dataType": "Long"}, {"name": "in_network_dataset", "isInputFile": true, "isOptional": true, "description": "The network dataset in which the turn feature class will participate. The resulting turn feature class will be added as a turn source to the network dataset. If no network dataset is specified, the turn feature class will be created as not participating in a network dataset. ", "dataType": "Network Dataset Layer"}, {"name": "in_template_feature_class", "isInputFile": true, "isOptional": true, "description": "The feature class used as a template to define the attribute schema of the new turn feature class. If the template feature class has the following fields, they are not created on the output turn feature class; NODE_, NODE#, JUNCTION, F_EDGE, T_EDGE, F-EDGE, T-EDGE, ARC1_,ARC2_,ARC1#,ARC2#,ARC1-ID, ARC2-ID, AZIMUTH, ANGLE. ", "dataType": "Feature Layer"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference to be applied to the output turn feature class. This parameter is ignored if the output location is a geodatabase feature dataset as the output turn feature class will inherit the spatial reference of the feature dataset. If you want to import the spatial reference from an existing feature class, specify its path as the parameter value. ", "dataType": "Spatial Reference"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the configuration keyword that determines the storage parameters of the new turn feature class. This parameter is used only if the output location is an ArcSDE geodatabase ", "dataType": "String"}, {"name": "spatial_grid_1", "isOptional": true, "description": "The Spatial Grid 1, 2, and 3 parameters are used to compute a spatial index and only apply to file geodatabases and certain ArcSDE geodatabase feature classes. If you are unfamiliar with setting grid sizes, leave these options as 0,0,0 and ArcGIS will compute optimal sizes for you. Since no features are written by this tool, the spatial index will be in an unbuilt state. The index will be built when features are written to the feature class such as by the Append tool or editing operations. For more information about this parameter, refer to the Add Spatial Index tool documentation. ", "dataType": "Double"}, {"name": "spatial_grid_2", "isOptional": true, "description": " Cell size of the second spatial grid. Leave the size at 0 if you only want one grid. Otherwise, set the size to at least three times larger than Spatial Grid 1. ", "dataType": "Double"}, {"name": "spatial_grid_3", "isOptional": true, "description": " Cell size of the third spatial grid. Leave the size at 0 if you only want two grids. Otherwise, set the size to at least three times larger than Spatial Grid 2. ", "dataType": "Double"}, {"name": "has_z", "isOptional": true, "description": " ENABLED \u2014 The coordinates in the new turn feature class will have elevation (Z) values. This value should be used if the input network dataset is specified and it supports connectivity based on z coordinate values of the network sources. DISABLED \u2014 The coordinates in the new turn feature class will not have elevation (Z) values.", "dataType": "Boolean"}]},
{"syntax": "UpgradeNetwork_na (in_network_dataset)", "name": "Upgrade Network (Network Analyst)", "description": "Upgrades the schema of the network dataset. Upgrading the network dataset allows the network dataset to make use of the new functionality available in the current software release. ", "example": {"title": "UpgradeNetwork example 1 (Python window)", "description": "Execute the tool using all the parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/Data/Socal.gdb\" arcpy.UpgradeNetwork_na ( \"Transportation/Streets_ND\" )"}, "usage": [" Before the network dataset can be upgraded, the geodatabase must be upgraded to the current release using the Upgrade Geodatabase tool. "], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": " The network dataset to be upgraded. The network dataset must be a geodatabase-based network dataset. ", "dataType": "Network Dataset Layer"}]},
{"syntax": "DissolveNetwork_na (in_network_dataset, out_workspace_location)", "name": "Dissolve Network (Network Analyst)", "description": " Creates a network dataset that minimizes the number of line features required to correctly model the input network dataset. The more efficient output network dataset reduces the time required to solve analyses, draw results, and generate driving directions. This tool outputs a new network dataset and source feature classes; the input network dataset and its source features remain unchanged. ", "example": {"title": "DissolveNetwork example 1 (Python window)", "description": "Execute the tool using all parameters. ", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.DissolveNetwork ( \"Transportation/Streets_ND\" , \"C:/Data/DissolvedNet.gdb\" )"}, "usage": ["This tool does not create a built network. Use the ", "Build Network", " tool to build the newly created network dataset prior to performing any analysis on it. ", "The network dataset and feature classes created in the output geodatabase workspace have the same feature dataset name, network dataset name, and feature class names as the input network dataset. The tool fails to execute if any datasets with these names already exist in the output geodatabase workspace. ", "Only fields from the source feature class that are used by the network dataset are transferred to the output line feature class. Examples of fields include:"], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": " The network dataset to be dissolved. The input network dataset must be a file or personal geodatabase network dataset with exactly one edge source. Any number of junction sources and turn sources is allowed. The edge source must have: The input network dataset must be built before it can be used in this tool. End point connectivity policy An elevation policy of either None or Elevation Fields", "dataType": "Network Dataset Layer"}, {"name": "out_workspace_location", "isOutputFile": true, "isOptional": false, "description": " The geodatabase workspace in which to create the dissolved network dataset. The workspace must be an ArcGIS 10 geodatabase or later, and it must be a different geodatabase than the one where the input network dataset resides. ", "dataType": "Workspace"}]},
{"syntax": "BuildNetwork_na (in_network_dataset)", "name": "Build Network (Network Analyst)", "description": "Reconstructs the network connectivity and attribute information of a network dataset. The network dataset needs to be rebuilt after making edits to the attributes or the features of a participating source feature class. After the source features are edited, the tool establishes the network connectivity only in the areas that have been edited to speed up the build process; however, when the network attributes are edited, the entire extent of the network dataset is rebuilt. This may be a slow operation on a large network dataset. ", "example": {"title": "BuildNetwork example 1 (Python window)", "description": "Execute the tool using all parameters. ", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/Paris.gdb\" arcpy.na.BuildNetwork ( \"Transportation/ParisMultimodal_ND\" )"}, "usage": ["An SDC network dataset cannot be built as it is read only."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset to be built. ", "dataType": "Network Dataset Layer"}]},
{"syntax": "UpdateAnalysisLayerAttributeParameter_na (in_network_analysis_layer, parameterized_attribute, attribute_parameter_name, {attribute_parameter_value})", "name": "Update Analysis Layer Attribute Parameter (Network Analyst)", "description": " Updates the network attribute parameter value for a network analysis layer. The tool should be used to update the value of an attribute parameter for a network analysis layer prior to solving with the  Solve  tool.  This ensures that the  solve operation uses the specified value of the attribute parameter to produce appropriate results. ", "example": {"title": "UpdateAnalysisLayerAttributeParameter example 1 (Python window)", "description": "Execute the tool using all the parameters.", "code": "import arcpy arcpy.na.UpdateAnalysisLayerAttributeParameter ( \"Route\" , \"Height Restriction\" , \"Vehicle Height (feet)\" , 12.0 )"}, "usage": ["Parameterized network attributes are used to model some dynamic aspect of an attribute's value. For example, a tunnel with a height restriction of 12 feet can be modeled using a parameter. In this case, the vehicle's height in feet should be specified as the parameter value. This restriction will then evaluate to true if the vehicle is higher than 12 feet. Similarly, a bridge could have a parameter to specify a weight restriction.", "This tool should only be used with network analysis layers having network attributes with parameters defined on them.", "This tool can be used to repeatedly change the value of an existing parameter before solving a network analysis layer.", "New attribute parameters can be created from the network dataset properties dialog box in the ", "Catalog", " window or in ArcCatalog."], "parameters": [{"name": "in_network_analysis_layer", "isInputFile": true, "isOptional": false, "description": "Network analysis layer for which the attribute parameter value will be updated. ", "dataType": "Network Analyst Layer"}, {"name": "parameterized_attribute", "isOptional": false, "description": "The network attribute whose attribute parameter will be updated. ", "dataType": "String"}, {"name": "attribute_parameter_name", "isOptional": false, "description": "The parameter of the network attribute that will be updated. The parameters of type Object cannot be updated using the tool. ", "dataType": "String"}, {"name": "attribute_parameter_value", "isOptional": true, "description": "The value that will be set for the attribute parameter. It can be a string, number, date, or Boolean (True, False). If the value is not specified, then the attribute parameter value is set to Null. If the attribute parameter has a restriction usage type, the value can be specified as a string keyword or a numeric value. The string keyword or the numeric value determines whether the restriction attribute prohibits, avoids, or prefers the network elements it is associated with. Furthermore, the degree to which network elements are avoided or preferred can be defined by choosing HIGH, MEDIUM, or LOW keywords. The following keywords are supported: Numeric values that are greater than one cause restricted elements to be avoided; the larger the number, the more the elements are avoided. Numeric values between zero and one cause restricted elements to be preferred; the smaller the number, the more restricted elements are preferred. Negative numbers prohibit restricted elements. If the parameter value holds an array, separate the items in the array with the localized separator character. For example, in the U.S., you would most likely use the comma character to separate the items. So representing an array of three numbers might look like the following: \"5,10,15\" . PROHIBITED AVOID_HIGH AVOID_MEDIUM AVOID_LOW PREFER_LOW PREFER_MEDIUM PREFER_HIGH", "dataType": "String"}]},
{"syntax": "Solve_na (in_network_analysis_layer, {ignore_invalids}, {terminate_on_solve_error}, {simplification_tolerance})", "name": "Solve (Network Analyst)", "description": "Solves the network analysis layer problem based on its network locations and properties. ", "example": {"title": "Solve example 1 (Python window)", "description": "Execute the tool using all the parameters.", "code": "import arcpy arcpy.na.Solve ( \"Route\" , \"HALT\" , \"TERMINATE\" , \"10 Meters\" )"}, "usage": ["When the solve fails, the warning and error messages provide useful information about the reasons for the failure.", "Be sure to specify all the parameters on the network analysis layer that are necessary to solve the problem before running this tool."], "parameters": [{"name": "in_network_analysis_layer", "isInputFile": true, "isOptional": false, "description": "The network analysis layer on which the analysis will be computed. ", "dataType": "Network Analyst Layer"}, {"name": "ignore_invalids", "isOptional": true, "description": "For vehicle routing problem network analysis layer, use HALT as the parameter value since the vehicle routing problem solver requires all the network locations to be valid. SKIP \u2014 The solver will skip over network locations that are unlocated and solve the analysis layer from valid network locations only. It will also continue solving if locations are on nontraversable elements or have other errors. This is useful if you know your network locations are not all correct, but you want to solve with the network locations that are valid. HALT \u2014 Do not solve if there are invalid locations. You can then correct these and re-run the analysis. ", "dataType": "Boolean"}, {"name": "terminate_on_solve_error", "isOptional": true, "description": " TERMINATE \u2014 The tool will fail to execute when the solver encounters an error. This is the default. When you use this option, the result object is not created when the tool fails to execute due to a solver error. You should obtain the geoprocessing messages from the ArcPy object. CONTINUE \u2014 The tool will not fail and continue execution even when the solver encounters an error. All of the error messages returned by the solver will be converted to warning messages. When you use this option, the result object is always created and the maxSeverity property of the result object is set to 1 even when the solver encounters an error. Use the getOutput method of the result object with an index value of 1 to determine if the solve was successful. ", "dataType": "Boolean"}, {"name": "simplification_tolerance", "isOptional": true, "description": " The tolerance that determines the degree of simplification for the output geometry. If a tolerance is specified, it must be greater than zero. You can choose a preferred unit; the default unit is decimal degrees. Specifying a simplification tolerance tends to reduce the time it takes to render routes or service areas. The drawback, however, is that simplifying geometry removes vertices, which may lessen the spatial accuracy of the output at larger scales. Because a line with only two vertices cannot be simplified any further, this parameter has no effect on drawing times for single-segment output, such as straight-line routes, OD cost matrix lines, and location-allocation lines. ", "dataType": "Linear unit"}]},
{"syntax": "MakeVehicleRoutingProblemLayer_na (in_network_dataset, out_network_analysis_layer, time_impedance, {distance_impedance}, {time_units}, {distance_units}, {default_date}, {capacity_count}, {time_window_factor}, {excess_transit_factor}, {UTurn_policy}, {restriction_attribute_name}, {hierarchy}, {hierarchy_settings}, {output_path_shape})", "name": "Make Vehicle Routing Problem Layer (Network Analyst)", "description": "Makes a vehicle routing problem (VRP) network analysis layer and sets its analysis properties. A vehicle routing problem analysis layer is useful for optimizing a set of routes using a fleet of vehicles.", "example": {"title": "MakeVehicleRoutingProblemLayer example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.MakeVehicleRoutingProblemLayer ( \"Transportation/Streets_ND\" , \"DeliveryRoutes\" , \"Minutes\" )"}, "usage": ["After creating the analysis layer with this tool, you can add network analysis objects to it using the ", "Add Locations", " tool, solve the analysis using the ", "Solve", " tool, and save the results on disk using ", "Save To Layer File", " tool. ", "When using this tool in geoprocessing models, if the model is run as a tool, the output network analysis layer must be made  a model parameter; otherwise, the output layer is not added to the table of contents in ArcMap."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset on which the vehicle routing problem analysis will be performed. The network dataset must have a time based cost attribute since the VRP solver minimizes time. ", "dataType": "Network Dataset Layer"}, {"name": "out_network_analysis_layer", "isOutputFile": true, "isOptional": false, "description": "Name of the vehicle routing problem network analysis layer to create. ", "dataType": "String"}, {"name": "time_impedance", "isOptional": false, "description": " The time cost attribute used to define the traversal time along the elements of the network. The time cost attribute is required, since the vehicle routing problem solver minimizes time. ", "dataType": "String"}, {"name": "distance_impedance", "isOptional": true, "description": " The distance cost attribute used to define the length along the elements of the network. The distance cost attribute is optional. ", "dataType": "String"}, {"name": "time_units", "isOptional": true, "description": "The time units used by the temporal fields of the analysis layer's sublayers and tables (network analysis classes). This does not have to be the same as the units of the time cost attribute. Seconds Minutes Hours Days", "dataType": "String"}, {"name": "distance_units", "isOptional": true, "description": " The distance units used by distance fields of the analysis layer's sublayers and tables (network analysis classes). This does not have to be the same as the units of the optional distance cost attribute. Miles Kilometers Feet Yards Meters Inches Centimeters Millimeters Decimeters NauticalMiles", "dataType": "String"}, {"name": "default_date", "isOptional": true, "description": "The implied date for time field values that don't have a date specified with the time. If a time field for an order object, such as TimeWindowStart1, has a time-only value, the date is assumed to be the Default Date. For example, if an order has a TimeWindowStart1 value of 9:00 AM and the Default Date is March 6, 2013, then the entire time value for the field is 9:00 A.M., March 6, 2013. The default date has no effect on time field values that already have a date. The day of the week can also be specified as the Default Date using the following dates. If your network dataset includes traffic data, the results of the analysis could change depending on the date that you specify here. For example, if your routes start at 8:00 a.m. on Sunday, when there is not much traffic, versus 8:00 a.m. on Monday during rush hour, the Monday route would take longer. Furthermore, the best path could change depending on traffic conditions. Today\u2014 12/30/1899 Sunday\u2014 12/31/1899 Monday\u2014 1/1/1900 Tuesday\u2014 1/2/1900 Wednesday\u2014 1/3/1900 Thursday\u2014 1/4/1900 Friday\u2014 1/5/1900 Saturday\u2014 1/6/1900", "dataType": "Date"}, {"name": "capacity_count", "isOptional": true, "description": "The number of capacity constraint dimensions required to describe the relevant limits of the vehicles. In an order delivery case, each vehicle may have a limited amount of weight and volume it can carry at one time based on physical and legal limitations. In this case, if you track the weight and volume on the orders, you can use these two capacities to prevent the vehicles from getting overloaded. The capacity count for this scenario is two (weight and volume). Depending on the problem, you may need to track different types or amounts of capacities. The capacities entered into the capacity fields (DeliveryQuantities and PickupQuantities for the Orders class and Capacities for the Routes class) are space-delimited strings of numbers, which can hold up to the number of values specified in Capacity Count. Each capacity dimension should appear in the same positional order for all capacity field values in the same VRP analysis layer. The capacities themselves are unnamed, so to avoid accidentally transposing capacity dimensions, ensure that the space-delimited capacity lists are always entered in the same order for all capacity field values. ", "dataType": "Long"}, {"name": "time_window_factor", "isOptional": true, "description": "This parameter allows you to rate the importance of honoring time windows without causing violations. A time window violation occurs when a route arrives at an order, depot, or break after a time window has closed. The violation is the interval between the end of the time window and the arrival time of a route. The VRP solution can change according to the value you choose for the Time Window Violation Importance parameter. The following list describes what the values mean and how the resulting VRP solution can vary: High \u2014 The solver tries to find a solution that minimizes time window violations at the expense of increasing the overall travel time. Choose High if arriving on time at orders is more important to you than minimizing your overall solution cost. This may be the case if you are meeting customers at your orders and you don't want to inconvenience them with tardy arrivals (another option is to use hard time windows that can't be violated at all).Given other constraints of a vehicle routing problem, it may be impossible to visit all the orders within their time windows. In this case, even a High setting might produce violations. Medium \u2014 This is the default setting. The solver looks for a balance between meeting time windows and reducing the overall solution cost. Low \u2014 The solver tries to find a solution that minimizes overall travel time, regardless of time windows. Choose Low if respecting time windows is less important than reducing your overall solution cost. You may want to use this setting if you have a growing backlog of service requests. For the purpose of servicing more orders in a day and reducing the backlog, you can choose Low even though customers will be inconvenienced with your fleet's late arrivals.", "dataType": "String"}, {"name": "excess_transit_factor", "isOptional": true, "description": "This parameter allows you to rate the importance of reducing excess transit time. Excess transit time is the amount of time exceeding the time required to travel directly between the paired orders. The excess time results from breaks or travel to other orders or depots between visits to the paired orders. The VRP solution can change according to the value you choose for the Excess Transit Time Importance. The following list describes what the values mean and how the resulting VRP solution can vary: High \u2014 The solver tries to find a solution with less excess transit time between paired orders at the expense of increasing the overall travel costs. It makes sense to use this setting if you are transporting people between paired orders and you want to shorten their ride time. This is characteristic of taxi services. Medium \u2014 This is the default setting. The solver looks for a balance between reducing excess transit time and reducing the overall solution cost. Low \u2014 The solver tries to find a solution that minimizes overall solution cost, regardless of excess transit time. This setting is commonly used with courier services. Since couriers transport packages as opposed to people, they don't need to worry about ride time. Using Low allows the couriers to service paired orders in the proper sequence and minimize the overall solution cost.", "dataType": "String"}, {"name": "UTurn_policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "restriction_attribute_name", "isOptional": false, "description": "List of restriction attributes to apply during the analysis. ", "dataType": "String"}, {"name": "hierarchy", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields an exact route for the network dataset.", "dataType": "Boolean"}, {"name": "hierarchy_settings", "isOptional": true, "description": " Prior to version 10 , this parameter allowed you to change the hierarchy ranges for your analysis from the default hierarchy ranges established in the network dataset. At version 10 , this parameter is no longer supported and should be specified as an empty string. If you wish to change the hierarchy ranges for your analysis, update the default hierarchy ranges in the network dataset. ", "dataType": "Network Analyst Hierarchy Settings"}, {"name": "output_path_shape", "isOutputFile": true, "isOptional": true, "description": " TRUE_LINES_WITH_MEASURES \u2014 The output routes will have the exact shape of the underlying network sources. Furthermore, the output includes route measurements for linear referencing. The measurements increase from the first stop and record the cumulative impedance to reach a given position. TRUE_LINES_WITHOUT_MEASURES \u2014 The output routes will have the exact shape of the underlying network sources. STRAIGHT_LINES \u2014 The output route shape will be straight lines connecting orders and depot visits as per the route sequence. NO_LINES \u2014 No shape will be generated for the output routes. You will also not be able to generate driving directions. ", "dataType": "String"}]},
{"syntax": "MakeServiceAreaLayer_na (in_network_dataset, out_network_analysis_layer, impedance_attribute, {travel_from_to}, {default_break_values}, {polygon_type}, {merge}, {nesting_type}, {line_type}, {overlap}, {split}, {excluded_source_name}, {accumulate_attribute_name}, {UTurn_policy}, {restriction_attribute_name}, {polygon_trim}, {poly_trim_value}, {lines_source_fields}, {hierarchy}, {time_of_day})", "name": "Make Service Area Layer (Network Analyst)", "description": "Makes a service area network analysis layer and sets its analysis properties. A service area analysis layer is useful in determining the area of accessibility within a given cutoff cost from a facility location. ", "example": {"title": "MakeServiceAreaLayer example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.MakeServiceAreaLayer ( \"Transportation/Streets_ND\" , \"FireStationCoverage\" , \"Minutes\" )"}, "usage": ["After creating the analysis layer with this tool, you can add network analysis objects to it using the ", "Add Locations", " tool, solve the analysis using the ", "Solve", " tool, and save the results on disk using ", "Save To Layer File", " tool. ", "When using this tool in geoprocessing models, if the model is run as a tool, the output network analysis layer must be made  a model parameter; otherwise, the output layer is not added to the table of contents in ArcMap."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset on which the service area analysis will be performed. ", "dataType": "Network Dataset Layer"}, {"name": "out_network_analysis_layer", "isOutputFile": true, "isOptional": false, "description": "Name of the service area network analysis layer to create. ", "dataType": "String"}, {"name": "impedance_attribute", "isOptional": false, "description": "The cost attribute to be used as impedance in the analysis. ", "dataType": "String"}, {"name": "travel_from_to", "isOptional": true, "description": "Specifies the direction of travel to or from the facilities. Using this option can result in different service areas on a network with one-way restrictions and having different impedances based on direction of travel. The service area for a pizza delivery store, for example, should be created away from the facility, whereas the service area of a hospital should be created toward the facility. TRAVEL_FROM \u2014 The service area is created in the direction away from the facilities. TRAVEL_TO \u2014 The service area is created in the direction towards the facilities.", "dataType": "String"}, {"name": "default_break_values", "isOptional": true, "description": "Default impedance values indicating the extent of the service area to be calculated. The default can be overridden by specifying the break value on the facilities. Multiple polygon breaks can be set to create concentric service areas. For instance, to find 2-, 3-, and 5-minute service areas for the same facility, specify \"2 3 5\" as the value for the Default break values parameter (the numbers 2, 3, and 5 should be separated by a space). ", "dataType": "String"}, {"name": "polygon_type", "isOptional": true, "description": "Specifies the type of polygons to be generated. If your data is of an urban area with a gridlike network, the difference between generalized and detailed polygons would be minimal. However, for mountain and rural roads, the detailed polygons may present significantly more accurate results than generalized polygons. SIMPLE_POLYS \u2014 Creates generalized polygons, which are generated quickly and are fairly accurate, except on the fringes. This is the default. DETAILED_POLYS \u2014 Creates detailed polygons, which accurately model the service area lines and may contain islands of unreached areas. This option is slower than generating generalized polygons. NO_POLYS \u2014 Turns off polygon generation for the case in which only service area lines are desired. ", "dataType": "String"}, {"name": "merge", "isOptional": true, "description": "Specifies the options to merge polygons that share similar break values. This option is applicable only when generating polygons for multiple facilities. NO_MERGE \u2014 Creates individual polygons for each facility. The polygons can overlap each other. NO_OVERLAP \u2014 Creates individual polygons that are closest for each facility. The polygons do not overlap each other. MERGE \u2014 Joins the polygons of multiple facilities that have the same break value. ", "dataType": "String"}, {"name": "nesting_type", "isOptional": true, "description": "Specifies the option to create concentric service area polygons as disks or rings. This option is applicable only when multiple break values are specified for the facilities. RINGS \u2014 Do not include the area of the smaller breaks. This creates polygons going between consecutive breaks. Use this option if you want to find the area from one break to another. DISKS \u2014 Creates the polygons going from the facility to the break. For instance, If you create 5- and 10-minute service areas, then the 10-minute service area polygon will include the area under the 5-minute service area polygon. Use this option if you want to find the entire area from the facility to the break for each break.", "dataType": "String"}, {"name": "line_type", "isOptional": true, "description": "Specifies the type of lines to be generated based on the service area analysis. Selecting the TRUE_LINES or TRUE_LINES_WITH_MEASURES option for large service areas will increase the amount of memory consumed by the analysis. NO_LINES \u2014 Do not generate lines. This is the default. TRUE_LINES \u2014 Lines are generated without measures. TRUE_LINES_WITH_MEASURES \u2014 Lines are generated with measures. The measure values are generated based on the impedance value on each end of the edge with the intermediate vertices interpolated. Do not use this option if faster performance is desired. ", "dataType": "String"}, {"name": "overlap", "isOptional": true, "description": "Determines if overlapping lines are generated when the service area lines are computed. OVERLAP \u2014 Include a separate line feature for each facility when the facilities have service area lines that are coincident. NON_OVERLAP \u2014 Include each service area line at most once and associate it with its closest (least impedance) facility.", "dataType": "Boolean"}, {"name": "split", "isOptional": true, "description": " SPLIT \u2014 Split every line between two breaks into two lines, each lying within its break. This is useful if you want to symbolize the service area lines by break. Otherwise, use the NO_SPLIT option for optimal performance. NO_SPLIT \u2014 The lines are not split at the boundaries of the breaks. This is the default. ", "dataType": "Boolean"}, {"name": "excluded_source_name", "isOptional": false, "description": "Specifies the list of network sources to be excluded when generating the polygons. The geometry of traversed elements from the excluded sources will be omitted from all polygons. This is useful if you have some network sources that you don't want to be included in the polygon generation because they create less accurate polygons or are inconsequential for the service area analysis. For example, while creating a drive time service area in a multimodal network of streets and rails, you should choose to exclude the rail lines from polygon generation so as to correctly model where a vehicle could travel. Excluding a network source from service area polygons does not prevent those sources from being traversed. Excluding sources from service area polygons only influences the polygon shape of the service areas. If you want to prevent traversal of a given network source, you must create an appropriate restriction when defining your network dataset. ", "dataType": "String"}, {"name": "accumulate_attribute_name", "isOptional": false, "description": "List of cost attributes to be accumulated during analysis. These accumulation attributes are purely for reference; the solver only uses the cost attribute specified by the Impedance attribute parameter to calculate the route. For each cost attribute that is accumulated, a Total_[Impedance] property is added to the routes that are output by the solver. ", "dataType": "String"}, {"name": "UTurn_policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "restriction_attribute_name", "isOptional": false, "description": "List of restriction attributes to apply during the analysis. ", "dataType": "String"}, {"name": "polygon_trim", "isOptional": true, "description": " TRIM_POLYS \u2014 Trims the polygons containing the edges at the periphery of the service area to be within the specified distance of these outer edges. This is useful if the network is very sparse and you don't want the service area to cover large areas where there are no features. NO_TRIM_POLYS \u2014 Do not trim polygons. ", "dataType": "Boolean"}, {"name": "poly_trim_value", "isOptional": true, "description": "Specifies the distance within which the service area polygons are trimmed. The parameter includes a value and units for the distance. The default value is 100 meters. ", "dataType": "Linear unit"}, {"name": "lines_source_fields", "isOptional": true, "description": " LINES_SOURCE_FIELDS \u2014 Add the SourceID, SourceOID, FromPosition, and ToPosition fields to the service area lines to hold information about the underlying source features traversed during the analysis. This can be useful to join the results of the service area lines to the original source data. NO_LINES_SOURCE_FIELDS \u2014 Do not add the source fields (SourceID, SourceOID, FromPosition, and ToPosition) to the service area lines.", "dataType": "Boolean"}, {"name": "hierarchy", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields a service area measured along all edges of the network dataset regardless of hierarchy level.", "dataType": "Boolean"}, {"name": "time_of_day", "isOptional": true, "description": " The time to depart from or arrive at the facilities of the service area layer. The interpretation of this value as a depart or arrive time depends on whether travel is away from or toward the facilities. If you have chosen a traffic-based impedance attribute, the solution will be generated given dynamic traffic conditions at the time of day specified here. A date and time can be specified as 5/14/2012 10:30 AM. Instead of using a particular date, a day of the week can be specified using the following dates. Repeatedly solving the same analysis, but using different Time of Day values, allows you to see how a facility's reach changes over time. For instance, the five-minute service area around a fire station may start out large in the early morning, diminish during the morning rush hour, grow in the late morning, and so on throughout the day. It represents the departure time if Travel From or To Facility is set to TRAVEL_FROM. It represents the arrival time if Travel From or To Facility is set to TRAVEL_TO. Today\u2014 12/30/1899 Sunday\u2014 12/31/1899 Monday\u2014 1/1/1900 Tuesday\u2014 1/2/1900 Wednesday\u2014 1/3/1900 Thursday\u2014 1/4/1900 Friday\u2014 1/5/1900 Saturday\u2014 1/6/1900", "dataType": "Date"}]},
{"syntax": "MakeRouteLayer_na (in_network_dataset, out_network_analysis_layer, impedance_attribute, {find_best_order}, {ordering_type}, {time_windows}, {accumulate_attribute_name}, {UTurn_policy}, {restriction_attribute_name}, {hierarchy}, {hierarchy_settings}, {output_path_shape}, {start_date_time})", "name": "Make Route Layer (Network Analyst)", "description": "Makes a route network analysis layer and sets its analysis properties. A route analysis layer is useful for determining the best route between a set of network locations based on a specified network cost.", "example": {"title": "MakeRouteLayer example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.MakeRouteLayer ( \"Transportation/Streets_ND\" , \"WorkRoute\" , \"Minutes\" )"}, "usage": ["After creating the analysis layer with this tool, you can add network analysis objects to it using the ", "Add Locations", " tool, solve the analysis using the ", "Solve", " tool, and save the results on disk using ", "Save To Layer File", " tool. ", "When using this tool in geoprocessing models, if the model is run as a tool, the output network analysis layer must be made  a model parameter; otherwise, the output layer is not added to the table of contents in ArcMap."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset on which the route analysis will be performed. ", "dataType": "Network Dataset Layer"}, {"name": "out_network_analysis_layer", "isOutputFile": true, "isOptional": false, "description": "Name of the route network analysis layer to create. ", "dataType": "String"}, {"name": "impedance_attribute", "isOptional": false, "description": "The cost attribute to be used as impedance in the analysis. ", "dataType": "String"}, {"name": "find_best_order", "isOptional": true, "description": " FIND_BEST_ORDER \u2014 The stops will be reordered to find the optimal route. This option changes the route analysis from a shortest-path problem to a traveling salesperson problem (TSP). USE_INPUT_ORDER \u2014 The stops will be visited in the input order. This is the default. ", "dataType": "Boolean"}, {"name": "ordering_type", "isOptional": true, "description": "Specifies the ordering of stops when FIND_BEST_ORDER is used. PRESERVE_BOTH \u2014 Preserves the first and last stops by input order as the first and last stops in the route. PRESERVE_FIRST \u2014 Preserves the first stop by input order as the first stop in the route, but the last stop is free to be reordered. PRESERVE_LAST \u2014 Preserves the last stop by input order as the last stop in the route, but the first stop is free to be reordered. PRESERVE_NONE \u2014 Frees both the first and last stop to be reordered. ", "dataType": "String"}, {"name": "time_windows", "isOptional": true, "description": "Specifies whether time windows will be used at the stops. USE_TIMEWINDOWS \u2014 The route will consider time windows on the stops. If a stop is arrived at before its time window, there will be wait time until the time window starts. If a stop is arrived at after its time window, there will be a time window violation. Total time window violation is balanced against minimum impedance when computing the route. This is a valid option only when the impedance is in time units. NO_TIMEWINDOWS \u2014 The route will ignore time windows on the stops. This is the default. ", "dataType": "Boolean"}, {"name": "accumulate_attribute_name", "isOptional": false, "description": "List of cost attributes to be accumulated during analysis. These accumulation attributes are purely for reference; the solver only uses the cost attribute specified by the Impedance attribute parameter to calculate the route. For each cost attribute that is accumulated, a Total_[Impedance] property is added to the routes that are output by the solver. ", "dataType": "String"}, {"name": "UTurn_policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "restriction_attribute_name", "isOptional": false, "description": "List of restriction attributes to apply during the analysis. ", "dataType": "String"}, {"name": "hierarchy", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields an exact route for the network dataset.", "dataType": "Boolean"}, {"name": "hierarchy_settings", "isOptional": true, "description": " Prior to version 10 , this parameter allowed you to change the hierarchy ranges for your analysis from the default hierarchy ranges established in the network dataset. At version 10 , this parameter is no longer supported and should be specified as an empty string. If you wish to change the hierarchy ranges for your analysis, update the default hierarchy ranges in the network dataset. ", "dataType": "Network Analyst Hierarchy Settings"}, {"name": "output_path_shape", "isOutputFile": true, "isOptional": true, "description": "Specifies the shape type for the route features that are output by the analysis. No matter which output shape type is chosen, the best route is always determined by the network impedance, never Euclidean distance. This means only the route shapes are different, not the underlying traversal of the network. TRUE_LINES_WITH_MEASURES \u2014 The output routes will have the exact shape of the underlying network sources. Furthermore, the output includes route measurements for linear referencing. The measurements increase from the first stop and record the cumulative impedance to reach a given position. TRUE_LINES_WITHOUT_MEASURES \u2014 The output routes will have the exact shape of the underlying network sources. STRAIGHT_LINES \u2014 The output route shape will be a single straight line between the stops. NO_LINES \u2014 No shape will be generated for the output routes. ", "dataType": "String"}, {"name": "start_date_time", "isOptional": true, "description": "Specifies a start date and time for the route. Route start time is mostly used to find routes based on the impedance attribute that varies with the time of the day. For example, a start time of 9 AM could be used to find a route that considers the rush hour traffic. The default value for this parameter is 8:00 AM. A date and time can be specified as 10/21/05 10:30 AM. If the route spans multiple days, and only the start time is specified, then the current date is used. Instead of using a particular date, a day of the week can be specified using the following dates. After the solve, the start and end time of the route are populated in the output routes. These start and end times are also used when directions are generated. Today\u2014 12/30/1899 Sunday\u2014 12/31/1899 Monday\u2014 1/1/1900 Tuesday\u2014 1/2/1900 Wednesday\u2014 1/3/1900 Thursday\u2014 1/4/1900 Friday\u2014 1/5/1900 Saturday\u2014 1/6/1900", "dataType": "Date"}]},
{"syntax": "MakeODCostMatrixLayer_na (in_network_dataset, out_network_analysis_layer, impedance_attribute, {default_cutoff}, {default_number_destinations_to_find}, {accumulate_attribute_name}, {UTurn_policy}, {restriction_attribute_name}, {hierarchy}, {hierarchy_settings}, {output_path_shape}, {time_of_day})", "name": "Make OD Cost Matrix Layer (Network Analyst)", "description": "Makes an origin\u2013destination (OD) cost matrix network analysis layer and sets its analysis properties. An OD cost matrix analysis layer is useful for representing a matrix of costs going from a set of origin locations to a set of destination locations.", "example": {"title": "MakeODCostMatrixLayer example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/Paris.gdb\" arcpy.na.MakeODCostMatrixLayer ( \"Transportation/ParisNet\" , \"DrivetimeCosts\" , \"Drivetime\" )"}, "usage": ["After creating the analysis layer with this tool, you can add network analysis objects to it using the ", "Add Locations", " tool, solve the analysis using the ", "Solve", " tool, and save the results on disk using ", "Save To Layer File", " tool. ", "When using this tool in geoprocessing models, if the model is run as a tool, the output network analysis layer must be made  a model parameter; otherwise, the output layer is not added to the table of contents in ArcMap."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset on which the OD cost matrix analysis will be performed. ", "dataType": "Network Dataset Layer"}, {"name": "out_network_analysis_layer", "isOutputFile": true, "isOptional": false, "description": "Name of the OD cost matrix network analysis layer to create. ", "dataType": "String"}, {"name": "impedance_attribute", "isOptional": false, "description": "The cost attribute to be used as impedance in the analysis. ", "dataType": "String"}, {"name": "default_cutoff", "isOptional": true, "description": "Default impedance value at which to cut off searching for destinations for a given origin. If the accumulated impedance becomes higher than the cutoff value, the traversal stops. The default can be overridden by specifying the cutoff value on the origins. ", "dataType": "Double"}, {"name": "default_number_destinations_to_find", "isOptional": true, "description": "Default number of destinations to find for each origin. The default can be overridden by specifying a value for the TargetDestinationCount property on the origins. ", "dataType": "Long"}, {"name": "accumulate_attribute_name", "isOptional": false, "description": "List of cost attributes to be accumulated during analysis. These accumulation attributes are purely for reference; the solver only uses the cost attribute specified by the Impedance attribute parameter to calculate the route. For each cost attribute that is accumulated, a Total_[Impedance] property is added to the routes that are output by the solver. ", "dataType": "String"}, {"name": "UTurn_policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "restriction_attribute_name", "isOptional": false, "description": "List of restriction attributes to apply during the analysis. ", "dataType": "String"}, {"name": "hierarchy", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields an exact route for the network dataset.", "dataType": "Boolean"}, {"name": "hierarchy_settings", "isOptional": true, "description": " Prior to version 10 , this parameter allowed you to change the hierarchy ranges for your analysis from the default hierarchy ranges established in the network dataset. At version 10 , this parameter is no longer supported and should be specified as an empty string. If you wish to change the hierarchy ranges for your analysis, update the default hierarchy ranges in the network dataset. ", "dataType": "Network Analyst Hierarchy Settings"}, {"name": "output_path_shape", "isOutputFile": true, "isOptional": true, "description": "No matter which output shape type is chosen, the best route is always determined by the network impedance, never Euclidean distance. This means only the route shapes are different, not the underlying traversal of the network. NO_LINES \u2014 No shape will be generated for the output routes. This is useful when you have large number of origins and destinations and are interested only in the OD cost matrix table (and not the output line shapes). STRAIGHT_LINES \u2014 The output route shape will be a single straight line between each of the origin-destination pairs. ", "dataType": "String"}, {"name": "time_of_day", "isOptional": true, "description": " Indicates the departure time from origins. If you have chosen a traffic-based impedance attribute, the solution will be generated given dynamic traffic conditions at the time of day specified here. A date and time can be specified as 5/14/2012 10:30 AM . Instead of using a particular date, a day of the week can be specified using the following dates. Today\u2014 12/30/1899 Sunday\u2014 12/31/1899 Monday\u2014 1/1/1900 Tuesday\u2014 1/2/1900 Wednesday\u2014 1/3/1900 Thursday\u2014 1/4/1900 Friday\u2014 1/5/1900 Saturday\u2014 1/6/1900", "dataType": "Date"}]},
{"syntax": "MakeLocationAllocationLayer_na (in_network_dataset, out_network_analysis_layer, impedance_attribute, {loc_alloc_from_to}, {loc_alloc_problem_type}, {number_facilities_to_find}, {impedance_cutoff}, {impedance_transformation}, {impedance_parameter}, {target_market_share}, {accumulate_attribute_name}, {UTurn_policy}, {restriction_attribute_name}, {hierarchy}, {output_path_shape}, {default_capacity}, {time_of_day})", "name": "Make Location-Allocation Layer (Network Analyst)", "description": "Makes a location-allocation network analysis layer and sets its analysis properties. A location-allocation analysis layer is useful for choosing a given number of facilities from a set of potential locations such that a demand will be allocated to facilities in an optimal and efficient manner.", "example": {"title": "MakeLocationAllocationLayer example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.MakeLocationAllocationLayer ( \"Transportation/Streets_ND\" , \"StoreLocations\" , \"Minutes\" )"}, "usage": ["After creating the analysis layer with this tool, you can add network analysis objects to it using the ", "Add Locations", " tool, solve the analysis using the ", "Solve", " tool, and save the results on disk using ", "Save To Layer File", " tool. ", "When using this tool in geoprocessing models, if the model is run as a tool, the output network analysis layer must be made  a model parameter; otherwise, the output layer is not added to the table of contents in ArcMap."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset on which the location-allocation analysis will be performed. ", "dataType": "Network Dataset Layer"}, {"name": "out_network_analysis_layer", "isOutputFile": true, "isOptional": false, "description": "Name of the location-allocation network analysis layer to create. ", "dataType": "String"}, {"name": "impedance_attribute", "isOptional": false, "description": "The cost attribute to be used as impedance in the analysis. ", "dataType": "String"}, {"name": "loc_alloc_from_to", "isOptional": true, "description": "Specifies the direction of travel between facilities and demand points when calculating the network costs. Using this option can affect the allocation of the demand points to the facilities on a network with one-way restrictions and different impedances based on direction of travel. For instance, a facility may be a 15-minute drive from the demand point to the facility, but only a 10-minute trip when traveling from the facility to the demand point. Fire departments commonly use the Facility to Demand setting, since they are concerned with the time it takes to travel from the fire station to the location of the emergency. A retail store is more concerned with the time it takes the shoppers to reach the store; therefore, stores commonly use the Demand to Facility option. FACILITY_TO_DEMAND \u2014 Direction of travel is from facilities to demand points. DEMAND_TO_FACILITY \u2014 Direction of travel is from demand points to facilities. ", "dataType": "String"}, {"name": "loc_alloc_problem_type", "isOptional": true, "description": "The problem type that will be solved. The choice of the problem type depends on the kind of facility being located. Different kinds of facilities have different priorities and constraints. MINIMIZE_IMPEDANCE \u2014 This option solves the warehouse location problem. It selects a set of facilities such that the total sum of weighted impedances (demand at a location times the impedance to the closest facility) is minimized. This problem type is often known as the P-Median problem. MAXIMIZE_COVERAGE \u2014 This option solves the fire station location problem. It chooses facilities such that all or the greatest amount of demand is within a specified impedance cutoff. MAXIMIZE_CAPACITATED_COVERAGE \u2014 This option solves the location problem where facilities have a finite capacity. It chooses facilities such that all or the greatest amount of demand can be served without exceeding the capacity of any facility. In addition to honoring capacity, it selects facilities such that the total sum of weighted impedance (demand allocated to a facility multiplied by the impedance to or from the facility) is minimized. MINIMIZE_FACILITIES \u2014 This option solves the fire station location problem. It chooses the minimum number of facilities needed to cover all or the greatest amount of demand within a specified impedance cutoff. MAXIMIZE_ATTENDANCE \u2014 This option solves the neighborhood store location problem where the proportion of demand allocated to the nearest chosen facility falls with increasing distance. The set of facilities that maximize the total allocated demand is chosen. Demand further than the specified impedance cutoff does not affect the chosen set of facilities. MAXIMIZE_MARKET_SHARE \u2014 This option solves the competitive facility location problem. It chooses facilities to maximize market share in the presence of competitive facilities. Gravity model concepts are used to determine the proportion of demand allocated to each facility. The set of facilities that maximizes the total allocated demand is chosen. TARGET_MARKET_SHARE \u2014 This option solves the competitive facility location problem. It chooses facilities to reach a specified target market share in the presence of competitive facilities. Gravity model concepts are used to determine the proportion of demand allocated to each facility. The minimum number of facilities needed to reach the specified target market share is chosen. ", "dataType": "String"}, {"name": "number_facilities_to_find", "isOptional": true, "description": "Specifies the number of facilities that the solver should locate. The facilities with a FacilityType value of Required are always part of the solution when there are more facilities to find than required facilities; any excess facilities to choose are picked from candidate facilities. Any facilities that have a FacilityType value of Chosen before solving are treated as candidate facilities at solve time. The parameter value is not considered for the MINIMIZE_FACILITIES problem type since the solver determines the minimum number of facilities to locate to maximize coverage. The parameter value is overridden for the TARGET_MARKET_SHARE problem type because the solver searches for the minimum number of facilities required to capture the specified market share. ", "dataType": "Long"}, {"name": "impedance_cutoff", "isOptional": true, "description": "Impedance Cutoff specifies the maximum impedance at which a demand point can be allocated to a facility. The maximum impedance is measured by the least-cost path along the network. If a demand point is outside the cutoff, it is left unallocated. This property might be used to model the maximum distance that people are willing to travel to visit your stores or the maximum time that is permitted for a fire department to reach anyone in the community. Demand points have a Cutoff_[Impedance] property, which, if set, overrides the Impedance Cutoff property of the analysis layer. You might find that people in rural areas are willing to travel up to 10 miles to reach a facility while urbanites are only willing to travel up to two miles. You can model this behavior by setting the impedance cutoff value of the analysis layer to 10 and setting the Cutoff_Miles value of the demand points in urban areas to 2. ", "dataType": "Double"}, {"name": "impedance_transformation", "isOptional": true, "description": " This sets the equation for transforming the network cost between facilities and demand points. This property, coupled with the Impedance Parameter , specifies how severely the network impedance between facilities and demand points influences the solver's choice of facilities. Demand points have an ImpedanceTransformation property, which, if set, overrides the Impedance Transformation property of the analysis layer. You might determine the impedance transformation should be different for urban and rural residents. You can model this by setting the impedance transformation for the analysis layer to match that of rural residents and setting the impedance transformation for the demand points in urban areas to match that of urbanites. LINEAR \u2014 The transformed network impedance between the facility and the demand point is the same as the shortest-path network impedance between them. With this option, the impedance parameter is always set to one. This is the default. POWER \u2014 The transformed network impedance between the facility and the demand point is equal to the shortest-path network impedance raised to the power specified by the impedance parameter. Use the POWER option with a positive impedance parameter to specify higher weight to nearby facilities. EXPONENTIAL \u2014 The transformed network impedance between the facility and the demand point is equal to the mathematical constant e raised to the power specified by the shortest-path network impedance multiplied with the impedance parameter. Use the EXPONENTIAL option with a positive impedance parameter to specify a very high weight to nearby facilities.Exponential transformations are commonly used in conjunction with an impedance cutoff.", "dataType": "String"}, {"name": "impedance_parameter", "isOptional": true, "description": "Provides a parameter value to the equations specified in the Impedance transformation parameter. The parameter value is ignored when the impedance transformation is of type linear. For power and exponential impedance transformations, the value should be nonzero. Demand points have an ImpedanceParameter property, which, if set, overrides the Impedance Parameter property of the analysis layer. You might determine that the impedance parameter should be different for urban and rural residents. You can model this by setting the impedance transformation for the analysis layer to match that of rural residents and setting the impedance transformation for the demand points in urban areas to match that of urbanites. ", "dataType": "Double"}, {"name": "target_market_share", "isOptional": true, "description": " Specifies the target market share in percentage to solve for when the Location-Allocation Problem Type parameter is set to TARGET_MARKET_SHARE. It is the percentage of the total demand weight that you want your solution facilities to capture. The solver chooses the minimum number of facilities required to capture the target market share specified by this numeric value. ", "dataType": "Double"}, {"name": "accumulate_attribute_name", "isOptional": false, "description": "List of cost attributes to be accumulated during analysis. These accumulation attributes are purely for reference; the solver only uses the cost attribute specified by the Impedance attribute parameter to calculate the route. For each cost attribute that is accumulated, a Total_[Impedance] property is added to the routes that are output by the solver. ", "dataType": "String"}, {"name": "UTurn_policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "restriction_attribute_name", "isOptional": false, "description": "List of restriction attributes to apply during the analysis. ", "dataType": "String"}, {"name": "hierarchy", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields an exact route for the network dataset.", "dataType": "Boolean"}, {"name": "output_path_shape", "isOutputFile": true, "isOptional": true, "description": " NO_LINES \u2014 No shape will be generated for the output of the analysis. STRAIGHT_LINES \u2014 The output line shapes will be straight lines connecting the solution facilities to their allocated demand points. ", "dataType": "String"}, {"name": "default_capacity", "isOptional": true, "description": " Specifies the default capacity of facilities when the Location-Allocation Problem Type parameter is set to MAXIMIZE_CAPACITATED_COVERAGE. This parameter is ignored for all other problem types. Facilities have a Capacity property, which, if set to a nonnull value, overrides the Default Capacity parameter for that facility. ", "dataType": "Double"}, {"name": "time_of_day", "isOptional": true, "description": " Indicates the time and date of departure. The departure time can be from facilities or demand points, depending on whether travel is from demand to facility or facility to demand. If you have chosen a traffic-based impedance attribute, the solution will be generated given dynamic traffic conditions at the time of day specified here. A date and time can be specified as 5/14/2012 10:30 AM. Instead of using a particular date, a day of the week can be specified using the following dates. Today\u2014 12/30/1899 Sunday\u2014 12/31/1899 Monday\u2014 1/1/1900 Tuesday\u2014 1/2/1900 Wednesday\u2014 1/3/1900 Thursday\u2014 1/4/1900 Friday\u2014 1/5/1900 Saturday\u2014 1/6/1900", "dataType": "Date"}]},
{"syntax": "MakeClosestFacilityLayer_na (in_network_dataset, out_network_analysis_layer, impedance_attribute, {travel_from_to}, {default_cutoff}, {default_number_facilities_to_find}, {accumulate_attribute_name}, {UTurn_policy}, {restriction_attribute_name}, {hierarchy}, {hierarchy_settings}, {output_path_shape}, {time_of_day}, {time_of_day_usage})", "name": "Make Closest Facility Layer (Network Analyst)", "description": "Makes a closest facility network analysis layer and sets its analysis properties. A closest facility analysis layer is useful in determining the closest facility or facilities to an incident based on a specified network cost.", "example": {"title": "MakeClosestFacilityLayer example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.MakeClosestFacilityLayer ( \"Transportation/Streets_ND\" , \"ClosestFireStations\" , \"Minutes\" )"}, "usage": ["After creating the analysis layer with this tool, you can add network analysis objects to it using the ", "Add Locations", " tool, solve the analysis using the ", "Solve", " tool, and save the results on disk using ", "Save To Layer File", " tool. ", "When using this tool in geoprocessing models, if the model is run as a tool, the output network analysis layer must be made  a model parameter; otherwise, the output layer is not added to the table of contents in ArcMap."], "parameters": [{"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset on which the closest facility analysis will be performed. ", "dataType": "Network Dataset Layer"}, {"name": "out_network_analysis_layer", "isOutputFile": true, "isOptional": false, "description": "Name of the closest facility network analysis layer to create. ", "dataType": "String"}, {"name": "impedance_attribute", "isOptional": false, "description": "The cost attribute to be used as impedance in the analysis. ", "dataType": "String"}, {"name": "travel_from_to", "isOptional": true, "description": "Specifies the direction of travel between facilities and incidents. Using this option can find different facilities on a network with one-way restrictions and different impedances based on direction of travel. For instance, a facility may be a 10-minute drive from the incident while traveling from the incident to the facility, but while traveling from the facility to the incident, it may be a 15-minute journey because of different travel time in that direction. Fire departments commonly use the TRAVEL_FROM setting since they are concerned with the time it takes to travel from the fire station (facility) to the location of the emergency (incident). A retail store (facility) is more concerned with the time it takes the shoppers (incidents) to reach the store; therefore, stores commonly use the TRAVEL_TO option. TRAVEL_FROM \u2014 Direction of travel is from facilities to incidents. TRAVEL_TO \u2014 Direction of travel is from incidents to facilities. ", "dataType": "String"}, {"name": "default_cutoff", "isOptional": true, "description": "Default impedance value at which to stop searching for facilities for a given incident. The default can be overridden by specifying the cutoff value on incidents when the TRAVEL_TO option is used or by specifying the cutoff value on facilities when the TRAVEL_FROM option is used. ", "dataType": "Double"}, {"name": "default_number_facilities_to_find", "isOptional": true, "description": "Default number of closest facilities to find per incident. The default can be overridden by specifying a value for the TargetFacilityCount property on the incidents. ", "dataType": "Long"}, {"name": "accumulate_attribute_name", "isOptional": false, "description": "List of cost attributes to be accumulated during analysis. These accumulation attributes are purely for reference; the solver only uses the cost attribute specified by the Impedance attribute parameter to calculate the route. For each cost attribute that is accumulated, a Total_[Impedance] property is added to the routes that are output by the solver. ", "dataType": "String"}, {"name": "UTurn_policy", "isOptional": true, "description": "The U-Turn policy at junctions. Allowing U-turns implies the solver can turn around at a junction and double back on the same street. Given that junctions represent street intersections and dead ends, different vehicles may be able to turn around at some junctions but not at others\u2014it depends on whether the junction represents an intersection or dead end. To accommodate, the U-turn policy parameter is implicitly specified by how many edges connect to the junction, which is known as junction valency. The acceptable values for this parameter are listed below; each is followed by a description of its meaning in terms of junction valency. If you need a more precisely defined U-turn policy, consider adding a global turn delay evaluator to a network cost attribute, or adjusting its settings if one exists, and pay particular attention to the configuration of reverse turns. Also, look at setting the CurbApproach property of your network locations. ALLOW_UTURNS \u2014 U-turns are permitted at junctions with any number of connected edges. This is the default value. NO_UTURNS \u2014 U-turns are prohibited at all junctions, regardless of junction valency. Note, however, that U-turns are still permitted at network locations even when this setting is chosen; however, you can set the individual network locations' CurbApproach property to prohibit U-turns there as well. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns are prohibited at all junctions, except those that have only one adjacent edge (a dead end). ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY \u2014 U-turns are prohibited at junctions where exactly two adjacent edges meet but are permitted at intersections (junctions with three or more adjacent edges) and dead ends (junctions with exactly one adjacent edge). Oftentimes, networks have extraneous junctions in the middle of road segments. This option prevents vehicles from making U-turns at these locations.", "dataType": "String"}, {"name": "restriction_attribute_name", "isOptional": false, "description": "List of restriction attributes to apply during the analysis. ", "dataType": "String"}, {"name": "hierarchy", "isOptional": true, "description": "The parameter is not used if a hierarchy attribute is not defined on the network dataset used to perform the analysis. In such cases, use \"#\" as the parameter value. USE_HIERARCHY \u2014 Use the hierarchy attribute for the analysis. Using a hierarchy results in the solver preferring higher-order edges to lower-order edges. Hierarchical solves are faster, and they can be used to simulate the preference of a driver who chooses to travel on freeways over local roads when possible\u2014even if that means a longer trip. This option is valid only if the input network dataset has a hierarchy attribute. NO_HIERARCHY \u2014 Do not use the hierarchy attribute for the analysis. Not using a hierarchy yields an exact route for the network dataset.", "dataType": "Boolean"}, {"name": "hierarchy_settings", "isOptional": true, "description": " Prior to version 10 , this parameter allowed you to change the hierarchy ranges for your analysis from the default hierarchy ranges established in the network dataset. At version 10 , this parameter is no longer supported and should be specified as an empty string. If you wish to change the hierarchy ranges for your analysis, update the default hierarchy ranges in the network dataset. ", "dataType": "Network Analyst Hierarchy Settings"}, {"name": "output_path_shape", "isOutputFile": true, "isOptional": true, "description": "Specifies the shape type for the route features that are output by the analysis. No matter which output shape type is chosen, the best route is always determined by the network impedance, never Euclidean distance. This means only the route shapes are different, not the underlying traversal of the network. TRUE_LINES_WITH_MEASURES \u2014 The output routes will have the exact shape of the underlying network sources. Furthermore, the output includes route measurements for linear referencing. The measurements increase from the first stop and record the cumulative impedance to reach a given position. TRUE_LINES_WITHOUT_MEASURES \u2014 The output routes will have the exact shape of the underlying network sources. STRAIGHT_LINES \u2014 The output route shape will be a single straight line between each paired incident and facility. NO_LINES \u2014 No shape will be generated for the output routes. ", "dataType": "String"}, {"name": "time_of_day", "isOptional": true, "description": " Specifies the time and date at which the routes should begin or end. The interpretation of this value depends on whether Time of Day Usage is set to START_TIME or END_TIME. If you have chosen a traffic-based impedance attribute, the solution will be generated given dynamic traffic conditions at the time of day specified here. A date and time can be specified as 5/14/2012 10:30 AM . Instead of using a particular date, a day of the week can be specified using the following dates. Today\u2014 12/30/1899 Sunday\u2014 12/31/1899 Monday\u2014 1/1/1900 Tuesday\u2014 1/2/1900 Wednesday\u2014 1/3/1900 Thursday\u2014 1/4/1900 Friday\u2014 1/5/1900 Saturday\u2014 1/6/1900", "dataType": "Date"}, {"name": "time_of_day_usage", "isOptional": true, "description": " Indicates whether the value of the Time of Day parameter represents the arrival or departure time for the route or routes. START_TIME \u2014 Time of Day is interpreted as the departure time from the facility or incident.When START_TIME is chosen, Time of Day indicates the solver should find the best route given a departure time. NOT_USED can be chosen if a value isn't specified in the Time of Day parameter. END_TIME \u2014 Time of Day is interpreted as the arrival time at the facility or incident. This option is useful if you want to know what time to depart from a location so that you arrive at the destination at the time specified in Time of Day. NOT_USED \u2014 When Time of Day doesn't have a value, NOT_USED is the only choice. When Time of Day has a value, NOT_USED isn't available.", "dataType": "String"}]},
{"syntax": "Directions_na (in_network_analysis_layer, file_type, out_directions_file, report_units, {report_time}, {time_attribute}, {language}, {style_name}, {stylesheet})", "name": "Directions (Network Analyst)", "description": "Generates turn-by-turn directions from a network analysis layer with routes. The directions can be written to a file in text, XML, or HTML format. If you provide an appropriate stylesheet, the directions  can be written to any other file format.", "example": {"title": "Directions example 1 (Python window)", "description": "Execute the Directions tool with all the parameters.", "code": "import arcpy arcpy.na.Directions ( \"Route\" , \"TEXT\" , \"C:/temp/Route_Directions.txt\" , \"Miles\" , \"REPORT_TIME\" , \"Minutes\" )"}, "usage": ["The tool solves the network analysis layer if it does not already have a valid result, so it is not required to solve the network analysis layer before generating directions."], "parameters": [{"name": "in_network_analysis_layer", "isInputFile": true, "isOptional": false, "description": "Network analysis layer for which directions will be generated. Directions can be generated only for route, closest facility, and vehicle routing problem network analysis layers. ", "dataType": "Network Analyst Layer"}, {"name": "file_type", "isOptional": false, "description": "The format of the output directions file. This parameter is ignored if the Stylesheet parameter has a value. XML \u2014 The output directions file will be generated as an XML file. Apart from direction strings and the length and time information for the routes, the file will also contain information about the maneuver type and the turn angle for each direction. TEXT \u2014 The output directions file will be generated as a simple TEXT file containing the direction strings, the length and, optionally, the time information for the routes. HTML \u2014 The output directions file will be generated as an HTML file containing the direction strings, the length and, optionally, the time information for the routes. ", "dataType": "String"}, {"name": "out_directions_file", "isOutputFile": true, "isOptional": false, "description": "The full path to the directions file that will be written. If you provide a stylesheet in the Stylesheet parameter, make sure the file suffix for Output Directions File matches the file type your stylesheet produces. ", "dataType": "File"}, {"name": "report_units", "isOptional": false, "description": "Specifies the linear units in which the length information will be reported in the directions file. For example, even though your impedance was in meters, you can show directions in miles. Feet \u2014 Yards \u2014 Miles \u2014 Meters \u2014 Kilometers \u2014 NauticalMiles \u2014", "dataType": "String"}, {"name": "report_time", "isOptional": true, "description": " NO_REPORT_TIME \u2014 Do not report travel times in the directions file. REPORT_TIME \u2014 Report travel times in the directions file. This is the default. ", "dataType": "Boolean"}, {"name": "time_attribute", "isOptional": true, "description": "The time-based cost attribute to provide travel times in the directions. The cost attribute must exist on the network dataset used by the input network analysis layer. ", "dataType": "String"}, {"name": "language", "isOptional": true, "description": "Choose a language in which to generate driving directions. The languages that are shown in the drop-down list depend on which ArcGIS language packs are installed on your computer. Note that if you are going to publish this tool as part of a service on a separate server, the ArcGIS language pack that corresponds with the language you choose must be installed on that server for the tool to function properly. Also, if a language pack isn't installed on your computer, the language won't appear in the drop-down list; however, you can type a language code instead of choosing one. ", "dataType": "String"}, {"name": "style_name", "isOptional": true, "description": " Choose the name of the formatting style for the directions. NA Desktop \u2014 Printable turn-by-turn directions NA Navigation \u2014 Turn-by-turn directions designed for an in-vehicle navigation device NA Campus \u2014 Turn-by-turn walking directions, which are designed for pedestrian routes", "dataType": "String"}, {"name": "stylesheet", "isOptional": true, "description": " The stylesheet for generating a given, formatted output file type (such as a PDF, Word, or HTML file). The suffix of the file in the Output Directions File parameter should match the file type that the stylesheet generates. The Directions tool overrides the Output File Type parameter if Stylesheet contains a value. If you want a head start on creating your own text and HTML stylesheets, copy and edit the stylesheets Network Analyst uses. You can find them in the following directory: [ArcGIS installation directory]\\ArcGIS\\Desktop 10.1 \\NetworkAnalyst\\Directions\\Styles. The HTML stylesheet is Dir2PHTML.xsl, and the text stylesheet is Dir2PlainText.xsl. ", "dataType": "File"}]},
{"syntax": "CalculateLocations_na (in_point_features, in_network_dataset, search_tolerance, search_criteria, {match_type}, {source_ID_field}, {source_OID_field}, {position_field}, {side_field}, {snap_X_field}, {snap_Y_field}, {distance_field}, {snap_Z_field}, {location_field}, {exclude_restricted_elements}, {search_query})", "name": "Calculate Locations (Network Analyst)", "description": "Adds fields to the input features that contain the network location of the features. The tool is used to store the network location information as feature attributes to quickly load the features as inputs for a network analysis layer.", "example": {"title": "CalculateLocations example 1 (Python window)", "description": "Calculate Locations for point features using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.CalculateLocations ( \"Analysis/Stores\" , \"Transportation/Streets_ND\" , \"5000 Meters\" ,[[ \"Streets\" , \"SHAPE\" ], [ \"Streets_ND_Junctions\" , \"NONE\" ]])"}, "usage": ["This tool is used to calculate locations fields that can be input to the ", "Add Locations", " tool. It should be used on features that will be used more than once as input to a network analysis layer. Once the locations have been computed, then the ", "Use Network Location Fields instead of geometry", " parameter on the ", "Add Locations", " tool can be used to quickly load the features as network locations. ", "This tool can also be used to re-calculate the network locations that are unlocated in your network analysis layer using a different set of search options. For example, If the stops in your route network analysis layer were initially added with a search tolerance of 500 meters and few of your stops were unlocated, you can select the unlocated stops, for example, using the ", "Select Layer By Attribute", " tool, and re-run this  tool specifying the stops sub layer as the ", "Input features", " with an increased search tolerance.  ", "This tool runs significantly faster if the feature classes used as the network sources in the network dataset have a valid and up-to-date spatial index. "], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input features for which the network locations will be calculated. For line and polygon features, since the network location information is stored in a blob field (specified in the Location ranges field parameter), only geodatabase feature classes are supported. ", "dataType": "Table View"}, {"name": "in_network_dataset", "isInputFile": true, "isOptional": false, "description": "The network dataset used to calculate the locations. If a sublayer of a network analysis layer is used as input features, the parameter must be set to the network dataset referenced by the network analysis layer. ", "dataType": "Network Dataset Layer"}, {"name": "search_tolerance", "isOptional": false, "description": "The search tolerance for locating the input features on the network. Features that are outside the search tolerance are left unlocated. The parameter includes a value and units for the tolerance. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Linear unit"}, {"name": "search_criteria", "isOptional": false, "description": "Specifies which sources in the network dataset will be searched when finding locations and what portions of geometry (also known as snap types) will be used. The parameter value is specified as a list with nested lists. The nested list is made up of two values indicating the name and the snap type for each network source. The snap type is specified using the SHAPE, MIDDLE, END, or NONE keywords. For example, when finding locations, the parameter value [[\"Streets\",\"SHAPE\"],[\"Streets_ND_Junctions\",\"NONE\"]] specifies that the search can locate on the shape of the Streets source but not on the Streets_ND_Junctions source. To specify multiple snap types for a single network source, use the combination of the snap type keywords separated by an underscore. For example, MIDDLE_END specifies that the locations can be snapped to the middle or end of the network source. For geodatabase network datasets, the snap types can be specified for each subtype of the network source. When calculating locations for line or polygon features, only the Shape snap type is used even if other snap types are specified. ", "dataType": "Value Table"}, {"name": "match_type", "isOptional": true, "description": "The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. MATCH_TO_CLOSEST \u2014 Matches the new network locations to the closest network source among all the sources that have a snap type specified in the search criteria. This is the default. PRIORITY \u2014 Matches the new network locations to the first network source having a snap type specified in the search criteria. The sources are searched in the priority order and the searching stops when the location is found within the search tolerance.", "dataType": "Boolean"}, {"name": "source_ID_field", "isOptional": true, "description": "Name of the field to be created or updated with the source ID of the computed network location. A field called SourceID is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "source_OID_field", "isOptional": true, "description": "Name of the field to be created or updated with the source OID of the computed network location. A field called SourceOID is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "position_field", "isOptional": true, "description": "Name of the field to be created or updated with the percent along of the computed network location. A field called PosAlong is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "side_field", "isOptional": true, "description": "Name of the field to be created or updated with the side of edge on which the point feature is located on the computed network location. A field called SideOfEdge is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "snap_X_field", "isOptional": true, "description": "Name of the field to be created or updated with the x-coordinate of the computed network location. A field called SnapX is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "snap_Y_field", "isOptional": true, "description": "Name of the field to be created or updated with the y-coordinate of the computed network location. A field called SnapY is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "distance_field", "isOptional": true, "description": "Name of the field to be created or updated with the distance of the point feature from the computed network location. A field called Distance is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "snap_Z_field", "isOptional": true, "description": " Name of the field to be created or updated with the z-coordinate of the computed network location. A field called SnapZ is created or updated by default. The parameter is not used when calculating locations for line or polygon features. In such cases, use \"#\" as the parameter value. When calculating locations for point features, the parameter is used only when the input network dataset supports connectivity based on z-coordinate values of the network sources. In all other cases, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "location_field", "isOptional": true, "description": " Name of the field to be created or updated with the location ranges of the computed network locations for the line or polygon features. A field called Locations is created or updated by default. The parameter is used only when the calculating locations for line or polygon features. For input point features, use \"#\" as the parameter value. ", "dataType": "Field"}, {"name": "exclude_restricted_elements", "isOptional": true, "description": "This parameter is applicable only when the input features are from the sub layer of a network analysis layer and are not barrier objects. In all other cases, use \"#\" as the parameter value. EXCLUDE \u2014 Specifies that the network locations are only placed on traversable portions of the network. This prevents placing network locations on elements that you can't reach due to restrictions or barriers. Before relocating your network locations using this option, make sure that you have already added all the restriction barriers to the network analysis layer to get expected results. INCLUDE \u2014 Specifies that the network locations are placed on all the elements of the network. The network locations that are relocated with this option may be unreachable during the solve process if they are placed on restricted elements.", "dataType": "Boolean"}, {"name": "search_query", "isOptional": false, "description": " Specifies a query to restrict the search to a subset of the features within a source feature class. This is useful if you don't want to find features that may be unsuited for a network location. For example, if you are loading centroids of polygons and don't want to locate on local roads, you can define a query that searches for major roads only. The parameter value is specified as a list with nested lists. The nested list is made up of two values indicating the name and the SQL expression for all of the network sources. The syntax for the SQL expression differs slightly depending on the type of the network source feature class. For example, if you're querying source feature classes stored in file or ArcSDE geodatabases, shapefiles, or SDC, enclose field names in double quotes: \"CFCC\" . If you're querying source feature classes stored in personal geodatabases, enclose fields in square brackets: [CFCC] . If you don't want to specify a query for a particular source, use \"#\" as the value for the SQL expression or exclude the source name and the SQL expression from the parameter value. If you don't want to specify a query for all of the network sources, use \"#\" as the parameter value. For example, the parameter value [[\"Streets\",\"\\\"CFCC\\\" = 'A15'\"], [\"Streets_ND_Junctions\",\"\"]] specifies an SQL expression for the Streets source feature class and no expression for Streets_ND_Junctions source feature class. Note that the double quotes used to enclose the field name CFCC are escaped using back slash characters to avoid a parsing error from the Python interpreter. ", "dataType": "Value Table"}]},
{"syntax": "AddLocations_na (in_network_analysis_layer, sub_layer, in_table, field_mappings, search_tolerance, {sort_field}, {search_criteria}, {match_type}, {append}, {snap_to_position_along_network}, {snap_offset}, {exclude_restricted_elements}, {search_query})", "name": "Add Locations (Network Analyst)", "description": "Adds network analysis objects to a network analysis layer. The objects are added to specific sublayers such as Stops and Barriers. Objects are input as features or records.", "example": {"title": "AddLocations example 1 (Python window)", "description": "Execute the tool using only the required parameters.", "code": "import arcpy arcpy.env.workspace = \"C:/ArcTutor/Network Analyst/Tutorial/SanFrancisco.gdb\" arcpy.na.AddLocations ( \"Route\" , \"Stops\" , \"Analysis/Hospital\" , \"\" , \"\" )"}, "usage": ["This tool can be run repeatedly to append network analysis objects to the same sublayer. For example, if Stops for a Route Layer come from two feature classes, the tool can be called twice, by using the APPEND option.", "If you want to delete the existing network analysis objects before loading new ones, use the CLEAR option. ", "This tool runs significantly faster if the feature classes used as the network sources in the network dataset have a valid and up-to-date spatial index. ", "When adding moving points, such as GPS coordinates captured from a receiver mounted on a vehicle, use the ", "bearing readings and a bearing tolerance", " to more accurately locate the points on the network."], "parameters": [{"name": "in_network_analysis_layer", "isInputFile": true, "isOptional": false, "description": "Network analysis layer to which the network analysis objects will be added. ", "dataType": "Network Analyst Layer"}, {"name": "sub_layer", "isOptional": false, "description": "The sublayer of the network analysis layer to which the network analysis objects will be added. ", "dataType": "String"}, {"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The feature class or the table that is the source for the new network analysis objects. ", "dataType": "Table View"}, {"name": "field_mappings", "isOptional": false, "description": "Sets the values for the properties of the network analysis objects. Properties can be set to a constant or mapped to a field from the input feature class or table. An NAClassFieldMappings object obtained from NAClassFieldMappings class is used to specify the parameter value. The NAClassFieldMappings object is a collection of NAClassFieldMap objects that allows you to specify the default values or map a field name from the input features for the properties of the network analysis object. If the data you are loading contains network locations or location ranges based on the network dataset used for the analysis, map the network location fields from your input features to the network location properties. Specifying the network location fields in the field mappings is similar to using the Use Network Location fields instead of geometry parameter from the tool dialog box. If you specify the field mapping for any of the network location properties, you need to specify the field mappings for the remaining network location properties to avoid a tool execution error. ", "dataType": "Network Analyst Class FieldMap"}, {"name": "search_tolerance", "isOptional": false, "description": "The search tolerance for locating the input features on the network. Features that are outside the search tolerance are left unlocated. The parameter includes a value and units for the tolerance. The parameter is not used when adding locations to the Line Barriers or Polygon Barriers sublayers. In such cases, use \"#\" as the parameter value. ", "dataType": "Linear unit"}, {"name": "sort_field", "isOptional": true, "description": "A field on which the network analysis objects are sorted as they are added to the network analysis layer. The default is the ObjectID field on the input feature class or the table. ", "dataType": "Field"}, {"name": "search_criteria", "isOptional": false, "description": "Specifies which sources in the network dataset will be searched when finding locations and what portions of geometry (also known as snap types) will be used. The parameter value is specified as a list with nested lists. The nested list is made up of two values indicating the name and the snap type for each network source. The snap type is specified using the SHAPE, MIDDLE, END, or NONE keywords. For example, when finding locations, the parameter value [[\"Streets\",\"SHAPE\"],[\"Streets_ND_Junctions\",\"NONE\"]] specifies that the search can locate on the shape of the Streets source but not on the Streets_ND_Junctions source. To specify multiple snap types for a single network source, use the combination of the snap type keywords separated by an underscore. For example, MIDDLE_END specifies that the locations can be snapped to the middle or end of the network source. When adding line or polygon network locations, only the Shape snap type is used even if other snap types are specified. ", "dataType": "Value Table"}, {"name": "match_type", "isOptional": true, "description": "The parameter is not used when adding locations to the Line Barriers or Polygon Barriers sublayers. In such cases, use \"#\" as the parameter value. MATCH_TO_CLOSEST \u2014 Matches the new network locations to the closest network source among all the sources that have a snap type specified in the search criteria. This is the default. PRIORITY \u2014 Matches the new network locations to the first network source having a snap type specified in the search criteria. The sources are searched in the priority order and the searching stops when the location is found within the search tolerance.", "dataType": "Boolean"}, {"name": "append", "isOptional": true, "description": "Specifies whether or not to append new network analysis objects to existing objects. APPEND \u2014 Adds the new network analysis objects to the existing set of objects in the selected sublayer. CLEAR \u2014 Deletes the existing network analysis objects and replaces them with the new objects. ", "dataType": "Boolean"}, {"name": "snap_to_position_along_network", "isOptional": true, "description": " Specifies that you want to snap the network locations along the network dataset or at some specified offset from the network dataset. The parameter is not used when adding locations to the Line Barriers or Polygon Barriers sublayers. In such cases, use \"#\" as the parameter value. NO_SNAP \u2014 The geometries of the network locations are based on the geometries of the input features. This is useful if you want to use the curb approach which requires that the network locations know which side of the edge they are on. SNAP \u2014 if you have point features, the point will be snapped to the network and you will not be able to use curb approach. This is useful if you don't care how a vehicle approaches a stop. If your input features are lines or polygons, use this parameter value. ", "dataType": "Boolean"}, {"name": "snap_offset", "isOptional": true, "description": "When snapping a point to the network, you can apply an offset distance. An offset distance of zero means the point will be coincident with the network feature (typically a line). To offset the point from the network feature, enter an offset distance. The offset is in relation to the original point location; that is, if the original point was on the left side, its new location will be offset to the left. If it was originally on the right side, its new location will be offset to the right. The parameter is not used when adding locations to the Line Barriers or Polygon Barriers sublayers. In such cases, use \"#\" as the parameter value. ", "dataType": "Linear unit"}, {"name": "exclude_restricted_elements", "isOptional": true, "description": " EXCLUDE \u2014 Specifies that the network locations are only placed on traversable portions of the network. This prevents placing network locations on elements that you can't reach due to restrictions or barriers. Before adding your network locations using this option, make sure that you have already added all the restriction barriers to the input network analysis layer to get expected results. This parameter is not applicable when adding barrier objects. In such cases, use \"#\" as the parameter value. INCLUDE \u2014 Specifies that the network locations are placed on all the elements of the network. The network locations that are added with this option may be unreachable during the solve process if they are placed on restricted elements.", "dataType": "Boolean"}, {"name": "search_query", "isOptional": false, "description": " Specifies a query to restrict the search to a subset of the features within a source feature class. This is useful if you don't want to find features that may be unsuited for a network location. For example, if you are loading centroids of polygons and don't want to locate on local roads, you can define a query that searches for major roads only. The parameter value is specified as a list with nested lists. The nested list is made up of two values indicating the name and the SQL expression for all of the network sources. The syntax for the SQL expression differs slightly depending on the type of the network source feature class. For example, if you're querying source feature classes stored in file or ArcSDE geodatabases, shapefiles, or SDC, enclose field names in double quotes: \"CFCC\" . If you're querying source feature classes stored in personal geodatabases, enclose fields in square brackets: [CFCC] . If you don't want to specify a query for a particular source, use \"#\" as the value for the SQL expression or exclude the source name and the SQL expression from the parameter value. If you don't want to specify a query for all of the network sources, use \"#\" as the parameter value. For example, the parameter value [[\"Streets\",\"\\\"CFCC\\\" = 'A15'\"], [\"Streets_ND_Junctions\",\"\"]] specifies an SQL expression for the Streets source feature class and no expression for Streets_ND_Junctions source feature class. Note that the double quotes used to enclose the field name CFCC are escaped using back slash characters to avoid a parsing error from the Python interpreter. ", "dataType": "Value Table"}]},
{"syntax": "AddFieldToAnalysisLayer_na (in_network_analysis_layer, sub_layer, field_name, field_type, {field_precision}, {field_scale}, {field_length}, {field_alias}, {field_is_nullable})", "name": "Add Field to Analysis Layer (Network Analyst)", "description": "Adds a field to a sublayer of a network analysis layer.", "example": {"title": "AddFieldToAnalysisLayer example 1 (Python window)", "description": "\r\nThe following Python Window script demonstrates how to add a UniqueID field to the Facilities sublayer of the Service Area network analysis layer.\r\n", "code": "import arcpy arcpy.na.AddFieldToAnalysisLayer ( \"Service Area\" , \"Facilities\" , \"UniqueID\" , \"LONG\" )"}, "usage": ["The tool is mostly used along with the ", "Add Locations", " tool to transfer fields from the input features to the sublayers. For example, if you want to transfer a field called UniqueID from your input features into the Facilities sublayer of the Service Area layer, use this tool to first add the UniqueID field to the Facilities sublayer, and then use the field mappings in the ", "Add Locations", " tool to provide input values for the UniqueID field. ", "Fields can be added to any of the sublayers of the network analysis layers."], "parameters": [{"name": "in_network_analysis_layer", "isInputFile": true, "isOptional": false, "description": "Network analysis layer to which the new field will be added. ", "dataType": "Network Analyst Layer"}, {"name": "sub_layer", "isOptional": false, "description": "The sublayer of the network analysis layer to which the new field will be added. ", "dataType": "String"}, {"name": "field_name", "isOptional": false, "description": "The name of the field that will be added to the specified sublayer of the network analysis layer. ", "dataType": "String"}, {"name": "field_type", "isOptional": false, "description": "The field type used in the creation of the new field. LONG \u2014 Numeric values without fractional values within a specific range. TEXT \u2014 Names or other textual qualities. FLOAT \u2014 Numeric values with fractional values within a specific range. DOUBLE \u2014 Numeric values with fractional values within a specific range. SHORT \u2014 Numeric values without fractional values within a specific range; coded values. DATE \u2014 Date and/or Time. BLOB \u2014 Images or other multimedia. ", "dataType": "String"}, {"name": "field_precision", "isOptional": true, "description": "Describes the number of digits that can be stored in the field. All digits are counted no matter what side of the decimal they are on. The parameter value is valid only for numeric field types. ", "dataType": "Long"}, {"name": "field_scale", "isOptional": true, "description": "Sets the number of decimal places stored in a field. This parameter is only used in Float and Double data field types. ", "dataType": "Long"}, {"name": "field_length", "isOptional": true, "description": "The length of the field being added. This sets the maximum number of allowable characters for each record of the field. This option is only applicable on fields of type text or blob. ", "dataType": "Long"}, {"name": "field_alias", "isOptional": true, "description": "The alternate name given to the field name. This name is used to give more descriptive names to cryptic field names. The field alias parameter only applies to geodatabases and coverages. ", "dataType": "String"}, {"name": "field_is_nullable", "isOptional": true, "description": "A geographic feature where there is no associated attribute information. These are different from zero or empty fields and are only supported for fields in a geodatabase. NON_NULLABLE \u2014 The field will not allow null values. NULLABLE \u2014 The field will allow null values. This is the default. ", "dataType": "Boolean"}]},
{"syntax": "TableToNetCDF_md (in_table, fields_to_variables, out_netCDF_file, {fields_to_dimensions})", "name": "Table to NetCDF (Multidimension)", "description": "Converts a table to a netCDF file.", "example": {"title": "TableToNetCDF example 1 (Python window)", "description": "Converts a table to a netCDF file.", "code": "import arcpy arcpy.TableToNetCDF_md ( \"c:/data/netcdfgisdata/rainfall.dbf\" , \"longitude longitude degree_east;latitude latitude degree_north\" , \"c:/output/rain.nc\" , \"station station\" )"}, "usage": ["The default variable name is the same as the field name specified in the ", "Fields to Variables", " parameter.", "The type of variable is the same as the type of field.", "The default dimension name is the same as the field name specified in the ", "Fields to Dimensions", " parameter.", " The size of a dimension is equal to the number of unique values in the respective field.", "If no field is specified as dimension, then a dimension named ", "RecordID", " is created in the output netCDF file.", "String fields may not be used to create dimensions in the netCDF file."], "parameters": [{"name": "in_table", "isInputFile": true, "isOptional": false, "description": "The input table. ", "dataType": "Table View"}, {"name": "fields_to_variables", "isOptional": false, "description": " The field or fields used to create variables in the netCDF file. field\u2014A field in the input table. {variable}\u2014The netCDF variable name. {units}\u2014The units of the data represented by the field.", "dataType": "Value Table"}, {"name": "out_netCDF_file", "isOutputFile": true, "isOptional": false, "description": " The output netCDF file. The filename must have a .nc extension. ", "dataType": "File"}, {"name": "fields_to_dimensions", "isOptional": false, "description": " The field or fields used to create dimensions in the netCDF file. Field\u2014A field in the input table. {dimension}\u2014The netCDF dimension name. {units}\u2014The units of the data represented by the field.", "dataType": "Value Table"}]},
{"syntax": "SelectByDimension_md (in_layer_or_table, {dimension_values}, {value_selection_method})", "name": "Select by Dimension (Multidimension)", "description": "Updates the netCDF layer display or netCDF table view based on the dimension value.", "example": {"title": "SelectByDimension example 1 (Python window)", "description": "Updates the layer based on the dimension value.", "code": "import arcpy arcpy.SelectByDimension_md ( \"rainfall\" ,[[ \"lat\" , 20 ]], \"BY_VALUE\" )"}, "usage": ["Inputs for this tool can be created using the ", "Make NetCDF Feature Layer", ", ", "Make NetCDF Raster Layer", ", or ", "Make NetCDF Table View", " tools.", "If a dimension is not specified, its value is set to the first value. The first value is considered the default value.", "Auxiliary coordinate variables are not listed in the ", "Dimension Values", " parameter drop-down list and cannot be set as the value of this parameter in a script.", "The BY_INDEX option matches the input value with the position or index of a dimension value. The index is 1 based, that is, the position starts at 1.", "This tool updates the input. In ModelBuilder, an output variable appears that can chain the updated input as input to another tool in the model, but the tool does not produce a new output."], "parameters": [{"name": "in_layer_or_table", "isInputFile": true, "isOptional": false, "description": "The input netCDF raster layer, netCDF feature layer, or netCDF table view. ", "dataType": "Raster Layer; Feature Layer; Table View"}, {"name": "dimension_values", "isOptional": false, "description": "A set of dimension\u2013value pairs used to specify a slice of a multidimensional variable. dimension\u2014A netCDF dimension. {value}\u2014A value of the dimension to specify a slice of a multidimensional variable. A drop-down arrow will appear if the number of available values is less than or equal to 200.", "dataType": "Value Table"}, {"name": "value_selection_method", "isOptional": true, "description": " Specifies the dimension value selection method. BY_VALUE \u2014 The input value is matched with the actual dimension value. BY_INDEX \u2014 The input value is matched with the position or index of a dimension value. The index is 0 based, that is, the position starts at 0.", "dataType": "String"}]},
{"syntax": "RasterToNetCDF_md (in_raster, out_netCDF_file, {variable}, {variable_units}, {x_dimension}, {y_dimension}, {band_dimension}, {fields_to_dimensions})", "name": "Raster to NetCDF (Multidimension)", "description": "Converts a raster dataset to a netCDF file.", "example": {"title": "RasterToNetCDF example 1 (Python window)", "description": "Converts a raster dataset to a netCDF file.", "code": "import arcpy arcpy.RasterToNetCDF_md ( \"C:/data/elevation\" , \"c:/output/elev.nc\" , \"elevation\" , \"meter\" , \"x\" , \"y\" ,)"}, "usage": ["The input can be any valid raster dataset or raster catalog.", "The default variable name is the same as the input raster name.", "The output netCDF variable type is either float or integer based on the input raster dataset type.", "The default x dimension and y dimension names are ", "x", " and ", "y", ", respectively.", "Band dimension is only applicable for a multiband raster.", "Field to dimension mapping is only applicable for a raster catalog.", "String fields may not be used to create dimensions in the netCDF file."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster dataset or raster catalog. ", "dataType": "Raster Layer; Raster Catalog"}, {"name": "out_netCDF_file", "isOutputFile": true, "isOptional": false, "description": " The output netCDF file. The filename must have a .nc extension. ", "dataType": "File"}, {"name": "variable", "isOptional": true, "description": "The netCDF variable name that will be used in the output netCDF file. This variable will contain the values of cells in the input raster. ", "dataType": "String"}, {"name": "variable_units", "isOptional": true, "description": "The units of the data contained within the variable. The variable name is specified in the Variable parameter. ", "dataType": "String"}, {"name": "x_dimension", "isOptional": true, "description": "The netCDF dimension name used to specify x, or longitude, coordinates. ", "dataType": "String"}, {"name": "y_dimension", "isOptional": true, "description": "The netCDF dimension name used to specify y, or latitude, coordinates. ", "dataType": "String"}, {"name": "band_dimension", "isOptional": true, "description": "The netCDF dimension name used to specify bands. ", "dataType": "String"}, {"name": "fields_to_dimensions", "isOptional": false, "description": " The field or fields used to create dimensions in the netCDF file. field\u2014A field in the input raster attribute table. {dimension}\u2014The netCDF dimension name. {units}\u2014The units of the data represented by the field.", "dataType": "Value Table"}]},
{"syntax": "MakeNetCDFTableView_md (in_netCDF_file, variable, out_table_view, {row_dimension}, {dimension_values}, {value_selection_method})", "name": "Make NetCDF Table View (Multidimension)", "description": " Makes a table view from a netCDF file.", "example": {"title": "MakeNetCDFTableView example 1 (Python window)", "description": "Creates a table view from a netCDF file.", "code": "import arcpy arcpy.MakeNetCDFTableView_md ( \"C:/data/netcdf/precipmonmean.nc\" , \"precip\" , \"precipmonmeantable\" , \"time\" )"}, "usage": ["Table views are tables stored in memory and are the same as the table view when a table is added to ArcMap.", "ArcCatalog does not display these table views, but they can be used as inputs to other geoprocessing tools in the current ArcGIS session. Once ArcGIS exits, the tables in memory are removed. To save the table view to a permanent table that can be used in later ArcGIS sessions, use the ", "Copy Rows", " tool.", "Table views created in ArcCatalog cannot be used in ArcMap.", "An existing table view will be overwritten if the same table view name is entered.", "The number of records in the table view is the same as the number of unique values in the row dimension. If multiple dimensions are selected, the number of records is the product of the number of unique values in those dimensions.", "The output table contains only one record if no row dimension is specified. The values in the fields represent the slice specified by the dimension-value pairs.", "The type of field is determined by the netCDF variable and dimension type.", "The first value of a non-row dimension is used to create a default table view of a multidimensional variable.", "\r\n The calendar attribute values noleap and 365_day, assigned to the time coordinate variable of the netCDF file, are not honored in ArcGIS."], "parameters": [{"name": "in_netCDF_file", "isInputFile": true, "isOptional": false, "description": " The input netCDF file. ", "dataType": "File"}, {"name": "variable", "isOptional": false, "description": "The netCDF variable, or variables, used to create fields in the table view. ", "dataType": "String"}, {"name": "out_table_view", "isOutputFile": true, "isOptional": false, "description": "The name of the output table view. ", "dataType": "Table View"}, {"name": "row_dimension", "isOptional": false, "description": "The netCDF dimension, or dimensions, used to create fields populated with unique values in the table view. The dimension, or dimensions, set here determine the number of rows in the table view and the fields that will be present. For instance, if stationID is a dimension in the netCDF file and has 10 values, by setting stationID as the dimension to use, 10 rows will be created in the table view. If stationID and time are used and there are 3 time slices, 30 rows will be created in the table view. ", "dataType": "String"}, {"name": "dimension_values", "isOptional": false, "description": "A set of dimension-value pairs used to specify a slice of a multidimensional variable. ", "dataType": "Value Table"}, {"name": "value_selection_method", "isOptional": true, "description": " Specifies the dimension value selection method. BY_VALUE \u2014 The input value is matched with the actual dimension value. BY_INDEX \u2014 The input value is matched with the position or index of a dimension value. The index is 0 based, that is, the position starts at 0.", "dataType": "String"}]},
{"syntax": "MakeNetCDFRasterLayer_md (in_netCDF_file, variable, x_dimension, y_dimension, out_raster_layer, {band_dimension}, {dimension_values}, {value_selection_method})", "name": "Make NetCDF Raster Layer (Multidimension)", "description": "Makes a raster layer from a netCDF file.", "example": {"title": "MakeNetCDFRasterLayer example 1 (Python window)", "description": "Creates a raster layer from a netCDF file.", "code": "import arcpy arcpy.MakeNetCDFRasterLayer_md ( \"C:/data/netcdf/rainfall.nc\" , \"pptx\" , \"lon\" , \"lat\" , \"rainfall\" )"}, "usage": ["To create a netCDF raster layer from a netCDF variable, the spacing between x-coordinates must be equal and the spacing between y-coordinates must be equal. If the coordinates are unequally spaced, create a netCDF feature layer, then interpolate to raster.", "The output raster layer type is either float or integer based on the netCDF variable type.", "The first variable in the netCDF file suitable for creating a raster is selected as the default variable.", "Auxiliary coordinate variables are listed in the ", "X Dimension", " and ", "Y Dimension", " drop-down lists and used during execution if specified. They are not listed in the ", "Dimension Values", " parameter drop-down list and cannot be set as the value of this parameter in a script.", "Band dimension is specified to create a multiband raster.", "The first value of a nonspatial dimension is used to create a default view of a multidimensional variable.", "To save the output layer, right-click the layer in the ArcMap table of contents and click ", "Save As Layer File", ", or use the ", "Save To Layer File", " tool.", "\r\n The calendar attribute values noleap and 365_day, assigned to the time coordinate variable of the netCDF file, are not honored in ArcGIS."], "parameters": [{"name": "in_netCDF_file", "isInputFile": true, "isOptional": false, "description": " The input netCDF file. ", "dataType": "File"}, {"name": "variable", "isOptional": false, "description": "The variable of the netCDF file used to assign cell values to the output raster. This is the variable that will be displayed, such as temperature or rainfall. ", "dataType": "String"}, {"name": "x_dimension", "isOptional": false, "description": "A netCDF dimension used to define the x, or longitude, coordinates of the output layer. ", "dataType": "String"}, {"name": "y_dimension", "isOptional": false, "description": "A netCDF dimension used to define the y, or latitude, coordinates of the output layer. ", "dataType": "String"}, {"name": "out_raster_layer", "isOutputFile": true, "isOptional": false, "description": "The name of the output raster layer. ", "dataType": "Raster Layer"}, {"name": "band_dimension", "isOptional": true, "description": "A netCDF dimension used to create bands in the output raster. Set this dimension if a multiband raster layer is required. For instance, altitude might be set as the band dimension to create a multiband raster where each band represents temperature at that altitude. ", "dataType": "String"}, {"name": "dimension_values", "isOptional": false, "description": " The value (such as 01/30/05) of the dimension (such as Time) or dimensions to use when displaying the variable in the output layer. By default, the first value of the dimension or dimensions will be used. This default value can also be altered on the netCDF tab of the Layer Properties dialog box. ", "dataType": "Value Table"}, {"name": "value_selection_method", "isOptional": true, "description": " Specifies the dimension value selection method. BY_VALUE \u2014 The input value is matched with the actual dimension value. BY_INDEX \u2014 The input value is matched with the position or index of a dimension value. The index is 0 based, that is, the position starts at 0.", "dataType": "String"}]},
{"syntax": "MakeNetCDFFeatureLayer_md (in_netCDF_file, variable, x_variable, y_variable, out_feature_layer, {row_dimension}, {z_variable}, {m_variable}, {dimension_values}, {value_selection_method})", "name": "Make NetCDF Feature Layer (Multidimension)", "description": "Makes a feature layer from a netCDF file.", "example": {"title": "MakeNetCDFFeatureLayer example 1 (Python window)", "description": "Creates a feature layer from a netCDF file.", "code": "import arcpy arcpy.MakeNetCDFFeatureLayer_md ( \"C:/data/netcdf/rainfall.nc\" , \"pptx\" , \"longitude\" , \"latitude\" , \"rainfall\" , \"station\" )"}, "usage": ["For very large netCDF files, there may be a delay between tool completion and the initial draw of the netCDF layer. Subsequent drawing of the layer will not have a delay.", "The netCDF feature layer can be used as input to any geoprocessing tool that accepts a feature class as input.", "The temporary feature layer can be saved as a layer file using the ", "Save To Layer File", " tool or saved as a new feature class using the ", "Copy Features", " tool.", "Layers created in ArcCatalog cannot be used in ArcMap unless they are saved to a layer file using the ", "Save To Layer File", " tool.", "An existing feature layer will be overwritten if the same layer name is specified.", "Auxiliary coordinate variables are listed in the ", "X Variable", " and ", "Y Variable", " drop-down lists and used during execution if specified. They are not listed in the ", "Dimension Values", " parameter drop-down list and cannot be set as the value of this parameter at the command line or in a script.", "The number of features in the layer is the same as the number of unique values in the row dimension. If multiple dimensions are selected, then the number of records is the product of the number of unique values in those dimensions.", "Only one feature is created when a row dimension is not specified.", "The type of field is determined by the netCDF variable type.", "The first value of a non-row dimension is used to create a default view of a multidimensional variable.", "\r\n The calendar attribute values noleap and 365_day, assigned to the time coordinate variable of the netCDF file, are not honored in ArcGIS."], "parameters": [{"name": "in_netCDF_file", "isInputFile": true, "isOptional": false, "description": " The input netCDF file. ", "dataType": "File"}, {"name": "variable", "isOptional": false, "description": "The netCDF variable, or variables, that will be added as fields in the feature attribute table. ", "dataType": "String"}, {"name": "x_variable", "isOptional": false, "description": "A netCDF coordinate variable used to define the x, or longitude, coordinates of the output layer. ", "dataType": "String"}, {"name": "y_variable", "isOptional": false, "description": "A netCDF coordinate variable used to define the y, or latitude, coordinates of the output layer. ", "dataType": "String"}, {"name": "out_feature_layer", "isOutputFile": true, "isOptional": false, "description": "The name of the output feature layer. ", "dataType": "Feature Layer"}, {"name": "row_dimension", "isOptional": false, "description": "The netCDF dimension, or dimensions, used to create features with unique values in the feature layer. The dimension or dimensions set here determine the number of features in the feature layer and the fields that will be presented in the feature layer's attribute table. For instance, if StationID is a dimension in the netCDF file and has 10 values, by setting StationID as the dimension to use, 10 features will be created (10 rows will be created in the feature layer's attribute table). If StationID and time are used, and there are 3 time slices, 30 features will be created (30 rows will be created in the feature layer's attribute table). If you will be animating the netCDF feature layer, it is recommended, for efficiency reasons, to not set time as a row dimension. Time will still be available as a dimension that you can set to animate through, but the attribute table will not store this information. ", "dataType": "String"}, {"name": "z_variable", "isOptional": true, "description": "A netCDF variable used to specify elevation values (z-values) for features. ", "dataType": "String"}, {"name": "m_variable", "isOptional": true, "description": "A netCDF variable used to specify linear measurement values (m-values) for features. ", "dataType": "String"}, {"name": "dimension_values", "isOptional": false, "description": " The value (such as 01/30/05) of the dimension (such as Time) or dimensions to use when displaying the variable in the output layer. By default, the first value of the dimension or dimensions will be used. This default value can also be altered on the netCDF tab of the Layer Properties dialog box. ", "dataType": "Value Table"}, {"name": "value_selection_method", "isOptional": true, "description": " Specifies the dimension value selection method. BY_VALUE \u2014 The input value is matched with the actual dimension value. BY_INDEX \u2014 The input value is matched with the position or index of a dimension value. The index is 0 based, that is, the position starts at 0.", "dataType": "String"}]},
{"syntax": "FeatureToNetCDF_md (in_features, fields_to_variables, out_netCDF_file, {fields_to_dimensions})", "name": "Feature to NetCDF (Multidimension)", "description": "Converts a point feature class to a netCDF file.", "example": {"title": "FeatureToNetCDF example 1 (Python window)", "description": "Converts a feature class to a netCDF file.", "code": "import arcpy arcpy.FeatureToNetCDF_md ( \"c:/data/spotelev.shp\" , [[ \"Shape.X\" , \"lon\" ], \"degree_east\" , [ \"Shape.Y\" , \"lat\" , \"degree_north\" ], [ \"elevation\" , \"elevation\" , \"meter\" ]], \"c:/output/pointelev01.nc\" , \"id\" )"}, "usage": ["The default variable name is the same as the input feature field name specified in the ", "Fields to Variables", " parameter.", "The type of variable is the same as the type of field.", "Special fields ", "Shape.X", " and ", "Shape.Y", " are always available in the ", "Fields to Variables", " drop-down list. They can be used for specifying variable names for x-coordinates and y-coordinates, respectively. If variable names are not specified or ", "Shape.X", " and ", "Shape.Y", " are not added to the list, the x- and y-coordinates are exported with default variable names. The default variable names for ", "Shape.X", " and ", "Shape.Y", " are ", "lon", " and ", "lat", ", respectively, when the feature is in a geographic coordinate system. In all other cases, the default variable names for ", "Shape.X", " and ", "Shape.Y", " are ", "x", " and ", "y", ", respectively.", "Special fields ", "Shape.Z", " and ", "Shape.M", " are available in the ", "Fields to Variables", " drop-down list for features with Z and M values. To export Z and M values, you must add ", "Shape.Z", " and ", "Shape.M", " to the list. The default variable names for ", "Shape.Z", " and ", "Shape.M", " are ", "z", " and ", "m", ", respectively.", "The default dimension name is the same as the input feature field name specified in the ", "Fields to Dimensions", " parameter.", " The size of a dimension is equal to the number of unique values in the respective field.", "If no field is specified as a row dimension, then a dimension ", "RecordID", " is created in the output netCDF file with a size equal to the number of features.", "String fields may not be used to create dimensions in the netCDF file."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class. ", "dataType": "Feature Layer"}, {"name": "fields_to_variables", "isOptional": false, "description": " The field or fields used to create variables in the netCDF file. Four special fields\u2014Shape.X, Shape.Y, Shape.Z, and Shape.M\u2014can be used for exporting x-coordinates or longitude, y-coordinates or latitude, Z values, and M values of input features, respectively. field\u2014A field in the input feature attribute table. {variable}\u2014The netCDF variable name. {units}\u2014The units of the data represented by the field.", "dataType": "Value Table"}, {"name": "out_netCDF_file", "isOutputFile": true, "isOptional": false, "description": " The output netCDF file. The filename must have a .nc extension. ", "dataType": "File"}, {"name": "fields_to_dimensions", "isOptional": false, "description": " The field or fields used to create dimensions in the netCDF file. field\u2014A field in the input feature attribute table. {dimension}\u2014The netCDF dimension name. {units}\u2014The units of the data represented by the field.", "dataType": "Value Table"}]},
{"syntax": "While_mb (in_values, {condition})", "name": "While (ModelBuilder)", "description": "Works exactly like 'while' in any scripting/programming language, executing \"while\" a condition is true or false for the input or set of inputs.\r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model.", "Replaces ", "iterate until a variable is false", " from the Iteration Options in ", "Model Properties", ". "], "parameters": [{"name": "in_values", "isInputFile": true, "isOptional": false, "description": " Input values to check whether to iterate or not. ", "dataType": "Multiple Value"}, {"name": "condition", "isOptional": true, "description": " Choose True or False to set the condition. True \u2014 Iteration will run until all the input values are True. This is the default. False \u2014 Iteration will run until all the input values are False.", "dataType": "String"}]},
{"syntax": "IterateWorkspaces_mb (in_folder, {wildcard}, {workspace_type}, {recursive})", "name": "Iterate Workspaces (ModelBuilder)", "description": " Iterates over workspaces in a folder. \r\n Learn how Iterate Workspaces works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", " Iterate workspace can be limited to iterate over workspace types such as Access, Coverage, FileGDB, Folder, or SDE.", "The tool has two outputs: Output Workspace and Name which could be used as ", "in-line variables", " ", "%Name%", " in other tools.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_folder", "isInputFile": true, "isOptional": false, "description": " Folder which stores the workspace to iterate. ", "dataType": "Folder"}, {"name": "wildcard", "isOptional": true, "description": " Combination of * and characters that help to limit the results. The asterisk is the same as saying ALL. If no wildcard is specified, all inputs will be returned. For example, it can be used to restrict Iteration over input names starting with a certain character or word (e.g., A* or Ari* or Land* ,and so on). ", "dataType": "String"}, {"name": "workspace_type", "isOptional": true, "description": " Choose the Workspace Type, such as Access, Coverage, FileGDB, Folder, or SDE. ACCESS \u2014 Only Access database will be the output. COVERAGE \u2014 Only Coverage will be the output. FILEGDB \u2014 Only File Geodatabase will be the output. FOLDER \u2014 Only Folder will be the output. SDE \u2014 Only SDE database will be the output.", "dataType": "String"}, {"name": "recursive", "isOptional": true, "description": " Determines if subfolders in the main folder will be iterated through recursively. Checked\u2014Will iterate through all subfolders. Unchecked\u2014Set as Default. Will not iterate through all subfolders.", "dataType": "Boolean"}]},
{"syntax": "IterateTables_mb (in_workspace, {wildcard}, {table_type}, {recursive})", "name": "Iterate Tables (ModelBuilder)", "description": "Iterates over tables in a workspace. \r\n Learn how Iterate Tables works in ModelBuilder \r\n", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool is intended for use in ModelBuilder and not in Python scripting.", " Tables in a folder can be restricted to iterate on table types such as DBASE and INFO.", " The first input table is used as a template for the output. This allows for selection of fields in subsequent tools.", "The tool has two outputs: Output Table and Name, which could be used as ", "in-line variable", " (e.g. ", "%Name%", ") in other tools.", "If an Iterator is added to a model, all tools in the model iterate for each value in the iterator. If you do not want to run each tool in the model for each iterated value, create a ", "sub-model/model within a model/nested model", " that contains only the iterator and add it as a model tool into the main model."], "parameters": [{"name": "in_workspace", "isInputFile": true, "isOptional": false, "description": " Workspace containing the tables to iterate. ", "dataType": "Workspace"}, {"name": "wildcard", "isOptional": true, "description": " Combination of * and characters that help to limit the results. The asterisk is the same as saying ALL. If no wildcard is specified, all inputs will be returned. For example, it can be used to restrict Iteration over input names starting with a certain character or word (e.g., A* or Ari* or Land* ,and so on). ", "dataType": "String"}, {"name": "table_type", "isOptional": true, "description": "For folder workspaces (shapefiles and coverages), choose the table type (DBASE or INFO). ", "dataType": "String"}, {"name": "recursive", "isOptional": true, "description": " Determines if subfolders in the main folder will be iterated through recursively. Checked\u2014Will iterate through all subfolders. Unchecked\u2014Set as Default. Will not iterate through all subfolders.", "dataType": "Boolean"}]},
{"syntax": "ExportWebMap_server (Web_Map_as_JSON, Output_File, {Format}, {Layout_Templates_Folder}, {Layout_Template})", "name": "Export Web Map (Server)", "description": "This tool takes the state of a web application (for example, included services, layer visibility settings, and client-side graphics) and returns a printable page layout or basic map of the specified area of interest. The input for  Export Web Map  is a piece of text in JavaScript object notation (JSON) format describing the layers, graphics, and other settings in the web map. The JSON must be structured according to the  the ExportWebMap specification  topic found  in the ArcGIS Help. This tool is shipped with ArcGIS Server to support web services for printing, including the preconfigured service named PrintingTools. The ArcGIS web APIs for JavaScript, Flex, and Silverlight use the Printing Tools service to generate images for effortless map printing.  ", "example": {"title": null, "description": null, "code": ""}, "usage": [], "parameters": [{"name": "Web_Map_as_JSON", "isOptional": false, "description": "A JSON representation of the state of the map to be exported as it appears in the web application. See the ExportWebMap specification to understand how this text should be formatted. The ArcGIS web APIs (for JavaScript, Flex, Silverlight, and so on) allow developers to easily get this JSON string from the map. ", "dataType": "String"}, {"name": "Output_File", "isOutputFile": true, "isOptional": false, "description": " Output file name. The extension of the file depends on the Format parameter. ", "dataType": "File"}, {"name": "Format", "isOptional": true, "description": " The format in which the map image for printing will be delivered. The following strings are accepted. For example: PNG8 (default if the parameter is left blank ) PDF PNG32 JPG GIF EPS SVG SVGZ", "dataType": "String"}, {"name": "Layout_Templates_Folder", "isOptional": true, "description": " Full path to the folder where map documents ( .mxd 's) to be used as Layout Templates are located. The default location is <install_directory>\\Templates\\ExportWebMapTemplates . ", "dataType": "Folder"}, {"name": "Layout_Template", "isOptional": true, "description": "Either a name of a template from the list or the keyword MAP_ONLY. When MAP_ONLY is chosen or an empty string is passed in, the output map does not contain any page layout surroundings (for example, title, legends, scale bar, and so forth). ", "dataType": "String"}]},
{"syntax": "ImportMapServerCache_server (input_service, source_cache_type, {source_cache_dataset}, {source_tile_package}, {upload_data_to_server}, {scales}, {num_of_caching_service_instances}, {import_extent}, {area_of_interest})", "name": "Import Map Server Cache (Server)", "description": " Imports tiles from a folder on disk into a map or image service cache. The source folder can be a child of a registered server cache directory, a folder into which a cache has been previously exported, or a tile package ( .tpk ).The target service must have the same tiling scheme and the storage format as the source cache.", "example": {"title": null, "description": "Import a CACHE_DATASET to a cached service for the default number of scales", "code": "# Name: ImportMapServerCache.py # Description: The following stand-alone script demonstrates how to import map #               server cache from a source directory with Tile Package to an existing service for  #               the default number of scales specified using an AOI by uploading data to remote server # To Import cache tiles for the scales specified for given feature class # Requirements: os, sys, time and traceback modules # Author: ESRI # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapServer\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName sourceCacheType = \"TILE_PACKAGE\" sourceCacheDataset = \"\" sourceTilePackage = \"C: \\\\ data \\\\ destination_folder \\\\ TPK \\\\ Rainfall.tpk\" uploadDataToServer = \"UPLOAD_DATA\" scales = [ 500000 , 250000 , 125000 , 64000 ] numOfCachingServiceInstances = \"2\" cacheDir = \"c: \\\\ arcgisserver \\\\ arcgiscache\" areaOfInterest = \"C:/data/101/Portland/Portland_Metro.shp\" importExtents = \"\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) # use \"scales[0]\",\"scales[-1]\",\"scales[0:3]\" try : starttime = time.clock () result = arcpy.ImportMapServerCache_server ( inputService , sourceCacheType , sourceCacheDataset , sourceTilePackage , uploadDataToServer , scales , numOfCachingServiceInstances , areaOfInterest , importExtents ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Imported Map server Cache Tiles successfully for \" + serviceName \" from \" + sourceTilePackage + \" to \" + cacheDir + \" \\n  using \" + areaOfInterest \" in \" + str ( elapsedtime ) + \" sec  \\n  on  \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 2  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) report.close () print \"Imported Map server Cache Tiles for the given feature class\""}, "usage": [], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service into which tiles will be imported. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "source_cache_type", "isOptional": false, "description": "Choose to import cache from a CACHE_DATASET or TILE_PACKAGE to a cached map or image service running on the server. CACHE_DATASET \u2014 Map or image service cache that is generated using ArcGIS Server. Usable in ArcMap and by ArcGIS Server map or image services. TILE_PACKAGE \u2014 A single compressed file where the cache dataset is added as a layer and consolidated so that it can be shared easily. Usable in ArcMap, as well as in ArcGIS Runtime and ArcGIS Mobile applications.", "dataType": "String"}, {"name": "source_cache_dataset", "isOptional": true, "description": "The path to the cache folder matching the data frame name. You do not have to enter a registered server cache directory; in fact, most of the time you'll enter a location on disk where tiles have been previously exported. This location should be accessible to the ArcGIS Server account. If the ArcGIS Server account cannot be granted access to this location set the upload_data_to_server to UPLOAD_DATA. ", "dataType": "Raster Dataset"}, {"name": "source_tile_package", "isOptional": true, "description": " The path to the tile package ( .tpk ) that will be imported. This location should be accessible to the ArcGIS Server account. When importing a tile package file to a cached map/image service, the upload_data_to_server parameter is ignored as it will be automatically be set to UPLOAD_DATA. ", "dataType": "File"}, {"name": "upload_data_to_server", "isOptional": true, "description": "Set this parameter to UPLOAD_DATA if the ArcGIS Server account does not have read access to the source cache. The tool will upload the source cache to the ArcGIS Server uploads directory before moving it to the ArcGIS Server cache directory. UPLOAD_DATA \u2014 Tiles are placed in the server uploads directory, then moved to the server cache directory. This is by default enabled when storage_format_type is TILE_PACKAGE. DO_NOT_UPLOAD \u2014 Tiles are imported directly into the server cache directory. The ArcGIS Server account must have read access to the source cache.", "dataType": "Boolean"}, {"name": "scales", "isOptional": false, "description": " A list of scale levels at which tiles will be imported. ", "dataType": "Double"}, {"name": "num_of_caching_service_instances", "isOptional": true, "description": "The total number of instances of the System/CachingTools service that you want to dedicate toward running this tool. You can increase the maximum number of instances per machine of the System/CachingTools service using the Service Editor window available through an administrative connection to ArcGIS Server. Ensure your server machines can support the chosen number of instances. ", "dataType": "Long"}, {"name": "import_extent", "isOptional": true, "description": " A rectangular extent defining the tiles to be imported into the cache. By default the extent is set to the full extent of the map service into which you are importing. Note the optional parameter on this tool Area Of Interest that allows you to spatially constrain the tiles imported using an irregular shape. If values are provided for both parameters, the Area Of Interest takes precedence over Import Extent . ", "dataType": "Extent"}, {"name": "area_of_interest", "isOptional": true, "description": " An area of interest polygon that spatially constrains where tiles are imported into the cache. This can be a feature class, or it can be a feature that you interactively define in ArcMap. This parameter is useful if you want to import tiles for irregularly shaped areas, as the tool clips the cache dataset which intersects the polygon at pixel resolution and then import it to the service cache directory. If you do not provide a value for this parameter, the value of the Import Extent parameter will be used. The default is to use the full extent of the map. ", "dataType": "Feature Set"}]},
{"syntax": "ExportMapServerCache_server (input_service, target_cache_path, export_cache_type, copy_data_from_server, storage_format_type, scales, {num_of_caching_service_instances}, {area_of_interest}, {export_extent})", "name": "Export Map Server Cache (Server)", "description": " Exports tiles from a map or image service cache as a cache dataset or as a tile package to a folder on disk. The tiles can either  be imported into other caches or they can be accessed from  ArcGIS for Desktop  or mobile devices as a raster dataset, independently from their parent service.", "example": {"title": null, "description": "Export cache tiles for a feature class while changing the storage format from EXPLODED to COMPACT", "code": "# Name: ExportMapServerCache.py # Description: The following stand-alone script demonstrates how to export cache #               as TILE_PACKAGE for default number of scales of a service, to a  #               TARGET_CACHE_PATH which is inaccessible to server instances using #               COPY_DATA_FROM_SERVER # Requirements: os, sys, time and traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapServer\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName targetCachePath = \"C:/temp/usa\" exportCacheType = \"TILE_PACKAGE\" copyDataFromServer = \"COPY_DATA\" storageFormat = \"COMPACT\" scaleValues = [ 500000 , 250000 , 125000 , 64000 ] numOfCachingServiceInstances = \"2\" exportExtents = \"\" areaOfInterest = \"\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) # use \"scaleValues[0]\",\"scaleValues[-1]\",\"scaleValues[0:3]\" try : starttime = time.clock () result = arcpy.ExportMapServerCache_server ( inputService , targetCachePath , exportCacheType , copyDataFromServer , storageFormat , scales , numOfCachingServiceInstances , areaOfInterest , exportExtents ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Exported cache successfully for mapservice \" + serviceName + \" to \" targetCachePath + \" in \" + str ( elapsedtime ) + \" sec  \\n  on\" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 1  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) print \"Exported Map server Cache \" report.close ()"}, "usage": ["The ArcGIS Server account must have write access to the target cache folder. If the ArcGIS Server account  cannot be granted write access to the target cache folder, but the ", "ArcGIS for Desktop", " client has write access to it,   then  choose the ", "Copy data from server", "  (", "copy_data_from_server", ") parameter. "], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service whose cache tiles will be exported. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "target_cache_path", "isOptional": false, "description": " The folder into which the cache will be exported. This folder does not have to be a registered server cache directory. The ArcGIS Server account must have write access to the target cache folder. If the server account cannot be granted write access to the destination folder but the ArcGIS for Desktop client has write access to it, then choose the Copy data from server parameter. ", "dataType": "Folder"}, {"name": "export_cache_type", "isOptional": false, "description": "Choose to export cache as a Cache Dataset or a Tile Package. Tile packages are suitable for ArcGIS Runtime and ArcGIS Mobile deployments. CACHE_DATASET \u2014 Map or image service cache that is generated using ArcGIS Server. Usable in ArcMap and by ArcGIS Server map or image services. This is the default. TILE_PACKAGE \u2014 A single compressed file where the cache dataset is added as a layer and consolidated so that it can be shared easily. Usable in ArcGIS for Desktop , as well as in ArcGIS Runtime and mobile applications.", "dataType": "String"}, {"name": "copy_data_from_server", "isOptional": false, "description": "Set this option to COPY_DATA if the ArcGIS Server account cannot be granted write access to the target folder and the ArcGIS for Desktop client has write access to it. The software exports the tiles in the server output directory before moving them to the target folder. COPY_DATA \u2014 Tiles are placed in the server output directory, then moved to the target folder. The ArcGIS for Desktop client must have write access to the target folder. DO_NOT_COPY \u2014 Tiles are exported directly into the target folder. The ArcGIS Server account must have write access to the target folder. This is the default.", "dataType": "Boolean"}, {"name": "storage_format_type", "isOptional": false, "description": "The storage format of the exported cache. COMPACT \u2014 Tiles are grouped in bundle files to save space on disk and allow for faster copying of caches. This is the default, if Export cache type ( export_cache_type in Python) is Tile package. EXPLODED \u2014 Each tile is stored as an individual file (in the way caches were always stored prior to ArcGIS Server 10).", "dataType": "String"}, {"name": "scales", "isOptional": false, "description": "A list of scale levels at which tiles will be exported. ", "dataType": "Double"}, {"name": "num_of_caching_service_instances", "isOptional": true, "description": "The total number of instances of the System/CachingTools service that you want to dedicate toward running this tool. You can increase the maximum number of instances per machine of the System/CachingTools service using the Service Editor window available through an administrative connection to ArcGIS Server. Ensure your server machines can support the chosen number of instances. ", "dataType": "Long"}, {"name": "area_of_interest", "isOptional": true, "description": " An area of interest (polygon) that spatially constrains where tiles are exported from the cache. This can be a feature class, or it can be a feature that you interactively define in ArcMap. This parameter is useful if you want to export irregularly shaped areas, as the tool clips the cache dataset at pixel resolution. If you do not specify an area of interest, the full extent of the map is exported. ", "dataType": "Feature Set"}, {"name": "export_extent", "isOptional": true, "description": " A rectangular extent defining the tiles to be exported. By default the extent is set to the full extent of the map service into which you are importing. Note the optional parameter on this tool Area of Interest that allows you to alternatively import using a polygon. It is recommended not to provide values for both the parameters for a job. If values are provided for both parameters, the Area of Interest takes precedence over Import Extent ", "dataType": "Extent"}]},
{"syntax": "ConvertMapServerCacheStorageFormat_server (input_service, {num_of_caching_service_instances})", "name": "Convert Map Server Cache Storage Format (Server)", "description": " Converts the storage of a map or image service cache between the exploded format and the compact format. The tool converts the format in place, meaning it does not make a copy of the existing format of the cache. Instead, it creates the new format of the cache in the same cache folder and deletes the old format. Make a backup of your cache before running this tool if you think you might want to go back to the old format.", "example": {"title": null, "description": "Converts the cache storage format of a service", "code": "# Name: ConvertMapServerCacheStorageFormat.py # Description: The following stand-alone script demonstrates how to convert map #               server cache storage format to the alteranate storage format # Requirements: os, sys, time, traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , string , datetime , traceback # Set environment settings env.workspace = \"C:/data\" # Set local variables for mapservice properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapServer\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName numOfCachingServiceInstances = \"2\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) # One can leave the variable for storage format marked \"COMPACT\" to \"\" (default) # Execute ConvertMapServerCacheFormat try : starttime = time.clock () result = arcpy.ConvertMapServerCacheStorageFormat_server ( inputService , numOfCachingServiceInstances ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Converted Map Server Cache Storage format for \" + serviceName + \" in \" str ( elapsedtime ) + \" sec  \\n  on\" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 1  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) report.close () print \"Converted Map Server Cache Storage format \""}, "usage": ["\r\nTo use this tool, specify the map or image service whose cache you want to convert. The tool detects the  current storage format and uses that information to automatically set the target format to the opposite format. You can choose how many service instances to dedicate to cache conversion by specifing a value in the  ", "Number of caching service instances", " (", "num_of_caching_service_instances", " in Python) parameter."], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service whose cache format you want to convert. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "num_of_caching_service_instances", "isOptional": true, "description": "The total number of instances of the System/CachingTools service that you want to dedicate toward running this tool. You can increase the maximum number of instances per machine of the System/CachingTools service using the Service Editor window available through an administrative connection to ArcGIS Server. Ensure your server machines can support the chosen number of instances. ", "dataType": "Long"}]},
{"syntax": "SendEmailWithZipFileAttachment_server (To, From, Subject, Text, Zip_File, Max_File_Size__MB_, SMTP_Email_Server, {User}, {Password})", "name": "Send Email With Zip File Attachment (Server)", "description": " Emails a file to an email address using an SMTP email server.", "example": {"title": null, "description": null, "code": ""}, "usage": [" Ask your system administrator for details about an SMTP email server that you can use. "], "parameters": [{"name": "To", "isOptional": false, "description": " The email address of the recipient. ", "dataType": "String"}, {"name": "From", "isOptional": false, "description": " The email address of the sender. ", "dataType": "String"}, {"name": "Subject", "isOptional": false, "description": " The text in the subject line of the email. ", "dataType": "String"}, {"name": "Text", "isOptional": false, "description": " The body text of the email. ", "dataType": "String"}, {"name": "Zip_File", "isOptional": false, "description": " The file to be attached to the email. ", "dataType": "File"}, {"name": "Max_File_Size__MB_", "isOptional": false, "description": " The maximum allowable size of an attachment. If you don't know what to use for Max File Size, check the attachment size limit of your SMTP mail server and the recipient email provider. ", "dataType": "Long"}, {"name": "SMTP_Email_Server", "isOptional": false, "description": " The SMTP email server that will deliver the email. ", "dataType": "String"}, {"name": "User", "isOptional": true, "description": " The user which will log in to the SMTP email server. ", "dataType": "String"}, {"name": "Password", "isOptional": true, "description": " The user password used to connect to the SMTP email server (if necessary). ", "dataType": "String"}]},
{"syntax": "ExtractDataTask_server (Layers_to_Clip, Area_of_Interest, Feature_Format, Raster_Format, Output_Zip_File)", "name": "Extract Data Task (Server)", "description": " Extracts the selected  layers in the specified area of interest to the selected formats and spatial reference, then returns all the data in a zip file.", "example": {"title": null, "description": null, "code": ""}, "usage": ["The drop-down for the ", "Feature Format", " and ", "Raster Format", " parameters contains a list of default formats. These default formats are set from the ", "Value List", " filter on the ", "Parameters", " tab of the model ", "Properties", ".  ", "You can add, modify, or remove the list of formats for either the ", "Feature Format", " parameter or the ", "Raster Format", " parameter. You will have to copy this model to your own custom toolbox before making any changes.", "Internally, this tool uses the ", "Export to CAD", " tool to convert data to the ", ".dgn", ", ", ".dwg", ", and ", ".dxf", " CAD formats. Each of these formats is in the default list and exports to the latest version of that file type. To change the version, edit the \"Short Name\" in the values list. For example, to output DXF R2005 rather than DXF R2007, change the string from ", "Autodesk AutoCAD - DXF_R2007 - .dxf", " to  ", "AutoCAD - DXF_R2005 - .dxf", "."], "parameters": [{"name": "Layers_to_Clip", "isOptional": false, "description": " The layers to be clipped. Layers must be either feature or raster layers. ", "dataType": "Layer"}, {"name": "Area_of_Interest", "isOptional": false, "description": " One or more polygons by which the layers will be clipped. ", "dataType": "Feature Set"}, {"name": "Feature_Format", "isOptional": false, "description": " The format in which the output features will be delivered. The string provided should be formatted as follows: For example: Internally, this tool uses the Export to CAD tool to convert data to the .dgn , .dwg , and .dxf CAD formats. The list of short names supported includes DGN_V8, DWG_R14, DWG_R2000, DWG_R2004, DWG_R2005, DWG_R2006, DWG_R2007, DWG_R2010, DXF_R14, DXF_R2000, DXF_R2004, DXF_R2005, DXF_R2006, DXF_R2007, and DXF_R2010. Exporting to nondefault formats is supported using the Quick Export tool and that requires the Data Interoperability extension be installed. The Data Interoperability extension is not installed by default with ArcGIS for Desktop or ArcGIS for Server . Name or format - Short Name - extension (if any) File Geodatabase - GDB - .gdb Shapefile - SHP - .shp Autodesk AutoCAD - DXF_R2007 - .dxf Autodesk AutoCAD - DWG_R2007 - .dwg Bentley Microstation Design (V8) - DGN_V8 - .dgn", "dataType": "String"}, {"name": "Raster_Format", "isOptional": false, "description": "The format in which the output raster datasets will be delivered. The string provided should be formatted as follows: Any of the following strings will work: Some of the above raster formats have limitations and not all data can be converted to the format. For a list of formats and their limitations, see Supported raster dataset file formats . Name of format - Short Name - extension (if any) Esri GRID - GRID File Geodatabase - GDB - .gdb ERDAS IMAGINE - IMG - .img Tagged Image File Format - TIFF - .tif Portable Network Graphics - PNG - .png Graphic Interchange Format - GIF - .gif Joint Photographics Experts Group - JPEG - .jpg Joint Photographics Experts Group - JPEG - .jp2 Bitmap - BMP - .bmp", "dataType": "String"}, {"name": "Output_Zip_File", "isOutputFile": true, "isOptional": false, "description": " The zip file that will contain the extracted data. ", "dataType": "File"}]},
{"syntax": "ExtractDataAndEmailTask_server (Layers_to_Clip, Area_of_Interest, Feature_Format, Raster_Format, To)", "name": "Extract Data and Email Task (Server)", "description": " Extracts the data in the specified layers and area of interest to the selected format and spatial reference, zips the data, and emails it to the specified address. This tool can be used to create a Data Extraction geoprocessing service.", "example": {"title": null, "description": null, "code": ""}, "usage": ["This tool will not function correctly unless it is edited and the email server is configured. To configure the email server settings, you will have to copy this model to your own custom toolbox before editing the model.  ", "The drop-down menu for the ", "Feature Format", " and ", "Raster Format", " parameters contains a list of default formats. These default formats are set from the ", "Value List", " filter on the ", "Parameters", " tab of the model ", "Properties", ".  ", "You can add, modify, or remove the list of formats for either the ", "Feature Format", " parameter or the ", "Raster Format", " parameter. You will have to copy this model to your own custom toolbox before making any changes.", " Internally, the ", "Extract Data Task", " tool uses the ", "Export to CAD", " tool to convert data to the ", ".dgn", ", ", ".dwg", ", and ", ".dxf", " CAD formats. Each of these formats is in the default list and exports to the latest version of that file type. To change the version, edit the \"Short Name\" in the values list. For example, to use DXF R2005 rather than DXF R2007, change the Short Name to DXF_R2005, do the following:", "The list of short names can be found in the ", "Export to CAD", " tool.", " By default, all supported raster formats are in the drop-down list of raster formats. ", "There are limitations to some of the formats such that the software will not be able to convert to the format specified. For more details on the limitations of the various raster formats, see the help topic ", "Supported raster dataset file formats", "."], "parameters": [{"name": "Layers_to_Clip", "isOptional": false, "description": " The layers to be clipped. Layers must be either feature or raster layers. ", "dataType": "Layer"}, {"name": "Area_of_Interest", "isOptional": false, "description": " One or more polygons by which the layers will be clipped. ", "dataType": "Feature Set"}, {"name": "Feature_Format", "isOptional": false, "description": " The format in which the output features will be delivered. The string provided should be formatted as follows: For example: Internally, this tool uses the Export to CAD tool to convert data to the .dgn , .dwg , and .dxf CAD formats. The list of short names supported includes DGN_V8, DWG_R14, DWG_R2000, DWG_R2004, DWG_R2005, DWG_R2006, DWG_R2007, DWG_R2010, DXF_R14, DXF_R2000, DXF_R2004, DXF_R2005, DXF_R2006, DXF_R2007, and DXF_R2010. Exporting to nondefault formats is supported using the Quick Export tool and that requires the Data Interoperability extension be installed. The Data Interoperability extension is not installed by default with ArcGIS for Desktop or ArcGIS for Server . Name or format - Short Name - extension (if any) File Geodatabase - GDB - .gdb Shapefile - SHP - .shp Autodesk AutoCAD - DXF_R2007 - .dxf Autodesk AutoCAD - DWG_R2007 - .dwg Bentley Microstation Design (V8) - DGN_V8 - .dgn", "dataType": "String"}, {"name": "Raster_Format", "isOptional": false, "description": "The format in which the output raster datasets will be delivered. The string provided should be formatted as follows: Any of the following strings will work: Some of the above raster formats have limitations and not all data can be converted to the format. For a list of formats and their limitations, see Supported raster dataset file formats . Name of format - Short Name - extension (if any) Esri GRID - GRID File Geodatabase - GDB - .gdb ERDAS IMAGINE - IMG - .img Tagged Image File Format - TIFF - .tif Portable Network Graphics - PNG - .png Graphic Interchange Format - GIF - .gif Joint Photographics Experts Group - JPEG - .jpg Joint Photographics Experts Group - JPEG - .jp2 Bitmap - BMP - .bmp", "dataType": "String"}, {"name": "To", "isOptional": false, "description": " The email address of the recipient. This tool will be able to email to this address if and only if the SMTP server has been configured within this model. ", "dataType": "String"}]},
{"syntax": "ExtractData_server (Layers_to_Clip, Area_of_Interest, Feature_Format, Raster_Format, Spatial_Reference, {Custom_Spatial_Reference_Folder}, Output_Zip_File)", "name": "Extract Data (Server)", "description": " Extracts selected  layers in the specified area of interest to a specific format and spatial reference.   The extracted data is then written to a zip file.", "example": {"title": null, "description": null, "code": ""}, "usage": ["A custom spatial reference name can be entered into the ", "Spatial Reference", " parameter provided that the folder containing the custom spatial reference is specified in the ", "Custom Spatial Reference Folder", " parameter."], "parameters": [{"name": "Layers_to_Clip", "isOptional": false, "description": " The layers to be clipped. Layers must be either feature or raster layers. ", "dataType": "Layer"}, {"name": "Area_of_Interest", "isOptional": false, "description": " One or more polygons by which the layers will be clipped. ", "dataType": "Feature Set"}, {"name": "Feature_Format", "isOptional": false, "description": " The format in which the output features will be delivered. The string provided should be formatted as follows: For example: Internally, this tool uses the Export to CAD tool to convert data to the .dgn , .dwg , and .dxf CAD formats. The list of short names supported includes DGN_V8, DWG_R14, DWG_R2000, DWG_R2004, DWG_R2005, DWG_R2006, DWG_R2007, DWG_R2010, DXF_R14, DXF_R2000, DXF_R2004, DXF_R2005, DXF_R2006, DXF_R2007, and DXF_R2010. Exporting to nondefault formats is supported using the Quick Export tool and that requires the Data Interoperability extension be installed. The Data Interoperability extension is not installed by default with ArcGIS for Desktop or ArcGIS for Server . Name or format - Short Name - extension (if any) File Geodatabase - GDB - .gdb Shapefile - SHP - .shp Autodesk AutoCAD - DXF_R2007 - .dxf Autodesk AutoCAD - DWG_R2007 - .dwg Bentley Microstation Design (V8) - DGN_V8 - .dgn", "dataType": "String"}, {"name": "Raster_Format", "isOptional": false, "description": "The format in which the output raster datasets will be delivered. The string provided should be formatted as follows: Any of the following strings will work: Some of the above raster formats have limitations and not all data can be converted to the format. For a list of formats and their limitations, see Supported raster dataset file formats . Name of format - Short Name - extension (if any) Esri GRID - GRID File Geodatabase - GDB - .gdb ERDAS IMAGINE - IMG - .img Tagged Image File Format - TIFF - .tif Portable Network Graphics - PNG - .png Graphic Interchange Format - GIF - .gif Joint Photographics Experts Group - JPEG - .jpg Joint Photographics Experts Group - JPEG - .jp2 Bitmap - BMP - .bmp", "dataType": "String"}, {"name": "Spatial_Reference", "isOptional": false, "description": "The spatial reference of the output data delivered by the tool. For standard Esri spatial references, the name you provide here should be the name of the desired coordinate system. This name corresponds to the name of the spatial reference's projection file. Alternatively, you can use the Well Known ID (WKID) of the coordinate system. For example: For any custom projection, the name specified should be the name of the custom projection file (without extension). The location of the custom projection files should be specified in the Custom_Spatial_Reference_Folder parameter. Sinusoidal (world) WGS 1984 Web Mercator NAD 1983 HARN StatePlane Oregon North FIPS 3601 WGS 1984 UTM Zone 11N 102003 54001 If you want the output to have the same coordinate system as the input, then use the string Same As Input .", "dataType": "String"}, {"name": "Custom_Spatial_Reference_Folder", "isOptional": true, "description": " The location of any custom projection file or files referenced in the Spatial Reference parameter. This is only necessary if the custom projection file is not in the default installation Coordinate System folder. ", "dataType": "Folder"}, {"name": "Output_Zip_File", "isOutputFile": true, "isOptional": false, "description": " The zip file that will contain the extracted data. ", "dataType": "File"}]},
{"syntax": "UpdateMapServerCache_server (server_name, object_name, data_frame, layer, {constraining_extent}, levels, update_mode, {thread_count}, {antialiasing}, {update_feature_class}, {ignore_status})", "name": "Update Map Server Cache (Server)", "description": "Updates an existing Map Service cache to replace missing tiles, overwrite outdated tiles, or add new tiles in new areas or, in the case of a multi-layer cache, from additional layers.", "example": {"title": null, "description": null, "code": "# Script Name: Update Fused Map Server Cache # Description: Updates a fused map server cache # Uncomment sys.argv[] lines to accept arguments from the command line. # Import standard library modules import sys , string , os , arcgisscripting # Create the Geoprocessor object gp = arcgisscripting.create () # Set the SOM server name # Example: \"mySOM\" server_name = \"mySOM\" #server_name = sys.argv[1] # Set the object_name # Example: \"MyServiceFolder/MyService\" object_name = \"MyServiceFolder/MyService\" #object_name = sys.argv[2] # Set the data frame # Example: \"Layers\" data_frame = \"Layers\" #data_frame = sys.argv[3] # Set the layers to cache. # Example: \"My First Layer;My Second Layer;My Third Layer\" layers = \"My First Layer;My Second Layer;My Third Layer\" #layers = sys.argv[4] # Set the extent to update in the cache. # Example: \"8 50 10 52\" constraining_extent = \"8 50 10 52\" #constraining_extent = sys.argv[5] # Set the scale levels for the cache. # Example: \"2000000;500000;250000\" scales = \"2000000;500000;250000\" #scales = sys.argv[6] # Set the update mode. # Example: \"Recreate Empty Tiles\" update_mode = \"Recreate All Tiles\" #update_mode = sys.argv[7] # Set number of instances to use while updating the cache # Example: \"3\" thread_count = \"3\" #thread_count = sys.argv[8] # Set antialiasing mode # Example: \"NONE\" antialiasing = \"ANTIALIASING\" #antialiasing = sys.argv[9] try : print 'Starting Cache Update' gp.UpdateMapServerCache ( server_name , object_name , data_frame , layers , constraining_extent , scales , update_mode , thread_count , antialiasing ) print 'Finished Cache Update' except : gp.AddMessage ( gp.GetMessages ( 2 )) print gp.GetMessages ( 2 )"}, "usage": ["Use Recreate Empty Tiles mode to add tiles in an extent not previously cached.", "Use Recreate All Tiles to update outdated tiles.", "Before running this tool, configure the Map Service to use as many instances as possible. This will dramatically decrease cache update time.", "This tool does not accept any Environment Setting that have been specified."], "parameters": [{"name": "server_name", "isOptional": false, "description": "The host name of the ArcGIS Server to use to update the cache. ", "dataType": "String"}, {"name": "object_name", "isOptional": false, "description": "The name of the Map Service to use to update the cache. ", "dataType": "String"}, {"name": "data_frame", "isOptional": false, "description": "The map frame to cache. ", "dataType": "String"}, {"name": "layer", "isOptional": false, "description": "Layers to remove from the cache. ", "dataType": "String"}, {"name": "constraining_extent", "isOptional": true, "description": "Extent in the cache to update. ", "dataType": "Extent"}, {"name": "levels", "isOptional": false, "description": "A list of scale levels to update. ", "dataType": "String"}, {"name": "update_mode", "isOptional": false, "description": "Choose a mode for updating the cache. The two modes are: Recreate Empty Tiles - Only tiles that are empty (have been deleted on disk), or that are new because the cache extent has changed or because new layers have been added to a multi-layer cache, will be created. Old tiles will be left unchanged. \u2014 Missing Value Recreate All Tiles - All tiles will be replaced and new tiles will be added if the extent has changed or if layers have been added to a multi-layer cache. \u2014 Missing Value", "dataType": "Boolean"}, {"name": "thread_count", "isOptional": true, "description": "Number of Map Server instances to use while updating the cache. ", "dataType": "Long"}, {"name": "antialiasing", "isOptional": true, "description": "Choose whether to use antialiasing when rendering the tiles. If ANTIALIASING is edges of lines, borders, and text will be smoothed. There is a performance cost for this option. No benefit will be gained on raster data. ", "dataType": "Boolean"}, {"name": "update_feature_class", "isOptional": true, "description": "A polygon feature class used to derive the extents for which the cache should be updated. ", "dataType": "Feature Class"}, {"name": "ignore_status", "isOptional": true, "description": "Choose Cache all features and ignore completion status field to ignore the cache completion status field and cache all feature extents. Choose Track cache completion status for each feature option to update the cache completion status to a field named Cached. A status of \"Yes\" is written to the Cached field after successful completion of cache generation for that feature. ", "dataType": "String"}]},
{"syntax": "UpdateGlobeServerCache_server (server_name, object_name, {update_extent}, layer, LOD_from, LOD_to, {thread_count}, update_mode)", "name": "Update Globe Server Cache (Server)", "description": "Updates an existing Globe Service cache to replace missing tiles, overwrite outdated tiles, or add new tiles in new areas. Update Globe Server Cache tool works on a single layer or on all layers of the Service. There are two modes of operation for this tool:", "example": {"title": null, "description": null, "code": "# Importing standard library modules import os , arcgisscripting # Create the geoprocessing object gp = arcgisscripting.create () gp.UpdateGlobeServerCache ( \"myglobeserver\" , \"Boundaries\" , \"Globe - 1:10000000\" , \"Countries - 1:2500000\" , 4 , \"'ESRI Countries (Small Scale)';'ESRI Countries Medium Scale)';LargeProvinces;'ESRI AWS Boundaries';'AND Countries'\" )"}, "usage": ["This tool should be used to update an existing globe server cache. To update a cache, specify the Globe server host. The tool will automatically list all server objects available on that server.", "Running update without specifying an extent will update the entire extent of the service being for the specified levels of detail.", "Update is useful to run when you need to update only a portion of the globe services cache. When specifying the 'from' and 'To' Levels of detail make sure you specify all levels you want tiles generated for. The From Level of Detail defines the lowest level of detail you'd want your data cache to begin with. The To Level of detail defines the highest resolution you'd want your data caching to have.", "This tool does not accept any Environment Setting that have been specified."], "parameters": [{"name": "server_name", "isOptional": false, "description": "The host name of the ArcGIS Server to use to update the cache. ", "dataType": "String"}, {"name": "object_name", "isOptional": false, "description": "The name of the Globe Service to use to update the cache. ", "dataType": "String"}, {"name": "update_extent", "isOptional": true, "description": "Choose an area of the layer for which the cache should be updated. You can do so by specifying the extent values or choosing an extent from an existing data source. Choosing a new cache extent will update tiles in every level of detail that intersects that extent. ", "dataType": "Extent"}, {"name": "layer", "isOptional": false, "description": "Update the data cache of the chosen layers. All layers are checked by default. If a layer is unchecked update will not affect the layer. ", "dataType": "String"}, {"name": "LOD_from", "isOptional": false, "description": "The minimum level of detail of the data cache building. Each level of detail corresponds to a fixed scale. These levels are fixed and correspond to the ArcGlobe's data tiling scheme. There are 21 levels of detail available: Globe - 1:10000000 | Continent - 1:5000000 | Countries - 1:2500000 | Country - 1:1250000 | States - 1:625000 | State - 1:312500 | Counties - 1:156250 | County - 1:78125 | Metropolitan Area - 1:39062 | Cities - 1:19531 | City - 1:9765 | Town - 1:4882 | Neighborhood - 1:2441 | City Blocks - 1:1220 | City Block - 1:610 | Buildings - 1:305 | Building - 1:152 | Houses - 1:76 | House Property - 1:38 | House - 1:19 | Rooms - 1:9 | Room - 1:4. ", "dataType": "String"}, {"name": "LOD_to", "isOptional": false, "description": "The maximum level of detail of the data cache building. Each level of detail corresponds to a fixed scale. These levels are fixed and correspond to the ArcGlobe's data tiling scheme. There are 21 levels of detail available: Globe - 1:10000000 | Continent - 1:5000000 | Countries - 1:2500000 | Country - 1:1250000 | States - 1:625000 | State - 1:312500 | Counties - 1:156250 | County - 1:78125 | Metropolitan Area - 1:39062 | Cities - 1:19531 | City - 1:9765 | Town - 1:4882 | Neighborhood - 1:2441 | City Blocks - 1:1220 | City Block - 1:610 | Buildings - 1:305 | Building - 1:152 | Houses - 1:76 | House Property - 1:38 | House - 1:19 | Rooms - 1:9 | Room - 1:4. ", "dataType": "String"}, {"name": "thread_count", "isOptional": true, "description": "Specifies the number of ArcGIS Server Globe Server Instances to use to create the cache. This number defaults to the maximum number of Globe Server instances specified in the Globe Service properties, but can be changed to a lesser number. ", "dataType": "Long"}, {"name": "update_mode", "isOptional": false, "description": "Choose a mode for updating the cache. The two modes are: Recreate Empty Tiles \u2014 Only tiles that are empty (have been deleted on disk), or that are new because the cache extent has changed or because new layers have been added to the globe service, will be created. Existing tiles will be left unchanged. Recreate All Tiles \u2014 All tiles, including existing tiles, will be replaced. Additionally new tiles will be added if a layers data extent has changed or new layers have been added to the globe service. ", "dataType": "Boolean"}]},
{"syntax": "ManageMapServerCacheTiles_server (input_service, scales, update_mode, {num_of_caching_service_instances}, {update_extent}, {area_of_interest}, {wait_for_job_completion})", "name": "Manage Map Server Cache Tiles (Server)", "description": "Creates and updates tiles in an existing map or image service cache. This tool is used to create new tiles, replace missing tiles, overwrite outdated tiles, and delete tiles.", "example": {"title": null, "description": "Create or update all the tiles in the cache using the RECREATE_ALL_TILES option", "code": "# Name: ManageMapServerCacheTiles.py # Description: The following stand-alone script demonstrates how to Recreate all  #               cache tiles for the default number of scales in the cache tiling #               scheme. # Requirements: os, sys, time and traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapServer\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName scales = \"\" numOfCachingServiceInstances = 2 updateMode = \"RECREATE_ALL_TILES\" areaOfInterest = \"\" waitForJobCompletion = \"WAIT\" updateExtents = \"\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = r'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) try : starttime = time.clock () result = arcpy.ManageMapServerCacheTiles_server ( inputService , scales , numOfCachingServiceInstances , updateMode , areaOfInterest , updateExtents , waitForJobCompletion ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Created cache tiles for given schema successfully for \" serviceName + \" in \" + str ( elapsedtime ) + \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 1  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) report.close () print \"Created Map server Cache Tiles \""}, "usage": ["This tool may take a long time to run for caches that cover a large geographic extent or very large map scales. If the tool is cancelled, tile creation is stopped, but the existing tiles are not deleted. This means you can cancel the tool if you run out of time  and re-run it later on the same cache using the RECREATE_EMPTY_TILES update mode. "], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service whose cache tiles you want to update. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "scales", "isOptional": false, "description": "The scale levels at which you will create or delete tiles when running this tool, depending on the Update Mode. ", "dataType": "Double"}, {"name": "update_mode", "isOptional": false, "description": "The mode for updating the cache. RECREATE_EMPTY_TILES \u2014 Only tiles that are empty will be created. Existing tiles will be left unchanged. RECREATE_ALL_TILES \u2014 Existing tiles will be replaced and new tiles will be added if the extent has changed. DELETE_TILES \u2014 Tiles will be deleted from the cache. The cache folder structure will not be deleted. If you wish to delete the entire cache, including the folder structure, use the Delete Map Server Cache tool.", "dataType": "String"}, {"name": "num_of_caching_service_instances", "isOptional": true, "description": "The total number of instances of the System/CachingTools service that you want to dedicate toward running this tool. You can increase the maximum number of instances per machine of the System/CachingTools service using the Service Editor window available through an administrative connection to ArcGIS Server. Ensure your server machines can support the chosen number of instances. ", "dataType": "Long"}, {"name": "update_extent", "isOptional": true, "description": "Rectangular extent at which to create or delete tiles, depending on the value of the update_mode parameter. It is not recommended you provide values for both update_extent and area_of_interest . If values for both parameters are provided, the value of area_of_interest will be used. ", "dataType": "Extent"}, {"name": "area_of_interest", "isOptional": true, "description": "Defines an area of interest to constrain where tiles will be created or deleted. This can be a feature class, or it can be a feature that you interactively define in ArcMap. This parameter is useful if you want to manage tiles for irregularly shaped areas. It's also useful in situations where you want to pre-cache some areas and leave less-visited areas uncached. If you do not provide a value for this parameter, the default is to use the full extent of the map. ", "dataType": "Feature Set"}, {"name": "wait_for_job_completion", "isOptional": true, "description": "This parameter allows you to watch the progress of the cache job running on the server. WAIT \u2014 The geoprocessing tool will continue to run in ArcGIS for Desktop while the cache job runs on ArcGIS for Server or ArcGIS Online. With this option, you can request detailed progress reports at any time and view the geoprocessing messages as they appear. This is the default option. It is recommended that you use this option in Python scripts. DO_NOT_WAIT \u2014 Unchecked\u2014The geoprocessing tool will submit the job to the server, allowing you to perform other geoprocessing tasks in ArcGIS for Desktop or even close ArcGIS for Desktop . This option is used when you choose to build a cache automatically at the time you publish the service, and you can also set this option on any other cache that you build. To track the status of the cache job, open ArcGIS for Desktop , right-click the service in the Catalog window, and click View Cache Status . You can also use the URL provided in the tool result message.This option is not available if the Status.gdb file geodatabase is not present in the service's cache directory.", "dataType": "Boolean"}]},
{"syntax": "ManageMapServerCacheScales_server (input_service, scales)", "name": "Manage Map Server Cache Scales (Server)", "description": "Updates the scale levels in an existing cached map or image service. Use this tool to add new scales or delete existing scales from a cache.", "example": {"title": null, "description": "This example modifies a map cache tiling scheme to contain four scales.", "code": "# Name: ManageMapServerCacheScales.py # Description: The following stand-alone script demonstrates how to add/delete #               MapServer cache scales for a map service with an existing schema #                # Requirements: os, sys, time & traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapServer\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName scales = \"250000;125000;64000;5250\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) try : starttime = time.clock () result = arcpy.ManageMapServerCacheScales_server ( inputService , scales ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Updated the number of cache scales successfully for \" + serviceName \" in \" + str ( elapsedtime ) + \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 1  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) report.close () print \"Updated Map server Cache scales \""}, "usage": ["If you remove scales from an existing cache, it will permanently delete all existing cached tiles within that level of detail."], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service for which you want to add or remove cache scales. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "scales", "isOptional": false, "description": "The scale values to be included in the updated tiling scheme. You must specify existing scale values if you want to keep them. Any existing scale levels you do not specify in this tool will be permanently deleted. For example, if you have four existing scales and you wish to add two scales, make sure your final list has a total of six scales. ", "dataType": "Value Table"}]},
{"syntax": "ManageGlobeServerCacheTiles_server (service, {update_extent}, in_layers, update_mode, {thread_count}, {area_of_interest}, {track_status})", "name": "Manage Globe Server Cache Tiles (Server)", "description": "Creates and updates tiles in an existing globe service cache. This tool is used to create new tiles or to replace missing tiles, overwrite outdated tiles, or add new tiles.  All these actions can be defined by rectangular extents or by a polygon feature class. When creating new tiles, you can choose whether to create only empty tiles or re-create all tiles.", "example": {"title": null, "description": null, "code": "#ManageGlobeServerCacheTiles For ArcGIS Server 10.1 Beta example (stand-alone script) # Name: ManageGlobeServerCacheTiles.py # Description: The following stand-alone script demonstrates how to update the #               globe map server cache tiles # Requirements: os, sys, time & traceback modules  # Author: ESRI # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default values # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" globeServiceName = \"tstGlobeService.GlobeService\" globeService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + globeServiceName inputLayers = \"\" updateExtents = \"\" updateMode = \"RECREATE_ALL_TILES\" numOfCachingServiceInstances = \"2\" areaOfInterest = \"\" ignoreStatus = \"DO_NOT_TRACK_STATUS\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) try : starttime = time.clock () result = arcpy.ManageGlobeServerCacheTiles_server ( globeService , inputLayers , updateMode , numOfCachingServiceInstances , areaOfInterest , ignoreStatus , updateExtents ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Created the GlobeServer cache successfully for mapservice \" globeService + \" \\n  in \" + str ( elapsedtime ) + \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 1  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) report.close () print \"Created the globe server cache successfully\""}, "usage": ["This tool should be used to update an existing globe server cache. To update a cache, specify the ArcGIS Server hosting the globe service. The tool will automatically list all globe services available on that server.", "Running update without specifying an extent will update the entire extent of the service.", " When specifying ", "level_from", " and ", "level_to", " for the input layers, make sure you specify all levels at which you want tiles generated. The ", "level_from", " defines the lowest level of detail you want your data cache to begin with. The ", "level_to", " defines the highest resolution you want your data caching to have."], "parameters": [{"name": "service", "isOptional": false, "description": "The globe service whose cache tiles you want to update. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog, select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/Seattle.GlobeServer . ", "dataType": "String"}, {"name": "update_extent", "isOptional": true, "description": "Rectangular extent at which tiles should be created or deleted, depending on the value of the update_mode parameter. You can type the extent values or choose an extent from an existing data source. ", "dataType": "Extent"}, {"name": "in_layers", "isInputFile": true, "isOptional": false, "description": "The layers to include in the cache. For each layer, you need to provide a level_from, which is the level of detail at which you would like to begin caching the layer, and a level_to, which is the level of detail at which you would like to end caching the layer. If the smallest and largest levels of detail are used for level_from and level_to, respectively, a full cache will be built for the layer. ", "dataType": "Value Table"}, {"name": "update_mode", "isOptional": false, "description": "Choose a mode for updating the cache. The two modes are: RECREATE_EMPTY_TILES \u2014 Only tiles that are empty will be created. Existing tiles will be left unchanged. RECREATE_ALL_TILES \u2014 All tiles, including existing tiles, will be replaced. Additionally, new tiles will be added if a layer's data extent has changed or new layers have been added to the globe service and listed in this tool.", "dataType": "String"}, {"name": "thread_count", "isOptional": true, "description": "The total number of instances of the System/CachingTools service that you want to dedicate toward running this tool. You can increase the maximum number of instances per machine of the System/CachingTools service using the Service Editor window available through an administrative connection to ArcGIS Server. Ensure your server machines can support the chosen number of instances. ", "dataType": "Long"}, {"name": "area_of_interest", "isOptional": true, "description": "Defines an area of interest to constrain where tiles will be created or deleted. This can be a feature class, or it can be a feature that you interactively define in ArcMap. This parameter is useful if you want to manage tiles for irregularly shaped areas. It's also useful in situations where you want to precache some areas and leave less-visited areas uncached. ", "dataType": "Feature Set"}, {"name": "track_status", "isOptional": true, "description": "This parameter allows you to track the status of your caching if you are creating tiles based on feature class boundaries (see the update_feature_class parameter). TRACK_STATUS \u2014 The feature class' Cached field is read (and created if it doesn't exist yet). Features containing No or null in this field are cached and will contain Yes when caching has completed for the feature. Features already marked Yes in this field are not cached. DO_NOT_TRACK_STATUS \u2014 The feature class' Cached field is ignored and tiles are created for all features in the feature class.", "dataType": "Boolean"}]},
{"syntax": "GenerateMapServerCacheTilingScheme_server (map_document, data_frame, tile_origin, output_tiling_scheme, num_of_scales, scales, dots_per_inch, tile_size)", "name": "Generate Map Server Cache Tiling Scheme (Server)", "description": "Generates an XML tiling scheme file that defines the scale levels, tile dimensions, and other properties for a map service cache. This tool is useful when creating a tiling scheme to use in multiple caches. You can load the tiling scheme file when you create a cache in  ArcGIS for Desktop  or ArcGIS Server Manager, or you can run  Create Map Server Cache  and pass in the tiling scheme file as a parameter. A tiling scheme describes how clients should reference the tiles in a cache and is a mapping between the spatial reference of the source map document and the tiling grid. The tiling grid uses a level of detail (scales), row, and column reference scheme. The scheme also defines the scale levels (levels of detail) at which the cache has tiles, the size of the tiles in pixels, and the screen resolution for which the tiles are intended to be most commonly displayed. A tiling scheme is needed to generate a map cache.", "example": {"title": null, "description": "Creates a map cache tiling scheme with four scales.", "code": "# Name: GeneateMapServerCacheTilingScheme.py # Description: The following stand-alone script demonstrates how to create map #               server cache schema using a given map document at a given #               \"pathForOutputXml\" # Requirements: os, sys, time & traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties mapDocument = \"C:/data/101/Portland/mxd/_M_Portland_classic_FGDB_Local.mxd\" dataFrame = \"\" outputTilingScheme = \"C:/data/port.xml\" tileOrigin = \"\" numOfScales = \"4\" scales = \"500000,250000,125000,64000\" tileSize = \"256 x 256\" dotsPerInch = \"96\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = r'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) try : starttime = time.clock () result = arcpy.GenerateMapServerCacheTilingScheme_server ( mapDocument , dataFrame , tileOrigin , outputTilingScheme , numOfScales , scales , dotsPerInch , tileSize ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \" Created MapServer cache tiling schema successfully using\" mapDocument + \" at \" + outputTilingScheme + \" in \" + str ( elapsedtime ) \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 1  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) print \"Created Map server Cache Tiling schema \" report.close ()"}, "usage": ["If you've already defined a cache in ", "ArcGIS for Desktop", " or ArcGIS Server Manager, then you do not have to run this tool. You will already have a tiling scheme (", "conf.xml", ") in the cache folder, which you can reference when you create other caches.", "By default, the tiling origin starts at the upper left of the coordinate system used by the service's source map document.", "Once the cache has been created, the scale levels are the only part of the tiling scheme that can be changed. Use ", "Manage Map Server Cache Scales", " to add or remove scale levels."], "parameters": [{"name": "map_document", "isOptional": false, "description": "The source map document to be used for the tiling scheme. ", "dataType": "File"}, {"name": "data_frame", "isOptional": false, "description": "The data frame to be used for the tiling scheme. ", "dataType": "String"}, {"name": "tile_origin", "isOptional": false, "description": "The upper left corner of the tiling scheme, in coordinates of the spatial reference of the source data frame. ", "dataType": "Point"}, {"name": "output_tiling_scheme", "isOutputFile": true, "isOptional": false, "description": "Path and file name of the tiling scheme file to create. ", "dataType": "File"}, {"name": "num_of_scales", "isOptional": false, "description": "Number of scale levels in the tiling scheme. ", "dataType": "Long"}, {"name": "scales", "isOptional": false, "description": "Scale levels to include in the tiling scheme. These are not represented as fractions. Instead, use 500 to represent a scale of 1:500, and so on. ", "dataType": "Value Table"}, {"name": "dots_per_inch", "isOptional": false, "description": "The dots per inch of the intended output device. If a DPI is chosen that does not match the resolution of the output device, the scale of the map tile will appear incorrect. The default value is 96. ", "dataType": "Long"}, {"name": "tile_size", "isOptional": false, "description": "The width and height of the cache tiles in pixels. The default is 256 by 256. For the best balance between performance and manageability, avoid deviating from standard dimensions of 256 by 256 or 512 by 512. ", "dataType": "String"}]},
{"syntax": "GenerateMapServerCache_server (server_name, object_name, data_frame, out_folder, tiling_scheme_type, scales_type, num_of_scales, dpi, tile_width, tile_height, {map_or_layers}, {tiling_schema}, {tile_origin}, {levels}, {layer}, {thread_count}, {Antialiasing}, {cache_format}, {tile_compression_quality})", "name": "Generate Map Server Cache (Server)", "description": "Generates a cache of static image tiles for an ArcGIS Server map service.", "example": {"title": null, "description": null, "code": "# Script Name: Generate Fused Map Server Cache # Description: Generates a fused map server cache using PNG8 image format # Uncomment sys.argv[] lines to accept arguments from the command line. # Import standard library modules import sys , string , os , arcgisscripting # Create the Geoprocessor object gp = arcgisscripting.create () # Set the path to the cache. # Example: \\\\\\\\myServer\\\\arcgiscache\\\\MyServiceFolder_MyService\\\\ out_folder = \" \\\\\\\\ myServer \\\\ arcgiscache \\\\ MyServiceFolder_MyService \\\\ \" #out_folder = sys.argv[1] # Set the SOM server name # Example: \"mySOM\" server_name = \"mySOM\" #server_name = sys.argv[2] # Set the object_name # Example: \"MyServiceFolder/MyService\" object_name = \"MyServiceFolder/MyService\" #object_name = sys.argv[3] # Set the data frame # Example: \"Layers\" data_frame = \"Layers\" #data_frame = sys.argv[4] # Set the map cache tiling origin # Example: \"-180 90\" tile_origin = \"-180 90\" #tile_origin = sys.argv[5] # Set the scale levels for the cache. # Example: \"2000000;500000;250000\" scales = \"2000000;500000;250000\" #scales = sys.argv[6] # Set the layers to cache. # Example: \"My First Layer;My Second Layer;My Third Layer\" layers = \"My First Layer;My Second Layer;My Third Layer\" #layers = sys.argv[7] # Set number of instances to use while updating the cache # Example: \"3\" thread_count = \"3\" #thread_count = sys.argv[8] # Set cache tile image format. # Example: \"PNG8\" cache_format = \"PNG8\" #layers = sys.argv[9] # Non-argument variable declarations tiling_scheme_type = \"NEW\" tiling_schema = \"\" scales_type = \"STANDARD\" num_of_scales = \"3\" dpi = \"96\" tile_width = \"512\" tile_height = \"512\" map_or_layers = \"FUSED\" antialiasing = \"ANTIALIASING\" tile_compression_quality = \"0\" try : print 'Starting Cache Generation' gp.GenerateMapServerCache ( server_name , object_name , data_frame , out_folder , tiling_scheme_type , scales_type , num_of_scales , dpi , tile_width , tile_height , map_or_layers , tiling_schema , tile_origin , scales , layers , thread_count , antialiasing , cache_format , tile_compression_quality ) print 'Finished Cache Generation' except : gp.AddMessage ( gp.GetMessages ( 2 )) print gp.GetMessages ( 2 )"}, "usage": ["This tool only works with ArcGIS Server map services.", "Before running this tool, configure the map service to use as many instances as possible. This will dramatically decrease cache generation time.", "Only one data frame can be cached at a time. If other data frames are needed, separate caches must be generated that associated with separate map services.", "The tiling scheme determines how the client should reference the tiles in the cache. A new tiling scheme can be created, or an existing tiling scheme created with the Generate Map Server Cache Tiling Scheme tool can be used.", "Use Fused caches for most cases. All the layers in the map are composited into single images for each specified scale level, then split into many tiles.", "Use Multi-layer caches for cases where you want to toggle layers on and off.", "The Number of Scales parameter is the number of different map scales the tool will create layers for in the cache.", "Computing large map scale levels for very large map extents (such as a large state, province, or country) can result in large creation times. For example, computing a tile size for map scales larger than 1:100,000 for all of North America.", "The primary benefit of employing image-compression in your cache is to reduce the size of the image tiles to improve performance over the network. Because less data is transmitted between the client and the server, it is possible to cache large, seamless raster datasets (as large as several terabytes) and serve them quickly to a client for display. An added benefit is the significantly reduced size of the cache on disk.", "When using this tool to add additional levels (scales) to an existing cache, make sure to specify the same compression factor that was used in the original levels.", "JPEG is best used with raster data. When used with vector data, lines and text may be blurred.", "JPEG file format is not supported for multi-layer caches.", "The cache tile format cannot be changed once the cache is generated. The cache must first be deleted before switching to a different file format.", "For best performance of streaming cache tiles to clients choose the JPEG image format. For the best combination of performance and transparency support, choose the PNG8 image format.", "The Antialiasing option smooths out the edges of lines and text, but offers little enhancement for raster images.", "This tool does not accept any Environment Settings that have been specified."], "parameters": [{"name": "server_name", "isOptional": false, "description": "The host name of the ArcGIS Server to use to generate the cache. ", "dataType": "String"}, {"name": "object_name", "isOptional": false, "description": "The name of the map service to use to generate the cache. ", "dataType": "String"}, {"name": "data_frame", "isOptional": false, "description": "The map documents data frame to cache. Make sure you select the same data frame that you selected for your map service. ", "dataType": "String"}, {"name": "out_folder", "isOutputFile": true, "isOptional": false, "description": "The parent directory for this map service cache. ", "dataType": "String"}, {"name": "tiling_scheme_type", "isOptional": false, "description": "Choose to use a NEW or PREDEFINED tiling scheme. A new tiling scheme can be defined in this tool, or a predefined scheme file (.xml) can be loaded. A predefined scheme can be created by running the Generate Map Server Cache Tiling Scheme GP Tool. NEW is the default. ", "dataType": "Boolean"}, {"name": "scales_type", "isOptional": false, "description": "Set the scale level STANDARD \u2014 Auto-generates the scales based on the number defined in the \"Number of Scales\" field. It will use levels that increase or decrease by half from 1:1000000 and will start with a level closest to the extent of the source map document. For example, if the source map document has an extent of 1:121,000,000 and 3 scale levels are defined, the map service will create a cache with scale-levels at: 1:128,000,000; 1:64,000,000; and 1:32,000,000. This is the default. CUSTOM \u2014 Permits the cache designer to create a number of scales defined by the \"Number of Scales\" field at any scale level desired. ", "dataType": "Boolean"}, {"name": "num_of_scales", "isOptional": false, "description": "The number of scale levels to create in the cache. ", "dataType": "Long"}, {"name": "dpi", "isOptional": false, "description": "The dots per inch of the intended output device. If a DPI is chosen that does not match the resolution of the output device, the scale of the map tile will appear incorrect. The default value is 96. ", "dataType": "Long"}, {"name": "tile_width", "isOptional": false, "description": "The width, in pixels, of output map tiles. Small widths map improve performance of the application requesting tiles from the cache as less data will travel over the wire. However, smaller tile size results in a larger cache size and longer creation time. The default tile width is 512. ", "dataType": "Long"}, {"name": "tile_height", "isOptional": false, "description": "The height, in pixels, of output map tiles. Small heights map improve performance of the application requesting tiles from the cache as less data will travel over the wire. However, smaller tile size results in a larger cache size and longer creation time. The default tile height is 512. ", "dataType": "Long"}, {"name": "map_or_layers", "isOptional": true, "description": "Choose to generate a FUSED or MULTI_LAYER cache: FUSED \u2014 Each tile is created as a fused image of all the layers in the source map document. MULTI_LAYER \u2014 There is one cache, but it is organized into sub-caches containing tiles from each layer in the source map document. ", "dataType": "Boolean"}, {"name": "tiling_schema", "isOptional": true, "description": "Path to a pre-defined tiling scheme. ", "dataType": "String"}, {"name": "tile_origin", "isOptional": true, "description": "Specify the origin (upper left corner) of the tiling scheme in coordinates of the spatial reference of the source map document. The extent of the source map document must be within (but does not need to coincide) with this region. ", "dataType": "String"}, {"name": "levels", "isOptional": false, "description": "Scales at which to cache the map service. ", "dataType": "String"}, {"name": "layer", "isOptional": false, "description": "Layers to include in the cache if MULTI_LAYER cache type is chosen. ", "dataType": "String"}, {"name": "thread_count", "isOptional": true, "description": "The number of MapServer instances to use while generating the cache. ", "dataType": "Long"}, {"name": "Antialiasing", "isOptional": true, "description": "Choose whether to use antialiasing when rendering the tiles. If ANTIALIASING is chosen, edges of lines, borders, and text will be smoothed. There is a performance cost for this option. No benefit will be gained on raster data. ", "dataType": "Boolean"}, {"name": "cache_format", "isOptional": true, "description": "Choose either PNG8, PNG24, PNG32 or JPEG file format for the tiles in the cache. PNG24 is the default. PNG8 \u2014 a lossless, 8-bit color, image format that uses an indexed color palette and an alpha table. Each pixel stores a value (0-255) that is used to look up the color in the color-palette and the transparency in the alpha table. 8-bit PNGs are similar to GIF images and enjoy the best support for transparent background by most web browsers. PNG24 \u2014 a lossless, three-channel image format that supports large color variations (16 million colors) and has limited support for transparency. Each pixel contains three 8-bit color channels and the file header contains the single color that represents the transparent background. The color representing the transparent background color can be set in ArcMap. Versions of Internet Explorer less than version 7 do not support this type of transparency. Caches using PNG24 are significantly larger than those using PNG8 or JPEG and will take more disk space and require greater bandwidth to serve clients. PNG32 \u2014 a lossless, four-channel image format that supports large color variations (16 million colors) and transparency. Each pixel contains three 8-bit color channels and one 8-bit alpha channel that represents the level of transparency for each pixel. While the PNG32 format allows for partially transparent pixels in the range from 0 to 255, the ArcGIS Server cache generation tool only writes fully transparent (0) or fully opaque (255) values in the transparency channel. Caches using PNG32 are significantly larger than the other supported formats and will take more disk space and require greater bandwidth to serve clients. JPEG \u2014 a lossy, three-channel image format that supports large color variations (16 million colors) but does not support transparency. Each pixel contains three 8-bit color channels. Caches using JPEG provide control over output quality and size and can be more compact than the PNG format. ", "dataType": "String"}, {"name": "tile_compression_quality", "isOptional": true, "description": "Enter a value between 1 and 100 for the JPEG compression quality. The default value is 75 for JPEG tile format and zero for other formats. Compression is supported only for JPEG format. The level of compression will depend on the data, but can also be controlled by changing the compression quality. Choosing a higher value will result in less compression and a higher-quality image. Choosing a lower value will result in more compression but a lower-quality image. ", "dataType": "Long"}]},
{"syntax": "GenerateGlobeServerCache_server (server_name, object_name, out_folder, lod_from, lod_to, {thread_count}, Layer)", "name": "Generate Globe Server Cache (Server)", "description": "Generates Globe data Caches based on ArcGlobe\u2019s Data tiling scheme. Use this tool to generate a globe data cache using the specified globe server object.", "example": {"title": null, "description": null, "code": "# Importing standard library modules import os , arcgisscripting # Create the geoprocessing object gp = arcgisscripting.create () gp.GenerateGlobeServerCache ( \"myglobeserver\" , \"Boundaries\" , \"Globe -  1 : 10000000 \",\" Countries - 1 : 2500000 \", 4,\"'ESRI Countries (Small  Scale ) ';' ESRI Countries ( Medium Scale ) ';LargeProvinces;' ESRI AWS Boundaries ';' AND Countries '\")"}, "usage": ["This tool is used to create a globe cache from an existing a Globe Server Object. To build a cache using the server object, specify the Globe server host. The tool will automatically list all server objects available on that server.", "You are able to generate data cache for all the layers contained in the server object or for a single layer.", "The data caching scheme is already pre defined. Hence, as a user, all you need to specify is the \u2018from\u2019 and \u2018To\u2019 Levels of detail. The From Level of Detail defines the lowest level of detail you would want your data cache to begin with. The To Level of detail defines the highest resolution you would want your data caching to have.", "Each data cache generation thread on the client must have access to the data, otherwise that thread will not be able to be used in the generation of the cache.", "This tool does not accept any Environment Settings that have been specified."], "parameters": [{"name": "server_name", "isOptional": false, "description": "The host name of the ArcGIS Server Object Manager (SOM) that will be used to generate the cache. ", "dataType": "String"}, {"name": "object_name", "isOptional": false, "description": "The name of the Globe Server configuration that will be used to generate the cache. ", "dataType": "String"}, {"name": "out_folder", "isOutputFile": true, "isOptional": false, "description": "The parent directory for this Globe Service where the data cache will reside for. The server cache directory must be registered with the server before running this tool. This directory must be read/write accessible to the ArcGIS Server Object Container account user on each SOC machine in the ArcGIS Server. ", "dataType": "String"}, {"name": "lod_from", "isOptional": false, "description": "Select the level-of-detail scale you would like to begin caching the layer. If the smallest and largest level-of-detail scales are used for the minimum and maximum, a full cache will be built for the layer. ", "dataType": "String"}, {"name": "lod_to", "isOptional": false, "description": "Select the level-of-detail scale you would like to begin caching the layer. If the smallest and largest level-of-detail scales are used for the minimum and maximum, a full cache will be built for the layer. ", "dataType": "String"}, {"name": "thread_count", "isOptional": true, "description": "The specified number of threads to attempt to create on the client. Each thread, in turn, will try to create a server context on the globe server object to generate the cache. ", "dataType": "Long"}, {"name": "Layer", "isOptional": false, "description": "Select the layers to include in the layer cache. ", "dataType": "String"}]},
{"syntax": "DeleteMapServerCache_server (input_service, {num_of_caching_service_instances})", "name": "Delete Map Server Cache (Server)", "description": "Deletes an existing map or image service cache, including all associated files on disk.", "example": {"title": null, "description": "This example uses DeleteMapServerCache to delete a cache for a map service named Rainfall.", "code": "# Name: DeleteMapServerCache.py # Description: The following stand-alone script demonstrates how to delete map server cache #              tiles if the corresponding cache schema or tiles has been created # Requirements: os, sys, time & traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of variables for mapservice properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapServer\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName numOfCachingServiceInstances = 2 currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) # To Recreate all the tiles for the default number of scales generated try : starttime = time.clock () result = arcpy.DeleteMapServerCache_server ( inputService , numOfCachingServiceInstances ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Deleted cache tiles & schema for mapservice \" + serviceName \" \\n   in \" + str ( elapsedtime ) + \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) print \"Deleted Map server Cache Tiles \" report.close ()"}, "usage": ["This is an unrecoverable operation so only use if you are sure you no longer need the cache. If you want to delete tiles but leave the cache folder structure and tiling scheme, use the ", "Manage Map Server Cache Tiles", " tool with the ", "Update Mode", " set to ", "Delete_Tiles", ".", "After this tool runs, the service will be restarted."], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service whose cache will be deleted. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "num_of_caching_service_instances", "isOptional": true, "description": "The total number of instances of the System/CachingTools service that you want to dedicate toward running this tool. You can increase the maximum number of instances per machine of the System/CachingTools service using the Service Editor window available through an administrative connection to ArcGIS Server. Ensure your server machines can support the chosen number of instances. The Delete Map Server Cache tool requires a minimum of two instances to run successfully. ", "dataType": "Long"}]},
{"syntax": "DeleteGlobeServerCache_server (service, Layer)", "name": "Delete Globe Server Cache (Server)", "description": "Deletes a layer or layers of an existing globe service cache and all tiles in them.", "example": {"title": null, "description": "This example deletes all layers of a cache for a globe service.", "code": "#DeleteGlobeServerCache example For ArcGIS Server 10.1 Beta(stand-alone script) # Name: DeleteGlobeServerCache.py # Description: The following stand-alone script deletes globe server cache # Requirements: os, sys, time & traceback modules # Author: ESRI # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" globeServiceName = \"tstGlobeService.GlobeService\" globeService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + globeServiceName inputLayers = \"\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) try : starttime = time.clock () result = arcpy.DeleteGlobeServerCache_server ( globeService , inputLayers ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Deleted the GlobeServer cache successfully for globeservice \" globeServiceName + \" \\n  in \" + str ( elapsedtime ) + \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) report.close () print \"Deleted the globe server cache successfully\""}, "usage": ["This is an unrecoverable operation so only use it if you are sure you no longer need the cache.", "This tool can be used to delete an existing globe server cache. To delete a cache, specify the ArcGIS Server (host) machine and select the globe service. The layers list will be populated with all available layers in the selected service.", "By default all layers of the service are displayed and selected. If you intend to delete only a particular layer's cache, make sure to unselect the ones for which you want to keep the cache.", "Delete Globe Server Cache", " deletes the entire cache folder. Note that after the ", " Delete Globe Server Cache", " tool runs, it will restart the service. This will, in turn, generate a new set of caches for each layer in the service that was deleted by the delete operation. This is because all globe services ", "require", " a cache configuration to exist on disk. The cache that gets created automatically on startup of a service is a skeleton representation that does not contain any tiles."], "parameters": [{"name": "service", "isOptional": false, "description": "The globe service whose layer caches you want to delete. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog, select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/Seattle.GlobeServer . ", "dataType": "String"}, {"name": "Layer", "isOptional": false, "description": "The layers in the globe service whose caches will be deleted. All layers of the service are included by default. If a layer is excluded the layer's cache will not be deleted. ", "dataType": "String"}]},
{"syntax": "CreateMapServerCache_server (input_service, service_cache_directory, tiling_scheme_type, scales_type, num_of_scales, dots_per_inch, tile_size, {predefined_tiling_scheme}, {tile_origin}, {scales}, {cache_tile_format}, {tile_compression_quality}, {storage_format}, {use_local_cache_dir})", "name": "Create Map Server Cache (Server)", "description": "Creates the tiling scheme and preparatory folders for a  map or image service cache. After running this tool, run  Manage Map Server Cache Tiles  to add tiles to the cache.", "example": {"title": null, "description": "This example creates a map cache using the STANDARD scale type.", "code": "# Name: CreateMapServerCache.py # Description: The following stand-alone script demonstrates how to create map #              using Custom scales & jpg image format. # Requirements: os, sys, time & traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , string , datetime , traceback # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapServer\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName serviceCacheDirectory = \"C: \\\\ arcgisserver \\\\ arcgiscache\" tilingSchemeType = \"NEW\" scalesType = \"CUSTOM\" numOfScales = \"4\" dotsPerInch = \"96\" tileSize = \"256 x 256\" predefinedTilingScheme = \"\" tileOrigin = \"\" scales = \"600265;350200;225400;44000\" cacheTileFormat = \"JPEG\" tileCompressionQuality = \"75\" storageFormat = \"COMPACT\" useLocalCacheDir = \"USE_LOCAL_DIR\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) try : starttime = time.clock () result = arcpy.CreateMapServerCache_server ( inputService , serviceCacheDirectory , tilingSchemeType , scalesType , numOfScales , dotsPerInch , tileSize , predefinedTilingScheme , tileOrigin , scales , cacheTileFormat , tileCompressionQuality , storageFormat , useLocalCacheDir ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Created cache schema with custom scales successfully for \" serviceName + \" in \" + str ( elapsedtime ) + \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 1  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) print \"Executed creation of map server Cache schema using custom scales\" report.close ()"}, "usage": ["Only one data frame can be cached at a time. If you need maps from multiple data frames, you must create separate map services and caches for each data frame.", "Once you create the tiling scheme you cannot modify it. However, you can add or delete scales using the ", "Manage Map Server Cache Scales", " tool.", "Raster data is best served with JPEG or MIXED image format. When using JPEG or MIXED with vector maps, use a high-compression quality value (such as 90) to reduce blurring of lines and text. Vector data can also be served in PNG format.", "The cache image format cannot be changed once the cache is generated. The cache must first be deleted before switching to a different  format."], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service to be cached. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "service_cache_directory", "isOptional": false, "description": "The parent directory for the cache. This must be a registered ArcGIS Server cache directory. ", "dataType": "String"}, {"name": "tiling_scheme_type", "isOptional": false, "description": "Choose to use a NEW or PREDEFINED tiling scheme. You can define a new tiling scheme with this tool or browse to a predefined tiling scheme file ( .xml ). A predefined scheme can be created by running the Generate Map Server Cache Tiling Scheme tool. NEW \u2014 You will define a new tiling scheme using the various other parameters in this tool to define scale levels, image format, storage format, and so on. This is the default. PREDEFINED \u2014 You will specify a tiling scheme .xml file that already exists on disk. You can create a tiling scheme file using the Generate Map Server Cache Tiling Scheme tool.", "dataType": "String"}, {"name": "scales_type", "isOptional": false, "description": "Specify how you will define the scales for the tiles. STANDARD \u2014 Autogenerates the scales based on the number defined in the Number of Scales ( num_of_scales in Python) parameter. It will use levels that increase or decrease by half from 1:1,000,000 and will start with a level closest to the extent of the source map document. For example, if the source map document has an extent of 1:121,000,000 and three scale levels are defined, the map service will create a cache with scale-levels at 1:128,000,000; 1:64,000,000; and 1:32,000,000. This is the default. CUSTOM \u2014 Permits the cache designer to enter any scales desired. ", "dataType": "String"}, {"name": "num_of_scales", "isOptional": false, "description": "The number of scale levels to create in the cache. This option is disabled if you create a custom list of scales. ", "dataType": "Long"}, {"name": "dots_per_inch", "isOptional": false, "description": "The dots per inch of the intended output device. If a DPI is chosen that does not match the resolution of the output device, the scale of the map tile will appear incorrect. The default value is 96. ", "dataType": "Long"}, {"name": "tile_size", "isOptional": false, "description": "The width and height of the cache tiles in pixels. The default is 256 by 256. For the best balance between performance and manageability, avoid deviating from standard widths of 256 by 256 or 512 by 512. ", "dataType": "Long"}, {"name": "predefined_tiling_scheme", "isOptional": true, "description": "Path to a predefined tiling scheme file (usually named conf.xml ). ", "dataType": "File"}, {"name": "tile_origin", "isOptional": true, "description": "The origin (upper left corner) of the tiling scheme in the coordinates of the spatial reference of the source map document. The extent of the source map document must be within (but does not need to coincide) with this region. ", "dataType": "Point"}, {"name": "scales", "isOptional": false, "description": "Scale levels available for the cache. These are not represented as fractions. Instead, use 500 to represent a scale of 1:500, and so on. ", "dataType": "Value Table"}, {"name": "cache_tile_format", "isOptional": true, "description": "Choose either PNG, PNG8, PNG24, PNG32, JPEG, or MIXED file format for the tiles in the cache. PNG8 is the default. PNG \u2014 Creates PNG format with varying bit depths. The bit depths are optimized according to the color variation and transparency values in a tile. PNG8 \u2014 A lossless, 8-bit color, image format that uses an indexed color palette and an alpha table. Each pixel stores a value (0\u2013255) that is used to look up the color in the color palette and the transparency in the alpha table. 8-bit PNGs are similar to GIF images and enjoy the best support for transparent background by most web browsers. PNG24 \u2014 A lossless, three-channel image format that supports large color variations (16 million colors) and has limited support for transparency. Each pixel contains three 8-bit color channels and the file header contains the single color that represents the transparent background. The color representing the transparent background color can be set in ArcMap. Versions of Internet Explorer less than version 7 do not support this type of transparency. Caches using PNG24 are significantly larger than those using PNG8 or JPEG and will take more disk space and require greater bandwidth to serve clients. PNG32 \u2014 A lossless, four-channel image format that supports large color variations (16 million colors) and transparency. Each pixel contains three 8-bit color channels and one 8-bit alpha channel that represents the level of transparency for each pixel. While the PNG32 format allows for partially transparent pixels in the range from 0 to 255, the ArcGIS Server cache generation tool only writes fully transparent (0) or fully opaque (255) values in the transparency channel. Caches using PNG32 are significantly larger than the other supported formats and will take more disk space and require greater bandwidth to serve clients. JPEG \u2014 A lossy, three-channel image format that supports large color variations (16 million colors) but does not support transparency. Each pixel contains three 8-bit color channels. Caches using JPEG provide control over output quality and size. MIXED \u2014 Creates PNG 32 anywhere that transparency is detected (in other words, anywhere that the data frame background is visible). Creates JPEG for the remaining tiles. This keeps the average file size down while providing you with a clean overlay on top of other caches.", "dataType": "String"}, {"name": "tile_compression_quality", "isOptional": true, "description": "Enter a value between 1 and 100 for the JPEG compression quality. The default value is 75 for JPEG tile format and zero for other formats. Compression is supported only for JPEG format. Choosing a higher value will result in a larger file size with a higher-quality image. Choosing a lower value will result in a smaller file size with a lower-quality image. ", "dataType": "Long"}, {"name": "storage_format", "isOptional": true, "description": "Determines the storage format of tiles. COMPACT \u2014 Group tiles into large files called bundles. This storage format is more efficient in terms of storage and mobility. EXPLODED \u2014 Store each tile as a separate file. ", "dataType": "String"}, {"name": "use_local_cache_dir", "isOptional": true, "description": "Determines whether bundle files should be written into a local directory on the server when creating a compact cache, instead of being written directly into the shared cache directory. USE_LOCAL_DIR \u2014 Tiles stored in bundles are written into a local directory, then copied into the shared cache directory as bundles are completed. This option improves performance when multiple machines are working on the caching job. This is the default when the storage format is Compact. DO_NOT_USE_LOCAL_DIR \u2014 Tiles are written directly into the shared cache directory. This is the only valid option when storage_format is Exploded.", "dataType": "Boolean"}]},
{"syntax": "UpdateDiagrams_schematics (in_container, {builder_options}, {recursive}, {diagram_type}, {last_update_criteria})", "name": "Update Diagrams (Schematics)", "description": "Updates schematic diagrams stored in a schematic dataset or schematic folder.  All diagrams or a subset of diagrams (for example, diagrams related to a specific diagram template or diagrams that have not been updated for a particular number of days) can be updated.  Only diagrams based on the Standard builder\u2014that is,  diagrams built from features organized into a geometric network or network dataset  and  schematic diagrams built from custom queries \u2014can be updated using this geoprocessing tool. Diagrams based on the  Network Dataset builder  and  XML builder  that require specific input data cannot be updated using this tool.", "example": {"title": "UpdateDiagrams - Example (Stand-alone Python script)", "description": "Update schematic diagrams contained in a specified schematic folder, implemented by a particular diagram template, or that have not been updated for a specified number of days.", "code": "# Name: UpdateDiagrams.py # Description: Update schematic diagrams # Requirement: ArcGIS Schematics extension # import system modules import arcpy msgNoLicenseAvailable = \"ArcGIS Schematics extension license required\" try : # Checks out the ArcGIS Schematics extension license if arcpy.CheckExtension ( \"Schematics\" ) == \"Available\" : arcpy.CheckOutExtension ( \"Schematics\" ) else : raise Exception ( msgNoLicenseAvailable ) # Sets environnement settings arcpy.env.overwriteOutput = True arcpy.env.workspace = \"C:\\ArcGIS\\ArcTutor\\Schematics\\Schematics_In_ArcMap\\ElecDemo.gdb\" # Updates diagrams stored on a specified schematic folder. For example, diagrams in the Feeders schematic folder arcpy.UpdateDiagrams_schematics ( \"ElecDemo\\Feeders\" ) # Updates diagrams based on a specified diagram template. For example, diagrams based on the GeoSchematic diagram template arcpy.UpdateDiagrams_schematics ( \"ElecDemo\" , \"#\" , \"RECURSIVE\" , \"GeoSchematic\" ) # Updates diagrams stored on a specified schematic folder that have not been updated for N days. For example, diagrams stored in the Inside_Plants schematic folder not updated for 7 days arcpy.UpdateDiagrams_schematics ( \"ElecDemo\\Inside Plants\" , \"#\" , \"RECURSIVE\" , \"#\" , \"7\" ) # Returns the ArcGIS Schematics extension license arcpy.CheckInExtension ( \"Schematics\" ) print \"Script completed successfully\" except Exception as e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"An error occured on line  %i \" % tb.tb_lineno print str ( e )"}, "usage": ["This tool is mostly used to batch diagram updates. But, when user data has been dropped and reloaded since the diagram generations, it can also be used to resynchronize the schematic features with their related features/objects based on GUIDs.", "If the ", "Diagram Template", " parameter is specified, only the diagrams based on that template will be updated.", "If the ", "Number of days without update", " parameter is specified, only the diagrams for which the last update date meets the criterion will be updated.", "By default, when the value specified for ", "Input Schematic Container", " is a schematic folder, the update process operates recursively on all diagrams contained in that schematic folder and on all those contained in its subfolders. If you do not want the diagrams contained in the subfolders to be updated, set 0 for the recursive parameter."], "parameters": [{"name": "in_container", "isInputFile": true, "isOptional": false, "description": "The schematic dataset or schematic folder in which the diagrams are stored. This container must already exist. ", "dataType": "Schematic Dataset; Schematic Folder"}, {"name": "builder_options", "isOptional": true, "description": "The schematic builder update options. They are optional. KEEP_MANUAL_MODIF \u2014 Default option. Use it if you want the schematic features that have been removed/reduced from the diagram not to reappear and the edited connections to be kept in the updated diagram. This is the default. NO_KEEP_MANUAL_MODIF \u2014 Use it if you want the removed/reduced schematic features and reconnected schematic feature links to be restored after the update. RESYNC_FROM_GUID \u2014 Use this particular option if you want to resynchronize the schematic related network feature/object information based on GUIDs. This option must be used to avoid errors or data corruption when diagrams are updated while user data has been dropped and reloaded since their generations. Note that when using this option, the process works on the GUIDs to try to reattach the schematic features in the diagrams to their expected related network features/objects, but the diagram contents are not updated when the process ends. Once the reattachment is done, the real update can be launched.", "dataType": "String"}, {"name": "recursive", "isOptional": true, "description": " RECURSIVE \u2014 Search recursively in subfolders. NO_RECURSIVE \u2014 Do not search recursively in subfolders.", "dataType": "Boolean"}, {"name": "diagram_type", "isOptional": false, "description": "The diagram template of the schematic diagram to update. ", "dataType": "String"}, {"name": "last_update_criteria", "isOptional": true, "description": "The number of days between diagram updates. The default is zero (0), meaning all diagrams will be updated daily. ", "dataType": "Long"}]},
{"syntax": "UpdateDiagram_schematics (in_diagram, {in_data}, {builder_options})", "name": "Update Diagram (Schematics)", "description": "Updates a schematic diagram.  Depending on the schematic builder, the diagram update can be based on feature layers, feature classes, object tables, a solved network analysis, or an XML file.", "example": {"title": "UpdateDiagram and Standard builder working from custom queries example 1 (stand-alone Python script)", "description": "Updates a sample schematic diagram entirely built from custom queries. In this case, only the diagram name parameter is required.", "code": "# Name: UpdateDiagramCustomQuery.py # Description: Update a schematic diagram entirely built from custom queries # Requirement: ArcGIS Schematics extension # import system modules import arcpy msgNoLicenseAvailable = \"ArcGIS Schematics extension license required\" try : # Checks out the ArcGIS Schematics extension license if arcpy.CheckExtension ( \"Schematics\" ) == \"Available\" : arcpy.CheckOutExtension ( \"Schematics\" ) else : raise Exception ( msgNoLicenseAvailable ) # Sets environment settings arcpy.env.overwriteOutput = True arcpy.env.workspace = \"C:\\ArcGIS\\ArcTutor\\Schematics\\Schematics_In_ArcMap\\ElecDemo.gdb\" # UpdateDiagram by only refreshing the attributes on schematic features contained in the input diagram; builder_options=REFRESH arcpy.UpdateDiagram_schematics ( \"ElecDemo\\Inside Plants\\Substation 08\" , \"#\" , \"REFRESH\" ) # UpdateDiagram by fully synchronizing the diagram content regarding the custom queries; no udpate parameters required arcpy.UpdateDiagram_schematics ( \"ElecDemo\\Inside Plants\\Substation 08\" ) # Returns the ArcGIS Schematics extension license arcpy.CheckInExtension ( \"Schematics\" ) print \"Script completed successfully\" except Exception as e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"An error occurred on line  %i \" % tb.tb_lineno print str ( e )"}, "usage": ["The ", "Input Data", " parameter is mandatory for diagrams working with the ", "Network Dataset builder", " and ", "XML builder", ". It is optional for diagrams working with the ", "Standard builder when it is configured to operate from a geometric network or network dataset", ". It must not be specified for diagrams that work with the ", "Standard builder when it is configured to operate from custom queries", ".", "When using the Update Diagram tool on diagrams implemented by the ", "Standard builder", " that have been generated from a geometric network trace, the update operates from the updated trace result based on the trace parameters that persisted in the schematic dataset the first time the diagram was generated from a highlighted trace.", "When the update must operate from features that compose a geometric network or network dataset, it is not necessary to specify all the feature layers in the ", "Input Data", " parameter. If the ", "Add connected nodes", " option is specified for the Standard builder properties, only the feature layers related to edge features or edge network elements can be specified, even if all the feature layers related to the geometric network or network dataset (junctions and edges) will be used for update.", "If a particular layout was saved for the specified schematic diagram, the schematic features that were in the diagram before updating are displayed according to their last saved position, and new schematic features that may have been introduced during the update are positioned at their geographic coordinates."], "parameters": [{"name": "in_diagram", "isInputFile": true, "isOptional": false, "description": " The schematic diagram layer to be updated. ", "dataType": "Schematic Layer"}, {"name": "in_data", "isInputFile": true, "isOptional": false, "description": " The input data on which the diagram update will be based. The Input Data parameter is not required for all predefined builders; it's an optional parameter: For diagram templates that work with the Standard builder configured to operate from custom queries, no input tables must be set. For diagram templates that work with the XML builder, the Input Data parameter must reference an XML file. For diagram templates that work with the Network Dataset builder, the input data must be specified. It must reference a unique network analysis layer. If the related network analysis is not performed, Schematics tries to perform it before using the solved analysis layer as input. For diagram templates that work with the Standard builder configured to operate from a geometric network or a network dataset, when the Input Data parameter is not specified, the Update Diagram tool works from the initial set of network features/objects used to generate the diagram or from the updated geometric network trace result based on the trace parameters that persisted in the schematic database if the diagram was generated from a highlighted trace. When the Input Data parameter is specified, it must reference at least a feature layer, feature class, or object table so the update operates on this data according to the Builder Options value.", "dataType": "Table View;Data Element;Layer"}, {"name": "builder_options", "isOptional": false, "description": " The schematic builder update options. Update options are optional. They depend on the builder related to the diagram template that implements the specified schematic diagram: Diagrams generated from custom queries (Standard builder): KEEP_MANUAL_MODIF , NO_KEEP_MANUAL_MODIF , or REFRESH . Diagrams generated from XML data (XML builder): KEEP_MANUAL_MODIF or NO_KEEP_MANUAL_MODIF . Diagrams generated from solver results on network datasets (Network Dataset builder): NO_MERGE_NODES;KEEP_MANUAL_MODIF , NO_MERGE_NODES;NO_KEEP_MANUAL_MODIF , MERGE_NODES;KEEP_MANUAL_MODIF or MERGE_NODES;NO_KEEP_MANUAL_MODIF . Diagrams generated from features organized into a geometric network or a network dataset (Standard builder): When the Input Data parameter is not specified\u2014 KEEP_MANUAL_MODIF , NO_KEEP_MANUAL_MODIF , or REFRESH When the Input Data parameter is specified\u2014 REBUILD;KEEP_MANUAL_MODIF , REBUILD;NO_KEEP_MANUAL_MODIF , APPEND;KEEP_MANUAL_MODIF , APPEND;NO_KEEP_MANUAL_MODIF , APPEND_QUICK;KEEP_MANUAL_MODIF or APPEND_QUICK;NO_KEEP_MANUAL_MODIF KEEP_MANUAL_MODIF \u2014 Default option for diagram based on the XML builder or on the Standard builder when no input data is set. Use it to synchronize the input diagram content either against the original selection/trace/query used to initially generate this diagram\u2014Standard diagram\u2014or from an updated version of the XML input data initially used to generate it\u2014XML diagram\u2014whereas the schematic features that may have been manually removed/reduced/reconnected are kept in the updated diagram. NO_KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the XML builder or on the Standard builder when no input data is set. Use it to synchronize the input diagram content either against the original selection/trace/query used to initially generate this diagram\u2014Standard diagram\u2014or from an updated version of the XML input data initially used to generate it\u2014XML diagram\u2014whereas the schematic features that may have been manually removed/reduced/reconnected are restored in the updated diagram. REFRESH \u2014 Option available for diagrams based on the Standard builder when no input data is set. Use it to simply refresh the attributes for all schematic features in the input diagram to the current state of the related network features in the geometric network or network dataset feature classes. REBUILD;KEEP_MANUAL_MODIF \u2014 Default option available for diagrams based on the Standard builder when input data is set. Use this option if you want the input diagram to be completely rebuilt according to the specified input feature layers, feature classes, or object tables, whereas the schematic features that may have been manually removed/reduced/reconnected are kept in the updated diagram. REBUILD;NO_KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Standard builder when input data is set. Use this option if you want the input diagram to be completely rebuilt according to the specified input feature layers, feature classes, or object tables, whereas the schematic features that may have been manually removed/reduced/reconnected are restored in the updated diagram. APPEND;KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Standard builder when input data is set. Use this option if you want to append schematic features associated with the specified input feature layers, feature classes, or object tables with a full synchronization of the input diagram content, whereas the schematic features that may have been manually removed/reduced/reconnected are kept in the updated diagram. APPEND;NO_KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Standard builder when input data is set. Use this option if you want to append schematic features associated with the specified input feature layers, feature classes, or object tables with a full synchronization of the input diagram content, whereas the schematic features that may have been manually removed/reduced/reconnected are restored in the updated diagram. APPEND_QUICK;KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Standard builder when input data is set. Use this option if you want to append schematic features associated with the specified input feature layers, feature classes, or object tables with a partial synchronization of the input diagram content, whereas the schematic features that may have been manually removed/reduced/reconnected are kept in the updated diagram. APPEND_QUICK;NO_KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Standard builder when input data is set. Use this option if you want to append schematic features associated with the specified input feature layers, feature classes, or object tables with a partial synchronization of the input diagram content, whereas the schematic features that may have been manually removed/reduced/reconnected are restored in the updated diagram. NO_MERGE_NODES;KEEP_MANUAL_MODIF \u2014 Default option for diagrams based on the Network Dataset builder. Use this option to update the input diagram from the specified solved network analysis layer without merging nodes that occur several times in this layer, whereas the schematic features that may have been manually removed/reduced/reconnected are kept in the updated diagram. NO_MERGE_NODES;NO_KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Network Dataset builder. Use this option to update the input diagram from the specified solved network analysis layer without merging nodes that occur several times in this layer, whereas the schematic features that may have been manually removed/reduced/reconnected are restored in the updated diagram. MERGE_NODES;KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Network Dataset builder. Use it to update the input diagram from the specified solved network analysis layer and merge nodes that occur several times in this layer, whereas the schematic features that may have been manually removed/reduced/reconnected are kept in the updated diagram. MERGE_NODES;NO_KEEP_MANUAL_MODIF \u2014 Option available for diagrams based on the Network Dataset builder. Use it to update the input diagram from the specified solved network analysis layer and merge nodes that occur several times in this layer, whereas the schematic features that may have been manually removed/reduced/reconnected are restored in the updated diagram.", "dataType": "String"}]},
{"syntax": "ConvertDiagram_schematics (in_diagram, out_location, {reuse_fc}, {export_related_attributes}, {container_geometry}, {config_keyword})", "name": "Convert Diagram To Features (Schematics)", "description": "Converts a schematic diagram to standard feature classes or shapefiles.   Schematic diagrams are all stored in hidden feature classes that are specific to Schematics and need the rest of the schematic configuration tables and information to be useful. This tool allows you to share diagrams with other people without having to provide the entire schematic dataset, which includes all the diagrams and the configuration.  If you plan to convert multiple diagrams, you can choose whether they will all be placed into the same set of standard feature classes or shapefiles or if each diagram will get its own set of feature classes or shapefiles.", "example": {"title": "ConvertDiagram example (script tool)", "description": "The following script demonstrates how to use the ConvertDiagram function in a script tool. This script tool loops on all diagrams contained in an input schematic container\u2014that is, a schematic dataset or schematic folder\u2014and converts them to standard features or shapefiles in an output location.", "code": "# Name: ConvertDiagrams.py # Description: Convert schematic diagrams # Requirement: ArcGIS Schematics extension # import arcpy, sys, math import arcpy , sys , math msgInputsErr = \"Invalid arguments.\" msgParseErr = \"Cannot parse input arguments.\" msgConvertErr = \"Error during conversion.\" msgNoLicenseAvailable = \"ArcGIS Schematics extension license required\" # Recursively searches for schematic diagrams in the folders def RecursiveSearch ( inCont , bRecursive ): try : childs = inCont.Children dataset = None for dataset in childs : if dataset.DataType == \"SchematicFolder\" and bRecursive : RecursiveSearch ( dataset , bRecursive ) elif dataset.DataType == \"SchematicDiagram\" : if diagramClassFilter == \"\" or diagramClassFilter == dataset.DiagramClassname : pathList.append ( dataset.CatalogPath ) except : raise try : # Checks out the ArcGIS Schematics extension license     if arcpy.CheckExtension ( \"Schematics\" ) == \"Available\" : arcpy.CheckOutExtension ( \"Schematics\" ) else : raise Exception ( msgNoLicenseAvailable ) except Exception as exc : print exc raise arcpy.env.overwriteOutput = True # Decodes parameters try : inputContainer = arcpy.GetParameterAsText ( 0 ) outputLocation = arcpy.GetParameterAsText ( 1 ) recursive = arcpy.GetParameterAsText ( 2 ) if recursive == \"false\" : recursive = None diagramClassFilter = arcpy.GetParameterAsText ( 3 ) if inputContainer == \"\" : raise Exception () except : print msgParseErr arcpy.AddError ( msgParseErr ) raise # Main code try : pathList = [] # List for diagram names to convert arcpy.SetProgressorLabel ( \"Searching diagrams to convert...\" ) RecursiveSearch ( arcpy.Describe ( inputContainer ), recursive ) arcpy.SetProgressor ( \"step\" , \"Converting...\" , 0 , len ( pathList ), 1 ) for path in pathList : # Execute convert mes = \"Converting Schematic Diagram : \" + path # Set the progressor label arcpy.SetProgressorLabel ( mes ) arcpy.AddMessage ( mes ) arcpy.ConvertDiagram_schematics ( path , outputLocation , \"REUSE_FC\" , \"NO_RELATED_ATTRIBUTES\" , \"#\" ) arcpy.SetProgressorPosition () # Returns the ArcGIS Schematics extension licence arcpy.CheckInExtension ( \"Schematics\" ) except : arcpy.AddError ( msgConvertErr ) raise"}, "usage": ["To convert a schematic diagram layer  to standard feature classes, browse to and select a geodatabase/feature dataset for the specified output location parameter. To convert a schematic diagram layer to shapefiles, browse to and select a folder.", "To convert several diagrams  into the same standard feature classes/shapefiles, check the ", "Reuse Existing Structure", " option. To convert several diagrams into different standard feature classes/shapefiles, uncheck the ", "Reuse Existing Structure", " option. ", "Check the ", "Export All Related Feature Attributes", " option if you want  to have all the real feature class attributes to be appended to the schematic records during the convert process. ", "The ", "Export All Related Attribute Features", " option only works for schematic feature classes for which the Associated Feature Class/Table field value is known. If no associated feature class/table is specified for a schematic feature class, its related attribute features cannot be exported.", "Select ", "POLYGON", " in ", "Container Geometry", " if you want all containers in the input schematic diagram to be converted as a polygon feature. If you want such containers to be converted as polyline (or point) features, select ", "POLYLINE", " (or ", "POINT", ").", "When working with the ", "Reuse Existing Structure", " option while ", "POLYGON", " (", "POLYLINE", " or ", "POINT", ") has been chosen for the first diagram conversion, there is no way to change the conversion to POLYLINE or POINT (POLYGON) for the next conversions. If you want to change it, you must remove the structure and convert your diagrams again.", "When the specified input schematic diagram layer has already been converted in the specified output location, it will be deleted before being re-created."], "parameters": [{"name": "in_diagram", "isInputFile": true, "isOptional": false, "description": "The schematic diagram layer to be converted. ", "dataType": "Schematic Layer"}, {"name": "out_location", "isOutputFile": true, "isOptional": false, "description": "Workspace or feature dataset in which the schematic diagram will be converted. This container must already exist. ", "dataType": "Workspace;Feature Dataset"}, {"name": "reuse_fc", "isOptional": false, "description": "Indicates whether the input schematic diagram layer will be converted in the same standard feature classes/shapefiles as the other diagrams based on the same diagram template. REUSE_FC \u2014 Must be used to convert the specified diagram into the same standard feature classes/shapefiles as the other diagrams\u2014based on the same diagram template\u2014have already been or will be converted. In this case, depending on the chosen output location, the specified diagram is converted: Into a feature dataset whose name corresponds to the diagram template name and each feature class name corresponding to the schematic feature class names. Into a folder whose name corresponds to the diagram template name and each shapefile name corresponding to the schematic feature class names. This is the default. NO_REUSE_FC \u2014 Converts the input diagram into a new feature dataset/folder whose name is the concatenation of the input diagram's diagram template name and the diagram name\u2014each feature class/shapefile name being the concatenation of the schematic feature class name and diagram name. ", "dataType": "Boolean"}, {"name": "export_related_attributes", "isOptional": true, "description": "Indicates whether all the attributes stored in the real feature classes/object tables associated with the schematic feature classes will also be converted. If no associated feature class/table is specified for a schematic feature class, no feature attributes can be converted. When using REUSE_FC and EXPORT_RELATED_ATTRIBUTES , the structure must already exist with the associated feature fields, so the related attributes are converted. NO_RELATED_ATTRIBUTES \u2014 Each schematic feature contained in the input diagram is converted with only the attributes contained in its schematic feature class. This is the default. EXPORT_RELATED_ATTRIBUTES \u2014 Each schematic feature contained in the input diagram is converted with the attributes contained in its schematic feature class and all the attributes related to its associated real feature. ", "dataType": "Boolean"}, {"name": "container_geometry", "isOptional": false, "description": "Indicates the geometry type of the features that will be created for the converted schematic containers contained in the input diagram. When using the Reuse Existing Structure option and the structure already exists with POLYGON (POLYLINE or POINT) feature classes created for container schematic features, there is no way to change the feature class type to POLYLINE or POINT (POLYGON) for the next conversions. POLYGON \u2014 Each container in the input diagram is converted as a polygon feature. This is the default. POLYLINE \u2014 Each container in the input diagram is converted as a polyline feature. POINT \u2014 Each container in the input diagram is converted as a point feature. ", "dataType": "String"}, {"name": "config_keyword", "isOptional": true, "description": "The configuration keyword that determines the storage parameters of the table in a relational database management system (RDBMS). This is for ArcSDE only. ", "dataType": "String"}]},
{"syntax": "CreateSchematicFolder_schematics (out_location, out_name)", "name": "Create Schematic Folder (Schematics)", "description": "Creates a schematic folder in a schematic dataset or schematic folder.", "example": {"title": "CreateSchematicFolder example (Stand-alone Python script)", "description": "Create a schematic folder and subfolder in a schematic dataset.", "code": "# Name: CreateSchematicFolder.py # Description: Create a schematic folder and subfolder # Requirement: ArcGIS Schematics extension # import system modules import arcpy msgNoLicenseAvailable = \"ArcGIS Schematics extension license required\" try : # Checks out the ArcGIS Schematics extension licence if arcpy.CheckExtension ( \"Schematics\" ) == \"Available\" : arcpy.CheckOutExtension ( \"Schematics\" ) else : raise Exception ( msgNoLicenseAvailable ) # Sets environnement settings arcpy.env.overwriteOutput = True arcpy.env.workspace = \"C:\\ArcGIS\\ArcTutor\\Schematics\\Schematics_In_ArcMap\\ElecDemo.gdb\" # Creates a new schematic folder, MySchematicRootFolder, at the ElecDemo schematic dataset root. arcpy.CreateSchematicFolder_schematics ( \"ElecDemo\" , \"MySchematicRootFolder\" ) # Creates a schematic subfolder, MySchematicSubFolder, in a schematic folder, MySchematicRootFolder. arcpy.CreateSchematicFolder_schematics ( \"ElecDemo\\MySchematicRootFolder\" , \"MySchematicSubFolder\" ) # Returns the ArcGIS Schematics extension licence arcpy.CheckInExtension ( \"Schematics\" ) print \"Script completed successfully\" except Exception as e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"An error occured on line  %i \" % tb.tb_lineno print str ( e )"}, "usage": ["The output schematic folder must not exist. (The Overwrite output of geoprocessing setting has no effect.)"], "parameters": [{"name": "out_location", "isOutputFile": true, "isOptional": false, "description": "The schematic dataset or schematic folder in which the folder will be created. This container must already exist. ", "dataType": "Schematic Dataset;Schematic Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "Name of the output schematic folder. ", "dataType": "String"}]},
{"syntax": "CreateDiagram_schematics (out_location, out_name, diagram_type, {in_data}, {builder_options})", "name": "Create Diagram (Schematics)", "description": "Creates a schematic diagram.  Depending on the schematic builder, diagram creation can be based on feature layers, feature classes, object tables, a network analysis layer, or XML data.", "example": {"title": "CreateDiagram and Standard builder working on geometric network data - Example 1 (Stand-alone Python script)", "description": "Create sample schematic diagrams from GIS features organized into a geometric network.", "code": "# Name: CreateDiagramStd.py # Description: Create schematic diagrams sample from GIS features organized into a geometric network # Requirement: ArcGIS Schematics extension # import system modules import arcpy msgNoLicenseAvailable = \"ArcGIS Schematics extension license required\" try : # Checks out the ArcGIS Schematics extension license if arcpy.CheckExtension ( \"Schematics\" ) == \"Available\" : arcpy.CheckOutExtension ( \"Schematics\" ) else : raise Exception ( msgNoLicenseAvailable ) # Sets environment settings arcpy.env.overwriteOutput = True arcpy.env.workspace = \"C:\\ArcGIS\\ArcTutor\\Schematics\\Schematics_In_ArcMap\\ElecDemo.gdb\" # CreateDiagram from a feature class, Feeder arcpy.CreateDiagram_schematics ( \"ElecDemo\" , \"FeederDiagram\" , \"GeoSchematic\" , \"ElectricNetwork\\Feeder\" ) # CreateDiagram from a feature layer # 1) Creates a layer with the selection InputLayer = arcpy.MakeFeatureLayer_management ( \"ElectricNetwork\\PrimaryLine\" , \"PrimaryLineLayer\" , \"PHASECODE =135\" ) # 2) Uses the layer as input of CreateDiagram arcpy.CreateDiagram_schematics ( \"ElecDemo\" , \"ElectricMainNetworkDiagram\" , \"GeoSchematic\" , InputLayer ) # Returns the ArcGIS Schematics extension license arcpy.CheckInExtension ( \"Schematics\" ) print \"Script completed successfully\" except Exception as e : # If an error occurred, print line number and error message import traceback , sys tb = sys.exc_info ()[ 2 ] print \"An error occured on line  %i \" % tb.tb_lineno print str ( e )"}, "usage": ["If the output schematic diagram name already exists, it may be deleted before being re-created. To avoid this deletion, you can uncheck the ", "Overwrite the outputs of geoprocessing operations", " box from the ", "Geoprocessing Options", " dialog box.", "Depending on the schematic builder related to the diagram template specified for the diagram to be created, the ", "Input Data", " parameter is required or not:"], "parameters": [{"name": "out_location", "isOutputFile": true, "isOptional": false, "description": "Schematic dataset or schematic folder in which the diagram will be created. This container must already exist. ", "dataType": "Schematic Dataset;Schematic Folder"}, {"name": "out_name", "isOutputFile": true, "isOptional": false, "description": "Name of the schematic diagram to be created. ", "dataType": "String"}, {"name": "diagram_type", "isOptional": false, "description": "Schematic diagram template of the schematic diagram to be created. ", "dataType": "String"}, {"name": "in_data", "isInputFile": true, "isOptional": false, "description": "The input data to generate the diagram. The Input Data parameter specifies the elements on which the diagram generation will be based. It must be specified for most of the predefined schematic builders: For diagram templates that work with the Standard builder when it is configured to operate from a geometric network or network dataset, XML builder, and Network Dataset builder, if no input data is specified, the diagram cannot be generated. For diagram templates that work with the Standard builder configured to operate from a geometric network or network dataset, the Input Data parameter must at least reference a feature layer, feature class, or object table. For diagram templates that work with the Network Dataset builder, the Input Data parameter must reference a unique network analysis layer. If the related network analysis is not performed, Schematics tries to perform it before using the solved analysis layer as input. For diagram templates related to the XML builder, the Input Data parameter is optional when the data for the diagram generation is provided by an external component. But when no external component is defined for the builder properties, the Input Data must reference an XML file. In both cases, the input XML data must be built according to the XMLBuilderDiagram XML Schema file . For diagram templates that work with the Standard builder configured to operate from custom queries, no input data is required.", "dataType": "Table View;Data Element;Layer"}, {"name": "builder_options", "isOptional": true, "description": "The schematic builder creation parameters. These parameters are required only for diagrams based on the Network Dataset builder to specify whether nodes will be merged. MERGE_NODES \u2014 Specifies the network element junctions, which are taken several times along the resultant route of a route analysis to be represented by a single schematic feature node in the generated schematic diagram. It also allows you to merge midspanjunction elements when they are at the same position in a service area analysis. NO_MERGE_NODES \u2014 Specifies that no schematic feature node will be merged (default).", "dataType": "String"}]},
{"syntax": "OrdinaryLeastSquares_stats (Input_Feature_Class, Unique_ID_Field, Output_Feature_Class, Dependent_Variable, Explanatory_Variables, {Coefficient_Output_Table}, {Diagnostic_Output_Table}, {Output_Report_File})", "name": "Ordinary Least Squares (OLS) (Spatial Statistics)", "description": "Performs global Ordinary Least Squares (OLS) linear regression to generate predictions or to model a dependent variable in terms of its relationships to a set of explanatory variables.   You can access the results of this tool (including the optional report file) from the  Results  window.  If you disable background processing, results will also be written to the  Progress  dialog box. \r\n Learn more about how Ordinary Least Squares regression works \r\n", "example": {"title": "OrdinaryLeastSquares example 1 (Python window)", "description": "The following Python window script demonstrates how to use the OrdinaryLeastSquares tool.", "code": "import arcpy arcpy.env.workspace = r\"c:\\data\" arcpy.OrdinaryLeastSquares_stats ( \"USCounties.shp\" , \"MYID\" , \"olsResults.shp\" , \"GROWTH\" , \"LOGPCR69;SOUTH;LPCR_SOUTH;PopDen69\" , \"olsCoefTab.dbf\" , \"olsDiagTab.dbf\" )"}, "usage": ["The primary output for this tool is the OLS summary report which is written to the ", "Results window", " or optionally written, with additional graphics, to the ", "Output Report File", " you specify.  Double-clicking the PDF report file in the ", "Results", " window will open it.  Right-clicking on the ", "Messages entry", " in the ", "Results", " window and selecting ", "View", " will also display the OLS summary report in a ", "Message", " dialog box.  ", "The OLS tool also produces an output feature class and optional tables with coefficient information and diagnostics. All of these are accessible from the ", "Results", " window. The output feature class is automatically added to the table of contents, with a hot/cold rendering scheme applied to model ", "residuals", ".  A full explanation of each output is provided in ", "Interpreting_OLS_results", ".", "If this tool is part of a custom model tool, the optional tables will only appear in the ", "Results", " window if they are set as ", "model parameters", " prior to running the tool.", "Results from OLS regression are only trustworthy if your data and regression model satisfy all of the assumptions inherently required by this method. Consult the table ", "Common Regression Problems, Consequences, and Solutions", " in ", "Regression Analysis Basics", " to ensure your model is properly specified.", "Dependent and Explanatory variables should be numeric fields containing a variety of values. OLS cannot solve when variables have all the same value (all the values for a field are 9.0, for example). Linear regression methods, like OLS, are not appropriate for predicting binary outcomes (for example, all of the values for the dependent variable are either 1 or 0).", "The ", "Unique ID", " field links model predictions to each feature. Consequently, the ", "Unique ID", " values must be unique for every feature, and typically should be a permanent field that remains with the feature class. If you don't have a ", "Unique ID", " field, you can easily create one by adding a new integer field to your feature class table and calculating the field values to be equal to the FID/OID field. You cannot use the FID/OID field directly for the ", "Unique ID", " parameter.", "Whenever there is statistically significant spatial autocorrelation of the regression residuals the OLS model will be considered misspecified and, consequently, results from OLS regression are unreliable. Be sure to run the ", "Spatial Autocorrelation", " tool on your regression residuals to assess this potential problem. Statistically significant spatial autocorrelation of regression residuals almost always indicates a key missing an explanatory variable.", "You should visually inspect the over- and underpredictions evident in your regression residuals to see if they provide clues about potential missing variables from your regression model. It sometimes helps to run ", "Hot Spot Analysis", " on the residuals to help you visualize spatial clustering of the over- and underpredictions.", "When misspecification is the result of trying to model nonstationary variables using a global model (OLS is a global model), then ", "Geographically Weighted Regression", " may be used to improve predictions and to better understand the ", "nonstationarity", " (regional variation) inherent in your explanatory variables.", "When the result of a computation is infinity or undefined, the output for nonshapefiles will be Null; for shapefiles the output will be -DBL_MAX (-1.7976931348623158e+308, for example).", "Model summary diagnostics are written to the OLS summary report and the optional diagnostic output table.  Both include diagnostics for the corrected Akaike Information Criterion (AICc), Coefficient of Determination, Joint F statistic, Wald statistic, Koenker's Breusch-Pagan statistic, and the Jarque-Bera statistic.  The diagnostic table also includes uncorrected AIC and Sigma-squared values.", "The optional coefficient and/or diagnostic output tables, if they already exist, will be overwritten when the Geoprocessing Option to ", "overwrite the outputs of geoprocessing operations", " is checked ON.", "This tool will optionally create a PDF report summarizing results.  PDF files do not automatically appear in the ", "Catalog", " window.  If you want PDF files to be displayed in ", "Catalog", ", open the ", "ArcCatalog", " application, select the ", "Customize", "  menu option, click  ", "ArcCatalog Options", ", and select the ", "File Types", " tab.  Click on the ", "New Type", " button and specify ", "PDF", ", as shown below, for ", "File Extension", ".", "On machines configured with the ArcGIS language packages for Chinese, Japanese, or Arabic, you might notice missing text and/or formatting problems in the PDF ", "Output Report File", ".  These problems can be corrected by ", "changing the font settings", ". ", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class containing the dependent and independent variables for analysis. ", "dataType": "Feature Layer"}, {"name": "Unique_ID_Field", "isOptional": false, "description": "An integer field containing a different value for every feature in the Input Feature Class. ", "dataType": "Field"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to receive dependent variable estimates and residuals. ", "dataType": "Feature Class"}, {"name": "Dependent_Variable", "isOptional": false, "description": "The numeric field containing values for what you are trying to model. ", "dataType": "Field"}, {"name": "Explanatory_Variables", "isOptional": false, "description": "A list of fields representing explanatory variables in your regression model. ", "dataType": "Field"}, {"name": "Coefficient_Output_Table", "isOptional": true, "description": "The full path to an optional table that will receive model coefficients, standard errors, and probabilities for each explanatory variable. ", "dataType": "Table"}, {"name": "Diagnostic_Output_Table", "isOptional": true, "description": "The full path to an optional table that will receive model summary diagnostics. ", "dataType": "Table"}, {"name": "Output_Report_File", "isOutputFile": true, "isOptional": true, "description": " The path to the optional PDF file you want the tool to create. This report file includes model diagnostics, graphs, and notes to help you interpret the OLS results. ", "dataType": "File"}]},
{"syntax": "GeographicallyWeightedRegression_stats (in_features, dependent_field, explanatory_field, out_featureclass, kernel_type, bandwidth_method, {distance}, {number_of_neighbors}, {weight_field}, {coefficient_raster_workspace}, {cell_size}, {in_prediction_locations}, {prediction_explanatory_field}, {out_prediction_featureclass})", "name": "Geographically Weighted Regression (GWR) (Spatial Statistics)", "description": "Performs Geographically Weighted Regression (GWR), a local form of linear regression used to model spatially varying relationships.  \r\n Learn more about how Geographically Weighted Regression works \r\n", "example": {"title": "GeographicallyWeightedRegression Example (Python Window)", "description": "The following Python Window script demonstrates how to use the GeographicallyWeightedRegression tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data\" arcpy.GeographicallyWeightedRegression_stats ( \"CallData.shp\" , \"Calls\" , \"BUS_COUNT;RENTROCC00;NoHSDip\" , \"CallsGWR.shp\" , \"ADAPTIVE\" , \"BANDWIDTH PARAMETER\" , \"#\" , \"25\" , \"#\" , \"CoefRasters\" , \"135\" , \"PredictionPoints\" , \"#\" , \"GWRCallPredictions.shp\" )"}, "usage": ["GWR constructs a separate equation for every feature in the dataset incorporating the dependent and explanatory variables of features falling within the bandwidth of each target feature. The shape and extent of the bandwidth is dependent on user input for the ", "Kernel type", ", ", "Bandwidth method", ", ", "Distance", ", and ", "Number of neighbors", " parameters with one restriction: when the number of neighboring features would exceed 1000, only the closest 1000 are incorporated into each local equation.", "GWR should be applied to datasets with several hundred features for best results. It is not an appropriate method for small datasets. The tool does not work with multipoint data.", "The GWR tool produces a variety of different outputs.  Right-clicking on the ", "Messages entry", " in the ", "Results window", " and selecting ", "View", " will display a  GWR tool execution summary report .", "The ", "_supp", " file is always created in the same location as the  ", "Output feature class", " unless the output feature class is created inside a ", "Feature Dataset", ".  When the output feature class is inside a feature dataset,  the ", "_supp", " table is created in the geodatabase containing the ", "feature dataset", ".", "Using projected data is always recommended; it is especially important whenever distance is a component of the analysis, as it is for GWR when you select ", "FIXED", " for ", "Kernel type", ". It is strongly recommended that your data is projected using a ", "Projected Coordinate System", " (rather than a ", "Geographic Coordinate System", ").", "Some of the computations done by the GWR tool take advantage of multiple CPUs in order to increase performance and will automatically use up to 8 threads/CPUs for processing.", "You should always begin regression analysis with ", "Ordinary_Least_Squares (OLS)", " regression.  First find a ", "properly specified OLS model", ", then use the same explanatory variables to run GWR (excluding any \"dummy\" explanatory variables representing different spatial regimes).    ", "Dependent and Explanatory variables should be numeric fields containing a variety of values. Linear regression methods, like GWR, are not appropriate for predicting binary outcomes (e.g., all of the values for the dependent variable are either 1 or 0).", "In global regression models, such as ", "Ordinary Least Squares Regression (OLS)", ", results are unreliable when two or more variables exhibit multicollinearity (when two or more variables are redundant or together tell the same \"story\"). GWR builds a local regression equation for each feature in the dataset. When the values for a particular explanatory variable cluster spatially, you will very likely have problems with local ", "multicollinearity", ". The ", "condition number", " in the output feature class indicates when results are unstable due to local multicollinearity. As a rule of thumb, do not trust results for features with a condition number larger than 30, equal to Null or, for shapefiles, equal to -1.7976931348623158e+308.", "Caution should be used when including nominal/categorical data in a GWR model. Where categories cluster spatially, there is strong risk of encountering local multicollinearity issues. The condition number included in the GWR output indicates when local collinearity is a problem (a condition number less than zero, greater than 30, or set to Null). Results in the presence of local multicollinearity are unstable.", "Do not use \"dummy\" explanatory variables to represent different spatial regimes in a GWR model (e.g., census tracts outside the urban core are assigned a value of 1, while all others are assigned a value of 0). Because GWR allows explanatory variable coefficients to vary, these spatial regime explanatory variables are unnecessary, and if included, will create problems with local ", "multicollinearity", ".  ", "To better understand regional variation among the coefficients of your explanatory variables, examine the optional raster coefficient surfaces created by GWR.  These raster surfaces are created in the ", "Coefficient raster workspace", ", if you specify one. For polygon data, you can use graduated color or cold-to-hot rendering on each coefficient field in the ", "Output feature class", " to examine changes across your study area.", "You may use GWR for prediction by supplying a ", "Predictions locations", " feature class (often this feature class is the same as the ", "Input feature class", "), the ", "Prediction explanatory variables", ", and an ", "Output prediction feature class", ".  There must be a one to one correspondence between the fields used to calibrate the regression model (the values entered for the ", "Explanatory variables", " field) and the fields used for prediction (the values entered for the ", "Prediction explanatory variables", " field).  The order of these variables must be the same.  Suppose, for example, you are modeling traffic accidents as a function of speed limits, road conditions, number of lanes, and number of cars.  You can predict the impact that changing speed limits or improving roads might have on traffic accidents by creating a new variables with  the amended speed limits and road conditions.  The existing variables would be used to calibrate the regression model and would be used for the   ", "Explanatory variables", " parameter. The amended variables would be used for predictions and would be entered as your ", "Prediction explanatory variables", ".", "If a ", "Prediction locations", " feature class is provided, but no ", "Prediction explanatory variables", " are specified, the ", "Output prediction feature class", " is created with computed coefficients for each location only (no predictions).", "A regression model is misspecified if it is missing a key explanatory variable. Statistically significant spatial autocorrelation of the regression residuals and/or unexpected spatial variation among the coefficients of one or more explanatory variables suggests that your model is misspecified. You should make every effort (through OLS residual analysis and GWR coefficient variation analysis, for example) to discover what these key missing variables are so they may be included in the model.", "Always question whether or not it makes sense for an explanatory variable to be nonstationary. For example, suppose you are modeling the density of a particular plant species as a function of several variables including ASPECT. If you find that the coefficient for the ASPECT variable changes across the study area, you are likely seeing evidence of a key missing explanatory variable (perhaps prevalence of competing vegetation, for example). You should make every effort to include all key explanatory variables in your regression model.", "Whenever using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may, consequently, store null values as zero or as some very small negative number (-DBL_MAX = -1.7976931348623158e+308). This can lead to unexpected results.  See also: ", "Geoprocessing_considerations_for_shapefile_output", ".", "When the result of a computation is infinity or undefined, the result for nonshapefiles will be Null; for shapefiles the result will be -DBL_MAX = -1.7976931348623158e+308.", "When you select either the AICc or CV ", "Bandwidth Method", ", GWR will find the optimal distance (for FIXED kernel) or optimal number of neighbors (for ADAPTIVE kernel).  Problems with local multicollinearity, however, will prevent both the AICc and CV Bandwidth methods from resolving an optimal distance/number of neighbors. If you get an error indicating ", "severe model design", " problems, try specifying a particular distance or neighbor count, then examining the condition numbers in the output feature class to see which features are associated with local collinearity problems", "Problems with local collinearity will prevent both the AICc and CV Bandwidth methods from resolving an optimal distance/number of neighbors. If you get an error indicating severe model design problems, try specifying a particular distance or neighbor count, then examining the condition numbers in the ", "Output feature class", " to see which features are associated with local multicollinearity problems.", "Severe model design", " errors, or errors indicating local equations do not include enough neighbors, often indicate a problem with global or local multicollinearity. To determine where the problem is, run your model using ", "OLS", " and examine the ", "VIF value", " for each explanatory variable. If some of the VIF values are large (above 7.5, for example), global multicollinearity is preventing GWR from solving. More likely, however, local multicollinearity is the problem. Try creating a thematic map for each explanatory variable. If the map reveals spatial clustering of identical values, consider removing those variables from the model or combining those variables with other explanatory variables in order to increase value variation. If, for example, you are modeling home values and have variables for both bedrooms and bathrooms, you may want to combine these to increase value variation, or to represent them as bathroom/bedroom square footage. Avoid using spatial regime dummy variables, spatially clustering categorical/nominal variables, or variables with very few possible values when constructing GWR models.", "GWR is a linear model subject to the same requirements as OLS. Review the section titled \"", "How Regression Models Go Bad", "\" in the ", "Regression Analysis Basics", " document as a check that your GWR model is properly specified."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature class containing the dependent and independent variables. ", "dataType": "Feature Layer"}, {"name": "dependent_field", "isOptional": false, "description": "The numeric field containing values for what you are trying to model. ", "dataType": "Field"}, {"name": "explanatory_field", "isOptional": false, "description": "A list of fields representing independent explanatory variables in your regression model. ", "dataType": "Field"}, {"name": "out_featureclass", "isOutputFile": true, "isOptional": false, "description": "The output feature class to receive dependent variable estimates and residuals. ", "dataType": "Feature Class"}, {"name": "kernel_type", "isOptional": false, "description": "Specifies if the kernel is constructed as a fixed distance, or if it is allowed to vary in extent as a function of feature density. FIXED \u2014 The spatial context (the Gaussian kernel) used to solve each local regression analysis is a fixed distance. ADAPTIVE \u2014 The spatial context (the Gaussian kernel) is a function of a specified number of neighbors. Where feature distribution is dense, the spatial context is smaller; where feature distribution is sparse, the spatial context is larger.", "dataType": "String"}, {"name": "bandwidth_method", "isOptional": false, "description": "Specifies how the extent of the kernel should be determined. When AICc or CV are selected, the tool will find the optimal distance/neighbor parameter for you. Typically you will select either AICc or CV if you don't know what to use for the distance ( kernel_type = FIXED) or the number_of_neighbors ( kernel_type = ADAPTIVE) parameters. If you select BANDWIDTH_PARAMETER you will need to specify a value for the distance or number_of_neighbors parameters. AICc \u2014 The extent of the kernel is determined using the Akaike Information Criterion (AICc). CV \u2014 The extent of the kernel is determined using Cross Validation. BANDWIDTH_PARAMETER \u2014 The extent of the kernel is determined by a fixed distance or a fixed number of neighbors.", "dataType": "String"}, {"name": "distance", "isOptional": true, "description": "Specifies a fixed bandwidth extent or distance whenever the kernel type is FIXED and the bandwidth method is BANDWIDTH_PARAMETER. ", "dataType": "Double"}, {"name": "number_of_neighbors", "isOptional": true, "description": "An integer reflecting the exact number of neighbors to include in the local bandwidth of the Gaussian kernel whenever the kernel type is ADAPTIVE and the bandwidth method is BANDWIDTH_PARAMETER. ", "dataType": "Long"}, {"name": "weight_field", "isOptional": true, "description": "The numeric field containing a spatial weighting for individual features. This weight field allows some features to be more important in the model calibration process than others. Primarily useful when the number of samples taken at different locations varies, values for the dependent and independent variables are averaged, and places with more samples are more reliable (should be weighted higher). If you have an average of 25 different samples for one location, but an average of only 2 samples for another location, you can use the number of samples as your weight field so that locations with more samples have a larger influence on model calibration than locations with few samples. ", "dataType": "Field"}, {"name": "coefficient_raster_workspace", "isOptional": true, "description": "A full pathname to the workspace where all of the coefficient rasters will be created. When this workspace is provided, rasters are created for the intercept and every explanatory variable. ", "dataType": "Folder"}, {"name": "cell_size", "isOptional": true, "description": "The cell size (a number) or reference to the cell size (a pathname to a raster dataset) to use when creating the coefficient rasters. The default cell size is the shortest of the width or height of the extent specified in the geoprocessing environment output coordinate system, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "in_prediction_locations", "isInputFile": true, "isOptional": true, "description": "A feature class containing features representing locations where estimates should be computed. Each feature in this dataset should contain values for all of the explanatory variables specified; the dependent variable for these features will be estimated using the model calibrated for the input feature class data. ", "dataType": "Feature Layer"}, {"name": "prediction_explanatory_field", "isOptional": false, "description": "A list of fields representing explanatory variables in the Prediction locations feature class. These field names should be provided in the same order (a one-to-one correspondance) as those listed for the input feature class Explanatory variables parameter. If no prediction explanatory variables are given, the output prediction feature class will only contain computed coefficient values for each prediction location. ", "dataType": "Field"}, {"name": "out_prediction_featureclass", "isOutputFile": true, "isOptional": true, "description": "The output feature class to receive dependent variable estimates for each feature in the Prediction locations feature class. ", "dataType": "Feature Class"}]},
{"syntax": "GenerateSpatialWeightsMatrix_stats (Input_Feature_Class, Unique_ID_Field, Output_Spatial_Weights_Matrix_File, Conceptualization_of_Spatial_Relationships, {Distance_Method}, {Exponent}, {Threshold_Distance}, {Number_of_Neighbors}, {Row_Standardization}, {Input_Table}, {Date_Time_Field}, {Date_Time_Interval_Type}, {Date_Time_Interval_Value})", "name": "Generate Spatial Weights Matrix (Spatial Statistics)", "description": "Constructs a spatial weights matrix (SWM) file to represent the spatial relationships among features in a dataset. \r\n Learn more about how Generate Spatial Weights Matrix works \r\n", "example": {"title": "GenerateSpatialWeightsMatrix example 1 (Python window)", "description": "The following Python window script demonstrates how to use the GenerateSpatialWeightsMatrix tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.GenerateSpatialWeightsMatrix_stats ( \"911Count.shp\" , \"MYID\" , \"euclidean6Neighs.swm\" , \"K_NEAREST_NEIGHBORS\" , \"#\" , \"#\" , \"#\" , 6 , \"NO_STANDARDIZATION\" )"}, "usage": ["Output from this tool is a spatial weights matrix file (SWM).  Tools, such as ", "Hot_Spot_Analysis", ", that require you to specify a ", "Conceptualization of Spatial Relationships", " will accept a ", "spatial weights matrix file", "; select ", "GET_SPATIAL_WEIGHTS_FROM_FILE", " for the ", "Conceptualization of Spatial Relationships", " parameter, and for the ", "Weights Matrix File", " parameter, specify the full path to the spatial weights file you create using this tool.", "This tool also reports characteristics of the resultant spatial weights matrix file: number of features, connectivity, minimum, maximum and average number of neighbors.  This summary is accessible from the ", "Results", " window and may be viewed by right-clicking on the ", "Messages entry", " in the ", "Results", " window and selecting ", "View", ".  Using this summary, ensure that all features have at least 1 neighbor.  In general, especially with large datasets, a minimum of 8 neighbors and a low value for feature connectivity is desirable.", "For space/time analyses, select \t\tSPACE_TIME_WINDOW for the ", "Conceptualization of Spatial Relationships", " parameter.  You define space by specifying a ", "Threshold Distance", " value; you define time by specifying a ", "Date/Time Field", " and both a ", "Date/Time Type", " (such as HOURS or DAYS) and a ", "Date/Time Interval Value", ".  The ", "Date/Time Interval Value", " is an Integer.  For example, if you enter ", "1000", " feet, select HOURS, and provide a ", "Date/Time Interval Value", " of ", "3", ", features within 1,000 feet and occuring within 3 hours of each other would be considered neighbors.", "The spatial weights matrix file (", "*.swm", ") was designed to allow you to generate, store, reuse, and share your conceptualization of the relationships among a set of features.  To improve performance the file is created in a binary file format.  Feature relationships are stored as a sparse matrix, so only nonzero relationships are written to the ", "SWM", " file.   In general, tools will perform well even when the ", "SWM", " file contains more than 15 million nonzero relationships.  If a memory error is encountered when using the ", "SWM", " file, however, you should revisit how you are defining your feature relationships.  As a rule of thumb, you should aim for a spatial weights matrix where every feature has at least 1 neighbor, most have about 8 neighbors, and no feature has more than about 1,000 neighbors.", "When using a distance-based ", "Conceptualization of Spatial Relationships", ", data should be in a ", "Projected Coordinate System", " (rather than a ", "Geographic Coordinate System", "). ", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "The ", "Unique ID", " field is linked to feature relationships derived from running this tool. Consequently, the ", "Unique ID", " values must be unique for every feature and typically should be in a permanent field that remains with the feature class. If you don't have a Unique ID field, you can easily create one by adding a new integer field (", "Add Field", ") to your feature class table and calculating the field values to be equal to the FID/OID field (", "Calculate Field", "). You cannot use the FID/OID field directly for the ", "Unique ID", " parameter.", "The ", "Number of Neighbors", " parameter may override the ", "Threshold Distance", " parameter for Inverse or Fixed Distance Conceptualizations of Spatial Relationships. If you specify a threshold distance of 10 miles and 3 for the number of neighbors, all features will receive a minimum of 3 neighbors even if the threshold has to be increased to find them.  The threshold distance is only increased in those cases where the minimum number of neighbors is not met.", "The ", "CONVERT_TABLE", " option for the ", "Conceptualization of Spatial Relationships", " parameter may be used to convert an ", "ASCII spatial weights matrix file", " to a ", "SWM formatted spatial weights matrix file", ".  First, you will need to put your ASCII weights into a ", "formatted", " table (using Excel, for example).", "If your table includes weights for ", "self-potential", ", they will be omitted from the SWM output file, and the default self-potential value will be used in analyses.  The default self-potential value for the ", "Hot_Spot_Analysis", " tool is one, but this value can be overwritten by specifying a ", "Self-Potential Field", " value; for all other tools, the default self-potential value is zero.", "For polygon features, you will almost always want to choose ", "Row", " for the ", "Row Standardization", " parameter.  ", "Row Standardization", " mitigates bias when the number of neighbors each feature has is a function of the aggregation scheme or sampling process, rather than reflecting the actual spatial distribution of the variable you are analyzing.", "The ", "Modeling Spatial Relationships", " help topic provides additional information about this tool's parameters.", "The tools that can use a spatial weights matrix file project feature ", "geometry", " to the output coordinate system prior to analysis and all mathematical computations are based on the output coordinate system. Consequently, if the output coordinate system setting does not match the input feature class ", "spatial reference", ", either make sure, for all analyses using the spatial weights matrix file, that the output coordinate system matches the settings used when the spatial weights matrix file was created, or ", "project", " the input feature class so that it does match the spatial reference associated with the spatial weights matrix file.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class for which spatial relationships of features will be assessed. ", "dataType": "Feature Class"}, {"name": "Unique_ID_Field", "isOptional": false, "description": "An integer field containing a different value for every feature in the Input Feature Class. ", "dataType": "Field"}, {"name": "Output_Spatial_Weights_Matrix_File", "isOutputFile": true, "isOptional": false, "description": "The full path for the spatial weights matrix file (SWM) you want to create. ", "dataType": "File"}, {"name": "Conceptualization_of_Spatial_Relationships", "isOptional": false, "description": "Specifies how spatial relationships among features are conceptualized. Note: Polygon Contiguity methods are only available with an ArcGIS for Desktop Advanced license. INVERSE_DISTANCE \u2014 The impact of one feature on another feature decreases with distance. FIXED_DISTANCE \u2014 Everything within a specified critical distance of each feature is included in the analysis; everything outside the critical distance is excluded. K_NEAREST_NEIGHBORS \u2014 The closest k features are included in the analysis; k is a specified numeric parameter. CONTIGUITY_EDGES_ONLY \u2014 Polygon features that share a boundary are neighbors. CONTIGUITY_EDGES_CORNERS \u2014 Polygon features that share a boundary and/or share a node are neighbors. DELAUNAY_TRIANGULATION \u2014 A mesh of nonoverlapping triangles is created from feature centroids; features associated with triangle nodes that share edges are neighbors. SPACE_TIME_WINDOW \u2014 Features within a specified critical distance and specified time interval of each other are neighbors. CONVERT_TABLE \u2014 Spatial relationships are defined in a table. ", "dataType": "String"}, {"name": "Distance_Method", "isOptional": true, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Exponent", "isOptional": true, "description": "Parameter for inverse distance calculation. Typical values are 1 or 2. ", "dataType": "Double"}, {"name": "Threshold_Distance", "isOptional": true, "description": "Specifies a cutoff distance for Inverse Distance and Fixed Distance conceptualizations of spatial relationships. Enter this value using the units specified in the environment output coordinate system. Defines the size of the Space window for the Space Time Window conceptualization of spatial relationships. A value of zero indicates that no threshold distance is applied. When this parameter is left blank, a default threshold value is computed based on output feature class extent and the number of features. ", "dataType": "Double"}, {"name": "Number_of_Neighbors", "isOptional": true, "description": "An integer reflecting either the minimum or the exact number of neighbors. For K Nearest Neighbors , each feature will have exactly this specified number of neighbors. For Inverse Distance or Fixed Distance each feature will have at least this many neighbors (the threshold distance will be temporarily extended to ensure this many neighbors, if necessary). When there are island polygons and one of the Contiguity Conceptualizations of Spatial Relationships is selected, then this specified number of nearest polygons will be associated with those island polygons. ", "dataType": "Long"}, {"name": "Row_Standardization", "isOptional": true, "description": "Row standardization is recommended whenever feature distribution is potentially biased due to sampling design or to an imposed aggregation scheme. ROW_STANDARDIZATION \u2014 Spatial weights are standardized by row. Each weight is divided by its row sum. NO_STANDARDIZATION \u2014 No standardization of spatial weights is applied. ", "dataType": "Boolean"}, {"name": "Input_Table", "isInputFile": true, "isOptional": true, "description": "A table containing numeric weights relating every feature to every other feature in the input feature class. Required fields are the Input Feature Class Unique ID field, NID (neighbor ID), and WEIGHT. ", "dataType": "Table"}, {"name": "Date_Time_Field", "isOptional": true, "description": "A date field with a timestamp for each feature. ", "dataType": "Field"}, {"name": "Date_Time_Interval_Type", "isOptional": true, "description": "The units to use for measuring time. SECONDS \u2014 Seconds MINUTES \u2014 Minutes HOURS \u2014 Hours DAYS \u2014 Days WEEKS \u2014 Weeks MONTHS \u2014 Months YEARS \u2014 Years", "dataType": "String"}, {"name": "Date_Time_Interval_Value", "isOptional": true, "description": "An Integer reflecting the number of time units comprising the time window. For example, if you select HOURS for the Date/Time Interval Type and 3 for the Date/Time Interval Value, the time window would be 3 hours; features within the specified space window and within the specified time window would be neighbors. ", "dataType": "Long"}]},
{"syntax": "GenerateNetworkSpatialWeights_stats (Input_Feature_Class, Unique_ID_Field, Output_Spatial_Weights_Matrix_File, Input_Network, Impedance_Attribute, {Impedance_Cutoff}, {Maximum_Number_of_Neighbors}, {Barriers}, {U-turn_Policy}, {Restrictions}, {Use_Hierarchy_in_Analysis}, {Search_Tolerance}, {Conceptualization_of_Spatial_Relationships}, {Exponent}, {Row_Standardization})", "name": "Generate Network Spatial Weights (Spatial Statistics)", "description": "Constructs a spatial weights matrix file ( .swm ) using a Network dataset, defining feature spatial relationships in terms of the underlying network structure.  \r\n Learn more about how Generate Network Spatial Weights works \r\n", "example": {"title": "GenerateNetworkSpatialWeights Example (Python Window)", "description": "The following Python Window script demonstrates how to use the GenerateNetworkSpatialWeights tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data\" arpcy.GenerateNetworkSpatialWeights_stats ( \"Hospital.shp\" , \"MyID\" , \"network6Neighs.swm\" , \"Streets_ND\" , \"MINUTES\" , 10 , 6 , \"#\" , \"ALLOW_UTURNS\" , \"#\" , \"USE_HIERARCHY\" , \"#\" , \"INVERSE\" , 1 , \"ROW_STANDARDIZATION\" )"}, "usage": ["Output from this tool is a spatial weights matrix file (", ".swm", ").  Tools that require you to specify a ", "Conceptualization of Spatial Relationships", " option will accept a ", "spatial weights matrix file", "; select ", "GET_SPATIAL_WEIGHTS_FROM_FILE", " for the ", "Conceptualization of Spatial Relationships", " parameter and, for the ", "Weights Matrix File", " parameter, specify the full pathname to the spatial weights file created using this tool.", "This tool was designed to work with point ", "Input Feature Class", " data only.", "A ", "spatial weights matrix", " quantifies the spatial relationships that\r\nexist among the features in your dataset.  Many tools in the ", "Spatial Statistics Toolbox", " evaluate each feature within the context of its neighboring features.  The spatial weights matrix file defines those neighbor relationships.  For this tool, neighbor relationships are based on the time or distance between features, in the case where travel is restricted to a network. \r\nFor more information about spatial weights and ", "spatial weights matrix files", ", see ", "Spatial_weights", ".", " ", "ESRI Data & Maps", ", free to ArcGIS users, contains StreetMap data including a prebuilt network dataset in SDC format. The coverage for this dataset is the United States and Canada.  These network datasets can be used directly by this tool. ", "The ", "Unique ID", " field is linked to feature relationships derived from running this tool. Consequently, the ", "Unique ID", " values must be unique for every feature and typically should be in a permanent field that remains with the feature class. If you don't have a Unique ID field, you can easily create one by adding a new integer field (", "Add Field", ") to your feature class table and calculating the field values to be equal to the FID/OID field (", "Calculate Field", "). You cannot use the FID/OID field directly for the ", "Unique ID", " parameter.", "The ", "Maximum Number of Neighbors", " parameter for this tool specifies the exact number of neighbors that will be associated with each feature. The ", "Impedance Cutoff", " overrides the number of neighbors parameter, so that some features may have fewer neighbors if the number of neighbors specified cannot be found within the cutoff distance/time.", "You can define spatial relationships using the hierarchy in the network dataset, if it has one, by checking the ", "Use Hierarchy in Analysis", " parameter. The hierarchy classifies network edges into primary, secondary, and local roads. When using the hierarchy of the network to create spatial relationships among features, preference will be given to travel on primary roads more than secondary roads and secondary roads more than local roads.", "This tool does not honor the Environment output coordinate system.  All feature ", "geometry", " is projected to match the ", "spatial reference", " associated with the ", "Network Dataset", " prior to analysis. The resultant spatial weights matrix file created by this tool will reflect spatial relationships defined using the Network Dataset spatial reference.   It is recommended that when performing analyses using a network spatial weights matrix file, the input feature class be projected to match the coordinate system of the network dataset used to create the network swm. ", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The point feature class for which Network spatial relationships among features will be assessed. ", "dataType": "Feature Class"}, {"name": "Unique_ID_Field", "isOptional": false, "description": "An integer field containing a different value for every feature in the input feature class. ", "dataType": "Field"}, {"name": "Output_Spatial_Weights_Matrix_File", "isOutputFile": true, "isOptional": false, "description": "The full path for the network spatial weights matrix (SWM) file created. ", "dataType": "File"}, {"name": "Input_Network", "isInputFile": true, "isOptional": false, "description": "The network dataset for which spatial relationships among features in the input feature class will be defined. ", "dataType": "Network Dataset Layer"}, {"name": "Impedance_Attribute", "isOptional": false, "description": "The type of cost units to use as impedance in the analysis. ", "dataType": "String"}, {"name": "Impedance_Cutoff", "isOptional": true, "description": "Specifies a cutoff value for Inverse and Fixed conceptualizations of spatial relationships. Enter this value using the units specified by the Impedance Attribute parameter. A value of zero indicates that no threshold is applied. When this parameter is left blank, a default threshold value is computed based on input feature class extent and the number of features. ", "dataType": "Double"}, {"name": "Maximum_Number_of_Neighbors", "isOptional": true, "description": "An integer reflecting the maximum number of neighbors to find for each feature. ", "dataType": "Long"}, {"name": "Barriers", "isOptional": true, "description": "The name of a point feature class with features representing blocked intersections, road closures, accident sites, or other locations where travel is blocked along the network. ", "dataType": "Feature Layer"}, {"name": "U-turn_Policy", "isOptional": true, "description": "Specifies optional U-turn restrictions. ALLOW_UTURNS \u2014 U-turns will be possible anywhere (default). NO_UTURNS \u2014 No u-turns will be allowed during navigation. ALLOW_DEAD_ENDS_ONLY \u2014 U-turns will be possible only at the dead ends (i.e., single-valent junctions). ", "dataType": "String"}, {"name": "Restrictions", "isOptional": false, "description": "A list of restrictions. Check ON the restrictions to be honored in spatial relationship computations. ", "dataType": "String"}, {"name": "Use_Hierarchy_in_Analysis", "isOptional": true, "description": "Specifies whether or not to use a hierarchy in the analysis. USE_HIERARCHY \u2014 Will use the network dataset's hierarchy attribute in a heuristic path algorithm to speed analysis. NO_HIERARCHY \u2014 Will use an exact path algorithm instead. If there is no hierarchy attribute, this option does not affect analysis. ", "dataType": "Boolean"}, {"name": "Search_Tolerance", "isOptional": true, "description": "The search threshold used to locate features in the Input Feature Class onto the Network Dataset. This parameter includes a search value and the units for the tolerance. ", "dataType": "Linear unit"}, {"name": "Conceptualization_of_Spatial_Relationships", "isOptional": true, "description": "Specifies how the weighting associated with each spatial relationship is specified. For INVERSE, features farther away have a smaller weight than features nearby. For FIXED, features within the Impedance Cutoff of a target feature are neighbors (weight of 1); features outside the Impedance Cutoff of a target feature are not (weight of 0). ", "dataType": "String"}, {"name": "Exponent", "isOptional": true, "description": "Parameter for the INVERSE Conceptualization of Spatial Relationships calculation. Typical values are 1 or 2. Weights drop off quicker with distance as this exponent value increases. ", "dataType": "Double"}, {"name": "Row_Standardization", "isOptional": true, "description": "Row standardization is recommended whenever feature distribution is potentially biased due to sampling design or to an imposed aggregation scheme. ROW_STANDARDIZATION \u2014 Spatial weights are standardized by row. Each weight is divided by its row sum. NO_STANDARDIZATION \u2014 No standardization of spatial weights is applied. ", "dataType": "Boolean"}]},
{"syntax": "StandardDistance_stats (Input_Feature_Class, Output_Standard_Distance_Feature_Class, Circle_Size, {Weight_Field}, {Case_Field})", "name": "Standard Distance (Spatial Statistics)", "description": "Measures the degree to which features are concentrated or dispersed around the geometric mean center. \r\n Learn more about how Standard Distance works \r\n", "example": {"title": "StandardDistance Example (Python Window)", "description": "The following Python Window script demonstrates how to use the StandardDistance tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.StandardDistance_stats ( \"AutoTheft.shp\" , \"auto_theft_SD.shp\" , \"1_STANDARD_DEVIATION\" , \"#\" , \"#\" )"}, "usage": ["The standard distance is a useful statistic as it provides a single summary measure of feature distribution around their center (similar to the way a standard deviation measures the distribution of data values around the statistical mean).", "The ", "Standard Distance", " tool creates a new feature class containing a circle polygon centered on the mean for each case. Each circle polygon is drawn with a radius equal to the standard distance. The attribute value for each circle polygon is its standard distance value.", "The ", "Case Field", " is used to group features prior to analysis.  When a ", "Case Field", " is specified, the input features are first grouped according to case field values, and then a standard distance circle is computed for each group. The case field can be of integer, date, or string type, and will appear as an attribute in the ", "Output Feature Class", ".  Records with NULL values for the ", "Case Field", " will be excluded from analysis.  ", "The standard distance calculation may be based on an optional ", "Weight Field", " (to get the standard distance of businesses weighted by employees, for example).  The Weight Field should be numeric.", "If the underlying spatial pattern of the input features is concentrated in the center with fewer features toward the periphery (spatial normal distribution), a one standard deviation circle polygon will cover approximately 68 percent of the features; a two standard deviation circle will contain approximately 95 percent of the features; and three standard deviations will cover approximately 99 percent of the features in the cluster.", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "A feature class containing a distribution of features for which the standard distance will be calculated. ", "dataType": "Feature Layer"}, {"name": "Output_Standard_Distance_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "A polygon feature class that will contain a circle polygon for each input center. These circle polygons graphically portray the standard distance at each center point. ", "dataType": "Feature Class"}, {"name": "Circle_Size", "isOptional": false, "description": "The size of output circles in standard deviations. The default circle size is 1; valid choices are 1, 2, or 3 standard deviations. 1_STANDARD_DEVIATION \u2014 2_STANDARD_DEVIATIONS \u2014 3_STANDARD_DEVIATIONS \u2014", "dataType": "String"}, {"name": "Weight_Field", "isOptional": true, "description": "The numeric field used to weight locations according to their relative importance. ", "dataType": "Field"}, {"name": "Case_Field", "isOptional": true, "description": "Field used to group features for separate standard distance calculations. The case field can be of integer, date, or string type. ", "dataType": "Field"}]},
{"syntax": "MedianCenter_stats (Input_Feature_Class, Output_Feature_Class, {Weight_Field}, {Case_Field}, {Attribute_Field})", "name": "Median Center (Spatial Statistics)", "description": "Identifies the location that minimizes overall Euclidean distance to the features in a dataset. \r\n Learn more about how_Median_Center_works \r\n", "example": {"title": "MedianCenter Example (Python Window)", "description": "The following Python Window script demonstrates how to use the MedianCenter tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.MedianCenter_stats ( \"coffee_shops.shp\" , \"coffee_MEDIANCENTER.shp\" , \"NUM_EMP\" , \"#\" , \"#\" )"}, "usage": ["While the ", "Mean_Center", " tool returns a point at the average X and average Y coordinate for all feature centroids, the median center uses an iterative algorithm to find the point that minimizes Euclidean distance to all features in the dataset.", "Both the ", "Mean_Center", " and ", "Median Center", " are measures of central tendency. The algorithm for the ", "Median Center", " tool is less influenced by data outliers.", "Calculations based on feature distances require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "The ", "Case Field", " is used to group features for separate median center computations.  When a Case Field is specified, the input features are first grouped according to case field values, and then a median center is calculated for each group.  The case field can be of integer, date, or string type, and will appear as an attribute in the ", "Output Feature Class", ".  Records with NULL values for the ", "Case Field", " will be excluded from analysis.  ", "The x and y values for the median center feature(s) are attributes in the output feature class. The values are stored in the fields XCOORD and YCOORD.", "The data median will be computed for all fields specified in the Attribute Field(s) parameter.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "A feature class for which the median center will be calculated. ", "dataType": "Feature Layer"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "A point feature class that will contain the features representing the median centers of the input feature class. ", "dataType": "Feature Class"}, {"name": "Weight_Field", "isOptional": true, "description": "The numeric field used to create a weighted median center. ", "dataType": "Field"}, {"name": "Case_Field", "isOptional": true, "description": "Field used to group features for separate median center calculations. The case field can be of integer, date, or string type. ", "dataType": "Field"}, {"name": "Attribute_Field", "isOptional": true, "description": "Numeric field(s) for which the data median value will be computed. ", "dataType": "Field"}]},
{"syntax": "MeanCenter_stats (Input_Feature_Class, Output_Feature_Class, {Weight_Field}, {Case_Field}, {Dimension_Field})", "name": "Mean Center (Spatial Statistics)", "description": "Identifies the geographic center (or the center of concentration) for a set of features. \r\n Learn more about how Mean Center works \r\n", "example": {"title": "MeanCenter Example (Python Window)", "description": "The following Python Window script demonstrates how to use the MeanCenter tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.MeanCenter_stats ( \"coffee_shops.shp\" , \"coffee_MEANCENTER.shp\" , \"NUM_EMP\" , \"#\" , \"#\" )"}, "usage": ["The mean center is a point constructed from the average x and y values for the input feature centroids.", "Use  ", "projected data", " with this tool to accurately measure distances.", "The x and y values for the mean center point features are attributes in the ", "Output Feature Class", ". The values are stored in the fields XCOORD and YCOORD.", "The ", "Case Field", " is used to group features for separate mean center computations.  When a Case Field is specified, the input features are first grouped according to case field values, and then a mean center is calculated for each group.  The case field can be of integer, date, or string type.    Records with NULL values for the ", "Case Field", " will be excluded from analysis.  ", "The ", "Dimension Field", " is any numeric field in the input feature class.  The Mean Center tool will compute the average for all values in that field and include the result in the output feature class.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "A feature class for which the mean center will be calculated. ", "dataType": "Feature Layer"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "A point feature class that will contain the features representing the mean centers of the input feature class. ", "dataType": "Feature Class"}, {"name": "Weight_Field", "isOptional": true, "description": "The numeric field used to create a weighted mean center. ", "dataType": "Field"}, {"name": "Case_Field", "isOptional": true, "description": "Field used to group features for separate mean center calculations. The case field can be of integer, date, or string type. ", "dataType": "Field"}, {"name": "Dimension_Field", "isOptional": true, "description": "A numeric field containing attribute values from which an average value will be calculated. ", "dataType": "Field"}]},
{"syntax": "DirectionalMean_stats (Input_Feature_Class, Output_Feature_Class, Orientation_Only, {Case_Field})", "name": "Linear Directional Mean (Spatial Statistics)", "description": "Identifies the mean direction, length, and geographic center for a set of lines. \r\n Learn more about how Linear Directional Mean works \r\n", "example": {"title": "LinearDirectionalMean Example (Python Window)", "description": "The following Python Window script demonstrates how to use the LinearDirectionalMean tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.DirectionalMean_stats ( \"AutoTheft_links.shp\" , \"auto_theft_LDM.shp\" , \"DIRECTION\" , \"#\" )"}, "usage": ["The input  must be a line feature class.", "Attribute values for the output line feature(s) include ", "CompassA", " for Compass Angle (clockwise from due North), ", "DirMean", " for Directional Mean (counterclockwise from due East), ", "CirVar", " for Circular Variance (an indication of how much line directions or orientations deviate from the directional mean), ", "AveX", " and ", "AveY", " for Mean Center X and Y Coordinates, and ", "AveLen", " for Mean Length. When a ", "Case Field", " is specified, it also will be added to the ", "Output Feature Class", ".", "Analogous to a standard deviation measure, the circular variance value tells how well the directional mean vector represents the set of input vectors. Circular variances range from 0 to 1. If all the input vectors have the exact same (or very similar) directions, the circular variance is small (near 0). When input vector directions span the entire compass, the circular variance is large (near 1).", "The ", "Case Field", " is used to group features for separate linear directional mean computations.  When a Case Field is specified, the input line features are first grouped according to case field values, and then an output line feature is created for each group. The case field can be of integer, date, or string type.  Records with NULL values for the ", "Case Field", " will be excluded from analysis.  ", "When measuring direction, the tool only considers the first and last points in a line. The tool does not consider all of the vertices along a line.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When this tool runs in ArcMap, the output feature class is automatically added to the Table of Contents (TOC) with default rendering (directional vectors). The rendering applied is defined by a layer file in ", "<ArcGIS>/ArcToolbox/Templates/Layers", ". You can reapply the default rendering, if needed, by ", "importing", " the template layer symbology.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class containing vectors for which the mean direction will be calculated. ", "dataType": "Feature Layer"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "A line feature class that will contain the features representing the mean directions of the input feature class. ", "dataType": "Feature Class"}, {"name": "Orientation_Only", "isOptional": false, "description": " DIRECTION \u2014 The From and To node information is ignored. ORIENTATION_ONLY \u2014 The From and To nodes are utilized in calculating the mean (default).", "dataType": "Boolean"}, {"name": "Case_Field", "isOptional": true, "description": "Field used to group features for separate directional mean calculations. The case field can be of integer, date, or string type. ", "dataType": "Field"}]},
{"syntax": "DirectionalDistribution_stats (Input_Feature_Class, Output_Ellipse_Feature_Class, Ellipse_Size, {Weight_Field}, {Case_Field})", "name": "Directional Distribution (Standard Deviational Ellipse) (Spatial Statistics)", "description": "Creates standard deviational ellipses to summarize the spatial characteristics of geographic features: central tendency, dispersion, and directional trends. \r\n Learn about how Directional Distribution (Standard Deviational Ellipse) works \r\n", "example": {"title": "DirectionalDistribution Example (Python Window)", "description": "The following Python Window script demonstrates how to use the DirectionalDistribution tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.DirectionalDistribution_stats ( \"AutoTheft.shp\" , \"auto_theft_SE.shp\" , \"1_STANDARD_DEVIATION\" , \"#\" , \"#\" )"}, "usage": ["The ", "Standard Deviational Ellipse", " tool creates a new ", "Output Feature Class", " containing elliptical polygons, one for each Case (", "Case Field", " parameter). The attribute values for these ellipse polygons include X and Y coordinates for the mean center, two standard distances (long and short axes), and the orientation of the ellipse. The fieldnames are CenterX, CenterY, XStdDist, YStdDist, and Rotation. When a ", "Case Field", " is provided, this field is added to the ", "Output Feature Class", ", as well.", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "When the underlying spatial pattern of features is concentrated in the center with fewer features toward the periphery (a spatial normal distribution), a one standard deviation ellipse polygon will cover approximately 68 percent of the features; two standard deviations will contain approximately 95 percent of the features; and three standard deviations will cover approximately 99 percent of the features in the cluster.", "The value in the output Rotation field represents the rotation of the long axis measured clockwise from noon.", "The ", "Case Field", " is used to group features prior to analysis.  When a ", "Case Field", " is specified, the input features are first grouped according to case field values, and then a standard deviational ellipse is computed for each group. The case field can be of integer, date, or string type.  Records with NULL values for the ", "Case Field", " will be excluded from analysis.  ", "The standard deviational ellipse calculation may be based on an optional ", "Weight Field", " (to get the ellipses for traffic accidents ", "weighted", " by severity, for example).  The Weight Field should be numeric.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "A feature class containing a distribution of features for which the standard deviational ellipse will be calculated. ", "dataType": "Feature Layer"}, {"name": "Output_Ellipse_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "A polygon feature class that will contain the output ellipse feature. ", "dataType": "Feature Class"}, {"name": "Ellipse_Size", "isOptional": false, "description": "The size of output ellipses in standard deviations. The default ellipse size is 1; valid choices are 1, 2, or 3 standard deviations. 1_STANDARD_DEVIATION \u2014 2_STANDARD_DEVIATIONS \u2014 3_STANDARD_DEVIATIONS \u2014", "dataType": "String"}, {"name": "Weight_Field", "isOptional": true, "description": "The numeric field used to weight locations according to their relative importance. ", "dataType": "Field"}, {"name": "Case_Field", "isOptional": true, "description": "Field used to group features for separate directional distribution calculations. The case field can be of integer, date, or string type. ", "dataType": "Field"}]},
{"syntax": "CentralFeature_stats (Input_Feature_Class, Output_Feature_Class, Distance_Method, {Weight_Field}, {Self_Potential_Weight_Field}, {Case_Field})", "name": "Central Feature (Spatial Statistics)", "description": "Identifies the most centrally located feature in a point, line, or polygon feature class. \r\n Learn more about how Central Feature works \r\n", "example": {"title": "CentralFeature example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CentralFeature tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.CentralFeature_stats ( \"coffee_shops.shp\" , \"coffee_CENTRALFEATURE.shp\" , \"EUCLIDEAN_DISTANCE\" , \"NUM_EMP\" , \"#\" , \"#\" )"}, "usage": ["The feature associated with the smallest accumulated distance to all other features in the dataset is the most centrally located feature; this feature is selected and copied to a newly created ", "Output Feature Class", ".", "Accumulated distances are measured using ", "EUCLIDEAN_DISTANCE", " or ", "MANHATTAN_DISTANCE", ", as specified by the ", "Distance Method", " parameter.", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "The ", "Case Field", " is used to group features for separate ", "Central Feature", " computations. The ", "Case Field", " can be of integer, date, or string type.  Records with NULL values for the ", "Case Field", " will be excluded from the analysis.  ", "Self-potential is the distance or weight between a feature and itself. Often this weight is zero, but in some cases you may want to specify another fixed value or a different value for every feature (perhaps ", "based on polygon size", ", for example).", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class containing a distribution of features from which to identify the most centrally located feature. ", "dataType": "Feature Layer"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The feature class that will contain the most centrally located feature in the Input Feature Class. ", "dataType": "Feature Class"}, {"name": "Distance_Method", "isOptional": false, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN_DISTANCE \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN_DISTANCE \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Weight_Field", "isOptional": true, "description": "The numeric field used to weight distances in the origin-destination distance matrix. ", "dataType": "Field"}, {"name": "Self_Potential_Weight_Field", "isOptional": true, "description": "The field representing self-potential\u2014the distance or weight between a feature and itself. ", "dataType": "Field"}, {"name": "Case_Field", "isOptional": true, "description": "Field used to group features for separate central feature computations. The case field can be of integer, date, or string type. ", "dataType": "Field"}]},
{"syntax": "HotSpots_stats (Input_Feature_Class, Input_Field, Output_Feature_Class, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Self_Potential_Field}, {Weights_Matrix_File})", "name": "Hot Spot Analysis (Getis-Ord Gi*) (Spatial Statistics)", "description": "Given a set of weighted features, identifies statistically significant hot spots and cold spots using the Getis-Ord Gi* statistic. \r\n Learn more about how Hot Spot Analysis (Getis-Ord Gi*) works \r\n", "example": {"title": "HotSpotAnalysis example 1 (Python window)", "description": "The following Python window script demonstrates how to use the HotSpotAnalysis tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.HotSpots_stats ( \"911Count.shp\" , \"ICOUNT\" , \"911HotSpots.shp\" , \"GET_SPATIAL_WEIGHTS_FROM_FILE\" , \"EUCLIDEAN_DISTANCE\" , \"NONE\" , \"#\" , \"#\" , \"euclidean6Neighs.swm\" )"}, "usage": ["This tool identifies statistically significant spatial clusters of high values (hot spots) and low values (cold spots). It creates a new ", "Output Feature Class", " with a z-score and p-value for each feature in the ", "Input Feature Class", ". It also returns the z-score and p-value field names as derived output values for potential use in custom models and scripts. ", "The z-scores and p-values are measures of statistical significance which tell you whether or not to reject the null hypothesis, feature by feature. In effect, they indicate whether the observed spatial clustering of high or low values is more pronounced than one would expect in a random distribution of those same values. ", "A high z-score and small p-value for a feature indicates a spatial clustering of high values. A low negative z-score and small p-value indicates a spatial clustering of low values. The higher (or lower) the z-score, the more intense the clustering. A z-score near zero indicates no apparent spatial clustering.", "The z-score is based on the randomization null hypothesis computation. For more information on z-scores, see ", "What is a z-score? What is a p-value?", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "The ", "Input Field", " should contain a variety of values. The math for this statistic requires some variation in the variable being analyzed; it cannot solve if all input values are 1, for example. If you want to use this tool to analyze the spatial pattern of incident data, consider  ", "aggregating your incident data", ".", "Your choice for the ", "Conceptualization of Spatial Relationships", " parameter should reflect inherent relationships among the features you are analyzing. The more realistically you can model how features interact with each other in space, the more accurate your results will be. Recommendations are outlined in ", "Selecting a Conceptualization of Spatial Relationships: Best Practices", ". Here are some additional tips: ", " The default ", "Distance Band or Threshold Distance", " will ensure each feature has at least one neighbor, and this is important. But often, this default will not be the most ", "appropriate distance", " to use for your analysis. Additional strategies for selecting an appropriate scale (distance band) for your analysis are outlined in ", "Selecting a fixed distance band value", ".", "When zero is entered for the ", "Distance Band or Threshold Distance", " parameter, all features are considered neighbors of all other features; when this parameter is left blank, the default distance will be applied.", "Weights for distances less than 1 become unstable when they are inverted. Consequently, the weighting for features separated by less than 1 unit of distance (common with ", "Geographic Coordinate System", " projections) are given a weight of 1. ", "Analysis on features with a ", "Geographic Coordinate System", " projection is not recommended when you select any of the inverse distance-based spatial conceptualization methods (", "INVERSE_DISTANCE", ", ", "INVERSE_DISTANCE_SQUARED", ", or ", "ZONE_OF_INDIFFERENCE", ").", "For the inverse distance options (", "INVERSE_DISTANCE", ", ", "INVERSE_DISTANCE_SQUARED", ", or ", "ZONE_OF_INDIFFERENCE", "), any two points that are coincident will be given a weight of one to avoid zero division. This assures features are not excluded from analysis.", "Additional options for the ", "Conceptualization of Spatial Relationships", " parameter, including space-time relationships, are available using the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools.  To take advantage of these additional options, use one of these tools to construct the ", "spatial weights matrix file", " prior to analysis; select ", "GET_SPATIAL_WEIGHTS_FROM_FILE", " for the ", "Conceptualization of Spatial Relationships", " parameter; and for the ", "Weights Matrix File", " parameter, specify the path to the spatial weights file you created.", "More information about space-time cluster analysis is provided in the ", "Space-Time Analysis", " documentation.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "If you provide a ", "Weights Matrix File", " with an ", "SWM", " extension, this tool is expecting a spatial weights matrix file created using either the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools;  otherwise, this tool is expecting an ", "ASCII formatted spatial weights matrix file", ". In some cases, behavior is different depending on which type of spatial weights matrix file you use:", "Running your analysis with an ", "ASCII formatted spatial weights matrix file", " is memory intensive.  For analyses on more than 5,000 features, consider converting your ASCII-formatted spatial weights matrix file into an ", "SWM formatted file", ".  First put your ASCII weights into a ", "formatted", " table (using Excel, for example).  Next, run the  ", "Generate Spatial Weights Matrix", " tool using ", "CONVERT_TABLE", " for the ", "Conceptualization of Spatial Relationships", " parameter.  The output will be an SWM-formatted spatial weights matrix file. ", "When this tool runs in ", "ArcMap", ", the ", "Output Feature Class", " is automatically added to the table of contents  with default rendering applied to the z-score field. The hot-to-cold rendering applied is defined by a layer file in ", "<ArcGIS>/Desktop10.x/ArcToolbox/Templates/Layers", ". You can reapply the default rendering, if needed, by ", "importing", " the template layer symbology.", "The ", "Output Feature Class", " includes a SOURCE_ID field which allows you to ", "Join", " it to the ", "Input Feature Class", ", if needed.", "The ", "Modeling Spatial Relationships", " help topic provides additional information about this tool's parameters.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information.", "Prior to ArcGIS ", "10.0", ", the output feature class was a duplicate of the input feature class with the z-score and p-value results fields added.   After ArcGIS ", "10.0", ", the output feature class only includes the z-score and p-value fields as well as the fields input for the analysis.  To join other input fields to the output feature class, use the SOURCE_ID field to join the fields using tools in the ", "Joins toolset", ". ", "Row Standardization", " has no impact on this tool: results from ", "Hot Spot Analysis (the Getis-Ord Gi* statistic)", " would be identical with or without row standardization.  The parameter is consequently disabled; it remains as a tool parameter only to support backwards compatibility."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class for which hot spot analysis will be performed. ", "dataType": "Feature Layer"}, {"name": "Input_Field", "isInputFile": true, "isOptional": false, "description": "The numeric count field (number of victims, crimes, jobs, and so on) to be evaluated. ", "dataType": "Field"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to receive the z-score and p-value results. ", "dataType": "Feature Class"}, {"name": "Conceptualization_of_Spatial_Relationships", "isOptional": false, "description": "Specifies how spatial relationships among features are conceptualized. INVERSE_DISTANCE \u2014 Nearby neighboring features have a larger influence on the computations for a target feature than features that are far away. INVERSE_DISTANCE_SQUARED \u2014 Same as INVERSE_DISTANCE except that the slope is sharper, so influence drops off more quickly, and only a target feature's closest neighbors will exert substantial influence on computations for that feature. FIXED_DISTANCE_BAND \u2014 Each feature is analyzed within the context of neighboring features. Neighboring features inside the specified critical distance receive a weight of 1 and exert influence on computations for the target feature. Neighboring features outside the critical distance receive a weight of zero and have no influence on a target feature's computations. ZONE_OF_INDIFFERENCE \u2014 Features within the specified critical distance of a target feature receive a weight of 1 and influence computations for that feature. Once the critical distance is exceeded, weights (and the influence a neighboring feature has on target feature computations) diminish with distance. CONTIGUITY_EDGES_ONLY \u2014 Only neighboring polygon features that share a boundary or overlap will influence computations for the target polygon feature. CONTIGUITY_EDGES_CORNERS \u2014 Polygon features that share a boundary, share a node, or overlap will influence computations for the target polygon feature. GET_SPATIAL_WEIGHTS_FROM_FILE \u2014 Spatial relationships are defined in a spatial weights file. The path to the spatial weights file is specified in the Weights Matrix File parameter. ", "dataType": "String"}, {"name": "Distance_Method", "isOptional": false, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN_DISTANCE \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN_DISTANCE \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Standardization", "isOptional": false, "description": "Row standardization has no impact on this tool: results from Hot Spot Analysis (the Getis-Ord Gi* statistic) would be identical with or without row standardization. The parameter is disabled; it remains as a tool parameter only to support backwards compatibility. NONE \u2014 No standardization of spatial weights is applied. ROW \u2014 Disabled. ", "dataType": "String"}, {"name": "Distance_Band_or_Threshold_Distance", "isOptional": true, "description": "Specifies a cutoff distance for Inverse Distance and Fixed Distance options. Features outside the specified cutoff for a target feature are ignored in analyses for that feature. However, for Zone of Indifference, the influence of features outside the given distance is reduced with distance, while those inside the distance threshold are equally considered. The distance value entered should match that of the output coordinate system. For the Inverse Distance conceptualizations of spatial relationships, a value of 0 indicates that no threshold distance is applied; when this parameter is left blank, a default threshold value is computed and applied. This default value is the Euclidean distance that ensures every feature has at least one neighbor. This parameter has no effect when Polygon Contiguity or Get Spatial Weights From File spatial conceptualizations are selected. ", "dataType": "Double"}, {"name": "Self_Potential_Field", "isOptional": true, "description": "The field representing self potential: the distance or weight between a feature and itself. ", "dataType": "Field"}, {"name": "Weights_Matrix_File", "isOptional": true, "description": "The path to a file containing weights that define spatial, and potentially temporal, relationships among features. ", "dataType": "File"}]},
{"syntax": "ClustersOutliers_stats (Input_Feature_Class, Input_Field, Output_Feature_Class, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Weights_Matrix_File})", "name": "Cluster and Outlier Analysis (Anselin Local Moran's I) (Spatial Statistics)", "description": "Given a set of weighted features, identifies statistically significant hot spots, cold spots, and spatial outliers using the Anselin Local Moran's I statistic. \r\n Learn more about how Cluster and Outlier Analysis (Anselin Local Moran's I) works \r\n", "example": {"title": "ClusterandOutlierAnalysis example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ClusterandOutlierAnalysis tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data/911calls\" arcpy.ClustersOutliers_stats ( \"911Count.shp\" , \"ICOUNT\" , \"911ClusterOutlier.shp\" , \"GET_SPATIAL_WEIGHTS_FROM_FILE\" , \"EUCLIDEAN_DISTANCE\" , \"NONE\" , \"#\" , \"euclidean6Neighs.swm\" )"}, "usage": ["This tool creates a new ", "Output Feature Class", " with the following attributes for each feature in the ", "Input Feature Class", ": Local Moran's I index, z-score, p-value, and cluster/outlier type (COType).  The field names of these attributes are also derived tool output values for potential use in custom models and scripts. ", "The z-scores and p-values are measures of statistical significance which tell you whether or not to reject the null hypothesis, feature by feature. In effect, they indicate whether the apparent similarity (a spatial clustering of either high or low values) or dissimilarity (a spatial outlier) is more pronounced than one would expect in a random distribution.", "A high positive z-score for a feature indicates that the surrounding features have similar values (either high values or low values). The COType field in the ", "Output Feature Class", " will be HH for a statistically significant (0.05 level) cluster of high values and LL for a statistically significant (0.05 level) cluster of low values.", "A low negative z-score (for example, < -1.96) for a feature indicates a statistically significant (0.05 level) spatial outlier. The COType  field in the ", "Output Feature Class", " will indicate if the feature has a high value and is surrounded by features with low values (HL) or if the feature has a low value and is surrounded by features with high values (LH).", "The z-score is based on the randomization null hypothesis computation. For more information on z-scores, see ", "What is a z-score? What is a p-value?", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "The ", "Input Field", " should contain a variety of values. The math for this statistic requires some variation in the variable being analyzed; it cannot solve if all input values are 1, for example. If you want to use this tool to analyze the spatial pattern of incident data, consider ", "aggregating your incident data", ".", "Your choice for the ", "Conceptualization of Spatial Relationships", " parameter should reflect inherent relationships among the features you are analyzing. The more realistically you can model how features interact with each other in space, the more accurate your results will be. Recommendations are outlined in ", "Selecting a Conceptualization of Spatial Relationships: Best Practices", ". Here are some additional tips: ", " The default ", "Distance Band or Threshold Distance", " will ensure each feature has at least one neighbor, and this is important. But often, this default will not be the most ", "appropriate distance", " to use for your analysis. Additional strategies for selecting an appropriate scale (distance band) for your analysis are outlined in ", "Selecting a fixed distance band value", ".", "When zero is entered for the ", "Distance Band or Threshold Distance", " parameter, all features are considered neighbors of all other features; when this parameter is left blank, the default distance will be applied.", "Weights for distances less than 1 become unstable when they are inverted. Consequently, the weighting for features separated by less than 1 unit of distance (common with ", "Geographic Coordinate System", " projections) are given a weight of 1. ", "Analysis on features with a ", "Geographic Coordinate System", " projection is not recommended when you select any of the inverse distance-based spatial conceptualization methods (", "INVERSE_DISTANCE", ", ", "INVERSE_DISTANCE_SQUARED", ", or ", "ZONE_OF_INDIFFERENCE", ").", "For the inverse distance options (", "INVERSE_DISTANCE", ", ", "INVERSE_DISTANCE_SQUARED", ", or ", "ZONE_OF_INDIFFERENCE", "), any two points that are coincident will be given a weight of one to avoid zero division. This assures features are not excluded from analysis.", "Additional options for the ", "Conceptualization of Spatial Relationships", " parameter, including space-time relationships, are available using the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools.  To take advantage of these additional options, use one of these tools to construct the ", "spatial weights matrix file", " prior to analysis; select ", "GET_SPATIAL_WEIGHTS_FROM_FILE", " for the ", "Conceptualization of Spatial Relationships", " parameter; and for the ", "Weights Matrix File", " parameter, specify the path to the spatial weights file you created.", "More information about space-time cluster analysis is provided in the ", "Space-Time Analysis", " documentation.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "If you provide a ", "Weights Matrix File", " with an ", "SWM", " extension, this tool is expecting a spatial weights matrix file created using either the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools;  otherwise, this tool is expecting an ", "ASCII formatted spatial weights matrix file", ". In some cases, behavior is different depending on which type of spatial weights matrix file you use:", "Running your analysis with an ", "ASCII formatted spatial weights matrix file", " is memory intensive.  For analyses on more than 5,000 features, consider converting your ASCII-formatted spatial weights matrix file into an ", "SWM formatted file", ".  First put your ASCII weights into a ", "formatted", " table (using Excel, for example).  Next, run the  ", "Generate Spatial Weights Matrix", " tool using ", "CONVERT_TABLE", " for the ", "Conceptualization of Spatial Relationships", " parameter.  The output will be an SWM-formatted spatial weights matrix file. ", "When this tool runs in ArcMap, the ", "Output Feature Class", " is automatically added to the Table of Contents (TOC) with default rendering applied to the COType field. The rendering applied is defined by a layer file in ", "<ArcGIS>/Desktop10.x/ArcToolbox/Templates/Layers", ". You can reapply the default rendering, if needed, by ", "importing", " the template layer symbology.", "The ", "Output Feature Class", " includes a SOURCE_ID field which allows you to ", "Join", " it to the ", "Input Feature Class", ", if needed.", "The ", "Modeling Spatial Relationships", " help topic provides additional information about this tool's parameters.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information.", "Prior to ArcGIS 10.0, the output feature class was a duplicate of the input feature class with the COType, z-score, and p-value results fields tacked on.   After ArcGIS 10.0, the output feature class only includes the results and fields used in  the analysis.  "], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class for which cluster/outlier analysis will be performed. ", "dataType": "Feature Layer"}, {"name": "Input_Field", "isInputFile": true, "isOptional": false, "description": "The numeric field to be evaluated. ", "dataType": "Field"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to receive the results fields. ", "dataType": "Feature Class"}, {"name": "Conceptualization_of_Spatial_Relationships", "isOptional": false, "description": "Specifies how spatial relationships among features are conceptualized. INVERSE_DISTANCE \u2014 Nearby neighboring features have a larger influence on the computations for a target feature than features that are far away. INVERSE_DISTANCE_SQUARED \u2014 Same as INVERSE_DISTANCE except that the slope is sharper, so influence drops off more quickly, and only a target feature's closest neighbors will exert substantial influence on computations for that feature. FIXED_DISTANCE_BAND \u2014 Each feature is analyzed within the context of neighboring features. Neighboring features inside the specified critical distance receive a weight of 1 and exert influence on computations for the target feature. Neighboring features outside the critical distance receive a weight of zero and have no influence on a target feature's computations. ZONE_OF_INDIFFERENCE \u2014 Features within the specified critical distance of a target feature receive a weight of 1 and influence computations for that feature. Once the critical distance is exceeded, weights (and the influence a neighboring feature has on target feature computations) diminish with distance. CONTIGUITY_EDGES_ONLY \u2014 Only neighboring polygon features that share a boundary or overlap will influence computations for the target polygon feature. CONTIGUITY_EDGES_CORNERS \u2014 Polygon features that share a boundary, share a node, or overlap will influence computations for the target polygon feature. GET_SPATIAL_WEIGHTS_FROM_FILE \u2014 Spatial relationships are defined in a spatial weights file. The path to the spatial weights file is specified in the Weights Matrix File parameter. ", "dataType": "String"}, {"name": "Distance_Method", "isOptional": false, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN_DISTANCE \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN_DISTANCE \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Standardization", "isOptional": false, "description": "Row standardization is recommended whenever the distribution of your features is potentially biased due to sampling design or an imposed aggregation scheme. NONE \u2014 No standardization of spatial weights is applied. ROW \u2014 Spatial weights are standardized; each weight is divided by its row sum (the sum of the weights of all neighboring features). ", "dataType": "String"}, {"name": "Distance_Band_or_Threshold_Distance", "isOptional": true, "description": "Specifies a cutoff distance for Inverse Distance and Fixed Distance options. Features outside the specified cutoff for a target feature are ignored in analyses for that feature. However, for Zone of Indifference, the influence of features outside the given distance is reduced with distance, while those inside the distance threshold are equally considered. The distance value entered should match that of the output coordinate system. For the Inverse Distance conceptualizations of spatial relationships, a value of 0 indicates that no threshold distance is applied; when this parameter is left blank, a default threshold value is computed and applied. This default value is the Euclidean distance that ensures every feature has at least one neighbor. This parameter has no effect when Polygon Contiguity or Get Spatial Weights From File spatial conceptualizations are selected. ", "dataType": "Double"}, {"name": "Weights_Matrix_File", "isOptional": true, "description": "The path to a file containing weights that define spatial, and potentially temporal, relationships among features. ", "dataType": "File"}]},
{"syntax": "SpatialAutocorrelation_stats (Input_Feature_Class, Input_Field, {Generate_Report}, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Weights_Matrix_File})", "name": "Spatial Autocorrelation (Global Moran's I) (Spatial Statistics)", "description": "Measures spatial autocorrelation based on feature locations and attribute values using the Global Moran's I statistic. You can access the results of this tool (including the optional report file) from the  Results  window.  If you disable background processing, results will also be written to the  Progress  dialog box. \r\n Learn more about how Spatial Autocorrelation (Global Moran's I) works \r\n", "example": {"title": "SpatialAutocorrelation example 1 (Python window)", "description": "The following Python window script demonstrates how to use the SpatialAutocorrelation tool.", "code": "import arcpy arcpy.env.workspace = r\"c:\\data\" arcpy.SpatialAutocorrelation_stats ( \"olsResults.shp\" , \"Residual\" , \"NO_REPORT\" , \"GET_SPATIAL_WEIGHTS_FROM_FILE\" , \"EUCLIDEAN DISTANCE\" , \"NONE\" , \"#\" , \"euclidean6Neighs.swm\" )"}, "usage": ["The Spatial Autocorrelation tool returns five values: the Moran's I Index, Expected Index, Variance, z-score, and p-value. These values are accessible from the ", "Results window", " and are also passed as derived output values for potential use in models or scripts. Optionally, this tool will create an HTML file with a graphical summary of results. Double-clicking on the HTML file in the ", "Results", " window will open the HTML file in the default Internet browser.  Right-clicking on the ", "Messages entry", " in the ", "Results", " window and selecting ", "View", " will display the results in a ", "Message dialog box", ".  If you execute this tool in the ", "foreground", ", output values will also be displayed in the ", "progress dialog box", ". ", "Given a set of features and an associated attribute, the Spatial Autocorrelation tool evaluates whether the pattern expressed is clustered, dispersed, or random. When the z-score or p-value indicates statistical significance, a positive Moran's I index value indicates tendency toward clustering while a negative Moran's I index value indicates tendency toward dispersion.", "The Global Moran's I tool calculates a ", "z-score", " and p-value to indicate whether or not you can reject the null hypothesis. In this case, the null hypothesis states that feature values are randomly distributed across the study area.", "The z-score is based on the randomization null hypothesis computation. For more information on z-scores, see ", "What is a z-score? What is a p-value?", "The ", "Input Field", " should contain a variety of values. The math for this statistic requires some variation in the variable being analyzed; it cannot solve if all input values are 1, for example. If you want to use this tool to analyze the spatial pattern of incident data, consider ", "aggregating your incident data", ".", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "In ArcGIS ", "10", ", optional graphical output is no longer displayed automatically.  Instead, an HTML file summarizing results is created.  To view results, double-click on the HTML file in the ", "Results window", ".  Custom scripts or model tools created prior to ArcGIS ", "10", " that use this tool may need to be rebuilt.  To rebuild these custom tools, open them, remove the ", "Display Results Graphically", " parameter, then resave.", "This tool will optionally create an HTML file summarizing results.  HTML files will not automatically appear in the ", "Catalog", " window.  If you want HTML files to be displayed in ", "Catalog", ", open the ", "ArcCatalog", " application, select the ", "Customize", "  menu option, click  ", "ArcCatalog Options", ", and select the ", "File Types", " tab.  Click on the ", "New Type", " button and specify ", "HTML", " for ", "File Extension", ".", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "Your choice for the ", "Conceptualization of Spatial Relationships", " parameter should reflect inherent relationships among the features you are analyzing. The more realistically you can model how features interact with each other in space, the more accurate your results will be. Recommendations are outlined in ", "Selecting a Conceptualization of Spatial Relationships: Best Practices", ". Here are some additional tips: ", " The default ", "Distance Band or Threshold Distance", " will ensure each feature has at least one neighbor, and this is important. But often, this default will not be the most ", "appropriate distance", " to use for your analysis. Additional strategies for selecting an appropriate scale (distance band) for your analysis are outlined in ", "Selecting a fixed distance band value", ".", "When zero is entered for the ", "Distance Band or Threshold Distance", " parameter, all features are considered neighbors of all other features; when this parameter is left blank, the default distance will be applied.", "Weights for distances less than 1 become unstable when they are inverted. Consequently, the weighting for features separated by less than 1 unit of distance (common with ", "Geographic Coordinate System", " projections) are given a weight of 1. ", "Analysis on features with a ", "Geographic Coordinate System", " projection is not recommended when you select any of the inverse distance-based spatial conceptualization methods (", "INVERSE_DISTANCE", ", ", "INVERSE_DISTANCE_SQUARED", ", or ", "ZONE_OF_INDIFFERENCE", ").", "For the inverse distance options (", "INVERSE_DISTANCE", ", ", "INVERSE_DISTANCE_SQUARED", ", or ", "ZONE_OF_INDIFFERENCE", "), any two points that are coincident will be given a weight of one to avoid zero division. This assures features are not excluded from analysis.", "Additional options for the ", "Conceptualization of Spatial Relationships", " parameter, including space-time relationships, are available using the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools.  To take advantage of these additional options, use one of these tools to construct the ", "spatial weights matrix file", " prior to analysis; select ", "GET_SPATIAL_WEIGHTS_FROM_FILE", " for the ", "Conceptualization of Spatial Relationships", " parameter; and for the ", "Weights Matrix File", " parameter, specify the path to the spatial weights file you created.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "If you provide a ", "Weights Matrix File", " with an ", "SWM", " extension, this tool is expecting a spatial weights matrix file created using either the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools;  otherwise, this tool is expecting an ", "ASCII formatted spatial weights matrix file", ". In some cases, behavior is different depending on which type of spatial weights matrix file you use:", "Running your analysis with an ", "ASCII formatted spatial weights matrix file", " is memory intensive.  For analyses on more than 5,000 features, consider converting your ASCII-formatted spatial weights matrix file into an ", "SWM formatted file", ".  First put your ASCII weights into a ", "formatted", " table (using Excel, for example).  Next, run the  ", "Generate Spatial Weights Matrix", " tool using ", "CONVERT_TABLE", " for the ", "Conceptualization of Spatial Relationships", " parameter.  The output will be an SWM-formatted spatial weights matrix file. ", "  It is possible to run out of memory when you run this tool.  This generally occurs when you select ", "Conceptualization of Spatial Relationships", " and/or  ", "Distance Band or Threshold Distance", " resulting in features having many, many neighbors.  You generally do not want to define spatial relationships so that features have thousands of neighbors.  You want all features to have at least one neighbor and almost all features to have at least eight neighbors.", "For polygon features, you will almost always want to choose ", "Row", " for the ", "Row Standardization", " parameter.  ", "Row Standardization", " mitigates bias when the number of neighbors each feature has is a function of the aggregation scheme or sampling process, rather than reflecting the actual spatial distribution of the variable you are analyzing.", "The ", "Modeling Spatial Relationships", " help topic provides additional information about this tool's parameters.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information.", "\r\n", "In ArcGIS version 9.2, the Global standardization option was removed. Global standardization returns the same results as no standardization. Models built with previous versions of ArcGIS that use the Global standardization option may need to be rebuilt."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class for which spatial autocorrelation will be calculated. ", "dataType": "Feature Layer"}, {"name": "Input_Field", "isInputFile": true, "isOptional": false, "description": "The numeric field used in assessing spatial autocorrelation. ", "dataType": "Field"}, {"name": "Generate_Report", "isOptional": true, "description": " NO_REPORT \u2014 No graphical summary will be created (default). GENERATE_REPORT \u2014 A graphical summary will be created as an HTML file.", "dataType": "Boolean"}, {"name": "Conceptualization_of_Spatial_Relationships", "isOptional": false, "description": "Specifies how spatial relationships among features are conceptualized. INVERSE_DISTANCE \u2014 Nearby neighboring features have a larger influence on the computations for a target feature than features that are far away. INVERSE_DISTANCE_SQUARED \u2014 Same as INVERSE_DISTANCE except that the slope is sharper, so influence drops off more quickly, and only a target feature's closest neighbors will exert substantial influence on computations for that feature. FIXED_DISTANCE_BAND \u2014 Each feature is analyzed within the context of neighboring features. Neighboring features inside the specified critical distance receive a weight of 1 and exert influence on computations for the target feature. Neighboring features outside the critical distance receive a weight of zero and have no influence on a target feature's computations. ZONE_OF_INDIFFERENCE \u2014 Features within the specified critical distance of a target feature receive a weight of 1 and influence computations for that feature. Once the critical distance is exceeded, weights (and the influence a neighboring feature has on target feature computations) diminish with distance. CONTIGUITY_EDGES_ONLY \u2014 Only neighboring polygon features that share a boundary or overlap will influence computations for the target polygon feature. CONTIGUITY_EDGES_CORNERS \u2014 Polygon features that share a boundary, share a node, or overlap will influence computations for the target polygon feature. GET_SPATIAL_WEIGHTS_FROM_FILE \u2014 Spatial relationships are defined in a spatial weights file. The path to the spatial weights file is specified in the Weights Matrix File parameter. ", "dataType": "String"}, {"name": "Distance_Method", "isOptional": false, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN_DISTANCE \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN_DISTANCE \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Standardization", "isOptional": false, "description": "Row standardization is recommended whenever the distribution of your features is potentially biased due to sampling design or an imposed aggregation scheme. NONE \u2014 No standardization of spatial weights is applied. ROW \u2014 Spatial weights are standardized; each weight is divided by its row sum (the sum of the weights of all neighboring features). ", "dataType": "String"}, {"name": "Distance_Band_or_Threshold_Distance", "isOptional": true, "description": "Specifies a cutoff distance for Inverse Distance and Fixed Distance options. Features outside the specified cutoff for a target feature are ignored in analyses for that feature. However, for Zone of Indifference, the influence of features outside the given distance is reduced with distance, while those inside the distance threshold are equally considered. The distance value entered should match that of the output coordinate system. For the Inverse Distance conceptualizations of spatial relationships, a value of 0 indicates that no threshold distance is applied; when this parameter is left blank, a default threshold value is computed and applied. This default value is the Euclidean distance that ensures every feature has at least one neighbor. This parameter has no effect when Polygon Contiguity or Get Spatial Weights From File spatial conceptualizations are selected. ", "dataType": "Double"}, {"name": "Weights_Matrix_File", "isOptional": true, "description": "The path to a file containing weights that define spatial, and potentially temporal, relationships among features. ", "dataType": "File"}]},
{"syntax": "MultiDistanceSpatialClustering_stats (Input_Feature_Class, Output_Table, Number_of_Distance_Bands, {Compute_Confidence_Envelope}, {Display_Results_Graphically}, {Weight_Field}, {Beginning_Distance}, {Distance_Increment}, {Boundary_Correction_Method}, {Study_Area_Method}, {Study_Area_Feature_Class})", "name": "Multi-Distance Spatial Cluster Analysis (Ripley's K Function) (Spatial Statistics)", "description": "Determines whether features, or the values associated with features, exhibit statistically significant clustering or dispersion over a range of distances.   \r\n Learn more about how Multi-Distance Spatial Cluster Analysis works \r\n", "example": {"title": "Multi-DistanceSpatialClusterAnalysis Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Multi-DistanceSpatialClusterAnalysis tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.MultiDistanceSpatialClustering_stats ( \"911Calls.shp\" , \"kFunResult.dbf\" , 11 , \"0_PERMUTATIONS_-_NO_CONFIDENCE_ENVELOPE\" , \"NO_REPORT\" , \"#\" , 1000 , 200 , \"REDUCE_ANALYSIS_AREA\" , \"MINIMUM_ENCLOSING_RECTANGLE\" , \"#\" )"}, "usage": ["Tool output is a table with fields: ", "ExpectedK", " and ", "ObservedK", " containing the expected and observed K values, respectively. Because the ", "L(d) transformation", " is applied, the ExpectedK values will always match the Distance value.  A field named ", "DiffK", " contains the Observed K values minus the Expected K values. If a confidence interval option is specified, two additional fields named ", "LwConfEnv", " and ", "HiConfEnv", " will be included in the ", "Output Table", " as well.  These fields contain confidence interval information for each iteration of the tool, as specified by the ", "Number of Distance Bands", " parameter. The K function will optionally create a graph layer summarizing results.  ", "When the observed K value is larger than the expected K value for a particular distance, the distribution is more clustered than a random distribution at that distance (scale of analysis). When the observed K value is smaller than the expected K value, the distribution is more dispersed than a random distribution at that distance. When the observed K value is larger than the HiConfEnv value, spatial clustering for that distance is statistically significant. When the observed K value is smaller than the LwConfEnv value, spatial dispersion for that distance is statistically significant.  Additional information about interpretation is found in ", "How Multi-Distance Spatial Cluster Analysis (Ripley's K-function) works", ".", "Enable  the ", "Display Results Graphically", " parameter to create a line graph summarizing tool results.  The expected results will be represented by a blue line while the observed results will be a red line. Deviation of the observed line above the expected line indicates that the dataset is exhibiting clustering at that distance. Deviation of the observed line below the expected line indicates that the dataset is exhibiting dispersion at that distance.  The line graph is created as a graph layer; graph layers are temporary and will be deleted when you close ArcMap.  If you right-click the graph layer and select ", "Save", ",  the graph can be written to a Graph File.  If you save your map document after saving your graph, a link to the graph file will be saved with your .mxd.  For more information about graph files, see ", "Exploring and visualizing data with graphs", ".", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "The ", "Weight Field", " is most appropriately used when it represents the number of incidents or counts.", "When no ", "Weight Field", " is specified, the largest ", "DiffK", " value tells you the distance where spatial processes promoting clustering are most pronounced.", "The following explains how the ", "confidence envelope", " is computed:", "When no ", "Weight Field", " is specified, the ", "confidence envelope", " is constructed by distributing points randomly in the study area and calculating ", "L(d)", " for that distribution. Each random distribution of the points is called a \"permutation\". If ", "99 permutations", " is selected, for example, the tool will randomly distribute the set of points 99 times for each iteration. After distributing the points 99 times the tool selects, for each distance, the Observed k value that deviated above and below the Expected k value by the greatest amount; these values become the confidence interval.", "When a ", "Weight Field", " is specified, only the weight values are randomly redistributed to compute confidence envelopes; the point locations remain fixed. In essence, when a Weight Field is specified, locations remain fixed and the tool evaluates the clustering of feature values in space. On the other hand, when no Weight Field is specified the tool analyzes clustering/dispersion of feature locations.", "Because the ", "confidence envelope", " is constructed from random permutations, the values defining the confidence envelope will change from one run to the next, even when parameters are identical.   If you set a seed value, however, for the ", "Random Number Generator", " geoprocessing environment, repeat analyses will produce consistent results.", "The number of permutations selected for the ", "Compute Confidence Envelope", " parameter may be loosely translated to confidence levels: 9 for 90%, 99 for 99%, and 999 for 99.9%.", "When no study area is specified, the tool uses a minimum enclosing rectangle as the study area polygon. Unlike the ", "extent", ", a minimum enclosing rectangle will not necessarily align with the x- and y-axes.", "The k-function statistic is very sensitive to the size of the study area. Identical arrangments of points can exhibit clustering or dispersion depending on the size of the study area enclosing them. Therefore, it is imperative that the study area boundaries are carefully considered. The picture below is a classic example of how identical feature distributions can be dispersed or clustered depending on the study area specified.", "\r\n", "A study area feature class is required if ", "USER_PROVIDED_STUDY_AREA_FEATURE_CLASS", " is chosen for the ", "Study Area Method", " parameter.  ", "If a ", "Study Area Feature Class", " is specified, it should have exactly one single part feature (the study area polygon).", "If no ", "Beginning Distance", " or ", "Distance Increment", " is specified, then default values are calculated for you based on the extent of the ", "Input Feature Class", ".", "The K function has an undercount bias for features located near the study area boundary. The ", "Boundary Correction Method", " parameter provides methods for addressing this bias.  ", "No specific boundary correction is applied. However, points in the ", "Input Feature Class", " that fall outside the user-specified study area are used in neighbor counts. This method is appropriate if you've collected data from a very large study area but only need to analyze smaller areas well within the boundaries of data collection.  ", "This method creates points outside the study area boundary that mirror those found inside the boundary in order to correct for underestimates near the edges. Points that are within a distance equal to the maximum distance band of an edge of the study area are mirrored. The mirrored points are used so that edge points will have more accurate neighbor estimates. The diagram below illustrates what points will be used in the calculation and which will be used only for edge correction.", "This edge correction technique shrinks the size of the analysis area by a distance equal to the largest distance band to be used in the analysis. After shrinking the study area, points found outside of the new study area will be considered only when neighbor counts are being assessed for points still inside the study area. They will not be used in any other way during the k-function calculation. The diagram below illustrates which points will be used in the calculation and which will be used only for edge correction.", "This method checks each point's distance from the edge of the study area and its distance to each of its neighbors. All neighbors that are further away from the point in question than the edge of the study area are given extra weighting. This edge correction method is only appropriate for square or rectangular shaped study areas, or when you select ", "MINIMUM_ENCLOSING_RECTANGLE", " for the ", "Study Area Method", " parameter. ", "When no boundary correction is applied, the undercount bias increases as the analysis distance increases.   If you enable the ", "Display Results Graphically", " parameter, you will notice that the ObservedK line droops at the larger distances.", "Mathematically, the Multi-Distance Spatial Cluster Analysis tool uses a common transformation of Ripley's k-function where the expected result with a random set of points is equal to the input distance. The transformation L(d) is shown below.", "where A is area, N is the number of points, d is the distance and k(i, j) is the weight, which (if there is no boundary correction) is 1 when the distance between i and j is less than or equal to d and 0 when the distance between i and j is greater than d. When edge correction is applied, the weight of k(i,j) is modified slightly.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class upon which the analysis will be performed. ", "dataType": "Feature Layer"}, {"name": "Output_Table", "isOutputFile": true, "isOptional": false, "description": "The table to which the results of the analysis will be written. ", "dataType": "Table"}, {"name": "Number_of_Distance_Bands", "isOptional": false, "description": "The number of times to increment the neighborhood size and analyze the dataset for clustering. The starting point and size of the increment are specified in the Beginning Distance and Distance Increment parameters, respectively. ", "dataType": "Long"}, {"name": "Compute_Confidence_Envelope", "isOptional": true, "description": "The confidence envelope is calculated by randomly placing feature points (or feature values) in the study area. The number of points/values randomly placed is equal to the number of points in the feature class. Each set of random placements is called a \"permutation\" and the confidence envelope is created from these permutations. This parameter allows you to select how many permutations you want to use to create the confidence envelope. 0_PERMUTATIONS_-_NO_CONFIDENCE_ENVELOPE \u2014 Confidence envelopes are not created. 9_PERMUTATIONS \u2014 Nine sets of points/values are randomly placed. 99_PERMUTATIONS \u2014 99 sets of points/values are randomly placed. 999_PERMUTATIONS \u2014 999 sets of points/values are randomly placed. ", "dataType": "String"}, {"name": "Display_Results_Graphically", "isOptional": true, "description": " NO_DISPLAY \u2014 No graphical summary will be created (default). DISPLAY_IT \u2014 A graphical summary will be created as a graph layer.", "dataType": "Boolean"}, {"name": "Weight_Field", "isOptional": true, "description": "A numeric field with weights representing the number of features/events at each location. ", "dataType": "Field"}, {"name": "Beginning_Distance", "isOptional": true, "description": "The distance at which to start the cluster analysis and the distance from which to increment. The value entered for this parameter should be in the units of the Output Coordinate System. ", "dataType": "Double"}, {"name": "Distance_Increment", "isOptional": true, "description": "The distance to increment during each iteration. The distance used in the analysis starts at the Beginning Distance and increments by the amount specified in the Distance Increment. The value entered for this parameter should be in the units of the Output Coordinate System. ", "dataType": "Double"}, {"name": "Boundary_Correction_Method", "isOptional": true, "description": "Method to use to correct for underestimates in the number of neighbors for features near the edges of the study area. NONE \u2014 No edge correction is applied. However, if the input feature class already has points that fall outside the study area boundaries, these will be used in neighborhood counts for features near boundaries. SIMULATE_OUTER_BOUNDARY_VALUES \u2014 This method simulates points outside the study area so that the number of neighbors near edges is not underestimated. The simulated points are the \"mirrors\" of points near edges within the study area boundary. REDUCE_ANALYSIS_AREA \u2014 This method shrinks the study area such that some points are found outside of the study area boundary. Points found outside the study area are used to calculate neighbor counts but are not used in the cluster analysis itself. RIPLEY'S_EDGE_CORRECTION_FORMULA \u2014 For all the points (j) in the neighborhood of point i, this method checks to see if the edge of the study area is closer to i, or if j is closer to i. If j is closer, extra weight is given to the point j. This edge correction method is only appropriate for square or rectangular shaped study areas. ", "dataType": "String"}, {"name": "Study_Area_Method", "isOptional": true, "description": "Specifies the region to use for the study area. The K Function is sensitive to changes in study area size so careful selection of this value is important. MINIMUM_ENCLOSING_RECTANGLE \u2014 Indicates that the smallest possible rectangle enclosing all of the points will be used. USER_PROVIDED_STUDY_AREA_FEATURE_CLASS \u2014 Indicates that a feature class defining the study area will be provided in the Study Area Feature Class parameter. ", "dataType": "String"}, {"name": "Study_Area_Feature_Class", "isOptional": true, "description": "Feature class that delineates the area over which the input feature class should be analyzed. Only to be specified if User-provided Study Area Feature Class is selected for the Study Area Method parameter. ", "dataType": "Feature Layer"}]},
{"syntax": "HighLowClustering_stats (Input_Feature_Class, Input_Field, {Generate_Report}, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Weights_Matrix_File})", "name": "High/Low Clustering (Getis-Ord General G) (Spatial Statistics)", "description": "Measures the degree of clustering for either high values or low values using the Getis-Ord General G statistic.   You can access the results of this tool (including the optional report file) from the  Results  window.  If you disable background processing, results will also be written to the  Progress  dialog box. \r\n Learn more about how High/Low Clustering: Getis-Ord General G works \r\n", "example": {"title": "HighLowClustering example 1 (Python window)", "description": "The following Python window script demonstrates how to use the High/Low Clustering tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.HighLowClustering_stats ( \"911Count.shp\" , \"ICOUNT\" , \"false\" , \"GET_SPATIAL_WEIGHTS_FROM_FILE\" , \"EUCLIDEAN_DISTANCE\" , \"NONE\" , \"#\" , \"euclidean6Neighs.swm\" )"}, "usage": ["The High/Low Clustering tool returns five values: Observed General G, Expected General G, Variance, z-score, and p-value. These values are accessible from the ", "Results window", " and are also passed as derived output values for potential use in models or scripts. Optionally, this tool will create an HTML file with a graphical summary of results. Double-clicking on the HTML file in the ", "Results", " window will open the HTML file in the default Internet browser.  Right-clicking on the ", "Messages entry", " in the ", "Results", " window and selecting ", "View", " will display the results in a ", "Message dialog box", ".   ", "The ", "Input Field", " should contain a variety of nonnegative", " values. You will get an error message if the ", "Input Field", " contains negative values. In addition, the math for this statistic requires some variation in the variable being analyzed; it cannot solve if all input values are 1, for example. If you want to use this tool to analyze the spatial pattern of incident data, consider ", "aggregating your incident data", ".", "The ", "z-score", " and ", "p-value", " are measures of statistical significance which tell you whether or not to reject the null hypothesis. For this tool, the null hypothesis states that the values associated with features are randomly distributed.", "The z-score is based on the randomization null hypothesis computation. For more information on z-scores, see ", "What is a z-score? What is a p-value?", "The higher (or lower) the ", "z-score", ", the stronger the intensity of the clustering. A z-score near zero indicates no apparent clustering within the study area. A positive z-score indicates clustering of high values. A negative z-score indicates clustering of low values.", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "In ArcGIS ", "10", ", optional graphical output is no longer displayed automatically.  Instead, an HTML file summarizing results is created.  To view results, double-click on the HTML file in the ", "Results window", ".  Custom scripts or model tools created prior to ArcGIS ", "10", " that use this tool may need to be rebuilt.  To rebuild these custom tools, open them, remove the ", "Display Results Graphically", " parameter, then resave.", "This tool will optionally create an HTML file summarizing results.  HTML files will not automatically appear in the ", "Catalog", " window.  If you want HTML files to be displayed in ", "Catalog", ", open the ", "ArcCatalog", " application, select the ", "Customize", "  menu option, click  ", "ArcCatalog Options", ", and select the ", "File Types", " tab.  Click on the ", "New Type", " button and specify ", "HTML", " for ", "File Extension", ".", "Your choice for the ", "Conceptualization of Spatial Relationships", " parameter should reflect inherent relationships among the features you are analyzing. The more realistically you can model how features interact with each other in space, the more accurate your results will be. Recommendations are outline in ", "Selecting a Conceptualization of Spatial Relationships: Best Practices", ". Here are some additional tips:  ", " The default ", "Distance Band or Threshold Distance", " will ensure each feature has at least one neighbor, and this is important. But often, this default will not be the most ", "appropriate distance", " to use for your analysis. Additional strategies for selecting an appropriate scale (distance band) for your analysis are outlined in ", "Selecting a fixed distance band value", ".", "When zero is entered for the ", "Distance Band or Threshold Distance", " parameter, all features are considered neighbors of all other features; when this parameter is left blank, the default distance will be applied.", "Weights for distances less than 1 become unstable when they are inverted. Consequently, the weighting for features separated by less than 1 unit of distance (common with ", "Geographic Coordinate System", " projections) are given a weight of 1. ", "Analysis on features with a ", "Geographic Coordinate System", " projection is not recommended when you select any of the inverse distance-based spatial conceptualization methods (", "INVERSE_DISTANCE", ", ", "INVERSE_DISTANCE_SQUARED", ", or ", "ZONE_OF_INDIFFERENCE", ").", "For the inverse distance options (not recommended for this tool), any two points that are coincident will be given a weight of one to avoid zero division. This assures features are not excluded from analysis.", "Additional options for the ", "Conceptualization of Spatial Relationships", " parameter, including space-time relationships, are available using the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools.  To take advantage of these additional options, use one of these tools to construct the ", "spatial weights matrix file", " prior to analysis; select ", "GET_SPATIAL_WEIGHTS_FROM_FILE", " for the ", "Conceptualization of Spatial Relationships", " parameter; and for the ", "Weights Matrix File", " parameter, specify the path to the spatial weights file you created.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "If you provide a ", "Weights Matrix File", " with an ", "SWM", " extension, this tool is expecting a spatial weights matrix file created using either the ", "Generate Spatial Weights Matrix", " or ", "Generate Network Spatial Weights", " tools;  otherwise, this tool is expecting an ", "ASCII formatted spatial weights matrix file", ". In some cases, behavior is different depending on which type of spatial weights matrix file you use:", "Running your analysis with an ", "ASCII formatted spatial weights matrix file", " is memory intensive.  For analyses on more than 5,000 features, consider converting your ASCII-formatted spatial weights matrix file into an ", "SWM formatted file", ".  First put your ASCII weights into a ", "formatted", " table (using Excel, for example).  Next, run the  ", "Generate Spatial Weights Matrix", " tool using ", "CONVERT_TABLE", " for the ", "Conceptualization of Spatial Relationships", " parameter.  The output will be an SWM-formatted spatial weights matrix file. ", "The ", "Modeling Spatial Relationships", " help topic provides additional information about this tool's parameters.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information.", "In ArcGIS 9.2, the Global standardization option was removed. Global standardization returns the same results as no standardization. Models built with previous versions of ArcGIS that use the Global standardization option may need to be rebuilt."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class for which the General G statistic will be calculated. ", "dataType": "Feature Layer"}, {"name": "Input_Field", "isInputFile": true, "isOptional": false, "description": "The numeric field to be evaluated. ", "dataType": "Field"}, {"name": "Generate_Report", "isOptional": true, "description": " NO_REPORT \u2014 No graphical summary will be created (default). GENERATE_REPORT \u2014 A graphical summary will be created as an HTML file.", "dataType": "Boolean"}, {"name": "Conceptualization_of_Spatial_Relationships", "isOptional": false, "description": "Specifies how spatial relationships among features are conceptualized. INVERSE_DISTANCE \u2014 Nearby neighboring features have a larger influence on the computations for a target feature than features that are far away. INVERSE_DISTANCE_SQUARED \u2014 Same as INVERSE_DISTANCE except that the slope is sharper, so influence drops off more quickly, and only a target feature's closest neighbors will exert substantial influence on computations for that feature. FIXED_DISTANCE_BAND \u2014 Each feature is analyzed within the context of neighboring features. Neighboring features inside the specified critical distance receive a weight of 1 and exert influence on computations for the target feature. Neighboring features outside the critical distance receive a weight of zero and have no influence on a target feature's computations. ZONE_OF_INDIFFERENCE \u2014 Features within the specified critical distance of a target feature receive a weight of 1 and influence computations for that feature. Once the critical distance is exceeded, weights (and the influence a neighboring feature has on target feature computations) diminish with distance. CONTIGUITY_EDGES_ONLY \u2014 Only neighboring polygon features that share a boundary or overlap will influence computations for the target polygon feature. CONTIGUITY_EDGES_CORNERS \u2014 Polygon features that share a boundary, share a node, or overlap will influence computations for the target polygon feature. GET_SPATIAL_WEIGHTS_FROM_FILE \u2014 Spatial relationships are defined in a spatial weights file. The path to the spatial weights file is specified in the Weights Matrix File parameter. ", "dataType": "String"}, {"name": "Distance_Method", "isOptional": false, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN_DISTANCE \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN_DISTANCE \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Standardization", "isOptional": false, "description": "Row standardization is recommended whenever the distribution of your features is potentially biased due to sampling design or an imposed aggregation scheme. NONE \u2014 No standardization of spatial weights is applied. ROW \u2014 Spatial weights are standardized; each weight is divided by its row sum (the sum of the weights of all neighboring features). ", "dataType": "String"}, {"name": "Distance_Band_or_Threshold_Distance", "isOptional": true, "description": "Specifies a cutoff distance for Inverse Distance and Fixed Distance options. Features outside the specified cutoff for a target feature are ignored in analyses for that feature. However, for Zone of Indifference, the influence of features outside the given distance is reduced with distance, while those inside the distance threshold are equally considered. The distance value entered should match that of the output coordinate system. For the Inverse Distance conceptualizations of spatial relationships, a value of 0 indicates that no threshold distance is applied; when this parameter is left blank, a default threshold value is computed and applied. This default value is the Euclidean distance that ensures every feature has at least one neighbor. This parameter has no effect when Polygon Contiguity or Get Spatial Weights From File spatial conceptualizations are selected. ", "dataType": "Double"}, {"name": "Weights_Matrix_File", "isOptional": true, "description": "The path to a file containing weights that define spatial, and potentially temporal, relationships among features. ", "dataType": "File"}]},
{"syntax": "AverageNearestNeighbor_stats (Input_Feature_Class, Distance_Method, {Generate_Report}, {Area})", "name": "Average Nearest Neighbor (Spatial Statistics)", "description": "Calculates a nearest neighbor index based on the average distance from each feature to its nearest neighboring feature.   You can access the results of this tool (including the optional report file) from the  Results  window.  If you disable background processing, results will also be written to the  Progress  dialog box. \r\n Learn more about how Average Nearest Neighbor Distance works \r\n", "example": {"title": "AverageNearestNeighbor example 1 (Python window)", "description": "The following Python window script demonstrates how to use the AverageNearestNeighbor tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.AverageNearestNeighbor_stats ( \"burglaries.shp\" , \"EUCLIDEAN_DISTANCE\" , \"NO_REPORT\" , \"#\" )"}, "usage": ["The Average Nearest Neighbor tool returns five values: Observed Mean Distance, Expected Mean Distance, Nearest Neighbor Index, z-score, and p-value. These values are accessible from the ", "Results window", " and are also passed as derived output values for potential use in models or scripts. Optionally, this tool will create an HTML file with a graphical summary of results. Double-clicking on the HTML entry in the ", "Results", " window will open the HTML file in the default Internet browser.  Right-clicking on the ", "Messages entry", " in the ", "Results", " window and selecting ", "View", " will display the results in a ", "Message dialog box", ".  ", "The ", "z-score and p-value", " results are measures of statistical significance which tell you whether or not to reject the null hypothesis. For the Average Nearest Neighbor statistic, the null hypothesis states that features are randomly distributed.", "The Nearest Neighbor Index is expressed as the ratio of the Observed Mean Distance to the Expected Mean Distance. The expected distance is the average distance between neighbors in a hypothetical random distribution. If the index is less than 1, the pattern exhibits clustering; if the index is greater than 1, the trend is toward dispersion or competition.", "The average nearest neighbor method is very sensitive to the Area value (small changes in the ", "Area", " parameter value can result in considerable changes in the results). Consequently, the Average Nearest Neighbor tool is most effective for comparing different features in a fixed study area. The picture below is a classic example of how identical feature distributions can be dispersed or clustered depending on the study area specified.  ", "If an ", "Area", " parameter value is not specified, then the area of the minimum enclosing rectangle around the input features is used. Unlike the ", "extent", ", a minimum enclosing rectangle will not necessarily align with the x- and y-axes.", "There are special cases of input features that would result in invalid (zero-area) minimum enclosing rectangles. In these cases, a small value derived from the input feature XY tolerance will be used to create the minimum enclosing rectangle. For example, if all features are coincident (that is, all have the exact same X and Y coordinates), the area for a very small square polygon around the single location will be used in calculations. Another example would be if all features align perfectly (for example, 3 points in a straight line); in this case the area of a rectangle polygon with a very small width around the features will be used in computations. It is always best to supply an ", "Area", " value when using the Average Nearest Neighbor tool.", "Although this tool will work with polygon or line data, it is most appropriate for event, incident, or other fixed-point feature data. For line and polygon features, the true geometric centroid for each feature is used in computations. For multipoint, polyline, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "In ArcGIS ", "10", ", optional graphical output is no longer displayed automatically.  Instead, an HTML file summarizing results is created.  To view results, double-click on the HTML file in the ", "Results window", ".  Custom scripts or model tools created prior to ArcGIS ", "10", " that use this tool may need to be rebuilt.  To rebuild these custom tools, open them, remove the ", "Display Results Graphically", " parameter, then resave.", "This tool will optionally create an HTML file summarizing results.  HTML files will not automatically appear in the ", "Catalog", " window.  If you want HTML files to be displayed in ", "Catalog", ", open the ", "ArcCatalog", " application, select the ", "Customize", "  menu option, click  ", "ArcCatalog Options", ", and select the ", "File Types", " tab.  Click on the ", "New Type", " button and specify ", "HTML", " for ", "File Extension", ".", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class, typically a point feature class, for which the average nearest neighbor distance will be calculated. ", "dataType": "Feature Layer"}, {"name": "Distance_Method", "isOptional": false, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN_DISTANCE \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN_DISTANCE \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Generate_Report", "isOptional": true, "description": " NO_REPORT \u2014 No graphical summary will be created (default). GENERATE_REPORT \u2014 A graphical summary will be created as an HTML file.", "dataType": "Boolean"}, {"name": "Area", "isOptional": true, "description": "A numeric value representing the study area size. The default value is the area of the minimum enclosing rectangle that would encompass all features (or all selected features). Units should match those for the Output Coordinate System. ", "dataType": "Double"}]},
{"syntax": "ManageMapServerCacheStatus_server (input_service, scales, manage_mode, num_of_caching_service_instances, output_folder, {report_extent}, {area_of_interest})", "name": "Manage Map Server Cache Status (Server)", "description": "Manages internal data kept by the server about the built tiles in a map or image service cache.", "example": {"title": null, "description": "Deletes the status information for a cache", "code": "# Name: ManageMapServerCacheStatus.py # Description: The following stand-alone script demonstrates how to delete  # Status of cache using ManageMapServerCachStatus tool   # Requirements: os, sys, time and traceback modules # Any line that begins with a pound sign is a comment and will not be executed # Empty quotes take the default value. # To accept arguments from the command line replace values of variables to #                                                           \"sys.argv[]\" # Import system modules import arcpy from arcpy import env import os , sys , time , datetime , traceback , string # Set environment settings env.workspace = \"C:/data\" # List of input variables for map service properties connectionFile = r\"C:\\Users\\<username>\\AppData\\Roaming\\ESRI\\Desktop10.1\\ArcCatalog\" server = \"arcgis on MyServer_6080 (publisher)\" serviceName = \"Rainfall.MapService\" inputService = connectionFile + \" \\\\ \" + server + \" \\\\ \" + serviceName scales = \"\" manageMode = \"DELETE_CACHE_STATUS\" numOfCachingServiceInstances = \"2\" outputFolder = \"\" areaOfInterest = \"\" reportExtents = \"\" currentTime = datetime.datetime.now () arg1 = currentTime.strftime ( \"%H-%M\" ) arg2 = currentTime.strftime ( \"%Y-%m- %d  %H:%M\" ) file = 'C:/data/report_ %s .txt' % arg1 # print results of the script to a report report = open ( file , 'w' ) # use \"scales[0]\",\"scales[-1]\",\"scales[0:3]\" try : starttime = time.clock () result = arcpy.ReportMapServerCacheStatus_server ( inputService , scales [ - 1 ], manageMode , numOfCachingServiceInstances , outputFolder , areaOfInterest , reportExtents ) finishtime = time.clock () elapsedtime = finishtime - starttime #print messages to a file while result.status < 4 : time.sleep ( 0.2 ) resultValue = result.getMessages () report.write ( \"completed \" + str ( resultValue )) print \"Reported the Bundle status for scale =\" + str ( scales [ - 1 ]) + \"of \" serviceName + \"at \" + outputFolder + \" \\n  using specified feature class \" areaOfInterest + \" in \" + str ( elapsedtime ) + \" sec  \\n  on \" + arg2 except Exception , e : # If an error occurred, print line number and error message tb = sys.exc_info ()[ 2 ] report.write ( \"Failed at step 3  \\n \" \"Line  %i \" % tb.tb_lineno ) report.write ( e.message ) report.close ()"}, "usage": [], "parameters": [{"name": "input_service", "isInputFile": true, "isOptional": false, "description": "The map or image service for which the status will be modified. This is a string containing both the server and service information. To see how to construct this string, open ArcCatalog , select your service in the Catalog tree, and note the text in the Location toolbar. Then change the backslashes to forward slashes, for example, GIS Servers/arcgis on MYSERVER (admin)/USA.MapServer . ", "dataType": "String"}, {"name": "scales", "isOptional": false, "description": " This parameter is intended for use with future releases of ArcGIS. ", "dataType": "Double"}, {"name": "manage_mode", "isOptional": false, "description": " DELETE_CACHE_STATUS \u2014 Deletes the status information used by the server. REBUILD_CACHE_STATUS \u2014 Deletes and then rebuilds the status information used by the server.", "dataType": "String"}, {"name": "num_of_caching_service_instances", "isOptional": false, "description": "The total number of instances of the System/CachingTools service that you want to dedicate toward running this tool. You can increase the maximum number of instances per machine of the System/CachingTools service using the Service Editor window available through an administrative connection to ArcGIS Server. Ensure your server machines can support the chosen number of instances. ", "dataType": "Long"}, {"name": "output_folder", "isOutputFile": true, "isOptional": false, "description": " This parameter is intended for use with future releases of ArcGIS. ", "dataType": "Folder"}, {"name": "report_extent", "isOptional": true, "description": "This parameter is intended for use with future releases of ArcGIS. ", "dataType": "Extent"}, {"name": "area_of_interest", "isOptional": true, "description": "This parameter is intended for use with future releases of ArcGIS. ", "dataType": "Feature Set"}]},
{"syntax": "SignInToPortal_server (username, password, portal_url)", "name": "Sign In To Portal (Server)", "description": "\r\nAllows you to sign in to portals. If you are publishing to an ArcGIS Online portal you need to be signed in to ArcGIS Online in order to publish. For those organizations that would like to use ArcGIS Online behind the firewall or in their own private cloud, there is a version that you can install and use on your own computer networks. It is called Portal for ArcGIS. Learn more about ArcGIS Online", "example": {"title": "Sign in to ArcGIS Online (Python window)", "description": "Signs in to ArcGIS Online with a specified user name and password.", "code": ""}, "usage": ["\r\n To sign into an ArcGIS Online portal, your esri global account needs to be registered as a member of ArcGIS Online. To create an esri global account and register it, go to ", "arcgis.com/home/signup.html", ".", "Use the ", "Sign Out From Portal", " tool to sign-out from the ArcGIS Online portal."], "parameters": [{"name": "username", "isOptional": false, "description": "The esri global account user name. ", "dataType": "String"}, {"name": "password", "isOptional": false, "description": " The esri global account password. ", "dataType": "Encrypted String"}, {"name": "portal_url", "isOptional": false, "description": "The URL for the ArcGIS Online portal for which you want to make a connection. For example, http://www.arcgis.com/ . The backslash at the end of the URL must be included. The default value is URL for the ArcGIS portal currently chosen for the user. ", "dataType": "String"}]},
{"syntax": "SignOutFromPortal_server ()", "name": "Sign Out From Portal (Server)", "description": "\r\nSigns out from the portal that you are currently signed in to. For those organizations that would like to use ArcGIS Online behind the firewall or in their own private cloud, there is a version that you can install and use on your own computer networks. It is called Portal for ArcGIS. Learn more about ArcGIS Online", "example": {"title": "Sign out from ArcGIS Online (Python window)", "description": "Signs out from ArcGIS Online.", "code": ""}, "usage": ["You can sign-in to an ArcGIS Online portal using the ", "Sign In To Portal", " tool, or in ", "ArcMap", ", click ", "File ", ">", " Sign In", " on the main menu."], "parameters": []},
{"syntax": "StageService_server (in_service_definition_draft, out_service_definition)", "name": "Stage Service (Server)", "description": "\r\n\r\nStages a  service definition .  A staged service definition ( .sd ) file contains all the necessary information needed to publish a GIS service, including data that must be copied to the server because it does not appear in the server's data store.", "example": {"title": "StageService example (Python window)", "description": "Stages a service definition.", "code": ""}, "usage": ["This tool converts a service definition draft (", ".sddraft", ") into a service definition that can then be input to the  ", "Upload_Service_Definition", " tool to upload and publish the  GIS service to a server.  Draft service definitions cannot be used to  publish a GIS service directly.", "Whenever you share a service using ", "ArcGIS for Desktop", ", the ", "Stage Service", " tool is run and you will see a result in the geoprocessing ", "Results", " window.  ", "Service definition drafts can be  created ", "using ArcGIS for Desktop", " or by using the ", "arcpy.mapping", " function ", "CreateMapSDDraft", ".", "Once staged, the input service definition draft is deleted."], "parameters": [{"name": "in_service_definition_draft", "isInputFile": true, "isOptional": false, "description": "Input draft service definition. Service definition drafts can be created using ArcGIS for Desktop. See the help topic About draft services for more information. You can also use the arcpy.mapping function CreateMapSDDraft to create draft service definitions. Once staged, the input draft service definition is deleted. ", "dataType": "File"}, {"name": "out_service_definition", "isOutputFile": true, "isOptional": false, "description": "Resulting service definition. The default is to write the service definition to the same directory as the draft service definition ", "dataType": "File"}]},
{"syntax": "UploadServiceDefinition_server (in_sd_file, in_server, {in_service_name}, {in_cluster}, {in_folder_type}, {in_folder}, in_startupType, {in_override}, {in_my_contents}, {in_public}, {in_organization}, {in_groups})", "name": "Upload Service Definition (Server)", "description": "\r\n\r\nUploads and publishes a GIS service to a specified GIS server based on a staged service definition ( .sd ) file.", "example": {"title": "UploadServiceDefinition example (Python window)", "description": "Uploads and publishes a service definition to a specified server.", "code": ""}, "usage": ["\r\nThis tool uploads and publishes a GIS service based on the input service definition.  Whenever you share a service using ", "ArcGIS for Desktop", ", this tool is run, and you will see a result in the geoprocessing ", "Results", " window.  ", "This tool does not upload and publish service definition draft (", ".sddraft", ") files. If you have a service definition draft, you can convert it to a staged service definition by using  the ", "Stage Service", " tool.", "You can create an ", "ArcGIS for Server", " connection using ", "ArcGIS for Desktop", " by ", "making a publisher connection to ArcGIS Server in ArcGIS Desktop", ".", " You can use the ", "Sign In To Portal", " tool to connect to an ArcGIS Online portal."], "parameters": [{"name": "in_sd_file", "isInputFile": true, "isOptional": false, "description": " The service definition ( .sd ) contains all the information needed to publish a GIS service. ", "dataType": "File"}, {"name": "in_server", "isInputFile": true, "isOptional": false, "description": "You can use ArcGIS for Server connections listed under the GIS Servers node in the Catalog window, or you can navigate to a different folder where you might have server connection files stored. If you are connecting to ArcGIS Online, make sure you type My Hosted Services for the server connection with each word capitalized and a space between each word. ", "dataType": "ServerConnection"}, {"name": "in_service_name", "isInputFile": true, "isOptional": true, "description": " Use this to override the service name currently specified in the service definition with a new name. ", "dataType": "String"}, {"name": "in_cluster", "isInputFile": true, "isOptional": true, "description": " Use this if you want to change the cluster to which the service has been assigned. You must choose from clusters that are available on the specified server. ", "dataType": "String"}, {"name": "in_folder_type", "isInputFile": true, "isOptional": false, "description": "Folder type is used to determine the source for the folder. The default is to get a folder from the service definition. You can also choose to get a list of folders already existing on the specified server, or you can specify a new folder to be created once you publish this service. NEW \u2014 Use this to create a new folder. EXISTING \u2014 Use this to specify a folder that exists on the server. FROM_SERVICE_DEFINITION \u2014 Use the folder already specified in the service definition. This is the default.", "dataType": "String"}, {"name": "in_folder", "isInputFile": true, "isOptional": true, "description": "Use this to specify the folder for the service. The default is to use the folder specified in the service definition. If you chose the NEW folder type, you use this parameter to enter a new folder name. If you chose the EXISTING folder type, you can choose from the existing folders on the server. ", "dataType": "String"}, {"name": "in_startupType", "isInputFile": true, "isOptional": false, "description": "Use this to determine the start/stop state of the service immediately after publishing. STARTED \u2014 The service starts immediately after publishing. STOPPED \u2014 The service does not start after publishing. You will have to start the service manually.", "dataType": "Boolean"}, {"name": "in_override", "isInputFile": true, "isOptional": true, "description": "Use this parameter if you want to override the sharing properties set in the service definition. These properties define if, and how, you are sharing your service with ArcGIS Online. Sharing your service with ArcGIS Online exposes it for others to use. You must be logged in to ArcGIS Online in order to override sharing properties. OVERRIDE_DEFINITION \u2014 Override the sharing properties set in the service definition with new values. USE_DEFINITION \u2014 The sharing properties currently set in the service definition will be used when the service is published. This is the default.", "dataType": "Boolean"}, {"name": "in_my_contents", "isInputFile": true, "isOptional": true, "description": "All shared services are available through My Contents. Even if you only want to share with a specific group in your organization, the service will also be shared through My Contents. You must be logged in to ArcGIS Online in order to override sharing properties. SHARE_ONLINE \u2014 Shares the service on ArcGIS Online. The service will be listed under My Content. NO_SHARE_ONLINE \u2014 The service will not be shared on ArcGIS Online and will be inaccessible to other ArcGIS Online users and clients on the web.", "dataType": "Boolean"}, {"name": "in_public", "isInputFile": true, "isOptional": true, "description": "Choose whether or not your service will be available to the public. You must be logged in to ArcGIS Online in order to override sharing properties. PUBLIC \u2014 Share the service with the public. PRIVATE \u2014 Do not share the service with the public.", "dataType": "Boolean"}, {"name": "in_organization", "isInputFile": true, "isOptional": true, "description": "You can share your service with your organization. You must be logged in to ArcGIS Online in order to override sharing properties. SHARE_ORGANIZATION \u2014 Share the service with your organization. NO_SHARE_ORGANIZATION \u2014 Do not share the service with your organization.", "dataType": "Boolean"}, {"name": "in_groups", "isInputFile": true, "isOptional": false, "description": "A list of group names with which to share the service. You must be logged in to ArcGIS Online in order to override sharing properties. ", "dataType": "String"}]},
{"syntax": "AddRepresentation_cartography (in_features, representation_name, {rule_id_field_name}, {override_field_name}, {geometry_editing_option}, {import_rule_layer}, {assign_rule_id_option})", "name": "Add Representation (Cartography)", "description": "Adds a feature class representation to a geodatabase feature class.", "example": {"title": "AddRepresentation tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the AddRepresentation tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.AddRepresentation_cartography ( \"C:/data/cartography.gdb/buildings/footprints\" , \"footprints_Rep\" , \"RuleID\" , \"Override\" , \"STORE_CHANGE_AS_OVERRIDE\" , \"C:/data/footprints.lyr\" , \"ASSIGN\" )"}, "usage": ["The input must be a geodatabase feature class.", "Two new fields with the specified field names will be appended to the input feature class attribute table to identify the representation rules that govern how each category of features will be portrayed, and to hold any feature-specific overrides to these rules.", "Specify an import rule layer to import symbol choices listed under the renderer type (i.e., Categories-Unique values) specified on the layer file. All of the symbol choices will be copied into this feature class representation as new representation rules. If an import rule layer is not specified, all features will be assigned to a single default representation rule.", "If the import rule layer has the same source feature class as the input feature class, you can use the ", "Assign Rule IDs", " parameter to assign representation rules to features to match the RuleID assignments of the import rule layer.", "If an import rule layer is specified and the ", "Assign Rule IDs", " parameter is set to ASSIGN, all features will be assigned to a representation rule based on the symbol choices listed under the renderer type (i.e., Categories-Unique values) specified on the layer file. If an import rule layer is specified, but the ", "Assign Rule IDs", " parameter is set to NO_ASSIGN, all features will be assigned to a single default representation rule.", "If <all other values> is used to symbolize features in the input layer, that symbol will become Rule ID 1 when a representation is added."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input geodatabase feature class to which a new feature class representation will be added. ", "dataType": "Feature Layer"}, {"name": "representation_name", "isOptional": false, "description": "The name of the feature class representation to be added. ", "dataType": "String"}, {"name": "rule_id_field_name", "isOptional": true, "description": "The name of the RuleID field, which will hold a reference to the representation rule for each feature. ", "dataType": "String"}, {"name": "override_field_name", "isOptional": true, "description": "The name of the Override field, which will hold overrides to representation rules for each feature. ", "dataType": "String"}, {"name": "geometry_editing_option", "isOptional": true, "description": "Specifies what will happen to the supporting feature class geometry when features are modified with the representation editing tools. STORE_CHANGE_AS_OVERRIDE \u2014 Geometry modification made to features with the representation editing tools or using any of the geoprocessing tools in the Cartography toolbox will be stored as shape overrides in the Override field. Supporting feature class geometry (stored in the Shape field of the feature class) will be unaffected. This is the default. MODIFY_FEATURE_SHAPE \u2014 Geometry modifications made to features with the representation editing tools, or using any of the geoprocessing tools in the Cartography toolbox, will modify the supporting feature class geometry (stored in the Shape field of the feature class.) No shape overrides will be stored. ", "dataType": "String"}, {"name": "import_rule_layer", "isOptional": true, "description": "A feature layer that symbolizes features with a feature class representation, from which the representation rules are imported. ", "dataType": "Layer"}, {"name": "assign_rule_id_option", "isOptional": true, "description": "Specifies whether or not to assign representation rules to features to match the RuleID assignments of the import rule layer. This option applies only when Import Rule Layer is specified. ASSIGN \u2014 assign RuleIDs to features to match the import rule layer. This is the default. NO_ASSIGN \u2014 Specifies not to match RuleIDs to features from the import rule layer. Features will be assigned to the default representation rule instead. ", "dataType": "String"}]},
{"syntax": "IntersectingLayersMasks_cartography (masking_layer, masked_layer, output_fc, reference_scale, spatial_reference, margin, method, mask_for_non_placed_anno, {attributes})", "name": "Intersecting Layers Masks (Cartography)", "description": "Creates masking polygons at a specified shape and size at the intersection of two symbolized input layers -- the Masking layer and the Masked layer. ", "example": {"title": "IntersectingLayersMasks tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the IntersectingLayersMasks tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.IntersectingLayersMasks_cartography ( \"C:/data/cartography.gdb/transportation/roads\" , \"C:/data/cartography.gdb/transportation/railroads\" , \"C:/data/cartography.gdb/transportation/ilm_polys\" , \"25000\" , \"\" , \"5 meters\" , \"EXACT_SIMPLIFIED\" , \"\" , \"ALL\" )"}, "usage": ["This tool accepts point, line, and polygon feature layers as well as geodatabase annotation layers as input.", "Masks will be created if the margin distance is 0 or negative. A margin size of 0 will create a polygon that represents the exact shape of the symbolized feature. A negative margin will result in a polygon smaller than the symbolized feature. Generally, a margin value larger than 0 will be specified to produce the desired masking effect.", "When creating masks, it is important to know that adding masks to maps adds complexity that will slow map drawing and affect map printing and exporting. Generally, there are three things to consider when creating masks for a map: (1) the number of masks, (2) the complexity of the masks, and (3) whether the masks will be used to mask polygon features filled with marker or line symbols. An increase in the number of masks, having more complex masks, and/or masking against marker or polygon fill symbols will result in slower drawing on your screen. Additionally, printing and exporting performance can be poor and even fail to produce valid output, first, because of the large amount of processing required to print and export maps with masks, and second, because of known limitations in how graphic file formats store map export results that have many complicated masks.", "When seeking to improve drawing performance as well as printing and exporting performance and reliability, the most important guideline to follow is to use the simplest masks necessary for the purposes of your map. In particular, when masking annotation text, you will find that CONVEX_HULL type masks are sufficient for many map purposes. If you need more detailed text masks, then use the EXACT_SIMPLIFIED type. Generally, when masking much text on a relatively large map, avoid using the EXACT type mask because it will create too many complicated masks to produce valid output efficiently.", "Margin values are specified in either page units or map units. Most of the time you will want to specify your margin distance value in page units.", "Margin value units are interpreted differently depending on which units you choose. If you choose points, inches, millimeters, or centimeters, then masks are created using the margin distance as calculated in page space (you can think of the margin as a distance measured on the paper). The reference scale parameter value is accounted for in this calculation.", "If you choose any other units for your margin, then masks are created using the margin distance as calculated in map space (you can think of the margin as a real-world distance measure on the earth). Also, in this case, the reference scale parameter value is not used as part of the calculation.", "If one of the input layers is an annotation layer, the reference scale will be automatically set to the reference scale of the layer's feature class to ensure accurate calculation of the mask. If two annotation layers are being intersected, they must have the same reference scale.", "When masking annotation projected on the fly, masks should be created using the map's spatial reference by properly setting it in the spatial reference parameter. Readability is preserved when text is projected on the fly, which is why there may be differences in the spatial area text occupies in different projections.", "Masks of annotation features are font specific. When using masks with text, it is important to ensure that the same font is used onscreen as in the output. To do this, choose to embed fonts in vector output or download SoftFonts to printers or plotters."], "parameters": [{"name": "masking_layer", "isOptional": false, "description": "The symbolized input layer, which will be intersected with the masked layer to create masking polygons. This is the layer that will be more prominently displayed when masking is applied to the masked layer. ", "dataType": "Layer"}, {"name": "masked_layer", "isOptional": false, "description": "The symbolized input layer to be masked. This is the layer that will be obscured by the masking polygons. ", "dataType": "Layer"}, {"name": "output_fc", "isOutputFile": true, "isOptional": false, "description": "The feature class that will contain the mask features. ", "dataType": "Feature Class"}, {"name": "reference_scale", "isOptional": false, "description": "The reference scale used for calculating the masking geometry when masks are specified in page units. This is typically the reference scale of the map. ", "dataType": "Double"}, {"name": "spatial_reference", "isOptional": false, "description": "The spatial reference for which the masking polygons will be created. This is not the spatial reference that will be assigned to the output feature class. It is the spatial reference of the map in which the masking polygons will be used since the position of symbology may change when features are projected. ", "dataType": "Spatial Reference"}, {"name": "margin", "isOptional": false, "description": "The space in page units surrounding the symbolized input features used to create the mask polygon. Typically, masking polygons are created with a small margin around the symbol to improve visual appearance. Margin values are specified in either page units or map units. Most of the time you will want to specify your margin distance value in page units. Margin value units are interpreted differently depending on which units you choose. If you choose points, inches, millimeters, or centimeters, then masks are created using the margin distance as calculated in page space (you can think of the margin as a distance measured on the paper). The reference scale parameter value is accounted for in this calculation. If you choose any other units for your margin, then masks are created using the margin distance as calculated in map space (you can think of the margin as a real-world distance measure on the earth). Also, in this case, the reference scale parameter value is not used as part of the calculation. ", "dataType": "Linear unit"}, {"name": "method", "isOptional": false, "description": "The type of masking geometry created. There are four types: BOX \u2014 A polygon representing the extent of the symbolized feature. CONVEX_HULL \u2014 The convex hull of the symbolized geometry of the feature. This is the default. EXACT_SIMPLIFIED \u2014 A generalized polygon representing the exact shape of the symbolized feature. Polygons created with this method will have a significantly smaller number of vertices compared to polygons created with the EXACT method. EXACT \u2014 A polygon representing the exact shape of the symbolized feature. ", "dataType": "String"}, {"name": "mask_for_non_placed_anno", "isOptional": false, "description": "Specifies whether to create masks for unplaced annotation. This option is only used when masking geodatabase annotation layers. ALL_FEATURES \u2014 Creates masks for all the annotation features. ONLY_PLACED \u2014 Only creates masks for features with a status of placed.", "dataType": "String"}, {"name": "attributes", "isOptional": true, "description": "Determines which attributes will be transferred from the input features to the output features. ONLY_FID \u2014 Only the FID field from the input features will be transferred to the output features. This is the default. NO_FID \u2014 All the attributes except the FID from the input features will be transferred to the output features. ALL \u2014 All the attributes from the input features will be transferred to the output features. ", "dataType": "String"}]},
{"syntax": "FeatureOutlineMasks_cartography (input_layer, output_fc, reference_scale, spatial_reference, margin, method, mask_for_non_placed_anno, {attributes})", "name": "Feature Outline Masks (Cartography)", "description": "Creates mask polygons at a specified distance and shape around the symbolized features in the input layer. ", "example": {"title": "FeatureOutlineMasks tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the FeatureOutlineMasks tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" sr = arcpy.SpatialReference ( 4326 ) arcpy.FeatureOutlineMasks_cartography ( \"C:/data/cartography.gdb/transportation/roads\" , \"C:/data/cartography.gdb/transportation/fom_polys\" , \"25000\" , sr , \"5 meters\" , \"EXACT_SIMPLIFIED\" , \"ALL_FEATURES\" , \"ALL\" )"}, "usage": ["This tool accepts point, line, and polygon feature layers as well as geodatabase annotation layers as input.", "When creating masks, it is important to know that adding masks to maps adds complexity that will slow map drawing and affect map printing and exporting. Generally, there are three things to consider when creating masks for a map: (1) the number of masks, (2) the complexity of the masks, and (3) whether the masks will be used to mask polygon features filled with marker or line symbols. An increase in the number of masks, having more complex masks, and/or masking against marker or polygon fill symbols will result in slower drawing on your screen. In addition, printing and exporting performance can be poor and even fail to produce valid output, first, because of the large amount of processing required to print and export maps with masks, and second, because of known limitations in how graphic file formats store map export results that have many complicated masks.", "When seeking to improve drawing performance as well as printing and exporting performance and reliability, the most important guideline to follow is to use the simplest masks necessary for the purposes of your map. In particular, when masking annotation text, you will find that CONVEX_HULL type masks are sufficient for many map purposes. If you need more detailed text masks, then use the EXACT_SIMPLIFIED type. Generally, when masking much text on a relatively large map, avoid using the EXACT type mask because it will create too many complicated masks to produce valid output efficiently.", "Margin values are specified in either page units or map units. Most of the time you will want to specify your margin distance value in page units.", "Margin value units are interpreted differently depending on which units you choose. If you choose points, inches, millimeters, or centimeters, then masks are created using the margin distance as calculated in page space (you can think of the margin as a distance measured on the paper). The reference scale parameter value is accounted for in this calculation.", "If you choose any other units for your margin, then masks are created using the margin distance as calculated in map space (you can think of the margin as a real-world distance measure on the earth). Also, in this case, the reference scale parameter value is not used as part of the calculation.", "Masks will be created if the margin distance is 0 or negative. A margin size of 0 will create a polygon that represents the exact shape of the symbolized feature. A negative margin will result in a polygon smaller than the symbolized feature. Generally, a margin value larger than 0 will be specified to produce the desired masking effect.", "If the input layer is an annotation layer, the reference scale will be automatically set to the reference scale of the layer's feature class to ensure accurate calculation of the mask.", "When masking annotation projected on the fly, masks should be created using the map's spatial reference by properly setting it in the spatial reference parameter. Readability is preserved when text is projected on the fly, which is why there may be differences in the spatial area that text occupies in different projections.", "Masks of annotation features are font specific. When using masks with text, it is important to ensure that the same font is used onscreen as in the output. To do this, choose to embed fonts in vector output or download SoftFonts to printers or plotters."], "parameters": [{"name": "input_layer", "isInputFile": true, "isOptional": false, "description": "The symbolized input layer from which the masks will be created. ", "dataType": "Layer"}, {"name": "output_fc", "isOutputFile": true, "isOptional": false, "description": "The feature class that will contain the mask features. ", "dataType": "Feature Class"}, {"name": "reference_scale", "isOptional": false, "description": "The reference scale used for calculating the masking geometry when masks are specified in page units. This is typically the reference scale of the map. ", "dataType": "Double"}, {"name": "spatial_reference", "isOptional": false, "description": "The spatial reference for which the masking polygons will be created. This is not the spatial reference that will be assigned to the output feature class. It is the spatial reference of the map in which the masking polygons will be used since the position of symbology may change when features are projected. ", "dataType": "Spatial Reference"}, {"name": "margin", "isOptional": false, "description": "The space in page units surrounding the symbolized input features used to create the mask polygon. Typically, masking polygons are created with a small margin around the symbol to improve visual appearance. Margin values are specified in either page units or map units. Most of the time you will want to specify your margin distance value in page units. Margin value units are interpreted differently depending on which units you choose. If you choose points, inches, millimeters, or centimeters, then masks are created using the margin distance as calculated in page space (you can think of the margin as a distance measured on the paper). The reference scale parameter value is accounted for in this calculation. If you choose any other units for your margin, then masks are created using the margin distance as calculated in map space (you can think of the margin as a real-world distance measure on the earth). Also, in this case, the reference scale parameter value is not used as part of the calculation. ", "dataType": "Linear unit"}, {"name": "method", "isOptional": false, "description": "The type of masking geometry created. There are four types: BOX \u2014 A polygon representing the extent of the symbolized feature. CONVEX_HULL \u2014 The convex hull of the symbolized geometry of the feature. This is the default. EXACT_SIMPLIFIED \u2014 A generalized polygon representing the exact shape of the symbolized feature. Polygons created with this method will have a significantly smaller number of vertices compared to polygons created with the EXACT method. EXACT \u2014 A polygon representing the exact shape of the symbolized feature. ", "dataType": "String"}, {"name": "mask_for_non_placed_anno", "isOptional": false, "description": "Specifies whether to create masks for unplaced annotation. This option is only used when masking geodatabase annotation layers. ALL_FEATURES \u2014 Creates masks for all the annotation features. ONLY_PLACED \u2014 Only creates masks for features with a status of placed.", "dataType": "String"}, {"name": "attributes", "isOptional": true, "description": "Determines which attributes will be transferred from the input features to the output features. ONLY_FID \u2014 Only the FID field from the input features will be transferred to the output features. This is the default. NO_FID \u2014 All the attributes except the FID from the input features will be transferred to the output features. ALL \u2014 All the attributes from the input features will be transferred to the output features.", "dataType": "String"}]},
{"syntax": "CulDeSacMasks_cartography (input_layer, output_fc, reference_scale, spatial_reference, margin, {attributes})", "name": "Cul-De-Sac Masks (Cartography)", "description": "Creates a feature class of polygon masks from a symbolized input line layer. ", "example": {"title": "CulDeSacMasks tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the CulDeSacMasks tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CulDeSacMasks_cartography ( \"C:/data/cartography.gdb/transportation/roads\" , \"C:/data/cartography.gdb/transportation/cds_polys\" , \"25000\" , \"\" , \"5 meters\" , \"ALL\" )"}, "usage": ["This tool only creates masks at the unconnected ends of lines (also called  culs-de-sac). A line end is considered connected if it shares its endpoint with the endpoint of another line. This tool only accepts line layers as input.", "This tool is specifically designed to provide line end masks in cases in which lines have been symbolized using a rounded end cap to smooth the transition of line connections. Rather than having lines end with a rounded end cap, the end of the line can be masked using the polygon feature class created with this tool.", "Masks will be created if the margin distance is 0 or negative. A margin size of 0 will create a polygon that represents the exact shape of the symbolized feature. A negative margin will result in a polygon smaller than the symbolized feature. Generally, a margin value larger than 0 will be specified to produce the desired masking effect.", "If the input line layer contains multipart line geometries, then cul-de-sac masks are created for all unconnected line ends, including the ends of parts within multipart lines.", "When creating masks, it is important to know that adding masks to maps adds complexity that will slow map drawing and affect map printing and exporting. Generally, there are three things to consider when creating masks for a map: (1) the number of masks, (2) the complexity of the masks, and (3) whether the masks will be used to mask polygon features filled with marker or line symbols. An increase in the number of masks, having more complex masks, and/or masking against marker or polygon fill symbols will result in slower drawing on your screen. In addition, printing and exporting performance can be poor and even fail to produce valid output, first, because of the large amount of processing required to print and export maps with masks, and second, because of known limitations in how graphic file formats store map export results that have many complicated masks.", "Margin values are specified in either page units or map units. Most of the time you will want to specify your margin distance value in page units.", "Margin value units are interpreted differently depending on which units you choose. If you choose points, inches, millimeters, or centimeters, then masks are created using the margin distance as calculated in page space (you can think of the margin as a distance measured on the paper). The reference scale parameter value is accounted for in this calculation.", "If you choose any other units for your margin, then masks are created using the margin distance as calculated in map space (you can think of the margin as a real-world distance measure on the earth). Also, in this case, the reference scale parameter value is not used as part of the calculation."], "parameters": [{"name": "input_layer", "isInputFile": true, "isOptional": false, "description": "Input line layer from which to create masks. ", "dataType": "Layer"}, {"name": "output_fc", "isOutputFile": true, "isOptional": false, "description": "The feature class that will contain the mask features. ", "dataType": "Feature Class"}, {"name": "reference_scale", "isOptional": false, "description": "The reference scale used for calculating the masking geometry when masks are specified in page units. This is typically the reference scale of the map. ", "dataType": "Double"}, {"name": "spatial_reference", "isOptional": false, "description": "The spatial reference for which the masking polygons will be created. This is not the spatial reference that will be assigned to the output feature class. It is the spatial reference of the map in which the masking polygons will be used since the position of symbology may change when features are projected. ", "dataType": "Spatial Reference"}, {"name": "margin", "isOptional": false, "description": "The space in page units surrounding the symbolized input features used to create the mask polygon. Typically, masking polygons are created with a small margin around the symbol to improve visual appearance. Margin values are specified in either page units or map units. Most of the time you will want to specify your margin distance value in page units. Margin value units are interpreted differently depending on which units you choose. If you choose points, inches, millimeters, or centimeters, then masks are created using the margin distance as calculated in page space (you can think of the margin as a distance measured on the paper). The reference scale parameter value is accounted for in this calculation. If you choose any other units for your margin, then masks are created using the margin distance as calculated in map space (you can think of the margin as a real-world distance measure on the earth). Also, in this case, the reference scale parameter value is not used as part of the calculation. ", "dataType": "Linear unit"}, {"name": "attributes", "isOptional": true, "description": "Determines which attributes will be transferred from the input features to the output features. ONLY_FID \u2014 Only the FID field from the input features will be transferred to the output features. This is the default. NO_FID \u2014 All the attributes except the FID from the input features will be transferred to the output features. ALL \u2014 All the attributes from the input features will be transferred to the output features. ", "dataType": "String"}]},
{"syntax": "MakeGridsAndGraticulesLayer_cartography (in_template, in_aoi, input_feature_dataset, output_layer, {name}, {refscale}, {rotation}, {mask_size}, {xy_tolerance}, {primary_coordinate_system}, {configure_layout}, {ancillary_coordinate_system_1}, {ancillary_coordinate_system_2}, {ancillary_coordinate_system_3}, {ancillary_coordinate_system_4})", "name": "Make Grids And Graticules Layer (Cartography)", "description": " Creates a grouped layer of feature classes depicting grid, graticule, and border features using predefined cartographic specifications. Grid layers are ideal for advanced grid definitions which are scale and extent specific. Each grid layer can be composed of a mask polygon, clip polygon, segments (line), gridlines (line), ticks (line), endpoints (point), points (point), and annotation feature classes. These components are stored as features in corresponding feature classes. These features classes are saved within a specified feature dataset in a geodatabase. There are seven feature classes that store the basic components of a grid. Each feature class is named with a three-letter prefix that helps identify the grid component the feature class holds.    These feature classes/components are: These feature classes can hold information for multiple grids. An eighth feature class (GRD_)  contains organizational information, such as map name and grid types that are used to organize your grids. The grid definition template is stored in a predefined XML file. This file stores specification properties for each grid, such as the number, color, and line weight of gridlines. When the definition template is applied, features are created according to the specification based on the current extent or extent of a selected feature (area of interest), scale, and coordinate systems.", "example": {"title": "MakeGridsAndGraticulesLayer tool example (stand-alone Python script)", "description": "This stand-alone script shows an example of using the MakeGridsAndGraticulesLayer tool to make a cartographic grid.", "code": "# Name: MakeGridsAndGraticulesLayer.py # Description: Create grid for a selected area of interest in a file # geodatabase # Import system module import arcpy # Enabling logging of the results arcpy.logHistory = True # Enable background geoprocessing  # arcpy.Command(\"force_run_in_background yes\")  #uncomment this to run the # GP tool in the background # Set overwrite outputs of geoprocessing operations to true arcpy.gp.overwriteOutput = 1 # Create a feature layer from the input area of interest feature class # to pass to the Selection geoprocess below  arcpy.MakeFeatureLayer_management ( \"C:/Base_Data/AOIs.gdb/QUAD_24K\" , \"QUAD_24K\" ) # Select a feature from the area of interest feature layer based on a # definition query (this query, formatted for a file geodatabase, has # been adjusted for use in python, in particular the use of \\ in # conjunction with the double quotes surrounding the field name in the # definition query) arcpy.SelectLayerByAttribute_management ( \"QUAD_24K\" , \"NEW_SELECTION\" , \" \\\" MSNAME \\\"  = 'Swift Minnesota'\" ) # Set the values of the tool's parameters using one of the grid # definition XML files located under the GridTemplates directory t = \"C:/Program Files/ArcGIS/Desktop10.1/GridTemplates/Quad_24K_NAD83.xml\" in_aoi = \"QUAD_24K\" input_feature_dataset = \"C:/Python_Output/grid_layers.gdb/QUAD_24K\" output_layer = \"QUAD_24K_NAD83_Grid\" name = \"MSNAME\" # Process: Make Grids and Graticules Layer (the '#' indicates use # default values) arcpy.MakeGridsAndGraticulesLayer_cartography ( t , in_aoi , input_feature_dataset , output_layer , name , \"#\" , \"#\" , \"#\" , \"#\" , \"#\" )"}, "usage": ["This tool is designed for projected maps that will be printed or exported. It is not meant for creating grids or graticules that dynamically update as the user navigates the map.", "You must input an area of interest to determine the extent of the grid layer to be created. If you are using a feature layer to define the area of interest, it must be a polygon feature class or polygon layer.", "In the ", "Make Grids and Graticules Layer", " dialog box, once the ", "Grid Template (XML file)", " parameter is defined, hover the pointer over or click the context-sensitive area immediately to the left of the parameter name to display grid type, description, rotation type, and scale type information for the defined XML grid definition template.", "The grid template XML file specifies grid components depicting measurements or locations for  primary and ancillary coordinate systems. All coordinate systems specified must share a common geographic coordinate system. If you want to change the primary coordinate system to one that uses a different datum than the default, say for example, you change the coordinate system from one that uses WGS 1984 to one that uses NAD 1983, you must change each default ancillary coordinate system to NAD 1983 as well.", "The grid template, area of interest, input feature dataset, and the primary and ancillary coordinate system parameters must use the same datum, for example, WGS 1984 or NAD 1983.", "The grid template XML file creates grid components for up to four ancillary coordinate systems. The number of  ancillary grids is specified by the XML file. You cannot add or delete ancillary coordinate systems, but you can override the default values. ", "The ", "Primary Coordinate System", " must be a projected coordinate system.", "Ancillary coordinate systems can be either a projected coordinate system or a geographic coordinate system.", "The spatial reference of the ", "Input Feature Dataset", " should have the same geographic coordinate system as specified by the grid template XML file.", "The spatial reference of the ", "Input Area of  Interest", " should have the same geographic coordinate system as specified by the grid template XML file.", "If you are accessing the tool from ArcMap, checking the ", "Configure data frame and layout using grid settings", " check box ensures that the data frame on the layout is adjusted to best fit the created grid. The data frame's coordinate system, scale, rotation, size, extent, and clipping may be altered to match the XML grid specification. This setting is only available when the tool is executed from ArcMap's layout view and is not being run in the background. The default is to have this check box unchecked, which will not change any of your data frame properties.", "When specifying a grid name that already exists in the output location, the existing grid will be overwritten.", "The following parameter values are automatically derived from the grid template XML:", "However, all of these default values can be overridden with new values."], "parameters": [{"name": "in_template", "isInputFile": true, "isOptional": false, "description": " The XML grid definition template stores the specification's graphic properties for each grid layer. In addition to the graphic properties, which cannot be altered before execution, the definition has specific default values, exposed as parameters, that can be modified before execution. Template files are located in the \\ArcGIS\\Desktop10.1\\GridTemplates directory. Additional grid templates can be obtained and shared using the ArcGIS Resource Center. The Esri Production Mapping extension provides a grid designer that allows you to create new templates as well as modify existing ones. ", "dataType": "File"}, {"name": "in_aoi", "isInputFile": true, "isOptional": false, "description": "The feature layer or x,y extent used to determine the extent of the grid layer created. The only extent options valid for this tool are Default, As Specified Below, or Same as Display. Selecting any other option will result in an error. Feature layer\u2014Indicates you can choose the layer to use for the area of interest. Only one selected feature will be used from this layer. For layers that have more than one feature, the tool will only create a grid layer based on the first feature. The first feature is based on object id. All remaining features are ignored. Extent\u2014Indicates you can use one of the following as the area of interest: The default area of interest of the data frame. As Specified Below\u2014When you specify an extent by directly adding coordinates, a spatial reference for those coordinates is derived from the following, in order: (1) ArcMap's focused data frame (2) if ArcMap is not active, the Cartographic Coordinate System environment setting. The same AOI as the display. ", "dataType": "Feature Layer; Extent"}, {"name": "input_feature_dataset", "isInputFile": true, "isOptional": false, "description": " The feature dataset that will store the features. Grid-specific feature classes will be created if they do not already exist. If they already exist, and a grid with the same name and type as the one being created also exists, it will be overwritten. A grid with the same name and type as the one created will always be overwritten, regardless of the geoprocessing overwrite outputs setting. ", "dataType": "Feature Dataset"}, {"name": "output_layer", "isOutputFile": true, "isOptional": false, "description": "The grouped layer of feature classes depicting grid, graticule, and border features. Each grid layer can be composed of a mask polygon, a clip polygon, segments(line), gridlines(line), ticks(line), endpoints(point), points(point), and annotation feature classes. This is a temporary layer that you must save in the ArcMap document or as a layer file. ", "dataType": "Group Layer"}, {"name": "name", "isOptional": true, "description": " The name for the cartographic grid created that allows for distinction between grids that are stored in the same feature dataset and set of feature classes, expressed in one of the following formats: String\u2014Enter a text string for the grid name. Field\u2014Enter a field from the feature layer. The value of the field for the selected feature is used to name the grid. An area of interest field name can be used when the Input Area of Interest parameter is defined as Feature Layer.", "dataType": "String; Field"}, {"name": "refscale", "isOptional": true, "description": "The scale at which the grid is created and should be viewed. When the reference scale from the XML grid definition file is defined as Use Environment, the reference scale is derived in the following order: The geoprocessing Reference Scale environment setting The active data frame's reference scale The active data frame's scale The value from the XML grid definition file", "dataType": "Double"}, {"name": "rotation", "isOptional": true, "description": "The rotation angle for the grid components. Rotation is used to provide annotation that is level with the page. Unless otherwise specified, rotation is calculated using the area of interest feature. When the rotation type from the XML grid definition file is defined as Use Environment, the rotation is derived in the following order: The active data frame's rotation The value from the XML grid definition file", "dataType": "Double"}, {"name": "mask_size", "isOptional": true, "description": " The mask is a polygon feature that forms an outer ring around the extent of the neatline and is used to mask data that falls in the area reserved for coordinate labels. Mask size defines the width of the polygon mask feature in map or page units. The data frame may have to be resized to fit around the edge of the mask while including the coordinate labels. ", "dataType": "Linear unit"}, {"name": "xy_tolerance", "isOptional": true, "description": "The minimum tolerated distance between geodatabase features, expressed in linear units. This value is defaulted from the value set in the XML. You can set the value higher for data with less coordinate accuracy and lower for data with extremely high accuracy. Features that fall within the set XY tolerance will be considered coincident. ", "dataType": "Linear unit"}, {"name": "primary_coordinate_system", "isOptional": true, "description": "The grid template XML file creates grid components depicting coordinates or locations for a primary coordinate system and up to four ancillary coordinate systems. The number of ancillary grids is specified by the file. You cannot add or delete ancillary coordinate systems. All coordinate systems specified must share a common geographic coordinate system. If you want to change the primary coordinate system to one that uses a different datum than the default, say for example, you change the coordinate system from one that uses WGS 1984 to one that uses NAD 1983, you must change each default ancillary coordinate system to NAD 1983 as well. This is the primary coordinate system for the grid layer being created. Typically, it will be the coordinate system of the final product or data frame. This coordinate system must be a projected coordinate system. When the Primary Coordinate System in the XML grid definition file is defined as Use Environment, the Primary Coordinate System is derived in the following order: In all cases user input takes the highest precedence. The geoprocessing Cartographic Coordinate System environment setting The active data frame's coordinate system, if it is a projected coordinate system The Fixed value from the XML grid definition file ", "dataType": "Spatial Reference"}, {"name": "configure_layout", "isOptional": true, "description": "Adjusts the data frame settings to ensure they match the grid layer. The data frame's coordinate system, scale, rotation, size, extent, and clipping may be altered to enforce consistency. This setting is only available when the tool is executed from ArcMap's layout view and is not being run in the background. The default is to have this check box unchecked. CONFIGURELAYOUT \u2014 Indicates data frame and layout are configured using grid settings. NO_CONFIGURELAYOUT \u2014 Indicates data frame and layout are not configured. This is the default.", "dataType": "Boolean"}, {"name": "ancillary_coordinate_system_1", "isOptional": true, "description": "The grid template XML file creates grid components depicting coordinates or locations for a primary coordinate system and up to four ancillary coordinate systems. The number of ancillary grids is specified by the file. You cannot add or delete ancillary coordinate systems. All coordinate systems specified must share a common geographic coordinate system. If you want to change the primary coordinate system to one that uses a different datum than the default, say for example, you change the coordinate system from one that uses WGS 1984 to one that uses NAD 1983, you must change each default ancillary coordinate system to NAD 1983 as well. This is the first ancillary coordinate system. ", "dataType": "Spatial Reference"}, {"name": "ancillary_coordinate_system_2", "isOptional": true, "description": "The grid template XML file creates grid components depicting coordinates or locations for a primary coordinate system and up to four ancillary coordinate systems. The number of ancillary grids is specified by the file. You cannot add or delete ancillary coordinate systems. All coordinate systems specified must share a common geographic coordinate system. If you want to change the primary coordinate system to one that uses a different datum than the default, say for example, you change the coordinate system from one that uses WGS 1984 to one that uses NAD 1983, you must change each default ancillary coordinate system to NAD 1983 as well. This is the second ancillary coordinate system. ", "dataType": "Spatial Reference"}, {"name": "ancillary_coordinate_system_3", "isOptional": true, "description": "The grid template XML file creates grid components depicting coordinates or locations for a primary coordinate system and up to four ancillary coordinate systems. The number of ancillary grids is specified by the file. You cannot add or delete ancillary coordinate systems. All coordinate systems specified must share a common geographic coordinate system. If you want to change the primary coordinate system to one that uses a different datum than the default, say for example, you change the coordinate system from one that uses WGS 1984 to one that uses NAD 1983, you must change each default ancillary coordinate system to NAD 1983 as well. This is the third ancillary coordinate system. ", "dataType": "Spatial Reference"}, {"name": "ancillary_coordinate_system_4", "isOptional": true, "description": "The grid template XML file creates grid components depicting coordinates or locations for a primary coordinate system and up to four ancillary coordinate systems. The number of ancillary grids is specified by the file. You cannot add or delete ancillary coordinate systems. All coordinate systems specified must share a common geographic coordinate system. If you want to change the primary coordinate system to one that uses a different datum than the default, say for example, you change the coordinate system from one that uses WGS 1984 to one that uses NAD 1983, you must change each default ancillary coordinate system to NAD 1983 as well. This is the fourth ancillary coordinate system. ", "dataType": "Spatial Reference"}]},
{"syntax": "DeleteGridsAndGraticules_cartography (in_grid_dataset, grid_name)", "name": "Delete Grids And Graticules (Cartography)", "description": " Deletes all the features associated with one or more selected grid and graticule layers from a feature dataset.", "example": {"title": "DeleteGridsAndGraticules tool Example (Python Window)", "description": "Deletes grid and graticule layers from a feature dataset.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Python_Output\" arcpy.DeleteGridsAndGraticules_cartography ( \"C:/Python_Output/grid_layers.gdb/QUAD_24K\" , \"Plaza Washington (Quad_24K_NAD83)\" )"}, "usage": [" You can select all the grid and graticule layers for deletion or specify only the ones you want deleted from the ", "Grid Name", " list.", " You can unselect all previously selected grid and graticule layers to prevent deletion of any grid and graticule layers or unselect only specific ones not to delete.", "A grid dataset that has been renamed cannot be used as an input dataset to this tool because the suffix of the grid feature classes will no longer match the name of the grid."], "parameters": [{"name": "in_grid_dataset", "isInputFile": true, "isOptional": false, "description": " The feature dataset location where the grid and graticule layers that can be deleted are stored. ", "dataType": "Feature Dataset"}, {"name": "grid_name", "isOptional": false, "description": " Lists the grid and graticule layers in the feature dataset that can be deleted. ", "dataType": "String"}]},
{"syntax": "ResolveRoadConflicts_cartography (in_layers, hierarchy_field, {out_displacement_features})", "name": "Resolve Road Conflicts (Cartography)", "description": " Resolves graphic conflicts among symbolized road features by adjusting portions of line segments.  \r\n Learn more about how Resolve Road Conflicts works \r\n", "example": {"title": "ResolveRoadConflicts tool example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ResolveRoadConflicts tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" env.referenceScale = \"50000\" arcpy.ResolveRoadConflicts_cartography ( \"C:/data/roads.lyr;C:/data/streets.lyr;C:/data/highways.lyr\" , \"hierarchy\" , \"C:/data/cartography.gdb/transportation/displace\" )"}, "usage": ["This tool is typically used when producing relatively large-scale products where it is preferable to display divided roads with multiple lanes that are visually distinct. At smaller scales, you may want to use the ", "Merge Divided Roads", " tool to display a single representative line for these features instead. If your workflow includes running both tools on the same collection of roads, it is advisable to merge roads prior to resolving road conflicts. ", " The ", "Hierarchy Field", " parameter is used to specify the hierarchical importance of each road class. Lower integers specify more significant roads, with hierarchy  equal to 1 for the most important roads. Movement will be minimized for the most important roads; lower hierarchy roads generally will be moved to accommodate higher hierarchy roads. The hierarchy field must be present and named the same for all input feature classes.", " This tool operates by assessing graphic conflicts of symbolized features. The symbology extent and the reference scale are considered in conjunction with one another. Run this tool only after you have finalized the appearance of your symbols and ensure that the reference scale corresponds to the final intended output scale.", "You can lock features from displacement  by calculating the ", "Hierarchy Field", " value equal to 0 (zero). This is useful when a road should not be moved because of its a  spatial relationship with other map features, especially continuous data like elevation, and should not be moved. ", "Processing large road datasets or a number of datasets together may exceed memory limitations. In this case, consider processing input data by partition by identifying a relevant  polygon feature class in the ", "Cartographic Partitions", " environment setting. Portions of the data, defined by partition boundaries, will be processed sequentially. The resulting feature classes will be seamless and consistent at partition edges. See ", "How Resolve Road Conflicts works", "  for more information about running this tool with partitioning.  ", "The optional ", "Output Displacement Feature Class", " parameter creates a feature class of  polygons that indicates the amount and direction of displacement that took place. This feature class can be used for visual inspection, for spatial querying, or as an input to the ", "Propagate Displacement", " tool. "], "parameters": [{"name": "in_layers", "isInputFile": true, "isOptional": false, "description": " The input feature layers containing symbolized road features that may be in conflict. ", "dataType": "Layer"}, {"name": "hierarchy_field", "isOptional": false, "description": " The field that contains hierarchical ranking of feature importance, where 1 is very important and larger integers reflect decreasing importance. A value of 0 (zero) locks the feature to ensure that it is not moved. The hierarchy field must be present and named the same for all input feature classes. ", "dataType": "String"}, {"name": "out_displacement_features", "isOutputFile": true, "isOptional": true, "description": " The output polygon features containing the degree and direction of road displacement, to be used by the Propagate Displacement tool to preserve spatial relationships. ", "dataType": "Feature Class"}]},
{"syntax": "ResolveBuildingConflicts_cartography (in_buildings, invisibility_field, in_barriers, building_gap, minimum_size, {hierarchy_field})", "name": "Resolve Building Conflicts (Cartography)", "description": " Resolves symbol conflicts among buildings and with respect to linear barrier features by moving or hiding buildings.  Learn more about how Resolve Building Conflicts works .", "example": {"title": "ResolveBuildingConflicts tool example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ResolveBuildingConflicts tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" env.referenceScale = \"50000\" arcpy.ResolveBuildingConflicts_cartography ( \"C:/data/footprints.lyr; C:/data/bldg_points.lyr\" , \"invisible\" , \"'C:/data/roads.lyr' 'true' '5 Meters';'C:/data/trails.lyr' 'false' '10 Meters';'C:/data/streams.lyr' 'false' '5 Meters'\" , \"10 meters\" , \"15 meters\" , \"bldg_hierarchy\" )"}, "usage": [" This tool operates by assessing graphic conflicts of symbolized features. The symbology extent and the reference scale are considered in conjunction with one another. Run this tool only after you have finalized the appearance of your symbols and ensure that the reference scale corresponds to the final intended output scale.", "The ", "Invisibility Field", " must be present and named the same for all input feature classes. Features that should remain visible are assigned a value of 0; those that should be removed from the display are assigned a value of 1. Use a layer definition query or a selection  to display the resulting simplified collection (that is, ", "invisibility <> 1", "). You can use multiple invisibility fields to store different results\u2014corresponding to different output scales\u2014on the same feature class.", "Before conflicts are assessed, polygonal buildings are enlarged to a minimum size specified by the ", "Minimum Allowable Building Size", " parameter. Minimum size is measured as a linear distance along the shortest side of a rotated bounding box best fit to the feature. To review the final size of buildings in the output results, add a double or float  field to any of the input building feature classes called RBC_SIZE. As the tool processes, this field will be updated with the shortest side of  a rotated bounding box around each feature.  ", "The ", "Hierarchy Field", " parameter is optional. If it is not specified, buildings will be assigned a relative importance based on the perimeter of the building and proximity to barriers. Larger buildings closer to more than one barrier will be assessed as more significant than smaller buildings relatively far from a barrier. You can use a partially populated Hierarchy field where only the significant buildings are attributed with a hierarchical value and all other features (with a NULL hierarchy value) will have their relative significance internally calculated.", "If the ", "Hierarchy Field", " parameter is used, buildings can be forced to stay visible by assigning them a hierarchy value of 0 (zero). They will not be masked by the tool. A building with hierarchy  of zero is considered locally important, so nearby buildings may be compromised more than they would if that building was not forced to stay visible. Hierarchy zero buildings may still be transformed (moved, rotated, or resized) in order to resolve conflicts and match other required parameters.", "Any buildings that are geometrically in conflict with barriers (that is, the actual geometry\u2014not just symbology\u2014of the building overlaps that of a barrier feature such as a road) will not be moved off of the barrier feature. These buildings may be flagged for masking in the course of conflict resolution processing, but they won't be moved. A conflict will remain.  ", "If the symbology of the barrier features is not symmetrically distributed across the geometry\u2014that is, the symbol is thicker on one side of the line than the other\u2014a larger building-to-barrier gap may appear on the side of the barrier with the thinner symbology. ", "Processing large datasets or a number of barrier layers together may exceed memory limitations. In this case, consider processing input building data by partition by identifying a relevant  polygon feature class in the ", "Partition Features", " environment setting. Portions of the building data, defined by partition boundaries, will be processed sequentially. The resulting layers will be seamless and consistent at partition edges. See ", "How_Resolve_Building_Conflicts_works", "  for more information about running this tool with partitioning.  "], "parameters": [{"name": "in_buildings", "isInputFile": true, "isOptional": false, "description": " The input layers containing building features that may be in conflict, or smaller than allowable size. Buildings can be either points or polygons. Buildings will be modified to resolve conflicts with other buildings and with barrier features. ", "dataType": "Layer"}, {"name": "invisibility_field", "isOptional": false, "description": " The field that stores the invisibility values that can be used to remove some buildings from display in order to resolve symbol conflicts. Buildings with a value of 1 should be removed from display; those with a value of zero should remain. Use a definition query on the layer to display visible buildings only. No features are deleted. ", "dataType": "String"}, {"name": "in_barriers", "isInputFile": true, "isOptional": false, "description": " The layers containing the linear or polygon features that are conflict barriers for input building features. Buildings will be modified to resolve conflicts between buildings and barriers. Orient value is Boolean, specifying whether buildings should be oriented to the barrier layer. Gap specifies the distance that buildings should move toward or away from the barrier layer. A unit must be entered with the value. If no unit is entered with the Gap value (that is, 10 instead of 10 meters ), the linear unit from the input feature's coordinate system will be used. A gap value of 0 (zero) will snap buildings directly to the edge of barrier line or outline symbology. A null (unspecified) gap value will mean that buildings will not be moved toward or away from barrier lines or outlines except for movement required by conflict resolution.", "dataType": "Value Table"}, {"name": "building_gap", "isOptional": false, "description": " The minimum allowable distance between symbolized buildings at scale. Buildings that are closer together will be displaced or hidden to enforce this distance. The minimum allowable distance is set relative to the reference scale (that is, 15 meters at 1:50,000 scale). The value is 0 if reference scale is not set. ", "dataType": "Linear unit"}, {"name": "minimum_size", "isOptional": false, "description": " The minimum allowable size of the shortest side of a rotated best-fit bounding box around the symbolized building feature drawn at the reference scale. Buildings with a bounding box side smaller than this value will be enlarged to meet it. Resizing may happen nonproportionally resulting in a change to building morphology. ", "dataType": "Linear unit"}, {"name": "hierarchy_field", "isOptional": true, "description": " The field that contains hierarchical ranking of feature importance, where 1 is very important and larger integers reflect decreasing importance. A value of 0 (zero) forced the building to retain visibility, although it may be moved somewhat to resolve conflicts. If this parameter is not used, feature importance will be assessed by the tool based on perimeter length and proximity to barrier features. ", "dataType": "String"}]},
{"syntax": "PropagateDisplacement_cartography (in_features, displacement_features, adjustment_style)", "name": "Propagate Displacement (Cartography)", "description": "Propagates the displacement resulting from road adjustment in the  Resolve Road Conflicts   and  Merge Divided Roads  tools to adjacent features to reestablish spatial relationships.  An optional output of both the  Resolve Road Conflicts   and  Merge Divided Roads  tools is a displacement feature class. Displacement features store the amount and direction of change from the initial state of the data before these tools are run. Displacement information can then be  applied to nearby features from different themes to ensure that spatial relationships are retained using this tool. For example, if roadways are separated by the  Resolve Road Conflicts  tool, it is often necessary to shift adjacent buildings along the roads accordingly.", "example": {"title": "PropagateDisplacement tool example 1 (Python window)", "description": "The following Python window script demonstrates how to use the PropagateDisplacement tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.PropagateDisplacement_cartography ( \"footprints.lyr\" , \"displacement.lyr\" , \"AUTO\" )"}, "usage": ["The location of input features are adjusted based on the vector displacements contained in the displacement features. Adjustments are a compromise of all displacements, such that large displacements that occurred near an input feature will have more influence than smaller displacements further away. Conceptually, this action is similar to a rubber-sheeting process that moves features in various directions by various amounts to fit them back to the spatial relationship that they originally had with the roads.", " This tool does not resolve graphic conflicts and in fact may introduce new conflicts. Topological errors that are introduced can be inspected using the ", "Detect Graphic Conflict", " tool. If you are using this tool to propagate  displacement  to building features, consider running the ", "Resolve Building Conflicts", " tool after propagation. ", " This tool operates by assessing graphic conflicts of symbolized features. The symbology extent and the reference scale are considered in conjunction with one another. Run this tool only after you have finalized the appearance of your symbols and ensure that the reference scale corresponds to the final intended output scale.", "This tool operates on the displacement output generated by the ", "Resolve Road Conflicts", "  and ", "Merge Divided Roads", " tools, which can be enabled to run by partitioning (using the ", "Cartographic Partitions", " geoprocessing environment variable) when large datasets are processed. In this case, a single seamless displacement feature class will generated.  Although this may be a very large feature class when generated with partitions, the information held within is not complex. This tool can manage the displacement polygons as inputs without using partitioning. See ", "Generalizing large datasets using partitions", " for more information about processing large datasets."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input feature layer containing features that may be in conflict. May be point, line, or polygon. ", "dataType": "Feature Layer"}, {"name": "displacement_features", "isOptional": false, "description": " The displacement polygon features created by the Resolve Road Conflicts or the Merge Divided Roads tools which contain the degree and direction of road displacement that took place. These polygons dictate the amount of displacement that will be propagated to the input features. ", "dataType": "Feature Layer"}, {"name": "adjustment_style", "isOptional": false, "description": "Defines the type of adjustment that will be used when displacing input features. AUTO \u2014 The tool will decide for each input feature whether a SOLID or an ELASTIC adjustment is most appropriate. In general, features with orthogonal shapes will have SOLID adjustment applied, while organically shaped features will have ELASTIC adjustment applied. This is the default. SOLID \u2014 The feature will be translated. All vertices will move the same distance and direction. Topological errors may be introduced. This option is most useful when input features have regular geometric shapes. ELASTIC \u2014 The vertices of the feature may be moved independently to best fit the feature to the road network. The shape of the feature may be modified slightly. Topological errors are less likely to be introduced. This option only applies to line and polygon input features. This option is most useful for organically shaped input features.", "dataType": "String"}]},
{"syntax": "DetectGraphicConflict_cartography (in_features, conflict_features, out_feature_class, {conflict_distance}, {line_connection_allowance})", "name": "Detect Graphic Conflict (Cartography)", "description": "Creates polygons where two or more symbolized features graphically conflict.", "example": {"title": "DetectGraphicConflict tool example1 (Python window)", "description": "The following Python window script demonstrates how to use the DetectGraphicConflict tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/cartography.gdb/buildings\" env.referenceScale = \"50000\" arcpy.DetectGraphicConflict_cartography ( \"footprints.lyr\" , \"roads.lyr\" , \"C:/data/cartography.gdb/buildings/dgc_polys\" , \"25 meters\" , \"0 meters\" )"}, "usage": ["The tool assesses conflicts among symbols, not geometry. The input layer and the conflict layer can be the same. ", "Input layers can be symbolized with standard symbols or representation symbology. Input layers can include feature class annotation, including symbol substitution. Shapefile, coverage, and CAD layers are also acceptable inputs. ", "The following inputs are not accepted by the tool: CAD, coverage, or VPF annotation, dimensions, charts, dot-density or proportional symbols, raster layers, Network Analyst layers, or 3D symbols.  ", "The output feature class stores polygons, each representing an area of graphic conflict between a symbolized input feature and a symbolized conflict feature. The feature IDs associated with the two conflicting features are stored with the conflict polygon in the ", "FID_<input_layer_name>", " and ", "FID_<conflict_layer_name>", " fields. In the event that the conflict layer is the same as the input layer, the second field will be named  ", "FID_<input_layer_name>_1", ". If no graphic conflicts are found, the output feature class will be empty.", "Use the ", "Conflict Distance", " parameter to detect areas where input and conflict symbology is closer than a certain distance. Temporary buffers one-half the size of the conflict distance value are created around symbols. Conflict polygons will be generated anywhere these buffers overlap. When the conflict distance is zero, conflicts are detected anywhere that  symbology itself actually overlaps; this is the default. The conflict calculation is based on a reference scale. If you access this tool from ArcMap, the reference scale of the data frame containing the input layers will be used, unless the Reference Scale environment setting has been set. This environment setting must be specified to execute this tool from ArcCatalog.", "Use the ", "Line Connection Allowance", " parameter to disregard symbol overlaps where line ends meet. This is useful if you use line symbol end caps to ensure that lines connect visually, but you don't want each instance detected as a conflict. The line connection allowance is in page units, related to the reference scale. It is equal to the radius of a circle, centered where lines join, within which graphic overlaps won't be detected; the default value is 1 point. Use a value that is at least one-half the line width of  line symbols to disregard these connections. A value of zero means no allowance and will detect a conflict at each line join in this case. This parameter is only considered when the input layer and the conflict layer are identical.", " This tool operates by assessing graphic conflicts of symbolized features. The symbology extent and the reference scale are considered in conjunction with one another. Run this tool only after you have finalized the appearance of your symbols and ensure that the reference scale corresponds to the final intended output scale."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing symbolized features. CAD, coverage, or VPF annotation, dimensions, charts, dot-density or proportional symbols, raster layers, Network Analyst layers, or 3D symbols are not acceptable inputs. ", "dataType": "Layer"}, {"name": "conflict_features", "isOptional": false, "description": "The feature layer containing symbolized features potentially in conflict with symbolized features in the input layer. ", "dataType": "Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to be created to store conflict polygons. It cannot be one of the feature classes associated with the input layers. ", "dataType": "Feature Class"}, {"name": "conflict_distance", "isOptional": true, "description": "Sets the conflict distance. Temporary buffers one-half the size of the conflict distance value are created around symbols in both the input and conflict layers. Conflict polygons will be generated where these buffers overlap. Conflict distance is measured in page units (Points, Inches, Millimeters, Centimeters). If you enter a conflict distance in map units, it will be converted to page units using the reference scale. The default conflict distance is 0, where no buffers are created and only symbols that physically overlap one another are detected as conflicts. ", "dataType": "Linear unit"}, {"name": "line_connection_allowance", "isOptional": true, "description": "The radius of a circle, centered where lines join, within which graphic overlaps won't be detected. This parameter is only considered when the input layer and the conflict layer are identical. Zero allowance will detect a conflict at each line join (if end caps are overlapping). Line connection allowance is calculated in page units (Points, Inches, Millimeters, Centimeters). If you enter a conflict distance in map units, it will be converted to page units using the reference scale. The value cannot be negative; the default value is 1 Point. ", "dataType": "Linear unit"}]},
{"syntax": "ThinRoadNetwork_cartography (in_features, minimum_length, invisibility_field, hierarchy_field)", "name": "Thin Road Network (Cartography)", "description": " Generates a simplified road network that retains connectivity and general character for display at a smaller scale. This tool does not generate new output. It assigns values in  Invisibility Field  in the input feature classes to identify features that are extraneous and can be removed from view to result in a simplified, yet representative, collection of roads. No feature geometry is altered or deleted.  Features are not actually deleted by  Thin Road Network . To actually remove features, consider using the  Trim Line  tool. The resulting simplified road collection  is determined by feature significance, importance, and density. Segments that participate in very long itineraries across the extent of the data  are more significant than those required only for local travel. Road classification, or importance, is specified by the  Hierarchy Field  parameter. The density of the resulting street network is determined by the  Minimum Length  parameter, which corresponds to the shortest segment that is visually sensible to show at scale.   Learn more about  how Thin Road Network works  and see a table of recommended minimum length values to use as a starting point. ", "example": {"title": "ThinRoadNetwork tool example (Python Window)", "description": "The following Python window script demonstrates how to use the ThinRoadNetwork tool in immediate mode:", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/cartography.gdb/transportation\" arcpy.ThinRoadNetwork_cartography ( \"roads.lyr\" , \"1000 meters\" , \"invisible\" , \"level\" )"}, "usage": ["The invisibility field must be present and named the same for all input feature classes. Features that should remain visible are assigned a value of 0; those that should be removed from the display are assigned a value of 1. Use a layer definition query or a selection  to display the resulting simplified collection (for example, ", "invisibility <> 1", "). You can use multiple invisibility fields to store different results\u2014corresponding to different output scales\u2014on the same feature class.", "The hierarchy field identifies the relative importance of features to help establish which features are significant. Hierarchy value 1 indicates the most important features with  importance decreasing as hierarchy value increases. For optimal results, use no more than five levels of hierarchy. Input roads with Hierarchy = 0 are considered \"locked\" and will remain visible, along with adjacent roads necessary for connectivity. The hierarchy field must be present and named the same for all input feature classes.", "The hierarchy is typically derived from a field that specifies road classification and corresponds to the way that roads are symbolized. It is not related to the concept of hierarchy used in network analysis. Hierarchy values equal to NULL are not accepted by the tool and will raise an error.", "The ", "Minimum Distance", " parameter defines a sense of the resolution or granularity of the resulting simplified road collection. It should correspond to a length that is visually significant to include at the final scale. The results of this tool are a balanced compromise between the requirements posed by hierarchy, visibility locking, resolution, and morphology and connectivity of the road geometry. Therefore, the minimum distance value cannot necessarily be measured directly in the resulting feature set. ", "Processing large road datasets or a number of datasets together may exceed memory limitations. In this case, consider processing input data by partition by identifying a relevant  polygon feature class in the ", "Cartographic Partitions", " environment setting. Portions of the data, defined by partition boundaries, will be processed sequentially. The resulting feature class(es) will be seamless and consistent at partition edges. See ", "How Thin Road Network works", "  for more information about running this tool with partitioning.  ", "The integrity of the results of this tool relies on the topological  integrity of the inputs. Proper connections must exist at intersections to faithfully represent the connectivity of the road network. See ", "How Thin Road Network works", " for more information about data requirements and other helpful tips. ", "If the inputs include a feature layer pointing to a representation, any geometry overrides associated with that representation will be used as the input geometry to determine the morphology and connectivity of the road collection. Similarly, representation visibility overrides will be honored, ensuring that invisible representations are not included in the resulting road collection and that adjacent roads are handled accordingly to maintain connectivity. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input linear roads that should be thinned to create a simplified collection for display at smaller scales. ", "dataType": "Feature Layer"}, {"name": "minimum_length", "isOptional": false, "description": " An indication of the shortest road segment that is sensible to display at the output scale. This controls the resolution, or density, of the resulting road collection. If the units are in point, millimeters, centimeters, or inches, the value is considered in page units and the reference scale is taken into account. ", "dataType": "Linear unit"}, {"name": "invisibility_field", "isOptional": false, "description": " The field that stores the results of the tool. Features that participate in the resulting simplified road collection have a value of 0 (zero). Those that are extraneous have a value of 1. A layer definition query can be used to display the resulting road collection. This field must be present and named the same for each input feature class. ", "dataType": "String"}, {"name": "hierarchy_field", "isOptional": false, "description": " The field that contains hierarchical ranking of feature importance, where 1 is very important and larger integers reflect decreasing importance. A value of 0 forces the feature to remain visible in the output collection. This field must be present and named the same for each input feature class. Hierarchy values equal to NULL are not accepted and will raise an error. ", "dataType": "String"}]},
{"syntax": "SmoothPolygon_cartography (in_features, out_feature_class, algorithm, tolerance, {endpoint_option}, {error_option})", "name": "Smooth Polygon (Cartography)", "description": "Smooths sharp angles in polygon outlines to improve aesthetic or cartographic quality.", "example": {"title": "SmoothPolygon Example (Python Window)", "description": "The following Python window script demonstrates how to use the SmoothPolygon tool in immediate mode.", "code": "import arcpy from arcpy import env import arcpy.cartography as CA env.workspace = \"C:/data\" CA.SmoothPolygon ( \"soils.shp\" , \"C:/output/output.gdb/smoothed_soils\" , \"PAEK\" , 100 )"}, "usage": [" There are two smoothing methods to choose from:", "Smoothing may introduce topological errors such as polygon outline crossings. Use the ", "FLAG_ERRORS", " option in the ", "Handle Topological Errors", " parameter to identify these errors. Two fields will be added\u2014InPoly_FID and SmoPlyFlag\u2014to contain input feature IDs and topological errors. Values of 1 in the SmoPlyFlag field indicate a topology error; 0 (zero) indicates no error. The InPoly_FID field links the output polygons to their input polygons. The ", "FLAG_ERRORS", " option cannot be used within an edit session.", "Invalid (self-intersecting) geometry may be created during the smoothing process and will be repaired but not improved. For example, if a polygon self-crosses it will become a multipart polygon but will still appear self-crossing."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The polygon features to be smoothed. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output polygon feature class to be created. ", "dataType": "Feature Class"}, {"name": "algorithm", "isOptional": false, "description": "Specifies the smoothing algorithm. PAEK \u2014 Acronym for Polynomial Approximation with Exponential Kernel. It calculates a smoothed polygon that will not pass through the input polygon vertices. This is the default. BEZIER_INTERPOLATION \u2014 Fits Bezier curves between vertices. The resulting polygon passes through the vertices of input polygons. This algorithm does not require a tolerance. Bezier curves will be approximated in shapefile output.", "dataType": "String"}, {"name": "tolerance", "isOptional": false, "description": "Sets a tolerance used by the PAEK algorithm. A tolerance must be specified, and it must be greater than zero. You can specify a preferred unit; the default is the feature unit. You must enter a 0 as a placeholder when using the BEZIER_INTERPOLATION smoothing algorithm. ", "dataType": "Linear unit"}, {"name": "endpoint_option", "isOptional": true, "description": "Specifies whether or not to preserve the endpoints for isolated polygon rings. This option works with the PAEK algorithm only. FIXED_ENDPOINT \u2014 Preserves the endpoint of an isolated polygon ring. This is the default. NOT_FIXED \u2014 Smooths through the endpoint of an isolated polygon ring. ", "dataType": "Boolean"}, {"name": "error_option", "isOptional": true, "description": "Specifies how the topological errors (possibly introduced in the process, such as line crossing or overlapping) will be handled. NO_CHECK \u2014 Specifies not to check for topological errors. This is the default. FLAG_ERRORS \u2014 Specifies to flag topological errors, if any are found. ", "dataType": "String"}]},
{"syntax": "SmoothLine_cartography (in_features, out_feature_class, algorithm, tolerance, {endpoint_option}, {error_option})", "name": "Smooth Line (Cartography)", "description": "Smooths sharp angles in lines to improve aesthetic or cartographic quality.", "example": {"title": "SmoothLine Example (Python Window)", "description": "The following Python window script demonstrates how to use the SmoothLine tool in immediate mode.", "code": "import arcpy from arcpy import env import arcpy.cartography as CA env.workspace = \"C:/data\" CA.SmoothLine ( \"contours.shp\" , \"C:/output/output.gdb/smoothed_contours\" , \"PAEK\" , 100 )"}, "usage": [" There are two smoothing methods to choose from:", "Smoothing may introduce topological errors such as line crossings. Use the ", "FLAG_ERRORS", " option in the ", "Handle Topological Errors", " parameter to identify these errors. Two fields will be added\u2014InLine_FID and  SmoLnFlag\u2014to contain input feature IDs and topological errors. Values of 1 in the  SmoLnFlag field indicate a topology error; 0 (zero) indicates no error. The InLineFID field links the output lines to their input lines. The ", "FLAG_ERRORS", " option cannot be used within an edit session.", "Invalid (self-intersecting) geometry may be created during the smoothing process and will be repaired but not improved. For example, if a line self-crosses it will become a multipart line but will still appear self-crossing."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The line features to be smoothed. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to be created. ", "dataType": "Feature Class"}, {"name": "algorithm", "isOptional": false, "description": "Specifies the smoothing algorithm. PAEK \u2014 Acronym for Polynomial Approximation with Exponential Kernel. It calculates a smoothed line that will not pass through the input line vertices. This is the default. BEZIER_INTERPOLATION \u2014 Fits Bezier curves between vertices. The resulting line passes through the vertices of the input line. This algorithm does not require a tolerance. Bezier curves will be approximated in shapefile output.", "dataType": "String"}, {"name": "tolerance", "isOptional": false, "description": "Sets a tolerance used by the PAEK algorithm. A tolerance must be specified, and it must be greater than zero. You can choose a preferred unit; the default is the feature unit. You must enter a 0 as a placeholder when using the BEZIER_INTERPOLATION smoothing algorithm. ", "dataType": "Linear unit"}, {"name": "endpoint_option", "isOptional": true, "description": "Specifies whether to preserve the endpoints for closed lines. This option works with the PAEK algorithm only. FIXED_CLOSED_ENDPOINT \u2014 Preserves the endpoint of a closed line. This is the default. NO_FIXED \u2014 Smooths through the endpoint of a closed line. ", "dataType": "Boolean"}, {"name": "error_option", "isOptional": true, "description": "Specifies how the topological errors (possibly introduced in the process, such as line crossing) will be handled. NO_CHECK \u2014 Specifies not to check for topological errors. This is the default. FLAG_ERRORS \u2014 Specifies to flag topological errors, if any are found. ", "dataType": "String"}]},
{"syntax": "SimplifyPolygon_cartography (in_features, out_feature_class, algorithm, tolerance, {minimum_area}, {error_option}, {collapsed_point_option})", "name": "Simplify Polygon (Cartography)", "description": "Simplifies polygons by removing extraneous bends while preserving essential shape.", "example": {"title": "SimplifyPolygon Example (Python Window)", "description": "The following Python window script demonstrates how to use the SimplifyPolygon tool in immediate mode.", "code": "import arcpy from arcpy import env import arcpy.cartography as CA env.workspace = \"C:/data\" CA.SimplifyPolygon ( \"soils.shp\" , \"C:/output/output.gdb/simplified_soils\" , \"POINT_REMOVE\" , 100 )"}, "usage": [" There are two simplification methods:", " The ", "Minimum Area", " parameter applies to simplified polygons only. Any polygons which are smaller than the minimum area after the simplification process is completed will be removed from the output feature class. For a group of adjacent polygons which share common edges, it applies to the total area of the group.", " The tool produces two output feature classes, a polygon feature class to store the simplified polygons and a point feature class to store points that represent any polygons that were collapsed to zero-area. The point output name and location is automatically derived from the output polygon name with a ", "_Pnt", " suffix. The polygon output will contain all the input fields; the point output will not contain any of the input fields.", " Multipart polygons are simplified as individual parts.", " There are three options for handling topological errors in the output:", "The point\r\noutput will be populated when the NO_CHECK or the FLAG_ERRORS are used, or if the ", "Keep\r\ncollapsed points", " check box is checked. If an input polygon contains\r\nmultiple parts and one of the parts becomes a collapsed point, the\r\npoint representing that part will also be included in the point\r\noutput."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The polygon features to be simplified. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output polygon feature class to be created. ", "dataType": "Feature Class"}, {"name": "algorithm", "isOptional": false, "description": "Specifies the polygon simplification algorithm. POINT_REMOVE \u2014 Keeps the so-called critical points that depict the essential shape of a polygon and removes all other points. This is the default. BEND_SIMPLIFY \u2014 Keeps the main shape of a polygon and removes extraneous bends in the boundary. ", "dataType": "String"}, {"name": "tolerance", "isOptional": false, "description": "The tolerance that determines the degree of simplification. A tolerance must be specified, and it must be greater than zero. You can choose a preferred unit; the default is the feature unit. For POINT_REMOVE algorithm, the tolerance you specify is the maximum allowable offset. For BEND_SIMPLIFY algorithm, the tolerance you specify is the length of the reference bend baseline.", "dataType": "Linear unit"}, {"name": "minimum_area", "isOptional": true, "description": "Sets the minimum area for a simplified polygon to be retained. The default value is zero, that is, to keep all polygons. You can choose a preferred unit for the specified value; the default is the feature unit. ", "dataType": "Areal unit"}, {"name": "error_option", "isOptional": true, "description": "Specifies how the topological errors (possibly introduced in the process, including line crossing, line overlapping, and collapsed zero-length lines) will be handled. NO_CHECK \u2014 Specifies not to check topological errors. This is the default. FLAG_ERRORS \u2014 Specifies to flag topological errors if any are found. RESOLVE_ERRORS \u2014 Specifies to resolve topological errors if any are found. ", "dataType": "String"}, {"name": "collapsed_point_option", "isOptional": true, "description": "Specifies whether to keep collapsed zero-area polygons as points if any are found in the process. This option applies only when NO_CHECK or FLAG_ERRORS is specified. KEEP_COLLAPSED_POINTS \u2014 Specifies to keep the collapsed zero-area polygons as points. The endpoints of the collapsed polygon boundaries will be stored in a point feature class at the output feature class location, taking the name of the output feature class plus a suffix _Pnt. This is the default. NO_KEEP \u2014 Specifies not to keep the collapsed zero-area polygons as points even if they are found in the process; therefore, the point feature class will be empty. ", "dataType": "Boolean"}]},
{"syntax": "SimplifyLine_cartography (in_features, out_feature_class, algorithm, tolerance, {error_resolving_option}, {collapsed_point_option}, {error_checking_option})", "name": "Simplify Line (Cartography)", "description": "Simplifies lines by removing extraneous bends while preserving essential shape. Learn more about how Simplify Line works", "example": {"title": "SimplifyLine example (Python window)", "description": "The following Python window script demonstrates how to use the SimplifyLine tool in immediate mode.", "code": "import arcpy from arcpy import env import arcpy.cartography as CA env.workspace = \"C:/data\" CA.SimplifyLine ( \"roads.shp\" , \"C:/output/output.gdb/simplified_roads\" , \"POINT_REMOVE\" , 20 )"}, "usage": [" There are two simplification methods:", "The ", "simplification tolerance", " value determines the degree of simplification. Set the tolerance equal to or greater than the minimum allowable spacing between graphic elements. Using the same tolerance, ", "point remove", " produces rougher and more simplified results than ", "bend simplify", ".", " The tool produces two output feature classes: a line feature class to store the simplified lines and a point feature class to store points that represent any lines that were collapsed to zero-length. The point output name and location is automatically derived from the output line name with a ", "_Pnt", " suffix. The line output will contain all the input fields; the point output will not contain any of the input fields.", " There are options for handling topological errors in the output:", " The ", "Check for topological errors", " and ", "Resolve topological errors", " parameters cannot be used within an edit session. Disable the ", "Check for topological errors", " parameter in order to run the tool within an edit session."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The line features to be simplified. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output line feature class to be created. ", "dataType": "Feature Class"}, {"name": "algorithm", "isOptional": false, "description": "Specifies the line simplification algorithm. POINT_REMOVE \u2014 Keeps the so-called critical points that depict the essential shape of a line and removes all other points. This is the default. BEND_SIMPLIFY \u2014 Keeps the main shape of a line and removes extraneous bends. ", "dataType": "String"}, {"name": "tolerance", "isOptional": false, "description": "The tolerance that determines the degree of simplification. A tolerance must be specified, and it must be greater than zero. You can choose a preferred unit; the default is the feature unit. For POINT_REMOVE algorithm, the tolerance you specify is the maximum allowable offset of each vertex from its original location. This value may be reduced locally in some areas when the option is used to resolve topological errors. For BEND_SIMPLIFY algorithm, the tolerance you specify is the length of the reference bend baseline.", "dataType": "Linear unit"}, {"name": "error_resolving_option", "isOptional": true, "description": "Specifies how the topological errors (possibly introduced in the process, including line crossing, line overlapping, and collapsed zero-length lines) will be handled. This parameter will be in effect when the error_checking_option is CHECK (the default). FLAG_ERRORS \u2014 Specifies to flag topological errors, if any are found. This is the default. RESOLVE_ERRORS \u2014 Specifies to resolve topological errors, if any are found. ", "dataType": "Boolean"}, {"name": "collapsed_point_option", "isOptional": true, "description": "Specifies whether to keep collapsed zero-length lines as points if any are found in the process. This option applies only when NO_CHECK is specified or when both FLAG_ERRORS and CHECK options are specified. KEEP_COLLAPSED_POINTS \u2014 Specifies to keep the collapsed zero-length lines as points. The endpoints of the collapsed lines will be stored in a point feature class at the output feature class location, taking the name of the output feature class plus a suffix _Pnt. This is the default. NO_KEEP \u2014 Specifies not to keep the collapsed zero-length lines as points even if they are found in the process; therefore, the point feature class will be empty. ", "dataType": "Boolean"}, {"name": "error_checking_option", "isOptional": true, "description": "Specifies how the topological errors (possibly introduced in the process, including line crossing, line overlapping, and collapsed zero-length lines) will be handled. CHECK \u2014 Specifies to check for topological errors and enables the error_resolving_option parameter. This is the default. NO_CHECK \u2014 Specifies not to check for topological errors and disables the error_resolving_option parameter. ", "dataType": "Boolean"}]},
{"syntax": "SimplifyBuilding_cartography (in_features, out_feature_class, simplification_tolerance, {minimum_area}, {conflict_option})", "name": "Simplify Building (Cartography)", "description": "Simplifies the boundary or footprint of building polygons while maintaining their essential shape and size.", "example": {"title": "SimplifyBuilding Example (Python Window)", "description": "The following Python window script demonstrates how to use the SimplifyBuilding function in immediate mode.", "code": "import arcpy from arcpy import env import arcpy.cartography as CA env.workspace = \"C:/data\" CA.SimplifyBuilding ( \"buildings.shp\" , \"C:/output/output.gdb/simplified_buildings\" , 10 )"}, "usage": [" The ", "Minimum area", " parameter applies to simplified buildings only. Any buildings which are smaller than the minimum area after the simplification process is completed will be removed from the output feature class.", "The output feature class will include a field called BLD_STATUS to indicate simplification status as follows:", "Prior to ArcGIS version 10, BLD_STATUS = 4 indicated simplified or partially simplified buildings connected with\r\nstraight lines. \r\nBLD_STATUS = 4 is no longer used. ", "If the ", "Check for spatial conflicts", " parameter is used, the tool will detect spatial conflicts and add a new field called SimBldFlag to the output to store conflict flags. A value of 0 means 'no conflict'; a value of 1 means 'conflict'.", "This tool cannot be executed within an edit session.", "Input Z values can be preserved if specified in the Environment Settings. Where output vertices are coincident to input feature vertices, Z values will be transferred to output vertices. Elsewhere, Z-values will be derived either from existing Z values or through interpolation.", "Invalid (self-intersecting) geometry may be created during the simplification process and will be repaired but not improved. For example, if a polygon crosses itself, the polygon will become a multiple-part polygon but will still look self-crossing."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The building polygons to be simplified. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to be created. ", "dataType": "Feature Class"}, {"name": "simplification_tolerance", "isOptional": false, "description": "Sets the tolerance for building simplification. A tolerance must be specified, and it must be greater than zero. You can choose a preferred unit; the default is the feature unit. ", "dataType": "Linear unit"}, {"name": "minimum_area", "isOptional": true, "description": "Sets the minimum area for a simplified building to be retained in feature units. The default value is zero, that is, to keep all buildings. You can specify a preferred unit; the default is the feature unit. ", "dataType": "Areal unit"}, {"name": "conflict_option", "isOptional": true, "description": "Specifies whether or not to check for potential conflicts, that is, overlapping or touching, among buildings. NO_CHECK \u2014 Specifies not to check for potential conflicts; the resulting buildings may overlap. This is the default. CHECK_CONFLICTS \u2014 Specifies to check for potential conflicts; the conflicting buildings will be flagged. ", "dataType": "Boolean"}]},
{"syntax": "MergeDividedRoads_cartography (in_features, merge_field, merge_distance, out_features, {out_displacement_features})", "name": "Merge Divided Roads (Cartography)", "description": " Generates single-line road features in place of matched pairs of  divided road lanes.  Matched pairs of roads or lanes are merged if they are the same road class, trend generally parallel to one another, and are within the merge distance apart. The road class is specified by the  Merge  Field  parameter. All nonmerged roads from the input collection are copied to the output feature class.  Learn more about how Merge Divided Roads works", "example": {"title": "MergeDividedRoads tool example (Python Window)", "description": "The following Python window script demonstrates how to use the MergeDividedRoads tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" env.referenceScale = \"50000\" arcpy.MergeDividedRoads_cartography ( \"roads.lyr\" , \"level\" , \"25 meters\" , \"C:/data/cartography.gdb/transportation/merged_roads\" , \"C:/data/cartography.gdb/transportation/displacement\" )"}, "usage": ["The output feature class contains single line features representing merged roads and copies of all unmerged input features. Merged features will inherit  the attribution from one of the two  input features.", "Input features with ", "Merge Field", " parameter values equal to zero are \"locked\" and will not be merged, even if adjacent features are not locked. ", "The optional ", "Output Displacement Feature Class", " parameter creates a feature class of  polygons that indicates the amount and direction of displacement that took place. This feature class can be used for visual inspection, for spatial querying, or as an input to the ", "Propagate Displacement", " tool. ", "If the input is a feature layer drawn with a representation, any shape overrides associated with the representation will be used as the input geometry considered by this tool, and the corresponding geometry in the Shape field will be ignored. The output feature class will contain the feature class representation, but all geometry will be stored in the output Shape field, not as representation shape overrides.", "Processing a large road dataset may exceed memory limitations. In this case, consider processing input data by partition by identifying a relevant  polygon feature class in the ", "Partition Features", " environment setting. Portions of the data, defined by partition boundaries, will be processed sequentially. The resulting feature classes will be seamless and consistent at partition edges. See ", "How Merge Divided Roads works", "  for more information about running this tool with partitioning.  "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input linear road features that contain matched pairs of divided road lanes that should be merged together to a single output line feature. ", "dataType": "Feature Layer"}, {"name": "merge_field", "isOptional": false, "description": " The field that contains road classification information. Only parallel, proximate roads of equal classification will be merged. A value of 0 (zero) locks a feature to prevent it from participating in merging. ", "dataType": "Field"}, {"name": "merge_distance", "isOptional": false, "description": " The minimum distance apart, in the specified units, for equal-class, relatively parallel road features to be merged. This distance must be greater than zero. If the units are in pt, mm, cm, or in, the value is considered as page units and takes into account the reference scale. ", "dataType": "Linear unit"}, {"name": "out_features", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing single-line merged road features and all unmerged road features. ", "dataType": "Feature Class"}, {"name": "out_displacement_features", "isOutputFile": true, "isOptional": true, "description": " The output polygon features containing the degree and direction of road displacement, to be used by the Propagate Displacement tool to preserve spatial relationships. ", "dataType": "Feature Class"}]},
{"syntax": "CollapseDualLinesToCenterline_cartography (in_features, out_feature_class, maximum_width, {minimum_width})", "name": "Collapse Dual Lines To Centerline (Cartography)", "description": "Derives centerlines from dual-line (or double-line) features, such as road casings, based on specified width tolerances.", "example": {"title": "CollapseDualLinesToCenterline Example (Python Window)", "description": "The following Python window script demonstrates how to use the CollapseDualLinesToCenterline tool in immediate mode.", "code": "import arcpy from arcpy import env import arcpy.cartography as CA env.workspace = \"C:/data\" CA.CollapseDualLinesToCenterline ( \"dual_line_roads.shp\" , \"C:/output/output.gdb/road_centerlines\" , 50 )"}, "usage": ["This tool is intended for regular, near parallel pairs of lines, such as large-scale road casings. Centerlines will be created only between open-ended lines not inside closed lines which are likely street blocks. The tool is not intended to simplify multiple-lane highways with interchanges, ramps, overpasses and underpasses, or railways with multiple, merging tracks. Use the ", "Merge Divided Roads", " tool instead.", "This tool cannot be executed within an edit session.", "The output feature class will not carry the geographic attributes from the input lines, but contain the following three new fields:", "You can use the LnType values to further inspect the unresolved intersections and wider roads; and you can use the LeftLn_FID and RightLn_FID fields along with Join Field tool to transfer the attributes from source lines to the centerlines as needed."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input dual-line features, such as road casings, from which centerlines are derived. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to be created. ", "dataType": "Feature Class"}, {"name": "maximum_width", "isOptional": false, "description": "Sets the maximum width of the dual-line features to derive centerline. A value must be specified, and it must be greater than zero. You can choose a preferred unit; the default is the feature unit. ", "dataType": "Linear unit"}, {"name": "minimum_width", "isOptional": true, "description": "Sets the minimum width of the dual-line features to derive centerline. The minimum width must be greater than or equal to zero, and it must be less than the maximum width. The default value is zero. You can specify a preferred unit; the default is the feature unit. ", "dataType": "Linear unit"}]},
{"syntax": "AggregatePolygons_cartography (in_features, out_feature_class, aggregation_distance, {minimum_area}, {minimum_hole_size}, {orthogonality_option}, {barrier_features}, {out_table})", "name": "Aggregate Polygons (Cartography)", "description": "Combines polygons within a specified distance of each other into new polygons.", "example": {"title": "AggregatePolygons tool example 1 (Python window)", "description": "The following Python window script demonstrates how to use the AggregatePolygons tool in immediate mode.", "code": "import arcpy arcpy.env.cartographicPartitions = \"C:/data/county.gdb/zipcodepoly\" buildings = \"C:/data/county.gdb/bldgspoly\" roads = \"C:/data/county.gdb/roadnetwork\" output = \"C:/data/county.gdb/BldgAggBarrierPartition\" output_table = \"C:/data/county.gdb/BldgAggBarrierPartition_Tbl\" arcpy.AggregatePolygons_cartography ( buildings , output , \"20 Meters\" , \"5 SquareMeters\" , \"0 SquareMeters\" , \"ORTHOGONAL\" , roads , output_table )"}, "usage": ["This tool is intended for moderate scale reduction and aggregation when input features can no longer be represented individually due to the limited map space or the required data resolution. Aggregation will only happen where two polygon boundaries are within the specified aggregation distance to each other. There will be no self-aggregation, meaning no aggregation within an input polygon feature itself along its boundary, and no aggregation between any parts of a multipart polygon feature.", "Using the orthogonal option will construct orthogonally shaped output features. This option is best suited for use with input features that have predominantly orthogonal edges. In some cases, less aggregation will occur in order to accommodate this. For example, two square buildings situated diagonally apart within the aggregation distance may not be aggregated because there is no clear connection that can be made while preserving orthogonality. The nonorthogonal option will produce more organically shaped results. ", "Use barrier features to prevent aggregation from occurring across boundaries. Examples include preventing land cover from aggregating across rivers or buildings aggregating across streets. Barriers can be either lines or polygons, and multiple barrier feature classes can be used simultaneously.", "If the input feature class is a layer referencing a representation, and there are shape overrides on any of the input features, the shape overrides will be assessed in the aggregation, not the feature geometry.", "The output feature class will not contain any geographic attributes from the input features. A one-to-many relationship table can be optionally created to  link the aggregated polygons to their source polygons. This link can become incorrect when any of the input or output features are modified. ", "If the input features contain Z values, the Z values can be preserved if specified in the environment settings. Where the output vertices are not changed, the input Z values will be carried over to the output vertices; otherwise, a Z value will be derived for  new vertices, either from existing Z values or through interpolation.", "Processing a large input dataset may exceed memory limitations. In this case, consider processing input data by partition by identifying a relevant polygon feature class in the ", "Cartographic Partitions", " environment setting. Portions of the data, defined by partition boundaries, will be processed sequentially. The resulting output feature class consistent at partition edges, but output features crossing partitions, will be split at the partition line. A field called ", "IS_SPLIT", " in the output feature class will have a value of ", "1", " in this instance. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The polygon features to be aggregated. If this is a layer referencing a representation and shape overrides are present on the input features, the overridden shapes, not the feature shapes, will be considered in aggregation processing. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to be created. ", "dataType": "Feature Class"}, {"name": "aggregation_distance", "isOptional": false, "description": "The distance to be satisfied between polygon boundaries for aggregation to happen. A distance must be specified, and it must be greater than zero. You can choose a preferred unit; the default is the feature unit. ", "dataType": "Linear unit"}, {"name": "minimum_area", "isOptional": true, "description": "The minimum area for an aggregated polygon to be retained. The default value is zero, that is, to keep all polygons. You can specify a preferred unit; the default is the feature unit. ", "dataType": "Areal unit"}, {"name": "minimum_hole_size", "isOptional": true, "description": "The minimum size of a polygon hole to be retained. The default value is zero, that is, to keep all polygon holes. You can specify a preferred unit; the default is the feature unit. ", "dataType": "Areal unit"}, {"name": "orthogonality_option", "isOptional": true, "description": "Specifies the characteristic of the output features when constructing the aggregated boundaries. NON_ORTHOGONAL \u2014 Organically shaped output features will be created. This is suitable for natural features, such as vegetation or soil polygons. This is the default. ORTHOGONAL \u2014 Orthogonally shaped output features will be created. This option is suitable for preserving the geometric characteristic of anthropogenic input features, such as building footprints.", "dataType": "Boolean"}, {"name": "barrier_features", "isOptional": true, "description": " The layers containing the line or polygon features that are aggregation barriers for input features. Features will not be aggregated across barrier features. Barrier features that are in geometric conflict with input features will be ignored. ", "dataType": "Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": true, "description": " A one-to-many relationship table that links the aggregated polygons to their source polygon features. This table contains two fields, OUTPUT_FID and INPUT_FID, storing the aggregated feature IDs and their source feature IDs, respectively. Use this table to derive necessary attributes for the output features from their source features. The default name for this table is the name of the output feature class, appended with _tbl. The default path is the same as the output feature class. No table is created when this parameter is left blank. ", "dataType": "Table"}]},
{"syntax": "StripMapIndexFeatures_cartography (in_features, out_feature_class, {use_page_unit}, {scale}, {length_along_line}, {length_perpendicular_to_line}, {page_orientation}, {overlap_percentage}, {starting_page_number}, {direction_type})", "name": "Strip Map Index Features (Cartography)", "description": " Creates a series of rectangular polygons, or index features, that follow a single linear feature or a group of linear features. These index features can be used with Data Driven Pages to define pages within a strip map, or set of maps that follow a linear feature. The resulting index features contain attributes that can be used to rotate and orient the map on the page and determine which index features, or pages, are next to the current page (to the left and right or to the top and bottom).", "example": {"title": "StripMapIndexFeatures tool Example #1 (Python Window)", "description": "Creates strip map index features based on the input line features with index feature dimensions specified for a layout page.", "code": ""}, "usage": ["The coordinate system of the ouput feature class is determined in this order:", "Line features are the only valid input.", " When you select ", "Use Page Unit and Scale Map", ", ", "Map Scale", " becomes a required parameter. If ArcMap is open the map scale of the active data frame will be used; otherwise, the default value is 1. If you are specifying the size of index features in map space and ", "Use Page Unit and Scale", " is not selected ", "Map Scale", " is not needed.", "Resulting index features are created with a number of attributes. These attributes include: PageNumber, GroupId, SeqId, PrevPage, NextPage, LeftPage, RightPage, TopPage, BottomPage and Angle.", " When ", "Use Page Unit and Scale", " is selected the units for ", "Length Along the Line", " and ", "Length Perpendicular to the Line", " automatically changes to the page units set in the active map document (if ArcMap is open) or to inches if you are using the tool outside of an ArcMap session. For best results these units should be specified in page units such as inches or centimeters. If ", "Use Page Units and Scale", " is not selected, units should be specified in map units such as meters, feet, kilometers, miles, or decimal degrees."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " Input polyline or polylines defining the path of the strip map index features. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " Resulting feature class of polygon index features. The coordinate system of the ouput feature class is determined in this order. If a coordinate system is specified by the Output Coordinate System variable in the Environment Settings the output feature class will use this coordinate system. If a coordinate system is not specified by the Output Coordinate System the ouput feature class will use the coordinate system of the input feature.", "dataType": "Feature Class"}, {"name": "use_page_unit", "isOptional": true, "description": "Indicates whether index feature size input is in page space. The default value is NO_USEPAGEUNIT. USEPAGEUNIT \u2014 Index polygon height and width are calculated in page space. NO_USEPAGEUNIT \u2014 Index polygon height and width are calculated in map space.", "dataType": "Boolean"}, {"name": "scale", "isOptional": true, "description": " Map scale must be specified if index feature lengths (along the line and perpendicular to the line) are to be calculated in page space. If ArcMap is open the default value will be the scale of the active data frame. If ArcMap is not open the default will be 1. ", "dataType": "Long"}, {"name": "length_along_line", "isOptional": true, "description": " Length of the polygon index feature along the input line feature specifed in either map or page units. The default value is determined by the spatial reference of the input line feature or features. This value will be 1/100 of the input feature class extent along the X axis. ", "dataType": "Linear unit"}, {"name": "length_perpendicular_to_line", "isOptional": true, "description": " Length of the polygon index feature perpendicular to the input line feature specifed in either map or page units. The default value is determined by the spatial reference of the input line feature or features. This value will be 1/2 the number used for the length along the line. ", "dataType": "Linear unit"}, {"name": "page_orientation", "isOptional": true, "description": " Used to determine the orientation of the input line features on the layout page. The default is HORIZONTAL. VERTICAL \u2014 The direction of the strip map series on the page is top to bottom. HORIZONTAL \u2014 The direction of the strip map series on the page is left to right.", "dataType": "String"}, {"name": "overlap_percentage", "isOptional": true, "description": " The approximate percentage of geographic overlap between an individual map page and its adjoining pages in the series. The default is 10. ", "dataType": "Double"}, {"name": "starting_page_number", "isOptional": true, "description": "Each grid index feature is assigned a sequential page number starting with a specified starting page number. The default value is 1. ", "dataType": "Long"}, {"name": "direction_type", "isOptional": true, "description": " Index features are created in a sequential order and require a starting point. Setting the direction type for the strip map provides a starting point. The default is WE_NS. This means that the starting point for the strip map is either at the western end of the line feature if the line feature's directional trend is West to East/East to West or, if the directional trend is North to South/South to North, the starting point would be the northern most point of the line feature. The direction type is also applied to secondary line features. WE_NS \u2014 West to East and North to South. WE_SN \u2014 West to East and South to North. EW_NS \u2014 East to West and North to South. EW_SN \u2014 East to West and South to North.", "dataType": "String"}]},
{"syntax": "GridIndexFeatures_cartography (out_feature_class, {in_features}, {intersect_feature}, {use_page_unit}, {scale}, {polygon_width}, {polygon_height}, {origin_coord}, {number_rows}, {number_columns}, {starting_page_number}, {label_from_origin})", "name": "Grid Index Features (Cartography)", "description": "Creates a grid of rectangular polygon features that can be used as an index to specify pages for a map book using Data Driven Pages. A grid can be created that only includes polygon features that intersect another feature layer.", "example": {"title": "GridIndexFeatures tool Example #1 (Python Window)", "description": "Creates GridIndexFeatures using the intersection of input features and specified index feature dimensions in map units.", "code": ""}, "usage": ["The coordinate system of the ouput feature class is determined in this order.", " Input Features", " must be points, lines, or polygons.", "When you select ", "Use Page Unit and Scale", ", ", "Map Scale", " becomes a required parameter.", "When using the tool dialog, changing the polygon width or height values when", " Grid Origin Coordinates", " have been specified, the number of rows and columns change automatically.", " When you add ", "Input Features", ", the ", "Grid Origin Coordinates", " are automatically calculated.", " Consider saving your output feature class to a geodatabase. Though saving the output to a shapefile is supported, if you intend on calculating spatial references for index layer features using either the ", "Calculate_Central_Meridian_And_Parallels", " or the ", "Calculate_UTM_Zone", " tools the generated coordinate string results may exceed the field character limits (255) of a shapefile. ", " When ", "Use Page Unit and Scale", " is selected, the ", "Polygon Width", " and ", "Polygon Height", " units automatically change to the page units set in the active map document. If you are using the tool outside of an ArcMap session the units default to inches. Though you have the option to specify map units, such as meters or miles, you should use units that are appropriate for your page.", "When ", "Use Page Unit and Scale", " is selected the map scale defaults to the scale value of the active data frame on the page layout of the active ArcMap document. If you are using the tool outside of an ArcMap session the scale defaults to 1.", " For best results all input feature layers and feature classes should be in the same coordinate system as the data frame, or if you are using the tool outside of an ArcMap session, all input feature layers and feature classes should be in the same coordinate system as the first input feature layer or feature class in the list."], "parameters": [{"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " Resulting feature class of polygon index features. The coordinate system of the ouput feature class is determined in this order. If a coordinate system is specified by the Output Coordinate System variable in the Environment Settings the output feature class will use this coordinate system. If a coordinate system is not specified by the Output Coordinate System the output feature class will use the coordinate system of the active data frame (ArcMap is open). If a coordinate system is not specified by the Output Coordinate System and there is no active data frame (ArcMap is not open) the ouput feature class will use the coordinate system of the first input feature. If a coordinate system is not specified by the Output Coordinate System, there is no active data frame (ArcMap is not open) and there is no specified input features the coordinate system of the ouput feature class will be unknown.", "dataType": "Feature Class"}, {"name": "in_features", "isInputFile": true, "isOptional": false, "description": " Input features can be used to define the extent of the polygon grid that is created. ", "dataType": "Feature Layer"}, {"name": "intersect_feature", "isOptional": true, "description": "Limits the output grid feature class to only areas that intersect input feature layers or datasets. When input features are specified, the default value is INTERSECTFEATURE. The intersection of input features will be used to create index features. INTERSECTFEATURE \u2014 Limits the output grid feature class to only areas that intersect input feature layers or datasets. NO_INTERSECTFEATURE \u2014 Output grid feature class is created using specified coordinates, rows, and columns.", "dataType": "Boolean"}, {"name": "use_page_unit", "isOptional": true, "description": "Indicates whether index polygon size input is in page units. The default is NO_USEPAGEUNIT. The tool uses map units by default. USEPAGEUNIT \u2014 Index polygon height and width are calculated in page units. NO_USEPAGEUNIT \u2014 Index polygon height and width are calculated in map units.", "dataType": "Boolean"}, {"name": "scale", "isOptional": true, "description": " Scale must be specified if the index polygon height and width are to be calculated in page units. If the tool is being used outside of an active ArcMap session the default scale value is 1. ", "dataType": "Long"}, {"name": "polygon_width", "isOptional": true, "description": " Width of the index polygon specifed in either map or page units. If page units are being used the default value is 1 inch. If map units are being used the default is 1 degree. ", "dataType": "Linear unit"}, {"name": "polygon_height", "isOptional": true, "description": " Height of the index polygon specifed in either map or page units. If page units are being used the default value is 1 inch. If map units are being used the default is 1 degree. ", "dataType": "Linear unit"}, {"name": "origin_coord", "isOptional": true, "description": " Coordinate for the lower left origin of the output grid feature class. If input features are specified the default value is determined by the extent of the union of extents for these features. If there are no input features specified, the default coordinates are 0 and 0. ", "dataType": "Point"}, {"name": "number_rows", "isOptional": true, "description": " Number of rows to create in the y direction from the point of origin. The default is 10. ", "dataType": "Long"}, {"name": "number_columns", "isOptional": true, "description": " Number of columns to create in the x direction from the point of origin. The default is 10. ", "dataType": "Long"}, {"name": "starting_page_number", "isOptional": true, "description": "Each grid index feature is assigned a sequential page number starting with a specified starting page number. The default is 1. ", "dataType": "Long"}, {"name": "label_from_origin", "isOptional": true, "description": "Page numbers (labels) starting with the specified starting page number (default is 1) begins with the cell in the lower left corner of the output grid. The default is NO_LABELFROMORIGIN. LABELFROMORIGIN \u2014 Page numbers (labels) starting with the specified starting page number (default is 1) begins with the polygon feature in the lower left corner of the output grid. NO_LABELFROMORIGIN \u2014 Page numbers (labels) starting with the specified starting page number (default is 1) begins with the cell in the upper left corner of the output grid.", "dataType": "Boolean"}]},
{"syntax": "CalculateUTMZone_cartography (in_features, in_field)", "name": "Calculate UTM Zone (Cartography)", "description": " Calculates a UTM zone of each feature based on the center point and stores this spatial reference string in a specified field. This field can be used in conjunction with Data Driven Pages to update the spatial reference to the correct UTM zone for each map.", "example": {"title": "CalculateUTMZone tool Example (Python Window)", "description": "Calculates a custom UTM zone for a set of features.", "code": ""}, "usage": [" Input features can be points, lines, or polygons.", " Geometries located at extreme latitudes, near either the North or the South poles, may not be appropriate for UTM zone calculation. You will receive a warning when a UTM zone cannot be calculated for a particular feature.", " The datum for the calculated UTM coordinate system string is taken from the datum of the active data frame's coordinate system. If the datum of the feature's data source is different than the data frame's, you will get a warning. A UTM zone is calculated, however, the result may not be accurate. The tool does not perform any geotransformations. In order to get the best results, the datum of your input features should be the same as the datum used by the data frame.", " The ", "UTM Zone Field", ", which contains the UTM spatial reference string, should have a length of 600 characters or more. Spatial reference strings vary in length but are usually quite long, and you need to ensure that the field has enough space. The tool will issue a warning if the spatial reference string has been truncated because the field length is not long enough. If you get this warning, it is likely that some of the spatial reference strings generated may have been truncated making them useless. When receiving the warning you should delete your results, create a new field of a sufficient length (> 600 characters), and rerun the tool.", " Use geodatabase feature classes. In most cases, the field size limitation of shapefiles makes it likely that spatial reference strings will be truncated if you run this tool on a shapefile."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "Input feature layer. ", "dataType": "Feature Layer"}, {"name": "in_field", "isInputFile": true, "isOptional": false, "description": " String field that stores the spatial reference string for the coordinate system. Field should have sufficient length (more than 600 characters) to hold the spatial reference string. ", "dataType": "Field"}]},
{"syntax": "CalculateGridConvergenceAngle_cartography (in_features, angle_field, {rotation_method}, {coordinate_sys_field})", "name": "Calculate Grid Convergence Angle (Cartography)", "description": "Calculates the rotation angle for true north based on the center point of each feature in a feature class and populates this value in a specified field. This field can be used in conjunction with Data Driven Pages to rotate each map to true north.", "example": {"title": "CalculateGridConvergenceAngle tool example 1 (Python window)", "description": "Calculates a true north rotation angle for a feature.", "code": ""}, "usage": ["This tool replaces the Calculate Geodesic Angle tool."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " Input feature class (points, multipoints, lines, and polygons). ", "dataType": "Feature Layer"}, {"name": "angle_field", "isOptional": false, "description": " Existing field that will be populated with the true north calculation value in decimal degrees. ", "dataType": "Field"}, {"name": "rotation_method", "isOptional": true, "description": " Method in which the rotation value is calculated. Geographic is the default value. GEOGRAPHIC \u2014 Angle is calculated clockwise with 0 at top. This is the default. ARITHMETIC \u2014 Angle is calculated counterclockwise with 0 at right. GRAPHIC \u2014 Angle is calculated counterclockwise with 0 at top.", "dataType": "String"}, {"name": "coordinate_sys_field", "isOptional": true, "description": " Field containing a projection engine string for a projected coordinate system to be used for angle calculation. The angle calculation for each feature will be based on the projected coordinate system projection engine string for the specific feature. In cases where there is an invalid value the tool will use the Cartographic coordinate system specified in the Cartography environment settings. The default is none, or no field specified. When no field is specified, the projected coordinate system used for calculation will be taken from the Cartography environment settings. ", "dataType": "Field"}]},
{"syntax": "CalculateCentralMeridianAndParallels_cartography (in_features, in_field, {standard_offset})", "name": "Calculate Central Meridian And Parallels (Cartography)", "description": " Calculates the central meridian and optional standard parallels based on the center point of a feature's extent; stores this coordinate system as a spatial reference string in a specified text field and repeats this for a set, or subset, of features.", "example": {"title": "CalculateCentralMeridianAndParallels tool Example (Python Window)", "description": "Calculates the central meridian and optional standard parallels for a set of features.", "code": ""}, "usage": [" Input features can be points, lines, or polygons", " Running this tool directly against a feature class that does not have a projection results in an error. The feature class must be projected.", " If you are using the tool while ArcMap is open, the resulting spatial reference string will be based on the current projection of the active data frame. The data frame must be in a projected coordinate system for the tool to work; otherwise, you will receive an error message. ", "The  ", "Data Driven Pages", " tool can use this string field to update the data frame coordinate system for each page.", " The ", "Coordinate System Field", " must be a text field and should have a length of 600 characters or more. Coordinate system strings vary in length but are usually quite long, and you need to ensure that the field has enough space. The tool will issue a warning if the coordinate system string has been truncated because the field length is not long enough. If you get this warning, it is likely that some of the coordinate system strings generated may have been truncated making them useless. When receiving the warning you should delete your results, create a new field of a sufficient length (> 600 characters) and rerun the tool.", " Use geodatabase feature classes as your input rather than shapefiles. In most cases, the field size limitation of shapefiles makes it likely that PE strings will be truncated if you run this tool on a shapefile.", " The ", "Standard Parallel Offset", " is a percentage of the latitudinal height extent of the input feature. The offset from the latitude of the center of the input feature is calculated using this percentage. A ", "Standard Parallel Offset", " value of 0 results in parallel values equal to the latitude of the input feature latitudinal center. A value of 0.5 (50%) results in parallels approximate to the latitudinal height, with parallels at the top and the bottom, of the feature. Values greater than 0.5 will place the parallels outside the latitudinal bounds of the input feature. The default value is 0.25. This places the parallels approximately halfway from the latitudinal center of the feature and its latitudinal edges. Negative values and values greater than 1 are acceptable inputs."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " Input feature layer. ", "dataType": "Feature Layer"}, {"name": "in_field", "isInputFile": true, "isOptional": false, "description": "Text field to store the coordinate system string. ", "dataType": "Field"}, {"name": "standard_offset", "isOptional": true, "description": " Percentage of the height of the input feature used to offset the standard parallels from the center latitude of the input feature. The default is 25% or 0.25. Negative values and values greater than 1 are acceptable inputs. ", "dataType": "Double"}]},
{"syntax": "CalculateAdjacentFields_cartography (in_features, in_field)", "name": "Calculate Adjacent Fields (Cartography)", "description": " The most common use case for using this tool is to populate fields that can be used to label the adjacent pages in a map book. This tool appends eight new fields (each field representing one of the eight points of the compass: North, Northeast, East, Southeast, South, Southwest, West and Northwest) to the input feature class and calculates values that identify the adjacent (neighboring) polygons, in each cardinal direction, for each feature in the input feature class.", "example": {"title": "CalculateAdjacentFields tool Example (Python Window)", "description": "Create and populate fields identifying adjacent features for a  polygon grid feature class.", "code": ""}, "usage": [" This tool creates eight new fields whose names are a combination of the in_field name and direction. Direction is abbreviated. For example, if the in_field name is \"PageName\" the new field names created by the tool would include: \"PageName_N\", \"PageName_NE\", \"PageName_E\", \"PageName_SE\", \"PageName_S\", \"PageName_SW\", \"PageName_W\" and \"PageName_NW\". If the in_field name is \"MyPoly\" the new field names would include: \"MyPoly_N\", \"MyPoly_NE\", \"MyPoly_E\", \"MyPoly_SE\", \"MyPoly_S\", \"MyPoly_SW\", \"MyPoly_W\" and \"MyPoly_NW\".", " You cannot specify names for the new fields. The tool uses default names, though you can change field names after the tool has been run.", " You can use fields from a joined table."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " Polygon grid index features to be appended with adjacent field data. ", "dataType": "Feature Layer"}, {"name": "in_field", "isInputFile": true, "isOptional": false, "description": " Field whose values will be used to populate adjacent field data. Use actual field names. Do not use field aliases. ", "dataType": "Field"}]},
{"syntax": "TiledLabelsToAnnotation_cartography (map_document, data_frame, polygon_index_layer, out_geodatabase, out_layer, anno_suffix, {reference_scale_value}, {reference_scale_field}, {tile_id_field}, {coordinate_sys_field}, {map_rotation_field}, {feature_linked}, {generate_unplaced_annotation})", "name": "Tiled Labels To Annotation (Cartography)", "description": "Converts labels to annotation for layers in a map document based on a polygon index layer.   The tool divides a map into tiles and creates annotation for each tile in turn. This is useful for converting a large number of labels to annotation. The polygon index layer can be one generated by the  Map Server Cache Tiling Scheme To Polygons  or  Grid Index Features  tools or any other polygon feature class that covers the area where you would like to create annotation.", "example": {"title": "TiledLabelsToAnnotation Example (Python Window)", "description": null, "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.TiledLabelsToAnnotation_cartography ( \"Annotation.mxd\" , \"Layers\" , \"Tiles\" , \"data.gdb\" , \"GroupAnno\" , \"Anno\" , \"\" , \"Tile_Scale\" , \"OID\" , \"\" , \"\" , \"FEATURE_LINKED\" , \"GENERATE_UNPLACED_ANNOTATION\" )"}, "usage": [" Label class scale ranges are respected. When the tool generates annotation for a specific reference scale it will only convert label classes that are turned on and visible at that scale. ", "Label class scale ranges are set on the layer's properties in ArcMap. Once you have configured your label properties, save the map document before running this tool.", "If you select tiles prior to running the tool, annotation will only be created for the selected tiles.", "To update annotation for only some tiles in the polygon index layer, first select the annotation features with that specific TileID value and delete them. Then select the polygon features and re-run the tool.", "One output of this tool is a series of group layers. One group layer will contain a group layer for each reference scale for which annotation was created.", "When working in ArcCatalog or ModelBuilder, you can use the ", "Save_To_Layer_File", " tool to write the output group layer to a layer file. When using ArcMap, the tool adds the group layer to the display if this option is checked in the geoprocessing options. The group layer that is created is temporary and will not persist after the session ends unless the document is saved.", "Group layers created in ArcCatalog cannot be used in ArcMap unless they are saved to a layer file using the ", "Save_To_Layer_File", " tool.", "An existing group layer will be overwritten if the same layer name is specified and if you explicitly state that overwriting outputs is allowed.", "If duplicate feature class names are found in the data frame, a number will be added to the annotation following the feature class name (e.g., Cities01Anno10000, Cities02Anno10000, and so on).", "Annotation feature classes will not be overwritten if a suffix is specified that already exists. In this case a number will be added to the annotation feature class suffix (e.g., CitiesAnno10000, CitiesAnno10000_1, and so on).", "The reference scale for the annotation feature classes can be specified in one of two ways. ", "If you are using a polygon index layer that was created by the the ", "Map_Server_Cache_Tiling Scheme To Polygons", " tool use the Tile_Scale field for the ", "Reference Scale Field", ". A new anotation feature class will be created for each layer/Tile_Scale combination.", "If you are producing annotation at a variety of reference scales, design your map for each of those scales and avoid setting a reference scale in the data frame.", " If a coordinate system field from the polygon index layer is provided the annotation for each tile will be projected into that coordinate system for the purpose of drawing and placement.", "Annotation that is feature linked is associated with a specific feature in another feature class in the geodatabase. If checked, when you create the output annotation feature class, a relationship class will be automatically generated as well.", "When creating feature-linked annotation the output workspace must be the same as that of the feature classes they are linked to.", "Some labels may not currently display on the map because there is no room for them. To convert these labels, check the ", "Generate Unplaced Annotation", " box. This saves the unplaced labels in the annotation feature class, allowing you to position them later in an ArcMap edit session."], "parameters": [{"name": "map_document", "isOptional": false, "description": " The source map document that contains the labels to convert to annotation. ", "dataType": "ArcMap Document"}, {"name": "data_frame", "isOptional": false, "description": " The data frame from the map document that contains the labels to convert to annotation. ", "dataType": "String"}, {"name": "polygon_index_layer", "isOptional": false, "description": " The polygon layer that contains tile features. ", "dataType": "Table View"}, {"name": "out_geodatabase", "isOutputFile": true, "isOptional": false, "description": " The workspace where the output feature classes will be saved. The workspace can be an existing geodatabase or an existing feature dataset. ", "dataType": "Workspace ;Feature Dataset"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": " The group layer that will contain the generated annotation. When working in ArcCatalog, you can use the Save To Layer File tool to write the output group layer to a layer file. When using ArcMap, the tool adds the group layer to the display if this option is checked in the geoprocessing options. The group layer that is created is temporary and will not persist after the session ends unless the document is saved. ", "dataType": "Group Layer"}, {"name": "anno_suffix", "isOptional": false, "description": " The suffix that will be added to each new annotation feature class. This suffix will be appended to the name of the source feature class for each new annotation feature class. The reference scale for the annotation will follow this suffix. ", "dataType": "String"}, {"name": "reference_scale_value", "isOptional": true, "description": " Enter the scale to use as a reference for the annotation. This sets the scale to which all symbol and text sizes in the annotation will be based. ", "dataType": "Double"}, {"name": "reference_scale_field", "isOptional": true, "description": " The field in the polygon index layer that will determine the reference scale of the annotation. This sets the scale to which all symbol and text sizes in the annotation will be based. ", "dataType": "Field"}, {"name": "tile_id_field", "isOptional": true, "description": " A field in the polygon index layer that uniquely identifies the tiled area. These values will populate the TileID field in the annotation feature class attribute table. ", "dataType": "Field"}, {"name": "coordinate_sys_field", "isOptional": true, "description": " A field in the polygon index layer that contains the coordinate system information for each tile. Due to the length required for a field to store coordinate system information, a polygon index layer that contains a coordinate system field must be a geodatabase feature class. ", "dataType": "Field"}, {"name": "map_rotation_field", "isOptional": true, "description": " A field in the polygon index layer that contains an angle by which the data frame is to be rotated. ", "dataType": "Field"}, {"name": "feature_linked", "isOptional": true, "description": "This parameter is only available with ArcGIS for Desktop Standard and ArcGIS for Desktop Advanced licenses. Choose whether the output annotation feature class will be linked to the features in another feature class. STANDARD \u2014 The output annotation feature class will not be linked to the features in another feature class. This is the default. FEATURE_LINKED \u2014 The output annotation feature class will be linked to the features in another feature class.", "dataType": "Boolean"}, {"name": "generate_unplaced_annotation", "isOptional": true, "description": "Choose whether to create unplaced annotation from unplaced labels. NOT_GENERATE_UNPLACED_ANNOTATION \u2014 Annotation will only be created for features that are currently labeled. This is the default. GENERATE_UNPLACED_ANNOTATION \u2014 Unplaced annotation are stored in the annotation feature class. The status field for these annotation is set to Unplaced.", "dataType": "Boolean"}]},
{"syntax": "MapServerCacheTilingSchemeToPolygons_cartography (map_document, data_frame, tiling_scheme, output_feature_class, use_map_extent, clip_to_horizon, {antialiasing}, {levels})", "name": "Map Server Cache Tiling Scheme To Polygons (Cartography)", "description": " Creates a new polygon feature class from an existing tiling scheme.  This tool subdivides a data frame extent using the same scales as an existing map service cache tiling scheme and creates tiles over a large area, or \"supertile\". Since the supertile extent is larger than the actual tiles defined in the scheme, tiles used as input into the  Tiled Labels to Annotation  tool can convert labels to annotation over a larger area at a time. This process minimizes annotation duplication across tiles.", "example": {"title": "MapServerCacheTilingSchemeToPolygons Example (Python Window)", "description": "The following stand-alone script demonstrates how to use the MapServerCacheTilingSchemeToPolygons function.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.MapServerCacheTilingSchemeToPolygons_cartography ( \"C:/data/Annotation.mxd\" , \"Layers\" , \"C:/Program Files/ArcGIS/Desktop10.1/TilingSchemes/ArcGIS_Online_Bing_Maps_Google_Maps.xml\" , \"Tiles\" , \"USE_MAP_EXTENT\" , \"CLIP_TO_HORIZON\" , \"NONE\" , \"\" )"}, "usage": [" There are several options for loading an existing tiling scheme:", "The output feature class from this tool may be used as input into the ", "Tiled Labels to Annotation", " tool.", "For the ", "Clip tiles at the coordinate system horizon", " parameter, the coordinate system horizon is the valid area of use for a particular geographic or projected coordinate system."], "parameters": [{"name": "map_document", "isOptional": false, "description": " The source map document. ", "dataType": "ArcMap Document"}, {"name": "data_frame", "isOptional": false, "description": " The data frame from the source map document. ", "dataType": "String"}, {"name": "tiling_scheme", "isOptional": false, "description": " Path to a predefined tiling scheme .xml file. ", "dataType": "File"}, {"name": "output_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output polygon feature class. ", "dataType": "Feature Class"}, {"name": "use_map_extent", "isOptional": false, "description": " Choose whether to produce tiles for the entire extent of the tiling scheme or only tiles that intersect the full extent of the data frame. USE_MAP_EXTENT \u2014 Polygon features will be created for the full extent of the data frame. This is the default. FULL_TILING_SCHEME \u2014 Polygon features will be created for the full extent of the tiling scheme.", "dataType": "Boolean"}, {"name": "clip_to_horizon", "isOptional": false, "description": "Choose whether to constrain the polygons to the valid area of use for the geographic or projected coordinate system of the data frame. CLIP_TO_HORIZON \u2014 Polygon features will only be created within the valid area of use for the geographic or projected coordinate system of the data frame. Tiles that are within the extent of the tiling scheme but outside the extent of the coordinate system horizon will be clipped. This is the default. UNIFORM_TILE_SIZE \u2014 Polygon features will be created for the full extent of the tiling scheme. Within each scale level, polygons will be of a uniform size and will not be clipped at the coordinate system horizon. This may create data that is outside the valid area of use for the geographic or projected coordinate system. If a scale within the tiling scheme would generate a tile that is larger than the spatial domain of the feature class, null geometry will be created for that feature. ", "dataType": "Boolean"}, {"name": "antialiasing", "isOptional": true, "description": "Choose whether to generate polygons that match map service caches with anti-aliasing enabled. A map service cache supertile is 2048 x 2048 pixels with antialiasing or 4096 x 4096 pixels without. To see if antialiasing was used in an existing cache, open the tiling scheme file, conf.xml, and check to see if the <Antialiasing> tag is set to true. ANTIALIASING \u2014 Polygon tiles will be created to match the supertile extent of a map service cache with antialiasing enabled. NONE \u2014 Polygon tiles will be created to match the supertile extent of a map service cache without antialiasing enabled. This is the default.", "dataType": "Boolean"}, {"name": "levels", "isOptional": false, "description": "The scale levels at which you will create polygons. To create polygons for all scale levels included in a tiling scheme, leave this parameter blank. You may choose to create polygons for all or only some of the scale levels that are included in your tiling scheme. To add additional scale levels, however, you will need to modify your tiling scheme file or create a new one. ", "dataType": "Double"}]},
{"syntax": "TrackIntervalsToFeature_TA (in_features, time_field, {track_id_field}, {calculation_method}, {time_field_format}, {locale_id}, {am_designator}, {pm_designator}, {distance_field_units}, {distance_field_name}, {duration_field_units}, {duration_field_name}, {speed_field_units}, {speed_field_name}, {course_field_units}, {course_field_name})", "name": "Track Intervals To Feature (Tracking Analyst)", "description": "Calculates values that are computed from the difference between successively ordered features in a track. New fields are added to the input feature class or layer to store the calculated values (distance, duration, speed, and course).", "example": {"title": "TrackIntervalsToFeature example using date field and default units and output field names", "description": "This sample shows how to run the tool on a feature class with a date field using the default units and output field names.", "code": "import arcpy arcpy.CheckOutExtension ( \"tracking\" ) in_features = \"C:\\Data\\Vehicles.gdb\\Planes\" time_field = \"DATE_TIME\" track_id_field = \"ACID\" arcpy.TrackIntervalsToFeature_ta ( in_features , time_field , track_id_field )"}, "usage": ["For this tool to work, the input feature class or layer must have date and time information contained in a single time field of data type text, short, long, float, double, or date. If the time field data type is date, the tool will automatically detect the format.   If the time field data type is anything other than  date, a time field format (and possibly other information for a time field data type of text) is required to parse the data values correctly.", "If the time field selected is text, short, long, float, or double, the time field format can be selected from a list of ", "supported time field formats", ", or you can define a custom time field format to interpret custom date and/or time values in a text field.  For more information about custom formats for text fields, refer to ", "converting string time values into date format", ".", "\r\nChoosing  the names of the output fields is optional.  If you do not enter output field names, the tool will automatically generate output field names that include an abbreviation of the units."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "time_field", "isOptional": false, "description": "The field in the input feature class or layer that contains date and time information. This tool requires date and time information to be contained in the same field, and the data type of the field must be Short, Long, Float, Double, Text, or Date. ", "dataType": "Field"}, {"name": "track_id_field", "isOptional": true, "description": "The field that contains data values which are used to group the input features into tracks. The data type of the field can be Short, Long, Float, Double, Text, or OID. ", "dataType": "Field"}, {"name": "calculation_method", "isOptional": true, "description": " Specifies which interval is used to calculate values for each feature. PREVIOUS_AND_CURRENT_FEATURE \u2014 Values are calculated using the interval between each feature and the previous feature in the track. CURRENT_AND_NEXT_FEATURE \u2014 Values are calculated using the interval between each feature and the next feature in the track.", "dataType": "String"}, {"name": "time_field_format", "isOptional": true, "description": " If the data type of the time field is anything other than Date, this parameter determines the format that will be used to interpret data values in the time field. Some examples of formats are: If the data type of the time field is Text, either a standard Esri text time format can be used or a custom format can be specified. However, custom formats cannot be used if you specified KEEP_ON_DISK for the storage policy. If the data type of the time field is numeric (Short, Long, Float, or Double), only standard Esri numeric time formats can be used. If the data type of the time field is Date, this parameter is not needed. \" yyyyMMdd \" (standard format valid for Text or Numeric time fields) \" yyyy/MM/dd HH:mm:ss \" (standard format valid only for Text time fields) \" MM-dd-yyyy hh:mm:ss tt \" (custom format valid only for Text time fields)", "dataType": "String"}, {"name": "locale_id", "isOptional": true, "description": "If the data type of the time field is Text, this parameter determines which locale will be used to interpret data values in the time field. For all time field data types other than Text, this parameter is not needed. If no locale is entered, the current locale of the operating system will be used. For a list of available locales supported by your system, open the tool dialog box and expand this drop-down list. When entering the locale as a parameter, it is recommended to use only the locale ID (LCID) assigned by Microsoft, which can be entered as a long integer such as 1033 . You can also enter the full string representation of the locale as a parameter, such as \"01033-English_(United_States)\" , but you must replace spaces with underscore characters. ", "dataType": "Long"}, {"name": "am_designator", "isOptional": true, "description": "If the time field data type is Text and the time format is a 12-hour clock representation including a time marker (\" t \" or \" tt \"), then this parameter determines the character (\" t \") or characters (\" tt \") that designate AM in the time field data values. If nothing is entered, then the default AM designator for the selected locale will be used. For all time field data types other than Text, this parameter is not needed. ", "dataType": "String"}, {"name": "pm_designator", "isOptional": true, "description": "If the time field data type is Text and the time format is a 12-hour clock representation including a time marker (\" t \" or \" tt \"), then this parameter determines the character (\" t \") or characters (\" tt \") that designate PM in the time field data values. If nothing is entered, then the default PM designator for the selected locale will be used. For all time field data types other than Text, this parameter is not needed. ", "dataType": "String"}, {"name": "distance_field_units", "isOptional": true, "description": " Specifies the distance units that will be used in the output distance field. INCHES \u2014 Inches FEET \u2014 Feet YARDS \u2014 Yards MILES \u2014 Miles NAUTICAL_MILES \u2014 Nautical Miles MILLIMETERS \u2014 Millimeters CENTIMETERS \u2014 Centimeters METERS \u2014 Meters KILOMETERS \u2014 Kilometers DECIMETERS \u2014 Decimeters", "dataType": "String"}, {"name": "distance_field_name", "isOptional": true, "description": " Specifies the name of the distance field that will be added to the input feature class or layer. If no field name is specified, a name will automatically be chosen. ", "dataType": "String"}, {"name": "duration_field_units", "isOptional": true, "description": " Specifies the time units that will be used in the output duration field. MILLISECONDS \u2014 Milliseconds SECONDS \u2014 Seconds MINUTES \u2014 Minutes HOURS \u2014 Hours DAYS \u2014 Days WEEKS \u2014 Weeks MONTHS \u2014 Months YEARS \u2014 Years", "dataType": "String"}, {"name": "duration_field_name", "isOptional": true, "description": " Specifies the name of the duration field that will be added to the input feature class or layer. If no field name is specified, a name will automatically be chosen. ", "dataType": "String"}, {"name": "speed_field_units", "isOptional": true, "description": " Specifies the speed units that will be used in the output speed field. MILES_PER_HOUR \u2014 Miles Per Hour FEET_PER_HOUR \u2014 Feet Per Hour KILOMETERS_PER_HOUR \u2014 Kilometers Per Hour MILES_PER_SECOND \u2014 Miles Per Second FEET_PER_SECOND \u2014 Feet Per Second METERS_PER_SECOND \u2014 Meters Per Second KNOTS \u2014 Knots", "dataType": "String"}, {"name": "speed_field_name", "isOptional": true, "description": " Specifies the name of the speed field that will be added to the input feature class or layer. If no field name is specified, a name will automatically be chosen. ", "dataType": "String"}, {"name": "course_field_units", "isOptional": true, "description": " Specifies the course units that will be used in the output course field. DEGREES \u2014 Degrees RADIANS \u2014 Radians", "dataType": "String"}, {"name": "course_field_name", "isOptional": true, "description": " Specifies the name of the course field that will be added to the input feature class or layer. If no field name is specified, a name will automatically be chosen. ", "dataType": "String"}]},
{"syntax": "TrackIntervalsToLine_TA (in_features, out_feature_class, time_field, {track_id_field}, {time_field_format}, {locale_id}, {am_designator}, {pm_designator}, {distance_field_units}, {distance_field_name}, {duration_field_units}, {duration_field_name}, {speed_field_units}, {speed_field_name}, {course_field_units}, {course_field_name})", "name": "Track Intervals To Line (Tracking Analyst)", "description": "Calculates values that are computed from the difference between successively ordered features in a track. A new line feature class is created to represent the track intervals and store the calculated values (distance, duration, speed, and course).", "example": {"title": "TrackIntervalsToLine example using date field and default units and output field names", "description": "This sample shows how to run the tool on a feature class with a date field using the default units and output field names.", "code": "import arcpy arcpy.CheckOutExtension ( \"tracking\" ) in_features = \"C:\\Data\\Vehicles.gdb\\Planes\" out_feature_class = \"C:\\Data\\Vehicles.gdb\\Plane_Intervals\" time_field = \"DATE_TIME\" track_id_field = \"ACID\" arcpy.TrackIntervalsToLine_ta ( in_features , out_feature_class , time_field , track_id_field )"}, "usage": ["For this tool to work, the input feature class or layer must have date and time information contained in a single time field of data type text, short, long, float, double, or date. If the time field data type is date, the tool will automatically detect the format.   If the time field data type is anything other than  date, a time field format (and possibly other information for a time field data type of text) is required to parse the data values correctly.", "If the time field selected is text, short, long, float, or double, the time field format can be selected from a list of ", "supported time field formats", ", or you can define a custom time field format to interpret custom date and/or time values in a text field.  For more information about custom formats for text fields, refer to ", "converting string time values into date format", ".", "This tool automatically generates the  output field names that store the calculated values in the output feature class, and the autogenerated name includes an abbreviation of the units used."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output line feature class that will be created. ", "dataType": "Feature Class"}, {"name": "time_field", "isOptional": false, "description": "The field in the input feature class or layer that contains date and time information. This tool requires date and time information to be contained in the same field, and the data type of the field must be short, long, float, double, text, or date. ", "dataType": "Field"}, {"name": "track_id_field", "isOptional": true, "description": "The field that contains data values that are used to group the input features into tracks. The data type of the field can be short, long, float, double, text, or OID. ", "dataType": "Field"}, {"name": "time_field_format", "isOptional": true, "description": " If the data type of the time field is anything other than date, this parameter determines the format that is used to interpret data values in the time field. Some examples of formats are as follows: If the data type of the time field is text, either a standard Esri text time format can be used, or a custom format can be specified. However, custom formats cannot be used if you specified KEEP_ON_DISK for the storage policy. If the data type of the time field is numeric (short, long, float, or double), only standard Esri numeric time formats can be used. If the data type of the time field is date, this parameter is not needed. \" yyyyMMdd \" (standard format valid for text or numeric time fields) \" yyyy/MM/dd HH:mm:ss \" (standard format valid only for text time fields) \" MM-dd-yyyy hh:mm:ss tt \" (custom format valid only for text time fields)", "dataType": "String"}, {"name": "locale_id", "isOptional": true, "description": "If the data type of the time field is text, this parameter determines which locale is used to interpret data values in the time field. For all time field data types other than text, this parameter is not needed. If no locale is entered, the current locale of the operating system is used. For a list of available locales supported by your system, open the tool dialog box and expand this drop-down list. When entering the locale as a parameter, it is recommended that you use only the locale ID (LCID) assigned by Microsoft, which can be entered as a long integer such as 1033 . You can also enter the full string representation of the locale as a parameter, such as \"01033-English_(United_States)\" , but you must replace spaces with underscore characters. ", "dataType": "Long"}, {"name": "am_designator", "isOptional": true, "description": "If the time field data type is text and the time format is a 12-hour clock representation including a time marker ( t or tt ), this parameter determines the character ( t ) or characters ( tt ) that designate AM in the time field data values. If nothing is entered, the default AM designator for the selected locale is used. For all time field data types other than text, this parameter is not needed. ", "dataType": "String"}, {"name": "pm_designator", "isOptional": true, "description": "If the time field data type is text and the time format is a 12-hour clock representation including a time marker ( t or tt ), this parameter determines the character ( t ) or characters ( tt ) that designate PM in the time field data values. If nothing is entered, the default PM designator for the selected locale is used. For all time field data types other than text, this parameter is not needed. ", "dataType": "String"}, {"name": "distance_field_units", "isOptional": true, "description": " Specifies the distance units that will be used in the output distance field. INCHES \u2014 Inches FEET \u2014 Feet YARDS \u2014 Yards MILES \u2014 Miles NAUTICAL_MILES \u2014 Nautical miles MILLIMETERS \u2014 Millimeters CENTIMETERS \u2014 Centimeters METERS \u2014 Meters KILOMETERS \u2014 Kilometers DECIMETERS \u2014 Decimeters", "dataType": "String"}, {"name": "distance_field_name", "isOptional": true, "description": " Specifies the name of the distance field that will be added to the input feature class or layer. If no field name is specified, a name is automatically chosen. ", "dataType": "String"}, {"name": "duration_field_units", "isOptional": true, "description": " Specifies the time units that will be used in the output duration field. MILLISECONDS \u2014 Milliseconds SECONDS \u2014 Seconds MINUTES \u2014 Minutes HOURS \u2014 Hours DAYS \u2014 Days WEEKS \u2014 Weeks MONTHS \u2014 Months YEARS \u2014 Years", "dataType": "String"}, {"name": "duration_field_name", "isOptional": true, "description": " Specifies the name of the duration field that will be added to the input feature class or layer. If no field name is specified, a name is automatically chosen. ", "dataType": "String"}, {"name": "speed_field_units", "isOptional": true, "description": " Specifies the speed units that will be used in the output speed field. MILES_PER_HOUR \u2014 Miles per hour FEET_PER_HOUR \u2014 Feet per hour KILOMETERS_PER_HOUR \u2014 Kilometers per hour MILES_PER_SECOND \u2014 Miles per second FEET_PER_SECOND \u2014 Feet per second METERS_PER_SECOND \u2014 Meters per second KNOTS \u2014 Knots", "dataType": "String"}, {"name": "speed_field_name", "isOptional": true, "description": " Specifies the name of the speed field that will be added to the input feature class or layer. If no field name is specified, a name is automatically chosen. ", "dataType": "String"}, {"name": "course_field_units", "isOptional": true, "description": " Specifies the course units that will be used in the output course field. DEGREES \u2014 Degrees RADIANS \u2014 Radians", "dataType": "String"}, {"name": "course_field_name", "isOptional": true, "description": " Specifies the name of the course field that will be added to the input feature class or layer. If no field name is specified, a name is automatically chosen. ", "dataType": "String"}]},
{"syntax": "MakeTrackingLayer_TA (in_features, out_layer, time_zone, adjusted_for_dst, storage_policy, start_time_field, {time_field_format}, {locale_id}, {am_designator}, {pm_designator}, {track_id_field})", "name": "Make Tracking Layer (Tracking Analyst)", "description": "This tool creates a tracking layer from a feature class or layer  containing temporal data.", "example": {"title": "MakeTrackingLayer example using feature class with time values stored in date field", "description": "This sample shows how to run the tool on the feature class whose time field is of field type date. This sample uses the Tracking Analyst tutorial data.", "code": "import arcpy arcpy.CheckOutExtension ( \"tracking\" ) inputGDB = \"C:/arcgis/ArcTutor/Tracking Analyst/Simple/Hurricanes.gdb/atlantic_hurricanes_2000\" timezone = \"Eastern_Standard_Time\" arcpy.MakeTrackingLayer_ta ( inputGDB , \"Hurricanes\" , timezone , \"ADJUSTED_FOR_DST\" , \"COPY_ALL_TO_MEMORY\" , \"Date_Time\" )"}, "usage": ["For this tool to work, the input feature class or layer must have date and time information contained in a single time field of data type text, short, long, float, double, or date. If the time field data type is date, the tool will automatically detect the format.   If the time field data type is anything other than  date, a time field format (and possibly other information for a time field data type of text) is required to parse the data values correctly.", "If the time field selected is text, short, long, float, or double, the time field format can be selected from a list of ", "supported time field formats", ", or you can define a custom time field format to interpret custom date and/or time values in a text field.  For more information about custom formats for text fields, refer to ", "converting string time values into date format", ".", "If possible, it is recommended to use a time field of data type date with this tool.  The ", "Convert Time Field", " geoprocessing tool can be used to create a date field in your input feature class or layer before using this tool.", "If your feature class or layer contains date and time information in two separate text  fields, the ", "Concatenate Date and Time Fields", " tool can be used to combine the information into a single text  field before using this tool.", "Specifying a track identifier field for the output tracking layer is optional.", "This tool allows you to specify a storage policy for the output tracking layer. The default setting is to store the output tracking layer in memory, but choosing the option to store the output tracking layer on disk allows you to create tracking layers from larger data sets.  If you choose to store the output layer on disk and  your time field is a text or numeric data type, only ", "supported time field formats", " can be used."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": "The name of the tracking layer to be created. The newly created tracking layer can be used as input to any geoprocessing tool that accepts a feature layer as input. ", "dataType": "Feature Layer"}, {"name": "time_zone", "isOptional": false, "description": "The time zone that the data in the input feature class was recorded in. For a list of available time zones supported by your system, you can open the tool dialog box and expand this drop-down. When you enter the time zone as a parameter, you must use a non-localized string representation of the appropriate Microsoft Time Zone ID, and replace any spaces with underscore characters. For example, the appropriate string representation of the Pacific time zone in the United States is \"Pacific_Standard_Time\". \"NO_TIME_ZONE\" can be used to specify no time zone for the output tracking layer. ", "dataType": "String"}, {"name": "adjusted_for_dst", "isOptional": false, "description": "If you chose a time zone for your data, this parameter specifies whether the data values in the input time field were recorded with an adjustment for Daylight Saving Time. ADJUSTED_FOR_DST \u2014 The data values in the input time field were recorded with an adjustment for Daylight Saving Time. NOT_ADJUSTED_FOR_DST \u2014 The data values in the input time field were recorded without an adjustment for Daylight Saving Time.", "dataType": "Boolean"}, {"name": "storage_policy", "isOptional": false, "description": "Determines the storage policy for the output tracking layer. COPY_ALL_TO_MEMORY \u2014 The output tracking layer will be stored completely in memory. KEEP_ON_DISK \u2014 The output tracking layer will use a disk-based storage system. This option should only be used when the input feature layer is very large.", "dataType": "String"}, {"name": "start_time_field", "isOptional": false, "description": "The field in the input feature class or layer that contains date and time information. This tool requires date and time information to be contained in the same field, and the data type of the field must be Short, Long, Float, Double, Text, or Date. ", "dataType": "Field"}, {"name": "time_field_format", "isOptional": true, "description": " If the data type of the time field is anything other than Date, this parameter determines the format that will be used to interpret data values in the time field. Some examples of formats are: If the data type of the time field is Text, either a standard Esri text time format can be used or a custom format can be specified. However, custom formats can not be used if you specified KEEP_ON_DISK for the storage policy. If the data type of the time field is numeric (Short, Long, Float, or Double), only standard Esri numeric time formats can be used. If the data type of the time field is Date, this parameter is not needed. \" yyyyMMdd \" (standard format valid for Text or Numeric time fields) \" yyyy/MM/dd HH:mm:ss \" (standard format valid only for Text time fields) \" MM-dd-yyyy hh:mm:ss tt \" (custom format valid only for Text time fields)", "dataType": "String"}, {"name": "locale_id", "isOptional": true, "description": "If the data type of the time field is Text, this parameter determines which locale will be used to interpret data values in the time field. For all time field data types other than Text, this parameter is not needed. If no locale is entered, the current locale of the operating system will be used. For a list of available locales supported by your system, open the tool dialog box and expand this drop-down. When entering the locale as a parameter, it is recommended to use only the locale ID (LCID) assigned by Microsoft, which can be entered as a long integer such as 1033 . You can also enter the full string representation of the locale as a parameter, such as \"01033-English_(United_States)\" , but you must replace spaces with underscore characters. ", "dataType": "Long"}, {"name": "am_designator", "isOptional": true, "description": "If the time field data type is Text and the time format is a 12-hour clock representation including a time marker (\" t \" or \" tt \"), then this parameter determines the character (\" t \") or characters (\" tt \") that designate AM in the time field data values. If nothing is entered, then the default AM designator for the selected locale will be used. For all time field data types other than Text, this parameter is not needed. ", "dataType": "String"}, {"name": "pm_designator", "isOptional": true, "description": "If the time field data type is Text and the time format is a 12-hour clock representation including a time marker (\" t \" or \" tt \"), then this parameter determines the character (\" t \") or characters (\" tt \") that designate PM in the time field data values. If nothing is entered, then the default PM designator for the selected locale will be used. For all time field data types other than Text, this parameter is not needed. ", "dataType": "String"}, {"name": "track_id_field", "isOptional": true, "description": "The field that contains data values that will be used to group features into tracks in the output tracking layer. The data type of the field can be Short, Long, Float, Double, Text, or OID. ", "dataType": "Field"}]},
{"syntax": "ConcatenateDateAndTimeFields_TA (Feature_Class, DateField, TimeField, OutputField)", "name": "Concatenate Date And Time Fields (Tracking Analyst)", "description": "Concatenates two separate date and time fields in a feature class or layer into a single field containing both the date and time. Tracking Analyst is designed to work with temporal data containing date and time information in a single field. If your data contains the date and time in two separate fields, this tool can be used to combine the information together into a new field that Tracking Analyst will understand.", "example": {"title": "ConcatenateDateAndTimeFields example (Python window)", "description": "This sample shows how to run the tool to concatenate the \"MY_DATE\" and \"MY_TIME\" fields, creating a new field named \"NEW_DATE_TIME\".", "code": "import arcpy arcpy.CheckOutExtension ( \"tracking\" ) inputGDB = \"C:/arcgis/ArcTutor/Tracking Analyst/My_Geodatabase.gdb/my_feature_class\" arcpy.ConcatenateDateAndTimeFields_ta ( inputGDB , \"MY_DATE\" , \"MY_TIME\" , \"NEW_DATE_TIME\" )"}, "usage": ["The input date and time fields must have a field data type of text.", "The new field added to the feature layer by this tool will also have a field data type of text."], "parameters": [{"name": "Feature_Class", "isOptional": false, "description": "The input feature class or layer. ", "dataType": "Feature Layer"}, {"name": "DateField", "isOptional": false, "description": "The text field in the input feature layer that contains date values. ", "dataType": "Field"}, {"name": "TimeField", "isOptional": false, "description": "The text field in the input feature layer that contains time values. ", "dataType": "Field"}, {"name": "OutputField", "isOptional": false, "description": "The name of the new concatenated date/time field to be created and added to the input feature layer. ", "dataType": "String"}]},
{"syntax": "GroupingAnalysis_stats (Input_Features, Unique_ID_Field, Output_Feature_Class, Number_of_Groups, Analysis_Fields, Spatial_Constraints, {Distance_Method}, {Number_of_Neighbors}, {Weights_Matrix_File}, {Initialization_Method}, {Initialization_Field}, {Output_Report_File}, {Evaluate_Optimal_Number_of_Groups})", "name": "Grouping Analysis (Spatial Statistics)", "description": "Groups features based on feature attributes and optional spatial/temporal constraints. \r\n Learn more about how Grouping Analysis works \r\n", "example": {"title": "GroupingAnalysis example 1 (Python window)", "description": "The following Python window script demonstrates how to use the GroupingAnalysis tool.", "code": "import arcpy import arcpy.stats as SS arcpy.env.workspace = r\"C:\\GA\" SS.GroupingAnalysis ( \"Dist_Vandalism.shp\" , \"TARGET_FID\" , \"outGSF.shp\" , \"4\" , \"Join_Count;TOTPOP_CY;VACANT_CY;UNEMP_CY\" , \"NO_SPATIAL_CONSRAINT\" , \"EUCLIDEAN\" , \"\" , \"\" , \"FIND_SEED_LOCATIONS\" , \"\" , \"outGSF.pdf\" , \"DO_NOT_EVALUATE\" )"}, "usage": ["This tool produces an output feature class with the fields used in the analysis plus a new Integer field called SS_GROUP.  Default rendering is based on the SS_GROUP field and shows you which group each feature falls into. If you indicate you want 3 groups, for example, each record will contain a 1, 2, or 3 for the SS_GROUP field.  When ", "NO_SPATIAL CONSTRAINT", " is selected for the ", "Spatial Constraints", " parameter, the output feature class will also contain a new binary field called SS_SEED.  The SS_SEED field indicates which features were used as starting points to grow groups.  The number of nonzero values in the SS_SEED field will match the value you entered for the ", "Number of Groups", " parameter.     ", "This tool will optionally create a PDF Report File when you specify a path for the ", "Output Report File", " parameter.  This report contains a variety of tables and graphs to help you understand the characteristics of the groups identified.  The PDF report file is  accessible through the ", "Results window", ".  ", "Creating the report file can add substantial processing time.  Consequently, while ", "Grouping Analysis", " will create the ", "Output Feature Class", " showing group membership, the PDF report file will not be created if you specify ", "more than 15 groups or more than 15 variables", ".  ", "The ", "Unique ID Field", " provides a way for you to link records in the ", "Output Feature Class", " back to data in the original input feature class. Consequently, the ", "Unique ID Field", " values must be unique for every feature, and typically should be a permanent field that remains with the feature class. If you don't have a ", "Unique ID Field", " in your dataset, you can easily create one by ", "adding", " a new integer field to your feature class table and ", "calculating", " the field values to be equal to the FID/OID field. You cannot use the FID/OID field directly for the ", "Unique ID Field", " parameter.", "The ", "Analysis Fields", " should be numeric and should contain a variety of values. Fields with no variation (that is, the same value for every record) will be dropped from the analysis but will be included in the ", "Output Feature Class", ".  Categorical fields may be used with the ", "Grouping Analysis", " tool if they are represented as dummy variables (a value of one  for all features in a category, zeros for all other features).", "The ", "Grouping Analysis", " tool will construct groups with or without space/time constraints.  For some applications you may not want to impose contiguity or other proximity requirements on the groups created.  In those cases you will set the ", "Spatial Constraints", " parameter to ", "NO_SPATIAL_CONSTRAINT", ".", "For some analyses, you will want groups to be spatially contiguous.  The CONTIGUITY options are enabled for polygon feature classes and indicate features can only be part of the same group if they share an edge (", "CONTIGUITY_EDGES_ONLY", ") or if they share either an edge or a vertex (", "CONTIGUITY_EDGES_CORNERS", ") with another member of the group.  ", "The ", "DELAUNAY_TRIANGULATION", " and ", "K_NEAREST_NEIGHBORS", " options are appropriate for point or polygon features when you want to ensure all group members are proximal.    These options indicate that a feature will only be included in a  group if at least one other feature is a natural neighbor (Delaunay Triangulation) or a K Nearest Neighbor.   K is the number of neighbors to consider and is specified using the ", "Number of Neighbors", " parameter.", "In order to create groups with both space and time constraints, use the ", "Generate Spatial Weights Matrix", " tool to first create a ", "spatial weights matrix file", " (SWM file) defining the space-time relationships among your features.  Next run ", "Grouping Analysis", " setting the ", "Spatial Constraints", " parameter to ", "GET_SPATIAL_WEIGHTS_FROM_FILE", " and the ", "Spatial Weights Matrix File", " parameter to the SWM file you created.", "Additional ", "Spatial Constraints", ", such as Fixed Distance, may be imposed by using the ", "Generate Spatial Weights Matrix", " tool to first create an SWM file and then providing the path to that file for the ", "Spatial Weights Matrix File", " parameter.  ", "Even though you may create a spatial weights matrix (SWM) file to define spatial constraints, there is no actual weighting being applied.  The SWM simply defines which features are contiguous or proximal.  Imposing a spatial constraint determines who can and cannot be members of the same group.  If you select ", "CONTIGUITY_EDGES_ONLY", ", for example, all the features in a single group will have at least one edge in common with another feature in the group.  This keeps the resultant groups spatially contiguous.", "Defining a spatial constraint ensures compact, contiguous, or proximal groups.  Including spatial variables in your list of ", "Analysis Fields", " can also encourage these group attributes.  Examples of spatial variables would be distance to freeway onramps, accessibility to job openings, proximity to shopping opportunities, measures of connectivity and even coordinates (X, Y).  Including variables representing time, day of the week, or temporal distance can encourage temporal compactness among group members.  ", "When there is a distinct spatial pattern to your features (an example would be three separate, spatially distinct, clusters), it can complicate the spatially constrained grouping algorithm.  Consequently, the grouping algorithm first determines if there are any disconnected groups.  If the number of disconnected groups is larger than the ", "Number of Groups", " specified, the tool cannot solve and will fail with an appropriate error message.  If the number of disconnected groups is exactly the same as the ", "Number of Groups", " specified, the spatial configuation of the features alone, determines group results, as shown in (A) below.  If the ", "Number of Groups", " specified is larger than the number of disconnected groups, grouping begins with the disconnected groups already determined.  For example, if there are three disconnected groups and the ", "Number of Groups", " specified is ", "4", ", one of the three groups will be divided to create a fourth group, as shown in (B) below.  ", " ", "In some cases, the ", "Grouping Analysis", " tool will not be able to meet the spatial constraints imposed, and some features will not be included with any group (the ", "SS_GROUP", " value will be -9999 with hollow rendering).  This happens if there are features with no neighbors.  To avoid this, use ", "K_NEAREST_NEIGHBORS", " which ensures all features have neighbors.  Increasing the ", "Number of Neighbors", " parameter will help resolve issues with disconnected groups.", "While there is a tendency to want to include as many ", "Analysis Fields", " as possible, for this tool it works best to start with a single variable and build.  Results are much easier to interpret with fewer analysis fields.  It is also easier to determine which variables are the best discriminators when there are fewer fields.", "When you select ", "NO_SPATIAL_CONSTRAINT", " for the ", "Spatial Constraints", " parameter, you have three options for the ", "Initialization Method", ": ", "FIND_SEED_LOCATIONS", ", ", "GET_SEEDS_FROM_FIELD", ", and ", "USE_RANDOM_SEEDS", ".  Seeds are the features used to grow individual groups.  If, for example, you enter a ", "3", " for the ", "Number of Groups", " parameter, the analysis will begin with three seed features.  The default option, ", "FIND_SEED_LOCATIONS", ", randomly selects the first seed, then makes sure that the subsequent seeds selected represent features that are far away from each other in data space.  Selecting initial seeds that capture different areas of data space improves performance.  Sometimes, you know that specific features reflect distinct characteristics that you will want represented by different groups.  In that case, create a seed field to identify those  distinctive features.  The seed field you create should have zeros for all but the initial seed features; the initial seed features should have a value of ", "1", ".  You will then select ", "GET_SEEDS_FROM_FIELD", " for the ", "Initialization Method", " parameter.  If you are interested in doing some kind of sensitivity analysis to see which features are always found in the same group, you might elect the ", "USE_RANDOM_SEEDS", " option for the ", "Initialization Method", " parameter. For this option, all of the seed features are randomly selected.", "Any values of ", "1", " in the ", "Initialization Field", " will be interpreted as a seed.  If there are more seed features than ", "Number of Groups", ", the seed features will be randomly selected from those identified by the  ", "Initialization Field", ".  If there are fewer seed features than specified by ", "Number of Groups", ", the additional seed features will be selected so they are far away (in data space) from those identified by the ", "Initialization Field", ".", "Sometimes you know the ", "Number of Groups", " most appropriate for your data.  In the case that you don't, however, you may have to try different numbers of groups, noting which values provide the best group differentiation.  When you check the ", "Evaluate Optimal Number of Groups", " parameter, a pseudo F-statistic will be computed for grouping solutions with 2 through 15 groups.  If no other criteria guide your choice for ", "Number of Groups", ", use a number associated with one of the largest pseudo F-statistic values.   The largest F-statistic values indicate solutions that perform best at maximizing both within group similarities and between group differences. When you specify an optional ", "Output Report File", ", that PDF report will include a graph showing the  F-statistic values for solutions with 2 through 15 groups.", "When you include a spatial or space-time constraint in your analysis, the pseudo F-Statistics are comparable (as long as the ", "Input Features", " and ", "Analysis Fields", " don't change).  Consequently, you can use the F-Statistic values to determine not only optimal  ", "Number of Groups", ", but also to help you make choices about the most effective ", "Spatial Constraints", " option, ", "Distance Method", ", and ", "Number of Neighbors", ".  ", "The K-Means algorithm used to partition features into groups when ", "NO_SPATIAL_CONSTRAINT", " is selected for the ", "Spatial Constraints", " parameter and ", "FIND_SEED_LOCATIONS", " or ", "USE_RANDOM_SEEDS", " is selected for the ", "Initialization Method", ", incorporates heuristics  and may return a different result each time you run the tool (even using the same data and the same tool parameters).   This is because there is a random component to finding the initial seed features used to grow the groups. ", "When a spatial constraint is imposed, there is no random component to the algorithm, so a single pseudo F-Statistic can be computed for 2 through 15 groups, and the highest F-Statistic values can be used to determine the optimal ", "Number of Groups", " for your analysis.  Because the ", "NO_SPATIAL_CONSTRAINT", " option is a heuristic solution, however, determining the optimal number of groups is more involved.  The F-Statistic may be different each time the tool is run, due to different initial seed features.  When a distinct pattern exists in your data, however, solutions from one run to the next will be more consistent.  Consequently, to help determine the optimal number of groups when the ", "NO_SPATIAL_CONSTRAINT", " option is selected, the tool solves the grouping analysis 10 times for 2, 3, 4, and up to 15 groups.  Information about the distribution of  these 10 solutions are then reported (min, max, mean, and median) to help you determine an  optimal number of groups for your analysis.  ", "The ", "Grouping Analysis", " tool returns three derived output values for potential use in custom models and scripts.  These are the pseudo F-Statistic for the ", "Number of Groups", " (Output_FStat), the largest pseudo F-Statistic for groups 2 through 15 (Max_FStat), and the number of groups associated with the largest pseudo F-Statistic value (Max_FStat_Group).  When you do not elect to ", "Evaluate Optimal Number of Groups", ", all of the derived output variables are set to None.", "The group number assigned to a set of features may change from one run to the next.   For example, suppose you partition features into two groups based on an income variable.  The first time you run the analysis you might see the high income features labeled as group 2 and the low income features labeled as group 1; the second time you run the same analysis, the high income features might be labeled as  group 1.  You might also see that some of the middle income features switch group membership from one run to another when ", "NO_SPATIAL_CONSTRAINT", " is specified. ", "While you can select to create a very large number of different groups, in most scenarios you will likely be partitioning features into just a few groups.  Because the graphs and maps become difficult to interpret with lots of groups, no report is created when you enter a value larger than ", "15", " for the ", "Number of Groups", " parameter or select more than 15 ", "Analysis Fields", ".  You can increase this limitation on the maximum number of groups, however.", "Because you have the Python source code for the ", "Grouping Analysis", " tool, you may override the 15 variable/15 group report limitation if desired.  This upper limit is set by two variables in both the ", "Partition.py", " script file and the tool's ", "validation code", " inside the Spatial Statistics Toolbox:", "This tool will optionally create a PDF report summarizing results.  PDF files do not automatically appear in the ", "Catalog", " window.  If you want PDF files to be displayed in ", "Catalog", ", open the ", "ArcCatalog", " application, select the ", "Customize", "  menu option, click  ", "ArcCatalog Options", ", and select the ", "File Types", " tab.  Click on the ", "New Type", " button and specify ", "PDF", ", as shown below, for ", "File Extension", ".", "On machines configured with the ArcGIS language packages for Chinese, Japanese, or Arabic, you might notice missing text and/or formatting problems in the PDF ", "Output Report File", ".  These problems can be corrected by ", "changing the font settings", ". ", "For more information about the ", "Output Report File", ", see ", "Learn more about how Grouping Analysis works"], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": "The feature class or feature layer you want to create groups for. ", "dataType": "Feature Layer"}, {"name": "Unique_ID_Field", "isOptional": false, "description": "An integer field containing a different value for every feature in the Input Features dataset. ", "dataType": "Field"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The new output feature class created containing all features, the analysis fields specified, and a field indicating which group each feature belongs to. ", "dataType": "Feature Class"}, {"name": "Number_of_Groups", "isOptional": false, "description": "The number of groups to create. The Output Report parameter will be disabled for more than 15 groups. ", "dataType": "Long"}, {"name": "Analysis_Fields", "isOptional": false, "description": "A list of fields you want to use to distinguish one group from another. The Output Report parameter will be disabled for more than 15 fields. ", "dataType": "Field"}, {"name": "Spatial_Constraints", "isOptional": false, "description": "Specifies if and how spatial relationships among features should constrain the groups created. CONTIGUITY_EDGES_ONLY \u2014 Groups contain contiguous polygon features. Only polygons that share an edge can be part of the same group. CONTIGUITY_EDGES_CORNERS \u2014 Groups contain contiguous polygon features. Only polygons that share an edge or a vertex can be part of the same group. DELAUNAY_TRIANGULATION \u2014 Features in the same group will have at least one natural neighbor in common with another feature in the group. Natural neighbor relationships are based on Delaunay Triangulation. Conceptually, Delaunay Triangulation creates a nonoverlapping mesh of triangles from feature centroids. Each feature is a triangle node and nodes that share edges are considered neighbors. K_NEAREST_NEIGHBORS \u2014 Features in the same group will be near each other; each feature will be a neighbor of at least one other feature in the group. Neighbor relationships are based on the nearest K features where you specify an Integer value, K, for the Number of Neighbors parameter. GET_SPATIAL_WEIGHTS_FROM_FILE \u2014 Spatial, and optionally temporal, relationships are defined by a spatial weights file (.swm). Create the spatial weights matrix file using the Generate Spatial Weights Matrix tool. NO_SPATIAL_CONSTRAINT \u2014 Features will be grouped using data space proximity only. Features do not have to be near each other in space or time to be part of the same group.", "dataType": "String"}, {"name": "Distance_Method", "isOptional": true, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Number_of_Neighbors", "isOptional": true, "description": "This parameter is enabled whenever the Spatial Constraints parameter is K_NEAREST_NEIGHBORS or one of the CONTIGUITY methods. The default number of neighbors is 8. For K_NEAREST_NEIGHBORS , this integer value reflects the exact number of nearest neighbor candidates to consider when building groups. A feature will not be included in a group unless one of the other features in that group is a K nearest neighbor. For the CONTIGUITY methods, this value reflects the exact number of neighbor candidates to consider for island polygons only. Since island polygons have no contiguous neighbors, they will be assigned neighbors that are not contiguous but are close by. ", "dataType": "Long"}, {"name": "Weights_Matrix_File", "isOptional": true, "description": "The path to a file containing spatial weights that define spatial relationships among features. ", "dataType": "File"}, {"name": "Initialization_Method", "isOptional": true, "description": "Specifies how initial seeds are obtained when the Spatial Constraint parameter selected is NO_SPATIAL_CONSTRAINT. Seeds are used to grow groups. If you indicate you want 3 groups, for example, the analysis will begin with three seeds. FIND_SEED_LOCATIONS \u2014 Seed features will be selected to optimize performance. GET_SEEDS_FROM_FIELD \u2014 Nonzero entries in the Initialization Field will be used as starting points to grow groups. USE_RANDOM_SEEDS \u2014 Initial seed features will be selected randomly.", "dataType": "String"}, {"name": "Initialization_Field", "isOptional": true, "description": "The numeric field identifying seed features. Features with a value of 1 for this field will be used to grow groups. ", "dataType": "Field"}, {"name": "Output_Report_File", "isOutputFile": true, "isOptional": true, "description": "The full path for the .pdf report file to be created summarizing group characteristics. This report provides a number of graphs to help you compare the characteristics of each group. Creating the report file can add substantial processing time. ", "dataType": "File"}, {"name": "Evaluate_Optimal_Number_of_Groups", "isOptional": true, "description": " EVALUATE \u2014 Groupings from 2 to 15 will be evaluated. DO_NOT_EVALUATE \u2014 No evaluation of the number of groups will be performed. This is the default.", "dataType": "Boolean"}]},
{"syntax": "ExploratoryRegression_stats (Input_Features, Dependent_Variable, Candidate_Explanatory_Variables, {Weights_Matrix_File}, {Output_Report_File}, {Output_Results_Table}, {Maximum_Number_of_Explanatory_Variables}, {Minimum_Number_of_Explanatory_Variables}, {Minimum_Acceptable_Adj_R_Squared}, {Maximum_Coefficient_p_value_Cutoff}, {Maximum_VIF_Value_Cutoff}, {Minimum_Acceptable_Jarque_Bera_p_value}, {Minimum_Acceptable_Spatial_Autocorrelation_p_value})", "name": "Exploratory Regression (Spatial Statistics)", "description": "\r\nThe Exploratory Regression tool evaluates all possible combinations of the input candidate  explanatory variables , looking for  OLS  models that best explain the  dependent variable  within the context of user-specified criteria. \r\n You can access the results of this tool (including the optional report file) from the  Results  window.  If you disable background processing, results will also be written to the  Progress  dialog box. Learn more about how Exploratory Regression works", "example": {"title": "ExploratoryRegression example 1 (Python window)", "description": "The following Python window script demonstrates how to use the ExploratoryRegression tool.", "code": "import arcpy , os arcpy.env.workspace = r\"C:\\ER\" arcpy.ExploratoryRegression_stats ( \"911CallsER.shp\" , \"Calls\" , \"Pop;Jobs;LowEduc;Dst2UrbCen;Renters;Unemployed;Businesses;NotInLF;  \\                                 ForgnBorn;AlcoholX;PopDensity;MedIncome;CollGrads;PerCollGrd;  \\                                 PopFY;JobsFY;LowEducFY\" , \"BG_911Calls.swm\" , \"BG_911Calls.txt\" , \"\" , \"MAX_NUMBER_ONLY\" , \"5\" , \"1\" , \"0.5\" , \"0.05\" , \"7.5\" , \"0.1\" , \"0.1\" )"}, "usage": ["The primary output for this tool is a report file which is written to the ", "Results", " window.  Right-clicking on the ", "Messages entry", " in the ", "Results", " window and selecting ", "View", " will display the ", "Exploratory Regression", " summary report in a ", "Message dialog box", ". ", "This tool will optionally create a text file report summarizing results.  This report file will be added to the table of contents (TOC) and may be viewed in ArcMap by right-clicking on it and selecting ", "Open", ".", "This tool also produces an optional table of all models meeting your maximum ", "coefficient p-value", " cutoff and ", "Variance Inflation Factor (VIF)", " value criteria.  A full explanation of the report elements and table is provided in ", "Interpreting Exploratory Regression Results", ". ", "This tool uses ", "Ordinary Least Squares (OLS)", " and ", "Spatial Autocorrelation (Global Moran's I)", ".\r\nThe optional ", "spatial weights matrix file", " is used with the ", "Spatial Autocorrelation (Global Moran's I)", " tool to assess model residuals; it is not used by the ", "OLS", " tool at all.  ", "This tool tries every combination of the ", "Candidate Explanatory Variables", " entered, looking for a properly specified OLS model.  Only when it finds a model that meets your threshold criteria for ", "Minimum Acceptable Adj R Squared", ", ", "Maximum Coefficient p-value Cutoff", ", ", "Maximum VIF Value Cutoff", " and ", "Minimum Acceptable Jarque-Bera p-value", " will it run the ", "Spatial Autocorrelation (Global Moran's I)", " tool on the model residuals to see if the under/over-predictions are clustered or not.  In order to provide at least some information about residual clustering in the case where none of the models pass all of these criteria, the ", "Spatial Autocorrelation (Global Moran's I)", " test is also applied to the residuals for the three models that have the highest Adjusted R", "2", " values and the three models that have the largest ", "Jarque-Bera p-values", ".", "Especially when there is strong spatial structure in your dependent variable, you will want to try to come up with as many candidate spatial explanatory variables as you can.  Some examples of spatial variables would be distance to major highways, accessibility to job opportunities, number of local shopping opportunities, connectivity measurements, or densities.  Until you find explanatory variables that capture the spatial structure in your dependent variable, model residuals will likely not pass the spatial autocorrelation test.  Significant clustering in regression residuals, as determined by the  ", "Spatial Autocorrelation (Global Moran's I)", " tool, indicates model misspecification.  Strategies for dealing with misspecification are outlined in ", "What they don't tell you about regression analysis", ".", "Because the ", "Spatial Autocorrelation (Global Moran's I)", " is not run for all of the models tested (see the previous usage tip), the table will have missing data for the SA (Spatial Autocorrelation) field.  Because ", ".dbf", " files do not store null values, these appear as very, very small (negative) numbers (something like -1.797693e+308).  For geodatabase tables, these missing values appear as null values.  A missing value indicates that the residuals for the associated model were not tested for spatial autocorrelation because the model did not pass all of the other model search criteria.", "The default ", "spatial weights matrix file", "  used to run the ", "Spatial Autocorrelation (Global Moran's I)", " tool is based on an  ", "8 nearest neighbor conceptualization of spatial relationships", ".  This default was selected primarily because it executes fairly quickly.  To define neighbor relationships differently, however, you can simply create your own spatial weights matrix file using the ", "Generate Spatial Weights Matrix File", " tool, then specify the name of that file for the ", "Input Spatial Weights Matrix File", " parameter.  Inverse Distance, Polygon Contiguity, or K Nearest Neighbors, are all appropriate ", "Conceptualizations of Spatial Relationships", " for testing regression residuals. ", "The spatial weights matrix file is only used to test model ", "residuals", " for spatial structure.  When a model is properly specified, the residuals are spatially random (large residuals are intermixed with small residuals; large residuals do not cluster together spatially).  "], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": "The feature class or feature layer containing the dependent and candidate explanatory variables to analyze. ", "dataType": "Feature Layer"}, {"name": "Dependent_Variable", "isOptional": false, "description": "The numeric field containing the observed values you want to model using OLS. ", "dataType": "Field"}, {"name": "Candidate_Explanatory_Variables", "isOptional": false, "description": "A list of fields to try as OLS model explanatory variables. ", "dataType": "Field"}, {"name": "Weights_Matrix_File", "isOptional": true, "description": "A file containing spatial weights that define the spatial relationships among your input features. This file is used to assess spatial autocorrelation among regression residuals. You can use the Generate Spatial Weights Matrix File tool to create this. When you do not provide a spatial weights matrix file, residuals are assessed for spatial autocorrelation based on each feature's 8 nearest neighbors. Note: The spatial weights matrix file is only used to analyze spatial structure in model residuals; it is not used to build or to calibrate any of the OLS models. ", "dataType": "File"}, {"name": "Output_Report_File", "isOutputFile": true, "isOptional": true, "description": "The report file contains tool results, including details about any models found that passed all the search criteria you entered. This output file also contains diagnostics to help you fix common regression problems in the case that you don't find any passing models. ", "dataType": "File"}, {"name": "Output_Results_Table", "isOutputFile": true, "isOptional": true, "description": " The optional output table created containing the explanatory variables and diagnostics for all of the models within the Coefficient p-value and VIF value cutoffs. ", "dataType": "Table"}, {"name": "Maximum_Number_of_Explanatory_Variables", "isOptional": true, "description": "All models with explanatory variables up to the value entered here will be assessed. If, for example, the Minimum_Number_of_Explanatory_Variables is 2 and the Maximum_Number_of Explanatory_Variables is 3, the Exploratory Regression tool will try all models with every combination of two explanatory variables, and all models with every combination of three explanatory variables. ", "dataType": "Long"}, {"name": "Minimum_Number_of_Explanatory_Variables", "isOptional": true, "description": "This value represents the minimum number of explanatory variables for models evaluated. If, for example, the Minimum_Number_of_Explanatory_Variables is 2 and the Maximum_Number_of_Explanatory_Variables is 3, the Exploratory Regression tool will try all models with every combination of two explanatory variables, and all models with every combination of three explanatory variables. ", "dataType": "Long"}, {"name": "Minimum_Acceptable_Adj_R_Squared", "isOptional": true, "description": " This is the lowest Adjusted R-Squared value you consider a passing model. If a model passes all of your other search criteria, but has an Adjusted R-Squared value smaller than the value entered here, it will not show up as a Passing Model in the Output Report File. Valid values for this parameter range from 0.0 to 1.0. The default value is 0.5, indicating that passing models will explain at least 50 percent of the variation in the dependent variable. ", "dataType": "Double"}, {"name": "Maximum_Coefficient_p_value_Cutoff", "isOptional": true, "description": " For each model evaluated, OLS computes explanatory variable coefficient p-values. The cutoff p-value you enter here represents the confidence level you require for all coefficients in the model in order to consider the model passing. Small p-values reflect a stronger confidence level. Valid values for this parameter range from 1.0 down to 0.0, but will most likely be 0.1, 0.05, 0.01, 0.001, and so on. The default value is 0.05, indicating passing models will only contain explanatory variables whose coefficients are statistically at the 95 percent confidence level (p-values smaller than 0.05). To relax this default you would enter a larger p-value cutoff, such as 0.1. If you are getting lots of passing models, you will likely want to make this search criteria more stringent by decreasing the default p-value cutoff from 0.05 to 0.01 or smaller. ", "dataType": "Double"}, {"name": "Maximum_VIF_Value_Cutoff", "isOptional": true, "description": " This value reflects how much redundancy (multicollinearity) among model explanatory variables you will tolerate. When the VIF (Variance Inflation Factor) value is higher than about 7.5, multicollinearity can make a model unstable; consequently, 7.5 is the default value here. If you want your passing models to have less redundancy, you would enter a smaller value, such as 5.0, for this parameter. ", "dataType": "Double"}, {"name": "Minimum_Acceptable_Jarque_Bera_p_value", "isOptional": true, "description": " The p-value returned by the Jarque-Bera diagnostic test indicates whether the model residuals are normally distributed. If the p-value is statistically significant (small), the model residuals are not normal and the model is biased. Passing models should have large Jarque-Bera p-values. The default minimum acceptable p-value is 0.1. Only models returning p-values larger than this minimum will be considered passing. If you are having trouble finding unbiased passing models, and decide to relax this criterion, you might enter a smaller minimum p-value such as 0.05. ", "dataType": "Double"}, {"name": "Minimum_Acceptable_Spatial_Autocorrelation_p_value", "isOptional": true, "description": " For models that pass all of the other search criteria, the Exploratory Regression tool will check model residuals for spatial clustering using Global Moran's I. When the p-value for this diagnostic test is statistically significant (small), it indicates the model is very likely missing key explanatory variables (it isn't telling the whole story). Unfortunately, if you have spatial autocorrelation in your regression residuals, your model is misspecified, so you cannot trust your results. Passing models should have large p-values for this diagnostic test. The default minimum p-value is 0.1. Only models returning p-values larger than this minimum will be considered passing. If you are having trouble finding properly specified models because of this diagnostic test, and decide to relax this search criteria, you might enter a smaller minimum such as 0.05. ", "dataType": "Double"}]},
{"syntax": "IncrementalSpatialAutocorrelation_stats (Input_Features, Input_Field, Number_of_Distance_Bands, {Beginning_Distance}, {Distance_Increment}, {Distance_Method}, {Row_Standardization}, {Output_Table}, {Output_Report_File})", "name": "Incremental Spatial Autocorrelation (Spatial Statistics)", "description": "\r\nMeasures spatial autocorrelation for a series of distances and optionally creates a line graph of those distances and their corresponding z-scores.  Z-scores reflect the intensity of spatial clustering, and statistically significant peak z-scores indicate distances where spatial processes promoting clustering are most pronounced.  These peak distances are often appropriate values to use for tools with a Distance Band or Distance Radius parameter.\r\n", "example": {"title": "IncrementalSpatialAutocorrelation example 1 (Python window)", "description": "The following Python window script demonstrates how to use the IncrementalSpatialAutocorrelation tool.", "code": "import arcpy , os import arcpy.stats as SS arcpy.env.workspace = r\"C:\\ISA\" SS.IncrementalSpatialAutocorrelation ( \"911CallsCount.shp\" , \"ICOUNT\" , \"20\" , \"\" , \"\" , \"EUCLIDEAN\" , \"ROW_STANDARDIZATION\" , \"outTable.dbf\" , \"outReport.pdf\" )"}, "usage": ["This tool can help you select an appropriate ", "Distance Threshold", " or ", "Radius", " for tools that have these parameters, such as ", "Hot Spot Analysis", " or ", "Point Density", ".", "The ", "Incremental Spatial Autocorrelation", " tool measures spatial autocorrelation for a series of distance increments and reports, for each distance increment, the associated Moran's Index, Expected Index, Variance, z-score and p-value.  These values are accessible from the ", "Results", " window by right-clicking on the  ", "Messages entry", " and selecting ", "View", ".  The tool tool also passes, as derived output, the first peak z-score and maximum peak z-score for potential use in models or scripts (see, for example, the sample script below).", "\r\nWhen more than one statistically significant peak is present, clustering is pronounced at each of those distances. Select the peak distance that best corresponds to the scale of analysis you are interested in; often this is the first statistically significant peak encountered.\r\n", "The ", "Input Field", " should contain a variety of values. The math for this statistic requires some variation in the variable being analyzed; it cannot solve if all input values are 1, for example. If you want to use this tool to analyze the spatial pattern of incident data, consider ", "aggregating your incident data", ".", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis.", "For polygon features, you will almost always want to choose ", "Row", " for the ", "Row Standardization", " parameter.  ", "Row Standardization", " mitigates bias when the number of neighbors each feature has is a function of the aggregation scheme or sampling process, rather than reflecting the actual spatial distribution of the variable you are analyzing.", "If no ", "Beginning Distance", " is given, the default distance is the distance at which each feature in the dataset has at least one neighbor. This may not be the most ", "appropriate beginning distance", " if your dataset includes spatial outliers. ", "If no ", "Increment Distance", " is given, the average nearest neighbor distance is used.", "  It is possible to run out of memory when you run this tool.  This generally occurs when you specify a ", "Beginning Distance", " and/or  ", "Increment Distance", " resulting in features having many, many neighbors.  You generally do not want to create spatial relationships where your features have thousands of neighbors.  Use a smaller value for ", "Increment Distance", " and temporarily ", "remove spatial outliers", " so that you can start with smaller ", "Beginning Distance", " values.", "Distances are always based on the ", "Output Coordinate System", " environment setting.   The default setting for the Output Coordinate System environment  is ", "Same as Input", ".  Input features are projected to the output ", "coordinate system", " prior to analysis.", "The optional ", "Output Table", " will contain the distance value at each iteration, the Moran's I Index value, the expected Moran's I index value, the variance, the z-score, and the p-value.  A peak would be an increase in the z-score value followed by a decrease in the z-score value.  For example, if this tool finds the following series of z-scores for 50, 100, and 150 meter distances, 2.95, 3.68, 3.12,  the peak would be 100 meters.  ", "The optional ", "Output Report File", " is created as a PDF file and may be accessed from the ", "Results", " window by double-clicking on the file name.", "This tool will optionally create a PDF report summarizing results.  PDF files do not automatically appear in the ", "Catalog", " window.  If you want PDF files to be displayed in ", "Catalog", ", open the ", "ArcCatalog", " application, select the ", "Customize", "  menu option, click  ", "ArcCatalog Options", ", and select the ", "File Types", " tab.  Click on the ", "New Type", " button and specify ", "PDF", ", as shown below, for ", "File Extension", ".", "On machines configured with the ArcGIS language packages for Chinese, Japanese, or Arabic, you might notice missing text and/or formatting problems in the PDF ", "Output Report File", ".  These problems can be corrected by ", "changing the font settings", ". ", "When no peak z-scores are identified, both the first peak z-score and maximum peak z-score derived output parameters return a blank.  "], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": "The feature class for which spatial autocorrelation will be measured over a series of distances. ", "dataType": "Feature Layer"}, {"name": "Input_Field", "isInputFile": true, "isOptional": false, "description": "The numeric field used in assessing spatial autocorrelation. ", "dataType": "Field"}, {"name": "Number_of_Distance_Bands", "isOptional": false, "description": "The number of times to increment the neighborhood size and analyze the dataset for spatial autocorrelation. The starting point and size of the increment are specified in the Beginning Distance and Distance Increment parameters, respectively. ", "dataType": "Long"}, {"name": "Beginning_Distance", "isOptional": true, "description": "The distance at which to start the analysis of spatial autocorrelation and the distance from which to increment. The value entered for this parameter should be in the units of the Output Coordinate System environment setting. ", "dataType": "Double"}, {"name": "Distance_Increment", "isOptional": true, "description": "The distance to increase after each iteration. The distance used in the analysis starts at the Beginning Distance and increases by the amount specified in the Distance Increment . The value entered for this parameter should be in the units of the Output Coordinate System environment setting. ", "dataType": "Double"}, {"name": "Distance_Method", "isOptional": true, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}, {"name": "Row_Standardization", "isOptional": true, "description": " NONE \u2014 No standardization of spatial weights is applied. ROW \u2014 Spatial weights are standardized; each weight is divided by its row sum (the sum of the weights of all neighboring features). ", "dataType": "Boolean"}, {"name": "Output_Table", "isOutputFile": true, "isOptional": true, "description": "The table to be created with each distance band and associated z-score result. ", "dataType": "Table"}, {"name": "Output_Report_File", "isOutputFile": true, "isOptional": true, "description": "The PDF file to be created containing a line graph summarizing results. ", "dataType": "File"}]},
{"syntax": "ExportXYv_stats (Input_Feature_Class, Value_Field, Delimiter, Output_ASCII_File, Add_Field_Names_to_Output)", "name": "Export Feature Attribute to ASCII (Spatial Statistics)", "description": "Exports feature class coordinates and attribute values to a space, comma, or semi-colon delimited ASCII text file.", "example": {"title": "ExportFeatureAttributeToASCII Example (Python Window)", "description": "The following Python Window script demonstrates how to use the ExportFeatureAttributeToASCII tool.", "code": "import arcpy arcpy.env.workspace = r\"c:\\data\" arcpy.ExportXYv_stats ( \"AidsByCaCnty.shp\" , \"HEPRATE\" , \"SPACE\" , \"aidsbycacnty.txt\" , \"ADD_FIELD_NAMES\" )"}, "usage": ["This tool may be used to export data for analysis with external software packages.", "The X and Y coordinate values are written to the text file with eight significant digits of precision.  Floating point attribute values are written to the text file with six significant digits.", "If this tool is part of a custom model tool, the output text file will only appear in the ", "Results", " window if it is set as a ", "model parameter", " prior to running the tool.", "When null values are encountered for a field value, they will be written to the output text file as \"NULL\".", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class from which to export feature coordinates and attribute values. ", "dataType": "Feature Layer"}, {"name": "Value_Field", "isOptional": false, "description": "The field or fields in the input feature class containing the values to export to an ASCII text file. ", "dataType": "Field"}, {"name": "Delimiter", "isOptional": false, "description": "Specifies how feature coordinates and attribute values will be separated in the output ASCII file. SPACE \u2014 Feature coordinates and attribute values will be separated by a space in the output. COMMA \u2014 Feature coordinates and attribute values will be separated by a comma in the output. SEMI-COLON \u2014 Feature coordinates and attribute values will be separated by a semicolon in the output. ", "dataType": "String"}, {"name": "Output_ASCII_File", "isOutputFile": true, "isOptional": false, "description": "The ASCII text file that will contain the feature coordinate and attribute values. ", "dataType": "File"}, {"name": "Add_Field_Names_to_Output", "isOptional": false, "description": " NO_FIELD_NAMES \u2014 No field names will be included in the output text file (default). ADD_FIELD_NAMES \u2014 Field names will be written to the output text file.", "dataType": "Boolean"}]},
{"syntax": "ConvertSpatialWeightsMatrixtoTable_stats (Input_Spatial_Weights_Matrix_File, Output_Table)", "name": "Convert Spatial Weights Matrix to Table (Spatial Statistics)", "description": "Converts a binary spatial weights matrix file ( .swm ) to a table.", "example": {"title": "Convert Spatial Weights Matrix to Table Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Convert Spatial Weights Matrix to Table tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data\" arcpy.ConvertSpatialWeightsMatrixtoTable_stats ( \"euclidean6Neighs.swm\" , \"euclidean6Neighs.dbf\" )"}, "usage": ["This tool allows you to edit a spatial weights matrix file, if necessary: "], "parameters": [{"name": "Input_Spatial_Weights_Matrix_File", "isInputFile": true, "isOptional": false, "description": "The full pathname for the spatial weights matrix file (.swm) you want to convert. ", "dataType": "File"}, {"name": "Output_Table", "isOutputFile": true, "isOptional": false, "description": "A full pathname to the table you want to create. ", "dataType": "Table"}]},
{"syntax": "CollectEvents_stats (Input_Incident_Features, Output_Weighted_Point_Feature_Class)", "name": "Collect Events (Spatial Statistics)", "description": "Converts event data, such as crime or disease incidents, to weighted point data.", "example": {"title": "CollectEvents Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Collect Events tool.", "code": "import arcpy arcpy.env.workspace = \"C:/Data\" arcpy.CollectEvents_stats ( \"911Copied.shp\" , \"911Count.shp\" , \"Count\" , \"#\" )"}, "usage": ["Collect Event combines coincident points: it creates a new ", "Output Feature Class", " containing all of the unique locations found in the ", "Input Feature Class", ". It then adds a field named ICOUNT to hold the sum of all incidents at each unique location.", "This tool will only combine features that have the exact same X and Y centroid coordinates. You may want to use the ", "Integrate", " tool to snap nearby features together prior to running the Collect Events tool. ", "The ", "Integrate", " tool permanently alters feature geometry; ", "always", " make a backup copy of your feature class prior to using ", "Integrate", ".", "The ", "Hot Spot Analysis (Getis-Ord Gi*)", ", ", "Cluster and Outlier Analysis (Local Moran's I)", ", and ", "Spatial Autocorrelation (Morans I)", " tools, for example, require weighted points rather than individual incidents. Collect Events can be used to create weights when the input feature class contains coincident features.", "Although this tool will work with polygon or line data, it is really only appropriate for event, incident, or other point feature data. For line and polygon features, feature coincidence is based on feature true geometric centroids. For multipoint, polyline, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area.", "If you want each individual point/part of multipoint/multipart data treated as singlepart features, run the ", "Multipart to Singlepart", " tool, then run ", "Collect Events", " on the single part feature class. For more information, see ", "Processing Multipoint Data", ".", "In addition to the ", "Output Feature Class", ", this function passes, as derived output values, the name of the count field and the maximum count value encountered for any one location. These derived output values are helpful when you use this tool in models or scripts.", "When this tool runs in ArcMap, the output feature class is automatically added to the Table of Contents (TOC) with default rendering applied to the ICOUNT field. The graduated circle rendering scheme is defined by a layer file in ", "<ArcGIS>/ArcToolbox/Templates/Layers", ". You can reapply the default rendering, if needed, by ", "importing", " the template layer symbology."], "parameters": [{"name": "Input_Incident_Features", "isInputFile": true, "isOptional": false, "description": "The features representing event or incident data. ", "dataType": "Feature Layer"}, {"name": "Output_Weighted_Point_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to contain the weighted point data. ", "dataType": "Feature Class"}]},
{"syntax": "CalculateDistanceBand_stats (Input_Features, Neighbors, Distance_Method)", "name": "Calculate Distance Band from Neighbor Count (Spatial Statistics)", "description": "Returns the minimum, the maximum, and the average distance to the specified Nth nearest neighbor (N is an input parameter) for a set of features.  Results are accessible from the  Results  window.", "example": {"title": "CalculateDistanceBandfromNeighborCount Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CalculateDistanceBandfromNeighborCount tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data\" mindist , avgdist , maxdist = arcpy.CalculateDistanceBand_stats ( \"Blocks\" , 10 , \"EUCLIDEAN_DISTANCE\" )"}, "usage": ["Given a set of features, this tool returns three numbers: the minimum, the maximum, and the average distance to a specified number of neighbors (", "N", "). Example: if you specify ", "8", " for the ", "Neighbors", " parameter, this tool creates a list of distances between every feature and its 8th nearest neighbor; from this list of distances it then calculates  the minimum, maximum, and average distance. ", "The output from this tool is written as messages to the ", "Results window", ".  Right-click on the ", "Messages", " entry and select ", "View", " to see results in a ", "Message dialog box", ".", "Some tools, such as ", "Hot Spot Analysis (Getis-Ord Gi*)", " and ", "Spatial_Autocorrelation (Global Moran's I)", ", allow you to specify a neighborhood ", "Distance Band or Threshold Distance", " value. By using the Maximum Distance output value from this tool for the ", "Distance Band or Threshold Distance", " parameter, you ensure every feature in the input feature class has at least ", "N", " neighbors. ", "This tool provides one strategy for determining a ", "Distance Band or Threshold Distance", " value to use with tools in the Spatial Statistics Toolbox such as ", "Hot Spot Analysis (Getis-Ord Gi*)", " or ", "Cluster and Outlier Analysis (Local Moran's I)", ". See ", "Selecting a Fixed Distance", " for additional strategies.  ", "The distances returned by this tool are in the units of the geoprocessing environment ", "Output_Coordinate_System", ".", "Calculations based on either ", "Euclidean or Manhattan distance", " require ", "projected data", " to accurately measure distances.", "For line and polygon features, feature centroids are used in distance computations. For multipoints, polylines, or polygons with multiple parts, the centroid is computed using the weighted mean center of all feature parts. The weighting for point features is 1, for line features is length, and for polygon features is area."], "parameters": [{"name": "Input_Features", "isInputFile": true, "isOptional": false, "description": "The feature class or layer used to calculate distance statistics. ", "dataType": "Feature Layer"}, {"name": "Neighbors", "isOptional": false, "description": "The number of neighbors (N) to consider for each feature. This number should be any integer between one and the total number of features in the feature class. A list of distances between each feature and its Nth neighbor is compiled, and the maximum, minimum, and average distances are output to the Results window. ", "dataType": "Long"}, {"name": "Distance_Method", "isOptional": false, "description": "Specifies how distances are calculated from each feature to neighboring features. EUCLIDEAN_DISTANCE \u2014 The straight-line distance between two points (as the crow flies) MANHATTAN_DISTANCE \u2014 The distance between two points measured along axes at right angles (city block); calculated by summing the (absolute) difference between the x- and y-coordinates ", "dataType": "String"}]},
{"syntax": "CalculateAreas_stats (Input_Feature_Class, Output_Feature_Class)", "name": "Calculate Areas (Spatial Statistics)", "description": "Calculates area values for each feature in a polygon feature class.", "example": {"title": "CalculateAreas Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CalculateAreas tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data\" arcpy.CalculateAreas_stats ( \"tracts.shp\" , \"tracts_with_area_field.shp\" )"}, "usage": ["The ", "F_AREA", " field created in the ", "Output Feature Class", " will be populated with values for the area of each polygon feature in square units of the ", "Output_Coordinate_System", ".", "There are alternative methods for creating an ", "Area", " field for polygon features including: ", "Calculate_Field", " and the ", "Geometry Calculator", ".", "The ", "Output Feature Class", " is a copy of the ", "Input Feature Class", " with the additional (or updated) ", "F_AREA ", "field containing polygon areas.", "This tool is useful for determining a weight for ", "intra-zonal interaction", ". ", "This tool can be used to calculate an Area value for a study area polygon. The ", "Average Nearest Neighbor", " tool, for example, has an ", "Area", " parameter.", "The F_AREA field is created in the ", "Output Feature Class", " to store calculated Area values. If a field of this name already exists in the ", "Input Feature Class", ", it will be overwritten in the ", "Output Feature Class", ".", "When using shapefiles, keep in mind that they cannot store null values. Tools or other procedures that create shapefiles from nonshapefile inputs may store or interpret null values as zero. In some cases, nulls are stored as very large negative values in shapefiles.  This can lead to unexpected results.  See ", "Geoprocessing considerations for shapefile output", " for more information."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": " The input polygon feature class. ", "dataType": "Feature Layer"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": " The output feature class. This feature class is a copy of the input feature class with field F_AREA added (or updated). The F_AREA field contains the polygon area. ", "dataType": "Feature Class"}]},
{"syntax": "ZRenderer_stats (input_feature_class, field_to_render, output_layer_file)", "name": "ZScore Rendering (Spatial Statistics)", "description": "Applies a cold (blue) to hot (red) color rendering scheme for a field of z-scores.", "example": {"title": "ZScore Rendering Example (Python Window)", "description": "The following Python Window script demonstrates how to use the ZScore Rendering tool.", "code": "import arcpy arcpy.env.workspace = r\"C:\\data\" arcpy.ZRenderer_stats ( \"hotspot_output.shp\" , \"GiInvDst\" , \"hotspot_output_rendered.lyr\" )"}, "usage": ["The Z Renderer creates a new layer file (", ".lyr", ") with z-scores rendered in the following manner:", "The Z Renderer is appropriate for symbolizing standard deviations including the output from both the ", "Hot Spot Analysis", " and ", "Cluster and Outlier Analysis", " tools.", "Beginning with the ArcGIS ", "10", " release, this tool is a built-in tool (rather than a Visual Basic executable).  While every effort was made not to break custom model and script tools developed prior to ArcGIS ", "10", ", there may be cases where older models that use this tool must be rebuilt in order for the model to run.", "Map layers can be used to define the ", "Input Feature Class", ". When using a layer with a selection, only the selected features are included in the analysis."], "parameters": [{"name": "input_feature_class", "isInputFile": true, "isOptional": false, "description": "The feature class containing a field with standardized z-scores. ", "dataType": "Feature Layer"}, {"name": "field_to_render", "isOptional": false, "description": "The name of the field containing the z-scores. ", "dataType": "Field"}, {"name": "output_layer_file", "isOutputFile": true, "isOptional": false, "description": "The new output layer file to store rendering information. You must include the .lyr extension as part of the file name. ", "dataType": "Layer File"}]},
{"syntax": "HotSpotsRendered_stats (Input_Feature_Class, Input_Field, Output_Layer_File, Output_Feature_Class, {Distance_Band_or_Threshold_Distance})", "name": "Hot Spot Analysis with Rendering (Spatial Statistics)", "description": "Calculates the Getis-Ord Gi* statistic for hot spot analysis and then applies a cold-to-hot type of rendering to the output z-scores.", "example": {"title": "HotSpotAnalysisWithRendering Example (Python Window)", "description": "The following Python Window script demonstrates how to use the HotSpotAnalysisWithRendering tool.", "code": "import arcpy arcpy.env.workspace = \"C:/data\" arcpy.HotSpotsRendered_stats ( \"911Count.shp\" , \"ICOUNT\" , \"911HotSpots_Rendered.lyr\" , \"911HotSpots.shp\" )"}, "usage": ["The Gi* rendered model combines the  ", "Hot Spot Analysis", " and ", "ZScore Rendering", " tools.", "Beginning with the ArcGIS 9.3 release, output from the ", "Hot Spot Analysis", " tool is automatically added to the TOC with default hot/cold rendering applied to the z-score field."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class for which hot spot analysis will be performed. ", "dataType": "Feature Layer"}, {"name": "Input_Field", "isInputFile": true, "isOptional": false, "description": "The numeric field (number of victims, jobs, incident severity, and so on) to be evaluated. ", "dataType": "Field"}, {"name": "Output_Layer_File", "isOutputFile": true, "isOptional": false, "description": "The layer file to store the cold-to-hot rendering information. You must include the .lyr extension as part of the file name. ", "dataType": "Layer File"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to receive the results fields. ", "dataType": "Feature Class"}, {"name": "Distance_Band_or_Threshold_Distance", "isOptional": true, "description": "Specifies a distance cutoff value. Features outside the specified Distance Band or Threshold Distance are ignored in the hot spot analysis. The value entered for this parameter should be in the units of the Input Feature Class' coordinate system. There is one exception: if the Output Coordinate System environment variable is set, the value entered for this parameter should be in the units of the coordinate system set in that environment. When this field is left blank, a default distance value will be computed and applied. ", "dataType": "Double"}]},
{"syntax": "CountRenderer_stats (input_feature_class, field_to_render, output_layer_file, number_of_classes, symbol_color, {maximum_field_value})", "name": "Count Rendering (Spatial Statistics)", "description": "Applies graduated circle rendering to a numeric field in a feature class.", "example": {"title": "CountRenderer example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CountRenderer tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data\" arcpy.CountRenderer_stats ( \"autotheft_weighted.shp\" , \"Count\" , \"auto_weight_rendered.lyr\" , \"5\" , \"mango\" , \"#\" )"}, "usage": ["The ", "Count Rendering", " tool renders quantities as circles; circle size reflects value magnitudes.  Tool output is a new layer file (", ".lyr", ").", "Rendering features quantitatively may reveal spatial patterns in the input count data.", "If no ", "Maximum Field Value", " is provided, it is set to the largest value found in the ", "Field to Render", ". Use the ", "Maximum Field Value", " parameter when you want to compare several graduated circle maps; setting the same ", "Maximum Field Value", " for a series of maps imposes a fixed circle size scaling even when the data ranges vary."], "parameters": [{"name": "input_feature_class", "isInputFile": true, "isOptional": false, "description": "The feature layer containing count data to be rendered. ", "dataType": "Feature Layer"}, {"name": "field_to_render", "isOptional": false, "description": "The name of the field containing count data. ", "dataType": "Field"}, {"name": "output_layer_file", "isOutputFile": true, "isOptional": false, "description": "The new output layer file containing rendering information. You must include the .lyr extension as part of the file name. ", "dataType": "Layer File"}, {"name": "number_of_classes", "isOptional": false, "description": "The number of classes into which the input feature class will be classified. ", "dataType": "Long"}, {"name": "symbol_color", "isOptional": false, "description": "The color of the graduated circles. MANGO \u2014 BRIGHT_RED \u2014 DARK_GREEN \u2014 GREEN \u2014 DARK_BLUE \u2014 BRIGHT_PINK \u2014 LIGHT_YELLOW \u2014 SKY_BLUE \u2014", "dataType": "String"}, {"name": "maximum_field_value", "isOptional": true, "description": "The maximum attribute value that will be rendered. Features with field values greater than this maximum value will not be drawn. ", "dataType": "Double"}]},
{"syntax": "CollectEventsRendered_stats (Input_Incident_Features, Output_Layer, Output_Weighted_Point_Feature_Class)", "name": "Collect Events with Rendering (Spatial Statistics)", "description": "Converts event data to weighted point data and then applies a graduated circle rendering to the resultant count field.", "example": {"title": "Collect Events with Rendering Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Collect Events with Rendering tool.", "code": "import arcpy arcpy.env.workspace = \"C:/Data\" arcpy.CollectEventsRendered_stats ( \"911.shp\" , \"911Count_rendered.lyr\" , \"911Count.shp\" )"}, "usage": ["The ", "Collect Events with Rendering", " tool is a model that combines the ", "Collect Events", " tool with the ", "Count Rendering", " tool.", "Beginning with ArcGIS 9.3, the output feature class from the ", "Collect Events", " tool is automatically added to the TOC with default graduated circle rendering applied to the ICOUNT field."], "parameters": [{"name": "Input_Incident_Features", "isInputFile": true, "isOptional": false, "description": "The features representing event or incident data. ", "dataType": "Feature Layer"}, {"name": "Output_Layer", "isOutputFile": true, "isOptional": false, "description": "The layer file (.lyr) to store the graduate circle rendering information. ", "dataType": "Layer File"}, {"name": "Output_Weighted_Point_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to contain the weighted point data. ", "dataType": "Feature Class"}]},
{"syntax": "ClustersOutliersRendered_stats (Input_Feature_Class, Input_Field, Output_Layer_File, Output_Feature_Class)", "name": "Cluster/Outlier Analysis with Rendering (Spatial Statistics)", "description": "Given a set of weighted features, identifies hot spots, cold spots, and spatial outliers using the Anselin Local Moran's I statistic. It then applies cold-to-hot rendering to the z-score results.", "example": {"title": "Cluster and Outlier Analysis with Rendering Example (Python Window)", "description": "The following Python Window script demonstrates how to use the Cluster and Outlier Analysis with Rendering tool.", "code": "import arcpy arcpy.env.workspace = \"c:/data/911calls\" arcpy.ClustersOutliersRendered_stats ( \"911Count.shp\" , \"ICOUNT\" , \"911ClusterOutlier_rendered.lyr\" , \"911ClusterOutlier.shp\" )"}, "usage": ["The ", "Cluster/Outlier Analysis with Rendering", " tool combines the ", "Clusters and Outlier Analysis", " and ", "ZScore Rendering", " tools in a model.  The ", "Output Layer File", " is automatically added to the TOC with hot/cold rendering applied to feature z-scores.", "Begining with ArcGIS 9.3, output from the ", "Clusters and Outlier Analysis", " is automatically added to the TOC with default rendering  applied to the ", "COTYPE", " field, showing statistically significant hot spots, cold spots and spatial outliers."], "parameters": [{"name": "Input_Feature_Class", "isInputFile": true, "isOptional": false, "description": "The feature class for which cluster analysis will be performed. ", "dataType": "Feature Layer"}, {"name": "Input_Field", "isInputFile": true, "isOptional": false, "description": "The field to be evaluated. ", "dataType": "Field"}, {"name": "Output_Layer_File", "isOutputFile": true, "isOptional": false, "description": "The output layer file to store rendering information. ", "dataType": "Layer File"}, {"name": "Output_Feature_Class", "isOutputFile": true, "isOptional": false, "description": "The output feature class to receive the results field, z-score, p-value, and cluster type designation. ", "dataType": "Feature Class"}]},
{"syntax": "Corridor (in_distance_raster1, in_distance_raster2)", "name": "Corridor (Spatial Analyst)", "description": "Calculates the sum of accumulative costs for two input accumulative cost rasters. \r\n Learn more about creating a least cost corridor", "example": {"title": "Corridor example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCorr = Corridor ( \"costraster\" , \"focalcost.tif\" ) outCorr.save ( \"c:/sapyexamples/output/corridor\" )"}, "usage": ["While any two rasters can be used for the input, to obtain a meaningful result they should be  unaltered accumulative cost output rasters.", "The order of the two inputs is irrelevant."], "parameters": [{"name": "in_distance_raster1", "isInputFile": true, "isOptional": false, "description": "The first input distance raster. It should be an accumulated cost distance output from a distance tool such as Cost Distance or Path Distance . ", "dataType": "Raster Layer"}, {"name": "in_distance_raster2", "isInputFile": true, "isOptional": false, "description": "The second input distance raster. It should be an accumulated cost distance output from a distance tool such as Cost Distance or Path Distance . ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PointDensity (in_point_features, population_field, {cell_size}, {neighborhood}, {area_unit_scale_factor})", "name": "Point Density (Spatial Analyst)", "description": "Calculates a magnitude per unit area from point features that fall within a neighborhood around each cell. \r\n Learn more about how Point Density works", "example": {"title": "PointDensity example 1 (Python window)", "description": "This example calculates a density raster from a point shape file.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" pdensOut = PointDensity ( \"rec_sites.shp\" , \"NONE\" , 60 , NbrCircle ( 2500 , \"MAP\" )) pdensOut.save ( \"C:/sapyexamples/output/pointdensity\" )"}, "usage": ["Larger values of the radius parameter produce a more generalized density raster. Smaller values produce a raster that shows more detail.", "Only the points that fall within the neighborhood are considered when calculating the density. If no points fall within the neighborhood at a particular cell, that cell is assigned NoData.", "If the area unit scale factor units are small, relative to the distance between the points, the output raster values may be small. To obtain larger values, use the area unit scale factor for larger units (for example, square kilometers versus square meters).", "The values on the output raster will always be floating point."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features for which to calculate the density. ", "dataType": "Feature Layer"}, {"name": "population_field", "isOptional": false, "description": "Field denoting population values for each point. The population field is the count or quantity to be used in the calculation of a continuous surface. Values in the population field may be integer or floating point. The options and default behaviours for the field are listed below. Use None if no item or special value will be used and each feature will be counted once. You can use Shape if input features contains Z. Otherwise, the default field is POPULATION . The following conditions may also apply. Use None if no item or special value will be used and each feature will be counted once. You can use Shape if input features contains Z. Otherwise, the default field is POPULATION . The following conditions may also apply. If there is no POPULATION field, but there is a POPULATIONxxxx field, this is used by default. The \" xxxx \" can be any valid character, such as POPULATION6 , POPULATION1974 , or POPULATIONROADTYPE . If there is no POPULATION field or POPULATIONxxxx field, but there is a POP field, this is used by default. If there is no POPULATION field, POPULATIONxxxx field, or POP field, but there is a POPxxxx field, this is used by default. If there is no POPULATION field, POPULATIONxxxx field, POP field, or POPxxxx field, NONE is used by default. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": " The cell size for the output raster dataset. This is the value in the environment if specifically set. If the environment is not set, then cell size is the shorter of the width or height of the output extent in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "neighborhood", "isOptional": true, "description": "Dictates the shape of the area around each cell used to calculate the density value. This is a Neighborhood class. There are four types of neighbourhood class: NbrAnnulus , NbrCircle , NbrRectangle , and NbrWedge . The forms and descriptions of the classes are: A torus (donut shaped) neighborhood defined by an inner radius and an outer radius. A circular neighborhood with the given radius. A rectangular neighborhood with the given width and height. A wedge (pie) shaped neighborhood. A wedge is specified by a start angle, an end angle, and a radius. The wedge extends counterclockwise from the starting angle to the ending angle. Angles are specified in arithmetic degrees (counterclockwise from the positive x-axis). Negative angles may be used. Defines the units as either the number of cells or as value in map units. The default is NbrCircle , where radius is the shortest of the width or height of the output extent in the output spatial reference, divided by 30. NbrAnnulus ({inner_radius}, {outer_radius}, {CELL | MAP}) A torus (donut shaped) neighborhood defined by an inner radius and an outer radius. NbrCircle ({radius}, {CELL | MAP}) A circular neighborhood with the given radius. NbrRectangle ({width}, {height}, {CELL | MAP}) A rectangular neighborhood with the given width and height. NbrWedge ({radius}, {start_angle}, {end_angle}, {CELL | MAP}) A wedge (pie) shaped neighborhood. A wedge is specified by a start angle, an end angle, and a radius. The wedge extends counterclockwise from the starting angle to the ending angle. Angles are specified in arithmetic degrees (counterclockwise from the positive x-axis). Negative angles may be used. {CELL | MAP} Defines the units as either the number of cells or as value in map units. ", "dataType": "Neighborhood"}, {"name": "area_unit_scale_factor", "isOptional": true, "description": "The desired area units of the output density values. A default unit is selected based on the linear unit of the projection of the output spatial reference. You can change this to the appropriate unit if you wish to convert the density output. Values for line density convert the units of both length and area. For example, if your input units are meters the default output area density units will be square kilometers for point features or kilometers per square kilometer for polyline features. The default density units based on the input feature units are: SQUARE_MAP_UNITS \u2014 If the units are unknown, points, or decimal degrees. SQUARE_MILES \u2014 For feet, yards, miles, or nautical miles. SQUARE_KILOMETERS \u2014 For meters or kilometers. SQUARE_INCHES \u2014 For inches. SQUARE_CENTIMETERS \u2014 For centimeters. SQUARE_MILLIMETERS \u2014 For millimeters.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "LineDensity (in_polyline_features, population_field, {cell_size}, {search_radius}, {area_unit_scale_factor})", "name": "Line Density (Spatial Analyst)", "description": "Calculates a magnitude per unit area from polyline features that fall within a radius around each cell. \r\n Learn more about how Line Density works", "example": {"title": "LineDensity example 1 (Python window)", "description": "This example calculates a density raster on a length field of a polyline shape file.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLDens = LineDensity ( \"roads.shp\" , \"LENGTH\" , 45 , 1000 , \"SQUARE_MILES\" ) outLDens.save ( \"C:/sapyexamples/output/ldensout\" )"}, "usage": ["Only the portion of a line within the neighborhood is considered when calculating the density. If no lines fall within the neighborhood at a particular cell, that cell is assigned NoData.", "Larger values of the radius parameter produce a more generalized density raster. Smaller values produce a raster that shows more detail.", "If the area unit scale factor units are small relative to the features (length of line sections), the output values may be small. To obtain larger values, use the area unit scale factor for larger units (for example, square kilometers versus square meters).", "The values on the output raster will always be floating point."], "parameters": [{"name": "in_polyline_features", "isInputFile": true, "isOptional": false, "description": "The input line features for which to calculate the density. ", "dataType": "Feature Layer"}, {"name": "population_field", "isOptional": false, "description": "Numeric field denoting population values (the number of times the line should be counted) for each polyline. Values in the population field may be integer or floating point. The options and default behaviours for the field are listed below. Use None if no item or special value will be used and each feature will be counted once. You can use Shape if input features contains Z. Otherwise, the default field is POPULATION . The following conditions may also apply. Use None if no item or special value will be used and each feature will be counted once. You can use Shape if input features contains Z. Otherwise, the default field is POPULATION . The following conditions may also apply. If there is no POPULATION field, but there is a POPULATIONxxxx field, this is used by default. The \" xxxx \" can be any valid character, such as POPULATION6 , POPULATION1974 , or POPULATIONROADTYPE . If there is no POPULATION field or POPULATIONxxxx field, but there is a POP field, this is used by default. If there is no POPULATION field, POPULATIONxxxx field, or POP field, but there is a POPxxxx field, this is used by default. If there is no POPULATION field, POPULATIONxxxx field, POP field, or POPxxxx field, NONE is used by default. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": " The cell size for the output raster dataset. This is the value in the environment if specifically set. If the environment is not set, then cell size is the shorter of the width or height of the output extent in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "search_radius", "isOptional": true, "description": "The search radius within which to calculate density. Units are based on the linear unit of the projection of the output spatial reference. For example, if the units are in meters, to include all features within a one-mile neighborhood, set the search radius equal to 1609.344 (1 mile = 1609.344 meters). The default is the shortest of the width or height of the output extent in the output spatial reference, divided by 30. ", "dataType": "Double"}, {"name": "area_unit_scale_factor", "isOptional": true, "description": "The desired area units of the output density values. A default unit is selected based on the linear unit of the projection of the output spatial reference. You can change this to the appropriate unit if you wish to convert the density output. Values for line density convert the units of both length and area. For example, if your input units are meters the default output area density units will be square kilometers for point features or kilometers per square kilometer for polyline features. The default density units based on the input feature units are: SQUARE_MAP_UNITS \u2014 If the units are unknown, points, or decimal degrees. SQUARE_MILES \u2014 For feet, yards, miles, or nautical miles. SQUARE_KILOMETERS \u2014 For meters or kilometers. SQUARE_INCHES \u2014 For inches. SQUARE_CENTIMETERS \u2014 For centimeters. SQUARE_MILLIMETERS \u2014 For millimeters.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "KernelDensity (in_features, population_field, {cell_size}, {search_radius}, {area_unit_scale_factor})", "name": "Kernel Density (Spatial Analyst)", "description": "Calculates a magnitude per unit area from point or polyline features using a kernel function to fit a smoothly tapered surface to each point or polyline. \r\n Learn more about how Kernel Density works", "example": {"title": "KernelDensity example 1 (Python window)", "description": "This example calculates a smoothed density raster from a point shape file.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outKDens = KernelDensity ( \"rec_sites.shp\" , \"NONE\" , 45 , 1200 , \"SQUARE_KILOMETERS\" ) outKDens.save ( \"C:/sapyexamples/output/kdensout\" )"}, "usage": ["Larger values of the search  radius parameter produce a smoother, more generalized density raster. Smaller values produce a raster that shows more detail.", "Only the points or portions of a line that fall within the neighborhood are considered in calculating density. If no points or line sections fall within the neighborhood of a particular cell, that cell is assigned NoData.", "If the area unit scale factor units are small relative to the features (distance between points or length of line sections, depending on feature type), the output values may be small. To obtain larger values, select the area unit scale factor for larger units (for example, square kilometers versus square meters)."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features (point or line) for which to calculate the density. ", "dataType": "Feature Layer"}, {"name": "population_field", "isOptional": false, "description": "Field denoting population values for each feature. The Population field is the count or quantity to be spread across the landscape to create a continuous surface. Values in the population field may be integer or floating point. The options and default behaviours for the field are listed below. Use None if no item or special value will be used and each feature will be counted once. You can use Shape if input features contains Z. Otherwise, the default field is POPULATION . The following conditions may also apply. Use None if no item or special value will be used and each feature will be counted once. You can use Shape if input features contains Z. Otherwise, the default field is POPULATION . The following conditions may also apply. If there is no POPULATION field, but there is a POPULATIONxxxx field, this is used by default. The \" xxxx \" can be any valid character, such as POPULATION6 , POPULATION1974 , or POPULATIONROADTYPE . If there is no POPULATION field or POPULATIONxxxx field, but there is a POP field, this is used by default. If there is no POPULATION field, POPULATIONxxxx field, or POP field, but there is a POPxxxx field, this is used by default. If there is no POPULATION field, POPULATIONxxxx field, POP field, or POPxxxx field, NONE is used by default. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": " The cell size for the output raster dataset. This is the value in the environment if specifically set. If the environment is not set, then cell size is the shorter of the width or height of the output extent in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "search_radius", "isOptional": true, "description": "The search radius within which to calculate density. Units are based on the linear unit of the projection of the output spatial reference. For example, if the units are in meters, to include all features within a one-mile neighborhood, set the search radius equal to 1609.344 (1 mile = 1609.344 meters). The default is the shortest of the width or height of the output extent in the output spatial reference, divided by 30. ", "dataType": "Double"}, {"name": "area_unit_scale_factor", "isOptional": true, "description": "The desired area units of the output density values. A default unit is selected based on the linear unit of the projection of the output spatial reference. You can change this to the appropriate unit if you wish to convert the density output. Values for line density convert the units of both length and area. For example, if your input units are meters the default output area density units will be square kilometers for point features or kilometers per square kilometer for polyline features. The default density units based on the input feature units are: SQUARE_MAP_UNITS \u2014 If the units are unknown, points, or decimal degrees. SQUARE_MILES \u2014 For feet, yards, miles, or nautical miles. SQUARE_KILOMETERS \u2014 For meters or kilometers. SQUARE_INCHES \u2014 For inches. SQUARE_CENTIMETERS \u2014 For centimeters. SQUARE_MILLIMETERS \u2014 For millimeters.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "SetNull (in_conditional_raster, in_false_raster_or_constant, {where_clause})", "name": "Set Null (Spatial Analyst)", "description": "Set Null sets identified cell locations to NoData based on a specified criteria.  It returns NoData if a conditional evaluation is true, and returns the value specified by another raster if it is false. \r\n\r\n Learn more about setting cell values to NoData with Set Null", "example": {"title": "SetNull example 1 (Python window)", "description": "In this example, any input cell with a value less than 0 will be set to NoData in the output raster, and the remaining cells will retain their original value.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSetNull = SetNull ( \"elevation\" , \"elevation\" , \"VALUE < 0\" ) outSetNull.save ( \"C:/sapyexamples/output/outsetnull.img\" )"}, "usage": ["If the evaluation of the where clause is true, the cell location on the output raster will be assigned NoData. If the evaluation is false, the output raster will be defined by the input false raster or constant value.", "If no where clause is specified, the output raster will have NoData wherever the conditional raster is not 0.", "The input conditional raster does not affect whether the output data type is integer or floating point. If the input false raster (or constant value) contains floating-point values, the output raster will be floating point. If it contains all integer values, the output will be an integer raster.", "The maximum length of the logical expression is 4,096 characters.", "In Python, the {where_clause} should be enclosed in quotes, for example, ", "\"Value > 5\"", "."], "parameters": [{"name": "in_conditional_raster", "isInputFile": true, "isOptional": false, "description": "Input raster representing the true or false result of the desired condition. ", "dataType": "Raster Layer"}, {"name": "in_false_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input whose values will be used as the output cell values if the condition is false. It can be an integer or a floating point raster, or a constant value. ", "dataType": "Raster Layer | Constant"}, {"name": "where_clause", "isOptional": true, "description": "A logical expression that determines which of the input cells are to be true or false. The expression follows the general form of an SQL expression. Consult the documentation for more information on the SQL reference for query expressions used in ArcGIS and specifying a query in Python . ", "dataType": "SQL Expression"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Pick (in_position_raster, in_rasters_or_constants)", "name": "Pick (Spatial Analyst)", "description": "The value from a position raster is used to determine from which raster in a list of input rasters the output cell value will be obtained.", "example": {"title": "Pick example 1 (Python window)", "description": "This example assigns the output value based on the order of several input rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPick = Pick ( \"cost\" , [ \"degs\" , \"negs\" , \"fourgrd\" ]) outPick.save ( \"C:/sapyexamples/output/outpick.tif\" )"}, "usage": ["The value of each cell of the position raster determines which input will be used to obtain the output raster value. For example, if a cell in the  position raster has a value of 1, the value from the first input in the raster list will be used for the output cell value.  If the position input has a value of 2, the output value will come from the second input in the raster list, and so on.", "The order of the input list is relevant for this tool. If the order of rasters changes, the results will change.", "If a cell value in the position raster is zero or negative, the result will be NoData. If the position value is larger than the number of rasters in the list, the result will be NoData.", "If the position raster is floating point, the values will be truncated to be integers before they are processed.", "Any cell with a NoData value on the position raster will receive NoData on the output raster.", "If any of the rasters in the input list is floating point, the output raster will be floating point. If they are all integer, the output raster will be integer."], "parameters": [{"name": "in_position_raster", "isInputFile": true, "isOptional": false, "description": "Input raster defining the position of the raster to use for the output value. The input can be an integer or float raster. ", "dataType": "Raster Layer"}, {"name": "in_rasters_or_constants", "isInputFile": true, "isOptional": false, "description": "The list of inputs from which the output value will be selected. The inputs can be integer or float rasters. A number can also be used as an input. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Con (in_conditional_raster, in_true_raster_or_constant, {in_false_raster_or_constant}, {where_clause})", "name": "Con (Spatial Analyst)", "description": "Performs a conditional if/else evaluation on each of the input cells of an input raster. \r\n\r\n Learn more about performing conditional evaluation with Con", "example": {"title": "Con example 1 (Python window)", "description": "In this example the original value will be retained in the output where the input conditional raster is greater than a value of 2000, and a value of NoData where it is not.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCon = Con ( \"elevation\" , \"elevation\" , \"\" , \"VALUE > 2000\" ) outCon.save ( \"C:/sapyexamples/output/outcon.img\" ) # Execute Con using a map algebra expression instead of a where clause outCon2 = Con ( Raster ( \"elevation\" ) > 2000 , \"elevation\" ) outCon2.save ( \"C:/sapyexamples/output/outcon2\" )"}, "usage": ["If either the true raster or optional false raster is floating point, the output raster will be floating point. If both the true expression and optional false raster are integer, the output raster will be integer.", "If the evaluation of the expression is nonzero, it is treated as True.", "If no input false raster or constant is specified, NoData will be assigned to those cells that do not result in True from the expression.", "If NoData does not satisfy the expression, it does not receive the value of the input false raster; it remains NoData.", "In Python, you can avoid using a {where_clause} which specifies the Value field by instead using a Map Algebra expression as the   ", "Input conditional raster", ".", " For example, the following expression: ", "For more information, see the code samples listed below or ", "Building complex statements", ".", "In order to use a {where_clause} in Python, it should be enclosed in quotes.  For example, ", "\"Population > 5000\"", ".  You can consult the help for more information on ", "specifying a query in Python", ".", "The maximum length of the logical expression is 4,096 characters."], "parameters": [{"name": "in_conditional_raster", "isInputFile": true, "isOptional": false, "description": "Input raster representing the true or false result of the desired condition. It can be of integer or floating point type. ", "dataType": "Raster Layer"}, {"name": "in_true_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input whose values will be used as the output cell values if the condition is true. It can be an integer or a floating point raster, or a constant value. ", "dataType": "Raster Layer | Constant"}, {"name": "in_false_raster_or_constant", "isInputFile": true, "isOptional": true, "description": "The input whose values will be used as the output cell values if the condition is false. It can be an integer or a floating point raster, or a constant value. ", "dataType": "Raster Layer | Constant"}, {"name": "where_clause", "isOptional": true, "description": "A logical expression that determines which of the input cells are to be true or false. The expression follows the general form of an SQL expression. Consult the documentation for more information on the SQL reference for query expressions used in ArcGIS and specifying a query in Python . ", "dataType": "SQL Expression"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CopyParcelFabric_fabric (in_parcels, output_parcels, {config_keyword})", "name": "Copy Parcel Fabric (Parcel Fabric)", "description": "\r\n Copies parcels from the input parcel fabric dataset or  layer to a new parcel fabric.\r\n", "example": {"title": "CopyParcelFabric example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CopyParcelFabric tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/OaklandCounty.gdb\" arcpy.CopyParcelFabric_fabric ( \"/Parceldata/ParcelFabric\" , \"/SubdivisionData/SubdivisionFabric\" ,)"}, "usage": ["\r\nIf ", "Input Parcels", " is a parcel fabric layer in the map and there is a selection present on the parcels sublayer, only the selected parcels are copied to the output parcel fabric.\r\n", "To overwrite an existing parcel fabric instead of creating a new parcel fabric, choose the ", "Overwrite the outputs of geoprocessing operations", " on the ", "Geoprocessing Options", " dialog box. To open the ", "Geoprocessing Options", " dialog box, click the ", "Geoprocessing", " menu in ArcMap and click ", "Geoprocessing Options", ". "], "parameters": [{"name": "in_parcels", "isInputFile": true, "isOptional": false, "description": " The parcels to be copied to another parcel fabric. ", "dataType": "Parcel Fabric Layer"}, {"name": "output_parcels", "isOutputFile": true, "isOptional": false, "description": " The new parcel fabric to which the parcels will be copied. If the parcel fabric exists and you have chosen to overwrite the outputs of geoprocessing operations by setting arcpy.env.overwriteOutput to True, the existing parcel fabric will be overwritten with a new parcel fabric containing the copied parcels. ", "dataType": "Parcel Fabric"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for parcel fabrics in file and ArcSDE geodatabases. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords for ArcSDE Enterprise Edition are set up by your database administrator. ", "dataType": "String"}]},
{"syntax": "AppendParcelFabric_fabric (in_parcels, target, unjoined_group, {parcels_schema_type}, {field_mapping_parcels}, {parcels_subtype}, {lines_schema_type}, {field_mapping_lines}, {lines_subtype}, {control_schema_type}, {field_mapping_control}, {control_subtype})", "name": "Append Parcel Fabric (Parcel Fabric)", "description": "\r\n Appends one or multiple parcel fabrics into an existing target parcel fabric. The spatial reference of the input parcel fabrics must match the spatial reference of the target parcel fabric.", "example": {"title": "AppendParcelFabric example 1 (Python window)", "description": "The following Python window script demonstrates how to use the AppendParcelFabric tool in immediate mode.", "code": "import arcpy arcpy.env.workspace = \"C:/data/OaklandCounty.gdb\" arcpy.AppendParcelFabric_fabric ( \"/ParcelData/CountyFabric\" , \"/SubdivisionData/Subdivision\" , \"JOINED_GROUP\" , \"TEST\" , \"\" , \"\" , \"TEST\" , \"\" , \"\" , \"TEST\" , \"\" , \"\" )"}, "usage": ["\r\nUse this tool to add new parcels into existing parcel fabrics. For example, you can append a new subdivision entered by the city\u2019s public works department into the county assessor\u2019s database of tax parcels.  Another example would be to stitch  parcel fabrics from different counties into one single parcel fabric.\r\n", "Coincident boundary points are merged between  appended fabrics so that the internal parcel fabric topology remains intact.", "The ", "Append parcels as Unjoined Group", " option appends new parcels as unjoined groups.  Unjoined parcels exist outside of the parcel fabric in their own local coordinate space.  Unjoined parcels can be joined to the parcel fabric at any time. You would want to append parcels as unjoined if additional work is required on the parcels before they become joined to the parcel fabric layer.", " Parcels from both the input parcel fabric and the target parcel fabric will remain intact after the append, even if the parcels overlap. For example, existing parcels are not split or modified by overlapping appended parcels. Coincident points are always merged.", "If TEST is specified for  ", "Parcels Schema Type", ", ", "Lines Schema Type", ", or  ", "Control Schema Type", ", the schema (field definitions) of these input parcel fabric tables must match the tables of the target parcel fabric in order for the parcels to be appended. The schema-type parameters only apply to attributes that you have added to the parcels, lines, or control points attribute tables. Parcel fabric system attributes remain the same across different parcel fabrics.   If  NO_TEST is specified, the attributes of the input parcels, lines, or control points fabric tables (field definitions) do not have to match the tables target fabric dataset. However, any fields from  input parcel fabric tables that do not match the fields of the target parcel fabric tables will not be mapped. Fields are only mapped between the input parcel fabric and target parcel fabric if the same fields exist in both fabrics.  ", "The spatial references of the input and target parcel fabric must match. The tool cannot append a parcel fabric that has a spatial reference that is different from the spatial reference of the target parcel fabric.", "The upgrade versions of the input and target parcel fabric must match. ", "Parcel fabric layers can be used as ", "Input Parcels", ". If a parcel fabric layer has a selection, only the selected parcels are used. ", "This  tool cannot use multiple input parcel fabric layers with the same name. Although ArcMap allows for the display of parcel fabric layers with the same name, these layers may not be used. To work around this limitation, use the  tool dialog box browse button to browse for the full paths of each of the ", "Input Parcels", ".", "To use the ", "Parcels Subtype", ", ", "Lines Subtype", ", and ", "Control Points Subtype", " parameters, the target parcel fabric must have defined subtype fields and assigned subtype codes on the parcels, lines, and control points tables. In the ", "Parcels Subtype", ", ", "Lines Subtype", ", or ", "Control Points Subtype", " parameters, provide a subtype description to assign that subtype to all new parcel fabric features that are appended to the target parcel fabric."], "parameters": [{"name": "in_parcels", "isInputFile": true, "isOptional": false, "description": " The input parcel fabrics that will be appended into the target parcel fabric. The spatial reference of the input parcel fabric must match that of the target parcel fabric. ", "dataType": "Parcel Fabric Layer"}, {"name": "target", "isOptional": false, "description": " The existing parcel fabric that the input parcel fabrics will be appended into. The spatial reference of the input parcel fabric must match that of the target parcel fabric. ", "dataType": "Parcel Fabric"}, {"name": "unjoined_group", "isOptional": false, "description": "Determines how parcels will be appended. UNJOINED_GROUP \u2014 Parcels will be appended as unjoined, stand-alone parcels to the parcel fabric. JOINED_GROUP \u2014 Parcels will be appended as joined parcels to the parcel fabric.", "dataType": "Boolean"}, {"name": "parcels_schema_type", "isOptional": true, "description": "Specifies if the schema (field definitions) of the input parcel fabric parcels table must match the schema of the target parcel fabric parcels table in order for data to be appended. TEST \u2014 Input parcel fabric parcels table schema (field definitions) must match the schema of the target parcel fabric parcels table. An error will be returned if the schemas do not match. NO_TEST \u2014 Input parcels table schema (field definitions) does not have to match that of the target parcels table. Any fields from the input parcels table that do not match the fields of the target parcels table will not be mapped to the target parcels table unless the same fields exist in the target parcels table.", "dataType": "String"}, {"name": "field_mapping_parcels", "isOptional": true, "description": "Lists the attribute fields that will be mapped to the target parcels table. The list includes the existing attribute fields of the target parcels table and attributes fields that match between the input parcels table and the target parcels table. Because the input parcel fabric is appended into an existing target parcel fabric that has a predefined schema (field definitions), the Parcels Extended Attribute Field Map control does not allow for fields to be added or removed from the target parcel fabric. ", "dataType": "Field Mappings"}, {"name": "parcels_subtype", "isOptional": true, "description": "A subtype description to assign that subtype to all new parcel features in a parcel fabric which is appended to the target parcel fabric. ", "dataType": "String"}, {"name": "lines_schema_type", "isOptional": true, "description": "Specifies if the schema (field definitions) of the input parcel fabric lines table must match the schema of the target parcel fabric lines table in order for data to be appended. TEST \u2014 Input parcel fabric lines table schema (field definitions) must match the schema of the target parcel fabric lines table. An error will be returned if the schemas do not match. NO_TEST \u2014 Input lines table schema (field definitions) does not have to match that of the target lines table. Any fields from the input lines table that do not match the fields of the target lines table will not be mapped to the target lines table unless the same fields exist in the target lines table.", "dataType": "String"}, {"name": "field_mapping_lines", "isOptional": true, "description": "Lists the attribute fields that will be mapped to the target lines table. The list includes the existing attribute fields of the target lines table and attributes fields that match between the input lines table and the target lines table. Because the input parcel fabric is appended into an existing target parcel fabric that has a predefined schema (field definitions), the Lines Extended Attribute Field Map control does not allow for fields to be added or removed from the target parcel fabric. ", "dataType": "Field Mappings"}, {"name": "lines_subtype", "isOptional": true, "description": "A subtype description to assign that subtype to all new line features in a parcel fabric which is appended to the target parcel fabric. ", "dataType": "String"}, {"name": "control_schema_type", "isOptional": true, "description": "Specifies if the schema (field definitions) of the input parcel fabric control table must match the schema of the target parcel fabric control table in order for data to be appended. TEST \u2014 Input parcel fabric control table schema (field definitions) must match the schema of the target parcel fabric control table. An error will be returned if the schemas do not match. NO_TEST \u2014 Input control table schema (field definitions) does not have to match that of the target control table. Any fields from the input control table that do not match the fields of the target control table will not be mapped to the target control table unless the same fields exist in the target control table.", "dataType": "String"}, {"name": "field_mapping_control", "isOptional": true, "description": "Lists the attribute fields that will be mapped to the target control table. The list includes the existing attribute fields of the target control table and attributes fields that match between the input control table and the target control table. Because the input parcel fabric is appended into an existing target parcel fabric that has a predefined schema (field definitions), the Control Points Extended Attribute Field Map control does not allow for fields to be added or removed from the target parcel fabric. ", "dataType": "Field Mappings"}, {"name": "control_subtype", "isOptional": true, "description": "A subtype description to assign that subtype to all new control point features in a parcel fabric which is appended to the target parcel fabric. ", "dataType": "String"}]},
{"syntax": "UpgradeParcelFabric_fabric (in_parcel_fabric)", "name": "Upgrade Parcel Fabric (Parcel Fabric)", "description": " Upgrades an existing parcel fabric to the latest released version of ArcGIS. An existing parcel fabric is upgraded to take advantage of the new parcel editing functionality available in the latest released version of ArcGIS.", "example": {"title": "UpgradeParcelFabric example (Python window)", "description": "The following Python window script demonstrates how to use the UpgradeParcelFabric tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Parcel_Editor_Tutorial/Encinitas_City/Cadastral.gdb\" arcpy.UpgradeParcelFabric_fabric ( \"Fabric\" )"}, "usage": ["To use the this tool, the geodatabase needs to be upgraded to the latest released version of ArcGIS. You can upgrade your geodatabase using the ", "Upgrade_Geodatabase", " tool.   ", "The geodatabase needs to be unversioned to be upgraded. If you do not want to upgrade your geodatabase, you can continue to work with an existing fabric that has not been upgraded in the latest released version of ArcGIS.", "This tool can also be accessed from the ", "Parcel Fabric Properties", " dialog box. To open the ", "Parcel Fabric Properties", " dialog box, right-click a parcel fabric dataset in the ", "Catalog", " window and click ", "Properties", ".  "], "parameters": [{"name": "in_parcel_fabric", "isInputFile": true, "isOptional": false, "description": "The parcel fabric dataset that will be upgraded to the latest released version of ArcGIS. ", "dataType": "Parcel Fabric"}]},
{"syntax": "MakeParcelFabricTableView_fabric (in_parcel_fabric, parcel_fabric_table, out_view)", "name": "Make Parcel Fabric Table View (Parcel Fabric)", "description": " Creates a table view from an input parcel fabric feature class or table. The table view that is created by the tool is temporary and will not persist after the session ends unless the document is saved. This tool is useful for accessing hidden, nonspatial parcel fabric tables, such as the  Plans table or the Accuracies table.", "example": {"title": "MakeParcelFabricTableView example (Python window)", "description": "The following Python window script demonstrates how to use the MakeParcelFabricTableView tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Parcel_Editor_Tutorial/Encinitas_City/Cadastral.gdb\" arcpy.MakeParcelFabricTableView_fabric ( \"FABRIC\" , \"Accuracy\" , \"FabricAccuraciesTable\" )"}, "usage": ["This tool is commonly used to create a table view with a selected set of attributes or fields.", "ArcCatalog", " does not display these table views but they can be used as inputs to other geoprocessing tools in the current ArcGIS session. Once the ArcGIS application is exited, the table views are deleted.", "Table views created in ", "ArcCatalog", " cannot be used in ", "ArcMap", ".", " Once an input parcel fabric has been specified, a table view can be created from any parcel fabric feature class or nonspatial table. ", "An existing table view will be overwritten if the same table view name is entered and if you explicitly state that overwriting output is allowed."], "parameters": [{"name": "in_parcel_fabric", "isInputFile": true, "isOptional": false, "description": " The parcel fabric dataset that contains the feature class or table that will be used to create a table view. ", "dataType": "Parcel Fabric Layer"}, {"name": "parcel_fabric_table", "isOptional": false, "description": " The parcel fabric feature class or nonspatial table that will be used to create a table view. ", "dataType": "String"}, {"name": "out_view", "isOutputFile": true, "isOptional": false, "description": " The name of the output table view. ", "dataType": "Table View"}]},
{"syntax": "MakeParcelFabricLayer_fabric (in_parcel_fabric, parcel_fabric_layer)", "name": "Make Parcel Fabric Layer (Parcel Fabric)", "description": " Creates a parcel fabric layer from an input parcel fabric. The parcel fabric layer that is created by the tool is temporary and will not persist after the session ends unless the document is saved.  This tool is needed if you would like a parcel fabric sublayer to participate in a geoprocessing model.", "example": {"title": "MakeParcelFabricLayer example (Python window)", "description": "The following Python window script demonstrates how to use the MakeParcelFabricLayer tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Parcel_Editor_Tutorial/Encinitas_City/Cadastral.gdb\" arcpy.MakeParcelFabricLayer_fabric ( \"Fabric\" , \"ParcelFabricLyr\" )"}, "usage": [" The sublayers of the output parcel fabric layer can be used as input to any geoprocessing tool that accepts a feature class as input and produces output that is derived from the  input feature class. Derived output does not modify the input dataset.", "The temporary parcel fabric layer and the sublayers can be saved as layer files.", "An existing layer will be overwritten if the same name is entered and if you explicitly state that overwriting output is allowed."], "parameters": [{"name": "in_parcel_fabric", "isInputFile": true, "isOptional": false, "description": " The parcel fabric dataset that will be used to create the parcel fabric layer. ", "dataType": "Parcel Fabric Layer"}, {"name": "parcel_fabric_layer", "isOptional": false, "description": " The output parcel fabric layer. ", "dataType": "Parcel Fabric Layer"}]},
{"syntax": "LoadTopologyToParcelFabric_fabric (target_parcel_fabric, in_topology_class, {in_point_class}, {linestring_minimum_segments}, {control_match_tolerance}, unjoined_group, {direction_units}, {direction_type}, compute_area, {area_units}, {radial_point_tolerance}, {accuracy_units})", "name": "Load A Topology To A Parcel Fabric (Parcel Fabric)", "description": " Loads polygon and line features that participate in a topology into a target parcel fabric. The topology requires a predefined set of topology rules:", "example": {"title": "LoadATopologyToAParcelFabric example 1 (Python window)", "description": "The following Python window script demonstrates how to use the LoadATopologyToAParcelFabric tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/Parcel_Editor_Tutorial/Encinitas_City/Cadastral.gdb\" arcpy.LoadTopologyToParcelFabric_fabric ( \"NewFabric\" , \"Enc_polgon\" , \"\" , \"100\" , \"0.5\" , \"JOINED_GROUP\" , \"DEGREES_MINUTES_SECONDS\" , \"NORTH AZIMUTH\" , \"COMPUTE_AREA\" , \"HECTARES\" , \"1\" , \"3 - 1908 to 1980\" )"}, "usage": ["Errors generated by the  polygon\u2014Boundary Must be Covered By (Line) topology rule can be marked as exceptions. This is to allow the migration of connection lines, unclosed parcels, and dependent lines.", "You can choose to load an entire line feature class and polygon feature class or only selected line and polygon features.  When loading large datasets, performance will depend on your existing computer memory specifications. This tool is best used for loading small- to medium-sized areas or sections of parcel data at a time. ", "The topology needs to be validated, clean, and free of errors for the polygon and line features to be migrated into the parcel fabric. If an edit is made to correct a topology error, make sure to validate the topology again to make sure the error no longer exists before running the tool.", "To validate an entire topology, use the ", "Validate Topology", " tool located in the Topology toolset in the Data Management toolbox.", "Instead of validating the entire topology, validate the current extent of the map display in ArcMap by clicking the ", "Validate Topology In Current Extent", " tool ", " on the ", "Topology", " toolbar.", " Either the polygon or the line feature class can be used as the  ", "Input (Topology) Feature Class", " parameter.  If there is a selection present on the feature layer of the input feature class in ArcMap, only the selected features will be migrated. If the line feature class is used as the input feature class, any selected lines that do not form a loop (dangling lines), matching the corresponding polygon in the polygon feature class, will be migrated as parts of ", "unclosed parcels", ". If you have a ", "Category", " field on your line feature class, you can set the category value of the dangling line to 3 to represent connection lines or 1 to represent dependent lines.  ", "COGO attributes are not required on the line feature class. If there are no COGO attributes on parcel lines, that is, parcel lines have been generated from  polygons, the importer will generate COGO attributes from the line geometry. If there are COGO attributes, the tool will migrate the existing COGO attributes to parcel fabric lines. ", "You can populate system attributes in parcel fabric tables using attributes on your source parcel polygon and parcel line feature classes. For example, to migrate parcel identification numbers (PINs) to the ", "Name", " field in the fabric parcels table, your source polygon feature class needs to have a PIN, NAME, LOT, or APN attribute. ", "Learn more about populating system attributes in parcel fabric tables", "If a line feature class is used as the input feature class to migrate unclosed parcels, you can populate system attributes in the parcels table using attributes on the source parcel line feature class. For example, to migrate a   parcel identification number (PIN) for an unclosed parcel, there needs to be a PIN, NAME, LOT, or APN attribute on your source parcel line feature class.", "Learn more about populating system attributes in parcel fabric tables", "User-defined attributes on the polygon and line feature classes can be migrated to the parcels and lines tables in the fabric provided that the same user-defined attribute is created  on the fabric tables table before migrating the data.", "Specify a point feature class for the ", "Input Point Features (optional)", " parameter if you want to migrate user-defined attributes on a point feature class to the points table in the parcel fabric. The same user-defined attribute should be created on the fabric points table before migration for the attribute values to be successfully migrated.", "When data is migrated to the fabric, fabric points are automatically created at the endpoints of fabric lines. The system X and Y attribute values of the fabric points are automatically populated with the point shape coordinates. Point features are only specified in this parameter to migrate user-defined attributes to the points table in the parcel fabric. As with polygon and line features, if there is a selection on the points feature layer, only the selected points will be migrated.", "The ", "Minimum Line String Segment Count (optional)", " is the minimum number of line segments a line feature can have before it is considered  as a line string or natural boundary.", "If a tolerance is specified for the ", "Control Match Tolerance (optional)", " parameter, any migrated fabric points that lie within the specified match tolerance of an existing control point in the fabric will be linked to the control point.", "If the ", "Import Parcels as Unjoined Group", " option is checked, parcels are migrated as an unjoined group. Unjoined parcels exist outside of the parcel fabric in their own local coordinate space.  Unjoined parcels can be joined to the parcel fabric at any time. You would want to migrate parcels as unjoined if additional work is required on the parcels before they become joined to the parcel fabric layer."], "parameters": [{"name": "target_parcel_fabric", "isOptional": false, "description": "The target parcel fabric where the data will be migrated. ", "dataType": "Parcel Fabric Layer"}, {"name": "in_topology_class", "isInputFile": true, "isOptional": false, "description": " Input feature class or layer that is part of a topology. The feature class can either be a line or polygon. ", "dataType": "Feature Layer"}, {"name": "in_point_class", "isInputFile": true, "isOptional": true, "description": " Input point feature class or layer. The point feature class does not need to be part of a topology. Only user-defined attributes on the input point feature class will be migrated to corresponding points in the parcel fabric. ", "dataType": "Feature Layer"}, {"name": "linestring_minimum_segments", "isOptional": true, "description": " The minimum number of segments a polyline feature can have before it is considered and migrated as a line string or natural boundary in the parcel fabric. The default is a minimum of 10 segments. ", "dataType": "Long"}, {"name": "control_match_tolerance", "isOptional": true, "description": "The tolerance in which new, migrated fabric points are linked with existing control points found in the fabric. The tolerance length units are the same as the length units of the coordinate system of the fabric. If a control match tolerance is not specified, the default of 0.1 meters is used. ", "dataType": "Linear unit"}, {"name": "unjoined_group", "isOptional": false, "description": "Determines how features will be migrated. UNJOINED_GROUP \u2014 Features will be migrated as unjoined, stand-alone parcels to the parcel fabric. JOINED_GROUP \u2014 Features will be migrated as joined parcels to the parcel fabric", "dataType": "Boolean"}, {"name": "direction_units", "isOptional": true, "description": "The direction units to be used when generating COGO bearing attributes for line features during the migration process. DEGREES_MINUTES_SECONDS \u2014 One degree equals 1/360 of a circle. Fractions of a degree are represented in minutes and seconds, where one minute equals 1/60 of a degree and one second equals 1/60 of a minute. Degrees Minutes and Seconds are stored as strings and interpreted accordingly. This is the default. DECIMAL_DEGREES \u2014 Similar to Degrees Minutes and Seconds, but fractions of degrees are represented as decimal values. Any number between 0 and 360 is valid. RADIANS \u2014 An angular unit of measure, where there are 2 pi or approximately 6.28318 in a complete circle. One radian is equivalent to about 57.296 degrees. Any number between 0 and 62318 is valid. GONS \u2014 The same angular unit of measure as gradians where the right angle is divided into 100 parts. One gon is equal to 1/400 of a circle. GRADIANS \u2014 An angular unit of measure where the right angle is divided into 100 parts. One gradian is equal to 1/400 of a circle.", "dataType": "String"}, {"name": "direction_type", "isOptional": true, "description": " The direction type to be used when generating COGO bearing attributes for line features during the migration process. SOUTH_AZIMUTH \u2014 Directions are measured clockwise from south. NORTH_AZIMUTH \u2014 Directions are measured clockwise from north. POLAR \u2014 Directions are measured counterclockwise from the positive x-axis. QUADRANT_BEARING \u2014 Directions are measured from a reference bearing of North or South, then East or West. NE bearings are measured clockwise from North. SE bearings are measured counterclockwise from South. SW bearings are measured clockwise from South. NW bearings are measured counterclockwise from North. This is the default.", "dataType": "String"}, {"name": "compute_area", "isOptional": false, "description": "Determines how features will be migrated. COMPUTE_AREA \u2014 Parcel area is computed from the polygon shape or COGO attributes. The Stated Area fabric system attribute on the fabric parcels table is populated with the computed value. NO_COMPUTE \u2014 Parcel area is not computed from the polygon shape or COGO attributes. The Stated Area fabric system attribute on the fabric parcels table is not populated.", "dataType": "Boolean"}, {"name": "area_units", "isOptional": true, "description": " If the Compute Area for New Parcels option is checked, select the area units to be used when computing parcel area during the migration process. SQUARE_METERS_HECTARE_OR_KILOMETERS \u2014 Depending on the size of the value, Square Meters, Hectares, or Kilometers is used as the unit of area. For example, if the area value is greater than 10,000, the area unit that will be used is Hectares. This is the default. ACRES_ROODS_OR_PERCHES \u2014 Depending on the size of the value, Acres, Roods, or Perches is used as the unit of area. For example, if the area value is greater than 160, the area unit that will be used is Acres. SQUARE_METERS \u2014 An International System of Units (SI) derived unit of area. Defined as the area of a square whose sides measure exactly one meter. HECTARES \u2014 An SI unit of area equal to 10,000 Square Meters. Symbolized as ha. ACRES \u2014 A United States Customary or Imperial unit of area equal to 4046.87 m\u00b2 or 44,560 Square Feet. SQUARE_RODS \u2014 A United States Customary or Imperial unit of area equal to 5.0292 meters or 16.5 Feet. A Rod is the same length as a Perch and 160 Rods equals one Acre. ROODS \u2014 A United States Customary or Imperial unit of area. One Acre equals four Roods and one Rood equals 40 Perches. PERCHES \u2014 A United States Customary or Imperial unit of area equal to a Square Rod. 160 Perches equals one Acre. SQUARE_FEET \u2014 A United States customary or Imperial unit of area. Defined as the area of a square whose sides measure exactly one Foot. One Foot equals 0.3048 Meters. SQUARE_US_FEET \u2014 A unit of area used when collecting survey data in the United States. One U.S. Foot equals 0.3048006 Meters. QUARTER_SECTIONS \u2014 An area of unit used under the Public Land Survey System in the United States. A Section is an area equal to one Square Mile or 640 Acres. A Quarter Section is one-fourth of a Square Mile and is equal to 160 Acres. SECTIONS \u2014 An area of unit used under the Public Land Survey System in the United States. A Section is an area equal to one Square Mile or 640 Acres. ", "dataType": "String"}, {"name": "radial_point_tolerance", "isOptional": true, "description": " The tolerance in which new, computed curve center points are matched with existing curve center points found in the fabric. Furthermore, if several computed curve center points lie within this tolerance, they are averaged and merged into a single center point. If a radial tolerance is not specified, the default of 0.5 meters is used. ", "dataType": "Linear unit"}, {"name": "accuracy_units", "isOptional": true, "description": " The accuracy category of the lines and polygons being migrated. Accuracy categories are defined by date of survey in the parcel fabric. Accuracy category 1 is the highest data accuracy (recently surveyed) and accuracy category 6 is the lowest data accuracy (year 1800 or lower). Accuracy categories are used in the fabric adjustment. 1_HIGHEST \u2014 Most recently surveyed and recorded data. Data accuracy is the highest. 2_AFTER_1980 \u2014 Data is surveyed and recorded after 1980. 3_1908_TO_1980 \u2014 Data is surveyed and recorded between 1908 and 1980. 4_1881_TO_1907 \u2014 Data is surveyed and recorded between 1881 and 1907. 5_BEFORE_1881 \u2014 Data is surveyed and recorded before 1881. Data accuracy is low. 6_1800 \u2014 Data is surveyed and recorded before 1800. Data accuracy is low. This is the default. 7_LOWEST \u2014 Data is unreliable and data accuracy is unknown. Data is excluded from influencing the outcome of a fabric adjustment.", "dataType": "String"}]},
{"syntax": "DelineateBuiltUpAreas_cartography (in_buildings, {identifier_field}, out_feature_class, grouping_distance, minimum_detail_size, {edge_features})", "name": "Delineate Built-Up Areas (Cartography)", "description": "\r\nCreates polygons to represent built-up areas by delineating densely clustered arrangements of buildings on small-scale maps. \r\n The boundaries\u2014or edges\u2014of the output polygons can be dictated by the location of other features such as roads or hydrology. Input buildings can be  attributed to identify which can be replaced in maps by the built-up area polygons for a more generalized depiction. ", "example": {"title": "DelineateBuiltUpAreas tool example (Python window)", "description": "The following Python window script demonstrates how to use the DelineateBuiltUpAreas tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" env.referenceScale = \"50000\" arcpy.DelineateBuiltUpAreas_cartography ( \"bldg_pnt;bldg_poly\" , \"inBUA\" , \"RoadNetwork\" , \"100 Meters\" , \"125 Meters\" , \"BUApolypoint\" )"}, "usage": ["\r\nInput buildings are clustered together based on the ", "Grouping Distance", " parameter to define dense arrangements of buildings that can be represented by a single built-up area polygon for smaller-scale display. Run the tool on the same input features with a different \r\n grouping distance to produce polygons representing  different degrees of building grouping that can be used for display at differing scales or even shown together at a single scale.", "Use ", "Edge Features", " to align the edges of built-up area polygons to other features displayed in the same map, such as roads or administrative areas. Polygon outlines snap only to those features that are generally trending in the same direction as the polygon edge and within the grouping distance away. ", "Use ", "Identifier Field", " to identify buildings that were considered in the formation of the built-up area polygons. When displaying built-up areas at midscale ranges, you might want to use a definition query on the building layers to draw only those buildings not represented by a built-up area. ", "To create a more simplified set of built-up area polygons for use at an even smaller scale, consider running the tool again, this time using the output built-up area polygons as the input layer. Use a larger ", "Minimum Detail Size", " value to reduce the complexity of the polygons and a different ", "edge features", " value (corresponding to features that will be drawn on the smaller-scale map), if applicable. ", "The output feature class includes a field called ", "bldg_count", ", which tells you how many buildings are replaced by each built-up area polygon. Use this field in a layer definition query to limit the display of built-up area polygons to only those that  represent a minimum number of buildings.    ", " When point buildings are used as inputs, the edges of the resulting built-up area polygons may cross  symbolized point buildings when drawn at scale. If this is undesirable, consider using the attributes in the identifier field to help identify buildings that fall near the edges of the built-up area polygons to filter them from display.  "], "parameters": [{"name": "in_buildings", "isInputFile": true, "isOptional": false, "description": " The layers containing buildings whose density and arrangement are used to define appropriate output built-up polygons. Multiple building layers can be assessed simultaneously. Building features can be points or polygons. ", "dataType": "Feature Layer"}, {"name": "identifier_field", "isOptional": true, "description": "A field on the input feature classes that will hold a status code indicating whether the input feature is part of the resulting built-up area . This field must be either short or long integer type and common in all input layers, if multiple input layers are used. 0 = The building is not represented by an output built-up area polygon. 1 = The building is represented by an output built-up area polygon and is within the resulting polygon. 2 = The building is represented by an output built-up area polygon and is outside the resulting polygon.", "dataType": "String"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing built-up area polygons representing clustered arrangements of input buildings. ", "dataType": "Feature Class"}, {"name": "grouping_distance", "isOptional": false, "description": " Buildings closer together than the grouping distance are considered collectively as candidates for representation by an output built-up area polygon. This distance is measured from the edges of polygon buildings and the centers of point buildings. ", "dataType": "Linear unit"}, {"name": "minimum_detail_size", "isOptional": false, "description": " Defines the relative degree of detail in the output built-up area polygons. This roughly translates to the minimum allowable diameter of a hole or cavity in the built-up area polygon. The actual size and shape of holes and cavities within the polygon is determined also by the arrangement of the input buildings, the grouping distance, and the presence of edge features, if they are used. ", "dataType": "Linear unit"}, {"name": "edge_features", "isOptional": false, "description": " The layers that can be used to define the edges of the built-up area polygons. Typically, these are roads, but other common examples are rivers, coastlines, or administrative areas. Built-up area polygons snap to an edge feature if one is generally aligned with the trend of the polygon edge and within the grouping distance away. Edge features can be lines or polygons. ", "dataType": "Feature Layer"}]},
{"syntax": "CollapseRoadDetail_cartography (in_features, collapse_distance, output_feature_class)", "name": "Collapse Road Detail (Cartography)", "description": "\r\nCollapses small, open  configurations of road segments that interrupt the general trend of a road network, such as traffic circles, for example, and replaces them with a simplified depiction.\r\n \r\n Configurations are collapsed regardless of  road class if the diameter across the open area is less than or equal to the  Collapse Distance    parameter. All uncollapsed roads from the input collection are copied to the output feature class.  To learn more, see  How Collapse Road Detail works .", "example": {"title": "CollapseRoadDetail tool example (Python window)", "description": "The following Python window script demonstrates how to use the CollapseRoadDetail tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/cartography.gdb/transportation\" arcpy.CollapseRoadDetail_cartography ( \"roads.lyr\" , \"250 Feet\" , \"roads_collapse_250\" )"}, "usage": ["Dense blocks of streets or other complex arrangements are not collapsed or thinned out. Consider using the ", "Thin Road Network", " tool to reduce the density of  streets. ", "\r\n\r\nCircles, or similar open road details,  that are connected to divided roads are not collapsed to avoid creating very small road segments. Consider running the ", "Merge Divided Roads", " tool first to create a single road and collapse the circles after.  ", "If a circle or other  open road detail cannot be modified without impacting network connectivity, collapse does not occur.", "Consider running this tool more than once at different collapse distances to create output suitable for different scales.", "Processing large road datasets  may exceed memory limitations. In this case, consider processing input data by partition by identifying a relevant  polygon feature class in the ", "Cartographic Partitions", " environment setting. Portions of the data, defined by partition boundaries, are processed sequentially. The output feature class is consistent at partition edges."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input features containing small enclosed road details, such as traffic circles, to be collapsed. ", "dataType": "Feature Layer"}, {"name": "collapse_distance", "isOptional": false, "description": " The diameter of or distance across the road detail that is to be considered for collapse. ", "dataType": "Linear unit"}, {"name": "output_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing the collapsed features\u2014features that were modified to accommodate the collapse\u2014and all unaffected features. ", "dataType": "Feature Class"}]},
{"syntax": "CreateCartographicPartitions_cartography (in_features, out_features, feature_count)", "name": "Create Cartographic Partitions (Cartography)", "description": "\r\n  Creates a mesh of polygon features that cover the input feature class where each polygon encloses no more than a specified number of  input features, determined by the density and distribution of the input features.\r\n  The resulting  partition feature class is ideally suited for the   Cartographic Partitions  geoprocessing environment setting. The Cartographic Partitions environment setting dictates to certain generalization or conflict-resolution geoprocessing tools to load and process input features by partition. These tools operate contextually, meaning that multiple features, possibly from multiple themes, must be loaded simultaneously. Memory limitations are encountered with large datasets. Partitioning allows large datasets to be processed by these tools in portions sequentially.", "example": {"title": "CreateCartographicPartitions tool example (Python window)", "description": "The following Python window script demonstrates how to use the CreateCartographicPartitions tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/cartography.gdb/transportation\" arcpy.CreateCartographicPartitions_cartography ( \"roads.lyr\" , \"partitions\" , \"50000\" )"}, "usage": ["This tool creates a polygon feature class designed to be used in the ", "Cartographic Partitions", " geoprocessing environment setting. Tools that honor this environment setting process input features in portions defined by the partition polygons to avoid exceeding memory limitations. The following tools honor the Cartographic Partitions environment setting:", "\r\nInput features should correspond to the features that you intend to process together for multiscale display.  \r\nFor example, if you intend to use the  ", "Thin Road Network", " tool to process a feature class containing streets together with another containing highways, enter both of these as inputs to the ", "Create Cartographic Partitions", " tool to create partitions that are relevant to the distribution and density of both datasets considered together.  "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input feature classes or layers whose feature distribution and density dictate the size and arrangement of output polygons. The input features are typically destined for subsequent processing with contextual generalization or conflict resolution geoprocessing tools. Typically, the input features, when considered simultaneously, would exceed memory limitations of generalization or conflict-resolution geoprocessing tools, so partitions are created to subdivide inputs for processing. ", "dataType": "Feature Layer"}, {"name": "out_features", "isOutputFile": true, "isOptional": false, "description": "The output polygon feature class of partitions, each of which encloses a manageable number of input features not exceeding the number specified by the Feature Count parameter. ", "dataType": "Feature Class"}, {"name": "feature_count", "isOptional": false, "description": "The ideal number of features to be enclosed by each polygon in the output feature class. The recommended count is 50,000 features, which is the default value. The feature count cannot be lower than 500. ", "dataType": "Long"}]},
{"syntax": "ContourAnnotation_cartography (in_features, out_geodatabase, contour_label_field, reference_scale_value, out_layer, contour_color, {contour_type_field}, {contour_alignment}, {enable_laddering})", "name": "Contour Annotation (Cartography)", "description": "\r\nCreates annotation for contour features. The tool creates an annotation feature class with corresponding mask polygons based on input contour features.", "example": {"title": "ContourAnnotation Example (Python Window)", "description": "The following stand-alone script demonstrates how to use the ContourAnnotation function.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data/data.gdb\" arcpy.ContourAnnotation_cartography ( \"Contours\" , \"C:/data/data.gdb\" , \"Contour\" , 50000 , \"ContourGroupLayer\" , \"BLACK\" , \"\" , \"PAGE\" , \"\" )"}, "usage": [" The output of this tool is a group layer.  The group layer will contain the input contour features, the annotation layer, and the mask polygons.", "An existing group layer will be overwritten if the same layer name is specified and if you explicitly state that overwriting outputs is allowed.", "When working in ArcCatalog or ModelBuilder, you can use the ", "Save_To_Layer_File", " tool to write the output group layer to a layer file. When using ArcMap, the tool adds the group layer to the display if this option is checked in the geoprocessing options. The group layer that is created is temporary and will not persist after the session ends unless the document is saved.", "Group layers created in ArcCatalog cannot be used in ArcMap unless they are saved to a layer file using the ", "Save_To_Layer_File", " tool.", "Annotation feature classes will not be overwritten if the tool is run multiple times on a single contour feature class. In this case, a number will be added to the annotation feature class  (e.g., ContourAnno, ContourAnno_1, and so on).", "Each mask will be created with a two-point margin around the annotation feature and a mask type of exact simplified, meaning that the mask will be a generalized polygon representing the exact shape of the annotation.", "There are three choices for the color of the contour layer and output annotation: black, brown, and blue.", "Contours that have been created using the ", "Contour With Barriers", " tool contain a Type field.    The Type field contains one or more of the following values:", "This Type field can be used as input for the ", "Contour Type Field", " parameter.   A separate annotation class will be created for annotation of each type."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The contour line feature class for which the annotation will be created. ", "dataType": "Feature Layer"}, {"name": "out_geodatabase", "isOutputFile": true, "isOptional": false, "description": " The workspace where the output feature classes will be saved. The workspace can be an existing geodatabase or an existing feature dataset. ", "dataType": "Workspace; Feature Dataset"}, {"name": "contour_label_field", "isOptional": false, "description": " The field in the input layer attribute table upon which the annotation text will be based. ", "dataType": "Field"}, {"name": "reference_scale_value", "isOptional": false, "description": " Enter the scale to use as a reference for the annotation. This sets the scale to which all symbol and text sizes in the annotation will be based. ", "dataType": "Double"}, {"name": "out_layer", "isOutputFile": true, "isOptional": false, "description": " The group layer that will contain the contour layer, the annotation, and the mask layer. When working in ArcCatalog, you can use the Save To Layer File tool to write the output group layer to a layer file. When using ArcMap, the tool adds the group layer to the display if this option is checked in the geoprocessing options. The group layer that is created is temporary and will not persist after the session ends unless the document is saved. ", "dataType": "Group Layer"}, {"name": "contour_color", "isOptional": false, "description": " The color of the output contour layer and annotation features. BLACK \u2014 The output contour layer and annotation features will be drawn in black. This is the default. BROWN \u2014 The output contour layer and annotation features will be drawn in brown. BLUE \u2014 The output contour layer and annotation features will be drawn in blue.", "dataType": "String"}, {"name": "contour_type_field", "isOptional": true, "description": " The field in the input layer attribute table containing a value for the type of contour feature. An annotation class will be created for each type value. ", "dataType": "Field"}, {"name": "contour_alignment", "isOptional": true, "description": " The annotation can be aligned to the contour elevations so that the top of the text is always placed uphill. This option allows the annotation to be placed upside down. The contour annotation can also be aligned to the page, ensuring that the text is never placed upside down. PAGE \u2014 The annotation will be aligned to the page, ensuring that the text is never placed upside down. This is the default. UPHILL \u2014 The annotation will be aligned to the contour elevations so that the top of the text is always placed uphill. This option allows the annotation to be placed upside down.", "dataType": "String"}, {"name": "enable_laddering", "isOptional": true, "description": "Placing annotation in ladders will place the text so it appears to step up and step down the contours in a straight path. These ladders will run from the top of a hill to the bottom, will not cross each other, will belong to a single slope, and will not cross any other slope. ENABLE_LADDERING \u2014 Annotation will step up and down the contours in a straight path. NOT_ENABLE_LADDERING \u2014 Annotation will not be placed up and down the contours in a straight path. This is the default.", "dataType": "Boolean"}]},
{"syntax": "AggregatePoints_cartography (in_features, out_feature_class, aggregation_distance)", "name": "Aggregate Points (Cartography)", "description": " Creates polygon features around clusters of proximate point features. ", "example": {"title": "AggregatePoints tool Example (Python Window)", "description": "The following Python Window script demonstrates how to use the AggregatePoints tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.AggregatePoints_cartography ( \"C:/data/cartography.gdb/crime/robberies\" , \"C:/data/cartography.gdb/crime/clustered_robberies\" , \"100 meters\" )"}, "usage": ["Polygons are created around clusters of three or more points within the aggregation distance.", "A one-to-many relationship table\u2014named the same as the output feature class appended with ", "_Tbl", "\u2014will be created that links the aggregated\r\npolygons to their source point features. This table is in the same location as the output feature class and contains two\r\nfields, OUTPUT_FID and INPUT_FID, storing the aggregated feature\r\nIDs and their source feature IDs, respectively. The link can become\r\nincorrect when any of the input or output features are modified.\r\nWith this table, you can derive necessary attributes for the output\r\nfeatures from their source features using appropriate geoprocessing\r\ntools.\r\n", "Point aggregation may introduce polygon holes or areas where adjacent polygon boundaries meet at one vertex."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input point features that will be assessed for proximity and clustering. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The feature class created to hold the polygons that represent the point clusters. ", "dataType": "Feature Class"}, {"name": "aggregation_distance", "isOptional": false, "description": " The distance between points that will be clustered. ", "dataType": "Linear unit"}]},
{"syntax": "SetRepresentationControlPointByAngle_cartography (in_features, maximum_angle)", "name": "Set Representation Control Point By Angle (Cartography)", "description": "Places a representation control point at vertices along a line or polygon boundary where the inner angle is less than or equal to a specified maximum angle.", "example": {"title": "SetRepresentationControlPointByAngle tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the SetRepresentationControlPointByAngle tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\" arcpy.SetRepresentationControlPointByAngle_cartography ( \"trails.lyr\" , \"135\" )"}, "usage": ["Input features must be a line or polygon layer symbolized with a representation.", "The modification to vertices is stored as a geometry override on the feature representation."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing line or polygon representations. ", "dataType": "Layer"}, {"name": "maximum_angle", "isOptional": false, "description": "The angle used to determine whether or not a vertex along a line or polygon boundary will be set as a representation control point. The angle value must be greater than zero and less than 180 decimal degrees. ", "dataType": "Double"}]},
{"syntax": "SetRepresentationControlPointAtIntersect_cartography (in_line_or_polygon_features, {in_features})", "name": "Set Representation Control Point At Intersect (Cartography)", "description": "This tool is commonly used to synchronize boundary symbology on adjacent polygons. It creates a representation control point at vertices that are shared by one or more line or polygon features. ", "example": {"title": "SetRepresentationControlPointAtIntersect tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the SetRepresentationControlPointAtIntersect tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\" arcpy.SetRepresentationControlPointAtIntersect_cartography ( \"parcels.lyr\" , \"roads.lyr\" )"}, "usage": ["Representation control points can be used to control the phasing of patterned representation symbology like dashed lines. Use this tool to synchronize phased outline symbology on adjacent polygons.", "The primary input must be a line or polygon feature layer that is symbolized with a feature class representation. The secondary input can be a point, line, or polygon feature class; it does not need to have a feature class representation; these features provide geometry for comparison to the primary input features. ", "Only existing vertices will be converted to representation control points. This tool will not create a representation control point at a location without an existing vertex. ", "The secondary input receives representation control points only if it contains a feature class representation. This allows features from both inputs to receive representation control points simultaneously. If the secondary input is not specified, the tool operates on the primary input only, allowing self-intersecting features to be processed.", "Modifications to the vertices are stored as geometry override on the feature representation.", "Both input layers can accept multipart geometry."], "parameters": [{"name": "in_line_or_polygon_features", "isInputFile": true, "isOptional": false, "description": "The input line or polygon feature layer symbolized with a feature class representation. ", "dataType": "Layer"}, {"name": "in_features", "isInputFile": true, "isOptional": true, "description": "The feature layer with coincident features. These features can be from a geodatabase, shapefile, or coverage. ", "dataType": "Feature Layer"}]},
{"syntax": "DisperseMarkers_cartography (in_point_features, minimum_spacing, {dispersal_pattern})", "name": "Disperse Markers (Cartography)", "description": "Finds representation markers that are overlapping or too close to one another and spreads them apart based on a minimum spacing and dispersal pattern.", "example": {"title": "DisperseMarkers tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the DisperseMarkers tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" env.referenceScale = \"50000\" arcpy.DisperseMarkers_cartography ( \"crime.lyr\" , \"2 Points\" , \"EXPANDED\" )"}, "usage": ["The input must be a point feature layer containing representations. Multipoint features are not a valid input. Location changes are stored as geometry overrides on the input representation.", "The analysis of marker groups is based on a minimum rectangular envelope around each marker. If any two marker envelopes are within the minimum spacing of each other or overlap, then the two markers are considered in the same marker group.", "Clustered or coincident representation markers are assessed and processed in groups. Within each group, markers are dispersed to the minimum spacing. Minimum spacing of zero will result in markers that touch at their edges.", "Graphical overlaps may be introduced between marker groups. Use the ", "Detect Graphic Conflict", " tool to identify conflicts."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point feature layer containing marker representations. ", "dataType": "Layer"}, {"name": "minimum_spacing", "isOptional": false, "description": "The minimum separation distance between individual markers, in page units. A distance must be specified and must be greater than or equal to zero. When a positive value is specified, markers will be separated by that value; when a value of zero is specified, markers will be touching. The default page unit is Points. ", "dataType": "Linear unit"}, {"name": "dispersal_pattern", "isOptional": true, "description": "Specifies the pattern in which the dispersed representation markers are placed. A group of markers will have a center of mass derived from the locations of each marker in the group. The center of mass is then used as the anchor point around which the dispersal pattern operates. EXPANDED \u2014 The general pattern of the markers will be maintained as they are spread apart. Markers that were exactly coincident are dispersed to a circle around their center of mass. This is the default. RANDOM \u2014 Representation markers are placed around the center of mass in a random dispersal that respects the minimum spacing. SQUARES \u2014 Representation markers are placed in multiple square rings around the center of mass, ensuring that all markers are placed as closely together as allowable by the minimum spacing parameter. RINGS \u2014 Representation markers are placed in multiple circular rings around the center of mass, ensuring that all markers are placed as closely together as allowable by the minimum spacing parameter. SQUARE \u2014 Representation markers are placed evenly around the center of mass in a single square pattern. RING \u2014 Representation markers are placed evenly around the center of mass in a single circular pattern. CROSS \u2014 Representation markers are spaced evenly on horizontal and vertical axes originating from the center of mass. X_CROSS \u2014 Representation markers are spaced evenly on 45\u00b0 axes originating from the center of mass. ", "dataType": "String"}]},
{"syntax": "CreateUnderpass_cartography (in_above_features, in_below_features, margin_along, margin_across, out_underpass_feature_class, out_mask_relationship_class, {where_clause}, {out_decoration_feature_class}, {wing_type}, {wing_tick_length})", "name": "Create Underpass (Cartography)", "description": "Allows intersecting lines to be displayed as underpassing one another by creating bridge parapets and masks to cover the underlying road segment. ", "example": {"title": "CreateUnderpass tool example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CreateUnderpass tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\" env.referenceScale = \"50000\" arcpy.CreateUnderpass_cartography ( \"roads.lyr\" , \"railroads.lyr\" , \"2 Points\" , \"1 Points\" , \"cartography.gdb/transportation/under_mask_fc\" , \"cartography.gdb/transportation/under_mask_rc\" , \"'RelationshipToSurface' = 3\" , \"cartography.gdb/transportation/tunnel\" , \"PARALLEL\" , \"1 Points\" )"}, "usage": ["Requires intersecting line features symbolized with stroke representations as inputs.", "This tool is identical to the ", "Create Overpass", " tool except that the ", "where_clause", " parameter selects from the ", "Input Below Features With Representations", " parameter in this tool, and selects from  the ", "Input Above Features With Representations", " in the Create Overpass tool.  ", "The ", "Input Above Features With Representations", " layer can be the same as the ", "Input Below Features With Representations", " layer in the case of self-overlapping features. When Input Above and Input Below representations are the same, an SQL expression is required for further refinement of feature selection."], "parameters": [{"name": "in_above_features", "isInputFile": true, "isOptional": false, "description": "The input line feature layer containing stroke representations that intersect\u2014and will be symbolized as passing above\u2014stroke representations in the Input Below Features. ", "dataType": "Layer"}, {"name": "in_below_features", "isInputFile": true, "isOptional": false, "description": "The input line feature layer containing stroke representations that intersect\u2014and will be symbolized as passing below\u2014stroke representations in the Input Above Features. These features will be masked by the polygons created in the Output Overpass feature class. ", "dataType": "Layer"}, {"name": "margin_along", "isOptional": false, "description": "Sets the length of the mask polygons along the Input Above Features by specifying the distance in page units that the mask should extend beyond the width of the stroke symbol of the Input Below features. The Margin Along must be specified, and it must be greater than or equal to zero. Choose a page unit for the margin; the default is points. ", "dataType": "Linear unit"}, {"name": "margin_across", "isOptional": false, "description": "Sets the width of the mask polygons across the Input Above Features by specifying the distance in page units that the mask should extend beyond the width of the stroke symbol of the Input Below Features. The Margin Across must be specified, and it must be greater than or equal to zero. Choose a page unit for the margin; the default is points. ", "dataType": "Linear unit"}, {"name": "out_underpass_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class that will be created to store polygons to mask the Input Below Features. ", "dataType": "Feature Class"}, {"name": "out_mask_relationship_class", "isOutputFile": true, "isOptional": false, "description": "The output relationship class that will be created to store links between Underpass mask polygons and the stroke representations of the Input Below Features. ", "dataType": "Relationship Class"}, {"name": "where_clause", "isOptional": true, "description": "A SQL expression used to select a subset of features in the Input Below Features with Representations parameter. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, or coverages, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In the Python window, enclose the {where_clause} in parentheses to ensure the spaces (which are delimiters between parameters) are correctly interpreted. For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}, {"name": "out_decoration_feature_class", "isOutputFile": true, "isOptional": true, "description": "The output line feature class that will be created to store parapet features. ", "dataType": "Feature Class"}, {"name": "wing_type", "isOptional": true, "description": "Specifies the wing style of the parapet features. ANGLED \u2014 Specifies that the wing tick of the parapet will be angled between the Input Above Features and the Input Below Features. This is the default. PARALLEL \u2014 Specifies that the wing tick of the overpass wing will be parallel to the Input Below Features. NONE \u2014 Specifies that no wing ticks will be created on the parapets. ", "dataType": "String"}, {"name": "wing_tick_length", "isOptional": true, "description": "Sets the length of the parapet wings in page units. The length must be greater than or equal to zero; the default length is 1. Choose a page unit (points, millimeters, and so on) for the length; the default is points. This parameter does not apply to the Wing Type - NONE. ", "dataType": "Linear unit"}]},
{"syntax": "CreateOverpass_cartography (in_above_features, in_below_features, margin_along, margin_across, out_overpass_feature_class, out_mask_relationship_class, {where_clause}, {out_decoration_feature_class}, {wing_type}, {wing_tick_length})", "name": "Create Overpass (Cartography)", "description": "Allows intersecting lines to be displayed as overpassing one another by creating bridge parapets and masks to cover the underlying road segment. ", "example": {"title": "CreateOverpass tool example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CreateOverpass tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\" env.referenceScale = \"50000\" arcpy.CreateOverpass_cartography ( \"roads.lyr\" , \"railroads.lyr\" , \"2 Points\" , \"1 Points\" , \"cartography.gdb/transportation/over_mask_fc\" , \"cartography.gdb/transportation/over_mask_rc\" , \"'Bridge_Category' = 3\" , \"cartography.gdb/transportation/bridge\" , \"ANGLED\" , \"1 Points\" )"}, "usage": ["Requires intersecting line features symbolized with stroke representations as inputs.", "The ", "Input Above Features With Representations", " layer can be the same as the ", "Input Below Features With Representations", " layer in the case of self-overlapping features. When Input Above and Input Below representations are the same, an SQL expression is required for further refinement of feature selection.", "This tool is identical to the ", "Create Underpass", " tool except that the", " where_clause", " parameter selects from the ", "Input Above Features With Representations", " parameter in this tool, and selects from  the ", "Input Below Features With Representations", " in the Create Underpass tool.  "], "parameters": [{"name": "in_above_features", "isInputFile": true, "isOptional": false, "description": "The input line feature layer containing stroke representations that intersect\u2014and will be symbolized as passing above\u2014stroke representations in the Input Below Features. ", "dataType": "Layer"}, {"name": "in_below_features", "isInputFile": true, "isOptional": false, "description": "The input line feature layer containing stroke representations that intersect\u2014and will be symbolized as passing below\u2014stroke representations in the Input Above Features. These features will be masked by the polygons created in the Output Overpass feature class. ", "dataType": "Layer"}, {"name": "margin_along", "isOptional": false, "description": "Sets the length of the mask polygons along the Input Above Features by specifying the distance in page units that the mask should extend beyond the width of the stroke symbol of the Input Below Features. The Margin Along must be specified, and it must be greater than or equal to zero. Choose a page unit (points, millimeters, and so on) for the margin; the default is points. ", "dataType": "Linear unit"}, {"name": "margin_across", "isOptional": false, "description": "Sets the width of the mask polygons across the Input Above Features by specifying the distance in page units that the mask should extend beyond the width of the stroke symbol of the Input Below Features. The Margin Across must be specified, and it must be greater than or equal to zero. Choose a page unit (points, millimeters, and so on) for the margin; the default is points. ", "dataType": "Linear unit"}, {"name": "out_overpass_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class that will be created to store polygons to mask the Input Below features. ", "dataType": "Feature Class"}, {"name": "out_mask_relationship_class", "isOutputFile": true, "isOptional": false, "description": "The output relationship class that will be created to store links between Overpass mask polygons and the stroke representations of the Input Below Features. ", "dataType": "Relationship Class"}, {"name": "where_clause", "isOptional": true, "description": "An SQL expression used to select a subset of features in the Input Above Features with Representations parameter. The syntax for the expression differs slightly depending on the data source. For example, if you're querying file or ArcSDE geodatabases, shapefiles, or coverages, enclose field names in double quotes: \"MY_FIELD\" If you're querying personal geodatabases, enclose fields in square brackets: [MY_FIELD] In the Python window, enclose the {where_clause} in parentheses to ensure the spaces (which are delimiters between parameters) are correctly interpreted. For more information on SQL syntax and how it differs between data sources, see the help topic SQL reference for query expressions used in ArcGIS . ", "dataType": "SQL Expression"}, {"name": "out_decoration_feature_class", "isOutputFile": true, "isOptional": true, "description": "The output line feature class that will be created to store parapet features. ", "dataType": "Feature Class"}, {"name": "wing_type", "isOptional": true, "description": "Specifies the wing style of the parapet features. ANGLED \u2014 Specifies that the wing tick of the parapet will be angled between the Input Above Features and the Input Below Features. This is the default. PARALLEL \u2014 Specifies that the wing tick of the overpass wing will be parallel to the Input Below Features. NONE \u2014 Specifies that no wing ticks will be created on the parapets. ", "dataType": "String"}, {"name": "wing_tick_length", "isOptional": true, "description": "Sets the length of the parapet wings in page units. The length must be greater than or equal to zero; the default length is 1. Choose a page unit (Points, Millimeters, and so on) for the length; the default is Points. This parameter does not apply to the Wing Type - NONE. ", "dataType": "Linear unit"}]},
{"syntax": "CalculatePolygonMainAngle_cartography (in_features, angle_field, {rotation_method})", "name": "Calculate Polygon Main Angle (Cartography)", "description": "Calculates the dominant angles of  input polygon features and assigns the values to a specified field in the feature class.", "example": {"title": "CalculatePolygonMainAngle tool example 1 (Python window)", "description": "The following Python window script demonstrates how to use the CalculatePolygonMainAngle tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\" arcpy.CalculatePolygonMainAngle_cartography ( \"cities\" , \"poly_angle\" , \"GEOGRAPHIC\" )"}, "usage": ["The dominant angle of a polygon is  the angle of  longest collection of segments that have similar orientation. This  angle  will be stored in the specified field in decimal degrees from true north.", "Use this tool  to determine the trend of a polygon and use the resulting angle to orient symbology such as markers or hatch lines within the polygon. This tool is meant for primarily orthogonal polygons rather than organically shaped ones."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input polygon features. ", "dataType": "Feature Layer"}, {"name": "angle_field", "isOptional": false, "description": "The field that will be updated with the polygon main angle values. ", "dataType": "Field"}, {"name": "rotation_method", "isOptional": true, "description": " Controls the method and origin point of rotation. GEOGRAPHIC \u2014 Angle is calculated clockwise with 0 at top/north. ARITHMETIC \u2014 Angle is calculated counterclockwise with 0 at the right/east. GRAPHIC \u2014 Angle is calculated counterclockwise with 0 at top/north. This is the default.", "dataType": "String"}]},
{"syntax": "CalculateLineCaps_cartography (in_features, {cap_type}, {dangle_option})", "name": "Calculate Line Caps (Cartography)", "description": "Modifies the cap type (ending style) for representation stroke symbols and stores it as a representation override. ", "example": {"title": "CalculateLineCaps tool Example (Python Window)", "description": "The following Python Window script demonstrates how to use the CalculateLineCaps tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\" arcpy.CalculateLineCaps_cartography ( \"roads.lyr\" , \"BUTT\" , \"CASED_LINE_DANGLE\" )"}, "usage": ["Representation stroke symbols that are relatively wide and drawn with multiple layers to display a \"cased road\" effect are often joined together at their endpoints with round line caps to prevent symbol gaps appearing at sharp angles between features. However, a round end cap is generally not desirable cartographic appearance for dead-end street (dangling line features that are not connected at their endpoint to another feature.) The line cap style is generally overriden to a butt or square style in these situations. This tool detects dangles and overrides the representation cap type of the stroke symbol."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing line representations. ", "dataType": "Layer"}, {"name": "cap_type", "isOptional": true, "description": "Defines how the ends of representation stroke symbols are drawn. The default cap type of representation strokes is round, where the symbol is terminated with a semicircle of radius equal to stroke width is centered at the line endpoint. This tool changes cap type to BUTT or SQUARE. BUTT \u2014 Specifies to end the representation stroke symbol exactly where the line geometry ends. This is the default. SQUARE \u2014 Specifies to end the representation stroke symbol with closed, square caps that extend past the endpoint of the line by half of the symbol width. ", "dataType": "String"}, {"name": "dangle_option", "isOptional": true, "description": "The Dangle parameter controls how line caps are calculated for adjoining line features that share an endpoint but are drawn with different representation symbology. CASED_LINE_DANGLE \u2014 Modifies the cap style for dangling lines (those not connected at their endpoints to another line) and also for the lines where a cased-line representation stroke symbol is joined at the endpoint of a single-line representation stroke symbol. This is the default. TRUE_DANGLE \u2014 Modifies the cap style only for endpoints that are not connected to another feature. ", "dataType": "String"}]},
{"syntax": "CalculateGeodesicAngle_cartography (in_features, angle_field)", "name": "Calculate Geodesic Angle (Cartography)", "description": "Calculates geodesic angles for the input features according to the defined coordinate system, and assigns the angle values to the specified field in the feature class that contains the input features.", "example": {"title": "CalculateGeodesicAngle tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the CalculateGeodesicAngle tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:\\data\" arcpy.CalculateGeodesicAngle_cartography ( \"cities\" , \"city_angle\" )"}, "usage": ["Angles are calculated according to the coordinate system of the active data frame of the current map document or the Coordinate System geoprocessing environment variable, if it is set. This environment setting must be set to execute the tool from ArcCatalog.", "The input features can be points, lines, or polygons. For a point feature, the point location will be used to calculate the geodesic angle. For a line or polygon feature, the center point (centroid) of geometry will be used to calculate the geodesic angle.", "A field must be present in the input feature attribute table to store the calculated angles. The stored angles are in decimal degrees."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input features for which geodesic angles will be computed. ", "dataType": "Feature Layer"}, {"name": "angle_field", "isOptional": false, "description": "The field that will be updated with the geodesic angle values in decimal degrees. ", "dataType": "Field"}]},
{"syntax": "AlignMarkerToStrokeOrFill_cartography (in_point_features, in_line_or_polygon_features, search_distance, {marker_orientation})", "name": "Align Marker To Stroke Or Fill (Cartography)", "description": "Align the representation marker symbols of a point feature class to the nearest stroke or fill representation symbols in a line or polygon feature class within a specified search distance.", "example": {"title": "AlignMarkerToStrokeOrFill tool Example (Python Window)", "description": "This stand-alone script shows an example of using the AlignMarkerToStrokeOrFill tool.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" env.referenceScale = \"50000\" arcpy.AlignMarkerToStrokeOrFill_cartography ( \"buildings.lyr\" , \"roads.lyr\" , \"2 Points\" , \"PERPENDICULAR\" )"}, "usage": ["Representation marker symbols are aligned by overriding the angle property of representation marker symbol layer.", "The search distance is the measurement from the graphical edge of the marker to the graphical edge of the nearest stroke or fill outline. Representation geometry (shape) overrides are honored. Representation markers beyond the search distance will not be rotated. A search distance of zero aligns only markers that are coincident to a representation stroke or fill symbol.", "Features that have null or invalid representation rules, or that have the representation visibility property off, will not be aligned. ", "Rotating markers may introduce graphic conflicts. The ", "Detect Graphic Conflict", " tool can be used to identify these areas."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point feature layer containing marker representations. ", "dataType": "Layer"}, {"name": "in_line_or_polygon_features", "isInputFile": true, "isOptional": false, "description": "The input line or polygon feature layer containing stroke or fill representations. ", "dataType": "Layer"}, {"name": "search_distance", "isOptional": false, "description": "The search distance from graphical marker edge to graphical stroke edge. A distance greater than or equal to zero must be specified. ", "dataType": "Linear unit"}, {"name": "marker_orientation", "isOptional": true, "description": "Specifies the representation marker orientation relative to the stroke or fill edge. PERPENDICULAR \u2014 aligns representation markers perpendicularly to the stroke or fill edge. This is the default. PARALLEL \u2014 aligns representation markers parallel to the stroke or fill edge. ", "dataType": "String"}]},
{"syntax": "UpdateOverride_cartography (in_features, representation, {update_option})", "name": "Update Override (Cartography)", "description": "Transfers feature representation overrides from the default override field to explicit fields as defined by the representation rules in the representation.", "example": {"title": "UpdateOverride tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the UpdateOverride tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.UpdateOverride_cartography ( \"footprints.lyr\" , \"footprints_Rep\" , \"BOTH\" )"}, "usage": ["Representation overrides are stored in the override field by default, for convenience. A manageable database model is one that leverages explicit fields to contain these overrides. Use this tool to expose overrides in the feature attribute table to support queries and selection. ", "The input must be a geodatabase feature class with at least one feature class representation.", "The explicit fields to be updated must be present on the input feature attribute table, and specified in the representation rules of the representation prior to using this tool.", "The explicit field to be used for field mapping can contain null values."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing at least one representation. ", "dataType": "Feature Layer"}, {"name": "representation", "isOptional": false, "description": "The representation containing overrides to be transferred to explicit fields. ", "dataType": "String"}, {"name": "update_option", "isOptional": true, "description": "Specifies the type of representation override to be transferred to explicit fields. REPRESENTATION_PROPERTY_OVERRIDE \u2014 Transfers representation property overrides only. This is the default. GEOMETRY_OVERRIDE \u2014 Transfers representation geometry overrides only, into the Shape field. The original geometry of the feature will be overwritten. BOTH \u2014 Transfers both representation property and geometry overrides. Representation geometry overrides will be transferred into the Shape field, overwriting the original geometry of the feature.", "dataType": "String"}]},
{"syntax": "SetLayerRepresentation_cartography (in_layer, representation)", "name": "Set Layer Representation (Cartography)", "description": "Sets a representation for a feature layer. The layer is temporary and stored in memory during the ArcGIS session, available for use in models and scripts.", "example": {"title": "SetLayerRepresentation tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the SetLayerRepresentation tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.SetLayerRepresentation_cartography ( \"footprints.lyr\" , \"buildings_Rep\" )"}, "usage": ["The input must be a geodatabase feature class with at least one feature class representation.", "The resulting layer can be used as a valid input for cartographic tools that require a layer input in especially in models and scripts.", "The temporary feature layer can be saved as a layer file using the ", "Save To Layer File", " tool or saved as a new feature class using the ", "Copy Features", " tool. Layers created in ArcCatalog cannot be used in ArcMap unless they are saved to a layer file using the ", "Save To Layer File", " tool."], "parameters": [{"name": "in_layer", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing at least one representation. ", "dataType": "Layer"}, {"name": "representation", "isOptional": false, "description": "The representation to be set for the input feature layer. ", "dataType": "String"}]},
{"syntax": "SelectFeatureByOverride_cartography (in_features, {select_option})", "name": "Select Feature By Override (Cartography)", "description": "Selects features that have representation geometry and/or property overrides.", "example": {"title": "SelectFeatureByOverride tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the SelectFeatureByOverride tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.SelectFeatureByOverride_cartography ( \"footprints.lyr\" , \"BOTH\" )"}, "usage": ["Input Features", " must be a layer whose source is a geodatabase feature class with at least one feature class representation.", "The output is a selected set, so the results can only be used in models, scripts, and in ArcMap sessions.", "Representation geometry overrides are sometimes called representation shape overrides."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing representations. ", "dataType": "Layer"}, {"name": "select_option", "isOptional": true, "description": "Specifies the type of representation override to use as a selection criterion. BOTH \u2014 Select features that have either representation geometry or property overrides. This is the default. GEOMETRY_OVERRIDE \u2014 Select all features that have representation geometry overrides. REPRESENTATION_PROPERTY_OVERRIDE \u2014 Select all features that have representation property overrides. ", "dataType": "String"}]},
{"syntax": "RemoveOverride_cartography (in_features, representation, {remove_option})", "name": "Remove Override (Cartography)", "description": "Removes geometry and/or property representation overrides from features participating in a feature class representation.", "example": {"title": "RemoveOverride tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the RemoveOverride tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.RemoveOverride_cartography ( \"footprints.lyr\" , \"footprints_Rep\" , \"BOTH\" )"}, "usage": ["Features will not be deleted, just representation overrides.", "The ", "Remove Option", " parameter specifies which type of representation override will be removed. "], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing at least one representation. ", "dataType": "Feature Layer"}, {"name": "representation", "isOptional": false, "description": "The representation from which representation overrides will be removed. ", "dataType": "String"}, {"name": "remove_option", "isOptional": true, "description": "Specifies which types of overrides will be removed. BOTH \u2014 Remove both geometry and property representation overrides. This is the default. GEOMETRY_OVERRIDE \u2014 Remove representation geometry overrides only. REPRESENTATION_PROPERTY_OVERRIDE \u2014 Remove representation property overrides only. ", "dataType": "String"}]},
{"syntax": "DropRepresentation_cartography (in_features, representation)", "name": "Drop Representation (Cartography)", "description": "Deletes a feature class representation from a geodatabase feature class.", "example": {"title": "DropRepresentation tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the DropRepresentation tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.DropRepresentation_cartography ( \"footprints.lyr\" , \"footprints_Rep\" )"}, "usage": ["Once a feature class representation is deleted from a feature class, all representation rules and feature overrides associated with that representation are deleted. No features from the source feature class are deleted.", "A feature class representation cannot be deleted from a feature class during an editing session. Deleting a representation is a schema change and schema changes are prevented during an editing session."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature layer containing representation(s). ", "dataType": "Feature Layer"}, {"name": "representation", "isOptional": false, "description": "The feature class representation to be deleted. ", "dataType": "String"}]},
{"syntax": "CalculateRepresentationRule_cartography (in_features, representation, representation_rule)", "name": "Calculate Representation Rule (Cartography)", "description": "Applies existing representation rules to features in a feature class representation by calculating the RuleID field.", "example": {"title": "CalculateRepresentationRule tool Example (Python Window)", "description": "The following Python window script demonstrates how to use the CalculateRepresentationRule tool in immediate mode.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CalculateRepresentationRule_cartography ( \"footprints.lyr\" , \"footprints_Rep\" , \"Rule_3\" )"}, "usage": ["The input must be a geodatabase feature class with at least one feature class representation.", "Specify a representation rule to assign to a feature. The selected feature or features will have their Rule ID field populated with the specified rule.", "If the specified Rule ID matches the current Rule ID for a selected feature or features, there will be no change."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature class that contains the features for which representation rules will be calculated. ", "dataType": "Feature Layer"}, {"name": "representation", "isOptional": false, "description": "The feature class representation that contains the representation rules that will be applied to features. This feature class representation must belong to the input feature class. ", "dataType": "String"}, {"name": "representation_rule", "isOptional": false, "description": "The representation rule to be applied to the input features by calculating the RuleID field. ", "dataType": "String"}]},
{"syntax": "MajorityFilter (in_raster, {number_neighbors}, {majority_definition})", "name": "Majority Filter (Spatial Analyst)", "description": "Replaces cells in a raster based on the majority of their contiguous neighboring cells. Learn more about how Majority Filter works", "example": {"title": "MajorityFilter example 1 (Python window)", "description": "This example filters the input raster using all eight neighbors, with the greater smoothing effect by requiring half of them to have the same value for replacement.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outMajFilt = MajorityFilter ( \"land\" , \"EIGHT\" , \"HALF\" ) outMajFilt.save ( \"c:/sapyexamples/output/outmajfilt\" )"}, "usage": ["The ", "Majority Filter", " tool must satisfy two criteria before a replacement can occur: the number of neighboring cells of a similar value must be large enough (either by being the majority of, or half of, all the cells), and those cells must be contiguous about the center of the filter kernel. The second criteria concerning the spatial connectivity of the cells minimizes the corruption of cellular spatial patterns.", "The use of FOUR for the number of neighbors will retain the corners of rectangular regions. The use of EIGHT will smooth the corners of rectangular regions.", "Contiguous is defined as sharing an edge for a kernel of EIGHT and sharing a corner for a kernel of FOUR.", "If the replacement threshold HALF is specified and two values occur as equal halves, a replacement will not occur if the value of the processing cell is the same as one of the halves. HALF allows more extensive filtering than MAJORITY.", "While the contiguity criterion is the same for edge and corner raster cells, they obey different MAJORITY and HALF rules. Using a kernel of FOUR, an edge or corner cell always requires two matching neighbors before replacement will occur. With a kernel of EIGHT, a corner cell must have all neighbors of the same value before it is changed, while an edge cell requires three contiguous neighbors, including one along the edge, before any change will occur.", "The output raster will be stabilized (will no longer change) after a few runs of ", "Majority Filter", "."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be filtered based on the the majority of contiguous neighboring cells. It must be of integer type. ", "dataType": "Raster Layer"}, {"name": "number_neighbors", "isOptional": true, "description": "Determines the number of neighboring cells to use in the kernel of the filter. FOUR \u2014 The kernel of the filter will be the four direct (orthogonal) neighbors to the present cell. This is the default. EIGHT \u2014 The kernel of the filter will be the eight nearest neighbors (a 3-by-3 window) to the present cell.", "dataType": "String"}, {"name": "majority_definition", "isOptional": true, "description": "Specifies the number of contiguous (spatially connected) cells that must be of the same value before a replacement will occur. MAJORITY \u2014 A majority of cells must have the same value and be contiguous. Three out of four or five out of eight connected cells must have the same value. HALF \u2014 Half of the cells must have the same value and be contiguous. Two out of four or four out of eight connected cells must have the same value. Using the HALF option will have a more smoothing effect.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Expand (in_raster, number_cells, zone_values)", "name": "Expand (Spatial Analyst)", "description": "Expands specified zones of a raster by a specified number of cells. \r\n  Learn more about how Expand works", "example": {"title": "Expand example 1 (Python window)", "description": "This example expands the zone specified by a list of values by two cells.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outExpand = Expand ( \"filter\" , 2 , [ 0 , 6 , - 3 ]) outExpand.save ( \"C:/sapyexamples/output/outexpand.img\" )"}, "usage": ["The specified zone values are considered to be foreground zones, while the remaining zone values are considered to be background zones. With this tool the foreground zones are allowed to expand into the background zones.", "When two foreground zones compete to expand into the same background zone, the conflict is resolved based on the value of the majority of surrounding cells.", "NoData cells are always considered background cells; therefore, neighboring cells of any value can expand into NoData cells. NoData cells will never expand into their neighbors."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster for which the identified zones are to be expanded It must be of integer type. ", "dataType": "Raster Layer"}, {"name": "number_cells", "isOptional": false, "description": "The number of cells to expand each specified zone by. The value must be an integer greater than 1. ", "dataType": "Long"}, {"name": "zone_values", "isOptional": false, "description": "The list of zone values to expand. The zone values must be integers. They can be in any order. ", "dataType": "Long"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BoundaryClean (in_raster, {sort_type}, {number_of_runs})", "name": "Boundary Clean (Spatial Analyst)", "description": "Smoothes the boundary between zones by expanding and shrinking it. \r\n Learn more about how Boundary Clean works", "example": {"title": "BoundaryClean example 1 (Python window)", "description": "This example smooths the boundary between zones in a descending order with a two-way run.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" OutBndCln = BoundaryClean ( \"land\" , \"DESCEND\" , \"TWO_WAY\" ) OutBndCln.save ( \"c:/sapyexamples/output/bndcln_des2\" )"}, "usage": ["All regions of less than three cells in the x- or y-direction will be changed.", "The shrinking that occurs with the ONE_WAY smoothing process (expansion-shrinking process run once)  or the first pass when the TWO_WAY smoothing process is different than the shrinking that occurs with the second pass of the TWO_WAY smoothing.", "In the first pass, for any processing cell in the expanded raster that has a neighbor of the original value of the processing cell, the original value of the processing cell will be recovered. In the second pass of TWO_WAY, any cell in the expanded raster that is not completely surrounded by eight cells of the same value will recover its original value.", "The expansion is identical for the first and second pass.", "Input cells of NoData have the lowest prioroty in the ONE_WAY sorting type, or in the first pass of TWO_WAY sorting.  In the second pass of TWO_WAY sorting, cells of NoData have the highest priority."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster for which the boundary between zones will be smoothed. It must be of integer type. ", "dataType": "Raster Layer"}, {"name": "sort_type", "isOptional": true, "description": "Specifies the type of sorting to use in the smoothing process. This determines the priority by which cells can expand into their neighbors. NO_SORT \u2014 Does no sorting by size. Zones with larger values have a higher priority to expand into zones with smaller values. This is the default. DESCEND \u2014 Sorts zones in descending order by size. Zones with larger total areas have a higher priority to expand into zones with smaller total areas. ASCEND \u2014 Sorts zones in ascending order by size. Zones with smaller total areas have a higher priority to expand into zones with larger total areas.", "dataType": "String"}, {"name": "number_of_runs", "isOptional": true, "description": "Specifies the number of directions in which the smoothing process will take place. TWO_WAY \u2014 Performs expansion and shrinking according to sorting type, then performs an additional shrinking and expansion with the priority reversed. This is the default. ONE_WAY \u2014 Performs expansion and shrinking once, according to sorting type.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Aggregate (in_raster, cell_factor, {aggregation_type}, {extent_handling}, {ignore_nodata})", "name": "Aggregate (Spatial Analyst)", "description": "Generates a reduced-resolution version of a raster. Each output cell contains the Sum, Minimum, Maximum, Mean, or Median of the input cells that are encompassed by the extent of that cell. \r\n Learn more about how Aggregate works", "example": {"title": "Aggregate example 1 (Python window)", "description": "This example aggregates a raster by averaging the values with a cell factor of 3 and outputs a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outAggreg = Aggregate ( \"highres\" , 3 , \"MAXIMUM\" , \"TRUNCATE\" , \"DATA\" ) outAggreg.save ( \"C:/sapyexamples/output/aggregate.tif\" )"}, "usage": ["If the values of the input raster are integer and any statistics type option other than Mean is used, the output raster will be integer. If the values of the input raster are floating point or the statistics type is Mean, the output raster will be floating point.", "The geoprocessing analysis environments ", "Extent", " and ", "Cell size", " are recognized by this tool. To determine the output raster's resolution when an integer cell size has been specified, multiply the cell resolution of the analysis environment by the input cell factor parameter. If the cell size for the analysis environment is set to the minimum or maximum of the inputs, the resolution of the output raster will be the product of the input raster's resolution multiplied by the specified cell factor."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to aggregate. It can be of integer or floating point type. ", "dataType": "Raster Layer"}, {"name": "cell_factor", "isOptional": false, "description": "The factor by which to multiply the cell size of the input raster to obtain the desired resolution for the output raster. For example, a cell factor value of three would result in an output cell size three times larger than that of the input raster. The value must be an integer greater than 1. ", "dataType": "Long"}, {"name": "aggregation_type", "isOptional": true, "description": "Establishes how the value for each output cell will be determined. The values of the input cells encompassed by the coarser output cell are aggregated by one of the following statistics: SUM \u2014 The sum (total) of the input cell values. This is the default. MAXIMUM \u2014 The largest value of the input cells. MEAN \u2014 The average value of the input cells. MEDIAN \u2014 The median value of the input cells. MINIMUM \u2014 The smallest value of the input cells.", "dataType": "String"}, {"name": "extent_handling", "isOptional": true, "description": "Defines how to handle the boundaries of the input raster when its rows or columns are not a multiple of the cell factor. If the number of rows and columns in the input raster is a multiple of the cell_factor , these keywords are not used. EXPAND \u2014 Expands the bottom or right boundaries of the input raster so the total number of cells in a row or column is a multiple of the cell factor. Expanded cells are given a value of NoData.With this option, the output raster can cover a larger spatial extent than the input raster. This is the default. This is the default. TRUNCATE \u2014 Reduces the number of rows or columns in the output raster by 1. This will truncate the remaining cells on the bottom or right boundaries of the input raster, making the number of rows or columns in the input raster a multiple of the cell factor.With this option, the output raster can cover a smaller spatial extent than the input raster.", "dataType": "Boolean"}, {"name": "ignore_nodata", "isOptional": true, "description": "Denotes whether NoData values are ignored by the aggregation calculation. DATA \u2014 Specifies that if NoData values exist for any of the cells that fall within the spatial extent of a larger cell on the output raster, the NoData values will be ignored when determining the value for output cell locations. Only input cells within the extent of the output cell that have data values will be used in determining the value of the output cell. This is the default. NODATA \u2014 Specifies that if any cell that falls within the spatial extent of a larger cell on the output raster has a value of NoData, the value for that output cell location will be NoData.When the NODATA keyword is used, it is implied that when cells within an aggregation contain the NoData value, there is insufficient information to perform the specified calculations necessary to determine an output value.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Sample (in_rasters, in_location_data, out_table, {resampling_type})", "name": "Sample (Spatial Analyst)", "description": "Creates a table that shows the values of cells from a raster, or set of rasters, for defined locations. The locations are defined by raster cells or by a set of points. \r\n Learn more about how Sample works", "example": {"title": "Sample example 1 (Python window)", "description": "Extract the cell values from multiple rasters to a table based on input locations.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" Sample ([ \"elevation\" , \"costraster\" ], \"observers.shp\" , \"c:/sapyexamples/output/samptable\" , \"NEAREST\" )"}, "usage": ["When the input location is a raster, the set of location cells consists of all cell that have a value of zero or greater. Cells that have NoData values are not included in the location set. A location raster can be easily created using the extraction tools.", "Locations that sample NoData cells in the input raster(s) will be given a NULL Value. For shapefiles, NULL is not supported and a value of 0 (zero) will be given.", "The values of the cells will remain integers for the rasters in input rasters that are integer, even if the Bilinear or Cubic options are selected for the resampling technique.", "The cell size and registration of the input rasters and the location raster should be the same.", "The output from the tool is a table."], "parameters": [{"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": "The list of rasters whose values will be sampled based on the input location data. ", "dataType": "Raster Layer"}, {"name": "in_location_data", "isInputFile": true, "isOptional": false, "description": "Data identifying positions at which you want a sample taken. This can be a raster or a point feature dataset. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "Output table holding the sampled cell values. ", "dataType": "Table"}, {"name": "resampling_type", "isOptional": true, "description": "Resampling algorithm used when sampling a raster. NEAREST \u2014 Nearest neighbor assignment. BILINEAR \u2014 Bilinear interpolation. CUBIC \u2014 Cubic convolution.", "dataType": "String"}]},
{"syntax": "ExtractValuesToPoints (in_point_features, in_raster, out_point_features, {interpolate_values}, {add_attributes})", "name": "Extract Values to Points (Spatial Analyst)", "description": "Extracts the cell values of a raster based on a set of point features and records the values in the attribute table of an output feature class.", "example": {"title": "ExtractValuesToPoints example 1 (Python window)", "description": "This example extracts the cell values from a raster based on locations defined by a point shapefile, and creates an output point feature class of those values.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" ExtractValuesToPoints ( \"rec_sites.shp\" , \"elevation\" , \"C:/sapyexamples/output/outValPnts\" , \"INTERPOLATE\" , \"VALUE_ONLY\" )"}, "usage": ["All fields from the input point feature class will be included in the output point feature class.", "The output feature class will have a new field added to it named ", "RASTERVALU", ".", "This field cannot already exist in the attribute table of the  input features.  If it does, an error will occur. If you wish to keep the original information, before performing ", "Extract Values to Points", " you can add a new field (for example, ", "RASVAL1", ") to the attribute table, calculate the values to it, and then delete the original ", "RASTERVALUE", " field.", "When ", "Extract Values to Points", " is used on a multiband raster, the ", "RASTERVALU", " field will contain values from the last band of the input raster. To extract values from multiple rasters or a multiband raster dataset, use the ", "Extract Multi Values To Points", " tool.", "For the ", "RASTERVALU", " field of the attribute table, NoData cells in the value raster will be given a value of -9999.", " The interpolation option determines how the values will be obtained from the raster. The default option is to use the value at the center of the cell being sampled. The interpolation option will use bilinear interpolation to interpolate a value for the cell center.", "If the input raster is floating-point type, the resulting output point dataset will only contain attributes from the input feature data and the value of the cell, as determined by the interpolation option.", "When adding the attributes from the input raster, if the output point feature dataset is a shapefile, there can be no fields in the input raster with a name more than 10 characters in length. If there are, these fields must be renamed before running the tool."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features defining the locations from which you want to extract the raster cell values. ", "dataType": "Feature Layer"}, {"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The raster dataset whose values will be extracted. It can be an integer or floating-point type raster. ", "dataType": "Raster Layer"}, {"name": "out_point_features", "isOutputFile": true, "isOptional": false, "description": "The output point feature dataset containing the extracted raster values. ", "dataType": "Feature Class"}, {"name": "interpolate_values", "isOptional": true, "description": "Specifies whether or not interpolation will be used. NONE \u2014 No interpolation will be applied; the value of the cell center will be used. INTERPOLATE \u2014 The value of the cell will be calculated from the adjacent cells with valid values using bilinear interpolation. NoData values will be ignored in the interpolation unless all adjacent cells are NoData.", "dataType": "Boolean"}, {"name": "add_attributes", "isOptional": true, "description": "Determines if the raster attributes are written to the output point feature dataset. VALUE_ONLY \u2014 Only the value of the input raster is added to the point attributes. This is the default. ALL \u2014 All the fields from the input raster (except Count) will be added to the point attributes.", "dataType": "Boolean"}]},
{"syntax": "ExtractMultiValuesToPoints (in_point_features, in_rasters, {bilinear_interpolate_values})", "name": "Extract Multi Values to Points (Spatial Analyst)", "description": "Extracts cell values at locations specified in a point feature class from one or more rasters, and records the values to the attribute table of the point feature class.", "example": {"title": "ExtractMultiValuesToPoints example 1 (Python window)", "description": "Extract the cell values from multiple rasters to attributes in a point shapefile feature class.", "code": "import arcpy from arcpy.sa import * from arcpy import env env.workspace = \"c:/sapyexamples/data\" ExtractMultiValuesToPoints ( \"observers.shp\" , [[ \"elevation\" , \"ELEV\" ], [ \"costraster\" , \"COST\" ], [ \"flowdir\" , \"DIR\" ]], \"NONE\" )"}, "usage": ["\r\n", "This tool modifies the input data. See ", "Tools with no outputs", " for more information and strategies to avoid undesired data modification.", " Any combination of rasters (single band or multiband) can be specified as input.", " A cell value will be extracted for each input raster and a new field containing the cell values for each input raster are appended to the input point feature class.", " Output field names are created from the name of the input raster by default. Otherwise you can specify a unique name for each field to store raster values.", " When the input is a multiband raster, a field will be added for all bands with a \"", "b1_, b2_, \u2026bn", "\" prefix added to the name of the output field denoting the band number.", " The interpolation option determines how the values will be obtained from the raster. The default option is to use the value at the center of the cell being sampled. The interpolation option will use bilinear interpolation to interpolate a value for the cell center.", " Shapefile formats have a field limitation of maximum 10 characters in length. Output fields appended to input shapefile will be truncated and made unique by default. This may make it hard to distinguish between input rasters if the names are long or very similar. In this case it is suggested to convert the features to a file geodatabase.", " NoData cells in the value raster will be given a NULL Value. For shapefiles, NULL is not supported and a value of 0 (zero) will be given."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": " The input point features to which you want to add raster values. ", "dataType": "Feature Layer"}, {"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": " The input raster (or rasters) values you want to extract based on the input point feature location. Optionally, you can supply the name for the field to store the raster value. By default, a unique field name will be created based on the input raster dataset name. ", "dataType": "Value Table"}, {"name": "bilinear_interpolate_values", "isOptional": true, "description": "Specifies whether or not interpolation will be used. NONE \u2014 No interpolation will be applied; the value of the cell center will be used. BILINEAR \u2014 The value of the cell will be calculated from the adjacent cells with valid values using bilinear interpolation. NoData values will be ignored in the interpolation unless all adjacent cells are NoData.", "dataType": "Boolean"}]},
{"syntax": "ExtractByRectangle (in_raster, rectangle, {extraction_area})", "name": "Extract by Rectangle (Spatial Analyst)", "description": "Extracts the cells of a raster based on a rectangle.", "example": {"title": "ExtractByRectangle example 1 (Python window)", "description": "This example extracts cells outside a rectangular extent to a new raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" rectExtract = ExtractByRectangle ( \"elevation\" , Extent ( 477625 , 213900 , 486400 , 224200 ), \"OUTSIDE\" ) rectExtract.save ( \"c:/sapyexamples/output/extrect\" )"}, "usage": ["The center of the cell is used to determine whether a cell is inside or outside a rectangle. If the center is within the outline of a rectangle, the cell is considered fully inside even if portions of the cell fall outside the rectangle.", "Cell locations that are not selected are assigned a value of NoData.", "When a ", "multiband raster", " is specified as input, a new multiband raster will be created as output. Each individual band in the input multiband raster will be analyzed accordingly.", "The default output format is an ", "Esri Grid stack", ". Note that the name of an Esri Grid stack cannot start with a number, use spaces, or be more than 9 characters in length.", "If the input is a layer created from a multiband raster with more than three bands, the extraction operation will only consider the bands that were loaded (symbolized) by the layer. As a result, the output multiband raster can only have three bands, corresponding to those used in the display of the input layer.", "If the input raster is integer, the output raster will be integer. If the input is floating point, the output will be floating point."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster from which cells will be extracted. ", "dataType": "Raster Layer"}, {"name": "rectangle", "isOptional": false, "description": "A rectangle that defines the area to be extracted. An Extent object is used to specify the coordinates. The form of the object is: where XMin and YMin define the lower-left coordinates of the area to be extracted, and XMax and YMax define the upper-right coordinates. The coordinates are specified in the same map units as the in_raster . Extent(XMin, YMin, XMax, YMax) where XMin and YMin define the lower-left coordinates of the area to be extracted, and XMax and YMax define the upper-right coordinates.", "dataType": "Extent"}, {"name": "extraction_area", "isOptional": true, "description": "Identifies whether to extract cells inside or outside the input rectangle. INSIDE \u2014 A keyword specifying that the cells inside the input rectangle should be selected and written to the output raster. All cells outside the rectangle will receive NoData values on the output raster. OUTSIDE \u2014 A keyword specifying that the cells outside the input rectangle should be selected and written to the output raster. All cells inside the rectangle will receive NoData values on the output raster.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ExtractByPolygon (in_raster, polygon, {extraction_area})", "name": "Extract by Polygon (Spatial Analyst)", "description": "Extracts the cells of a raster based on a polygon.", "example": {"title": "ExtractByPolygon example 1 (Python window)", "description": "This example extracts cells from a raster based on the defined polygon coordinates.", "code": "import arcpy from arcpy import env from arcpy.sa import * polyPoints = [ arcpy.Point ( 743050 , 4321275 ), arcpy.Point ( 743100 , 4321200 ), arcpy.Point ( 743500 , 4322000 ), arcpy.Point ( 742900 , 4321800 )] env.workspace = \"C:/sapyexamples/data\" extPolygonOut = ExtractByPolygon ( \"soil\" , polyPoints , \"INSIDE\" ) extPolygonOut.save ( \"c:/sapyexamples/output/extpoly\" )"}, "usage": ["To extract based on a polygon in a feature class instead of providing a series of x,y pairs, you can use the ", "Extract By Mask", " tool.", "The center of the cell is used to determine whether a cell is inside or outside a polygon. If the center is within the arcs of the polygon, the cell is considered fully inside even if portions of the cell fall outside the polygon.", "The polygon has a limit of 1,000 vertices. Polygon vertices must be entered in a clockwise order. The first and last vertex must be the same to close the polygon if multiple polygons are to be used.  If the last points are not identical, the polygon will be closed automatically.  The arcs of the polygon can cross one another, but convoluted polygons are not recommended.", "Cell locations that are not selected are assigned a value of NoData.", "When a ", "multiband raster", " is specified as input, a new multiband raster will be created as output. Each individual band in the input multiband raster will be analyzed accordingly.", "The default output format is an ", "Esri Grid stack", ". Note that the name of an Esri Grid stack cannot start with a number, use spaces, or be more than 9 characters in length.", "If the input is a layer created from a multiband raster with more than three bands, the extraction operation will only consider the bands that were loaded (symbolized) by the layer. As a result, the output multiband raster can only have three bands, corresponding to those used in the display of the input layer.", "If the input raster is integer, the output raster will be integer. If the input is floating point, the output will be floating point."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster from which cells will be extracted. ", "dataType": "Raster Layer"}, {"name": "polygon", "isOptional": false, "description": "A polygon (or polygons) that defines the area of the input raster to be extracted. Each polygon part is a list of vertices defined by Point classes. Optionally a Polygon class can be used to define a list of polygon parts. The points are specified as x,y coordinate pairs. The form of the object is: Note that the last coordinate should be the same as the first in order to close the polygon. [[point(x 1 ,y 1 ), point(x 2 ,y 2 ),point(x n ,y n ),...point(x 1 ,y 1 )], [point(x' 1 ,y' 1 ), point(x' 2 ,y' 2 ),point(x' n ,y' n ),...,point(x' 1 ,y' 1 )]", "dataType": "Point"}, {"name": "extraction_area", "isOptional": true, "description": "Identifies whether to extract cells inside or outside the input polygon. INSIDE \u2014 A keyword specifying that the cells inside the input polygon should be selected and written to the output raster. All cells outside the polygon will receive NoData values on the output raster. OUTSIDE \u2014 A keyword specifying that the cells outside the input polygon should be selected and written to the output raster. All cells inside the polygon will receive NoData values on the output raster.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ExtractByPoints (in_raster, points, {extraction_area})", "name": "Extract by Points (Spatial Analyst)", "description": "Extracts the cells of a raster based on a set of coordinate points.", "example": {"title": "ExtractByPoints example 1 (Python window)", "description": "This example extracts cells from a raster based on the specified point coordinates.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" pointList = [ arcpy.Point ( 743050 , 4321275 ), arcpy.Point ( 743100 , 4321200 ), arcpy.Point ( 734500 , 4322000 )] outPointExtract = ExtractByPoints ( \"soil\" , pointList , \"INSIDE\" ) outPointExtract.save ( \"c:/sapyexamples/output/pntextract\" )"}, "usage": ["Cell locations that are not selected are assigned a value of NoData.", "When a ", "multiband raster", " is specified as input, a new multiband raster will be created as output. Each individual band in the input multiband raster will be analyzed accordingly.", "The default output format is an ", "Esri Grid stack", ". Note that the name of an Esri Grid stack cannot start with a number, use spaces, or be more than 9 characters in length.", "If the input is a layer created from a multiband raster with more than three bands, the extraction operation will only consider the bands that were loaded (symbolized) by the layer. As a result, the output multiband raster can only have three bands, corresponding to those used in the display of the input layer.", "If the input raster is integer, the output raster will be integer. If the input is floating point, the output will be floating point."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster from which cells will be extracted. ", "dataType": "Raster Layer"}, {"name": "points", "isOptional": false, "description": "A Python list of Point class objects denote the locations where values will be extracted from the raster. The point objects are specified in a list of x,y coordinate pairs. The form of the object is: The points are in the same map units as in_raster . [point(x 1 ,y 1 ), point(x 2 ,y 2 ),...]", "dataType": "Point"}, {"name": "extraction_area", "isOptional": true, "description": "Identifies whether to extract cells based on the specified point locations (inside) or outside the point locations (outside) . INSIDE \u2014 A keyword specifying that the cell in which the selected point falls will be written to the output raster. All cells outside the box will receive NoData on the output raster. OUTSIDE \u2014 A keyword specifying that the cells outside the input points should be selected and written to the output raster.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ExtractByMask (in_raster, in_mask_data)", "name": "Extract by Mask (Spatial Analyst)", "description": "Extracts the cells of a raster that correspond to the areas defined by a mask.", "example": {"title": "ExtractByMask example 1 (Python window)", "description": "This example extracts cells from a raster within a mask defined by an input polygon shapefile feature class.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outExtractByMask = ExtractByMask ( \"elevation\" , \"mask.shp\" ) outExtractByMask.save ( \"C:/sapyexamples/output/maskextract\" )"}, "usage": ["The ", "Extract by Mask", " tool is similar to setting the ", "Mask", " environment, except that the input mask is only used on the immediate instance, while a mask set in the environment is applied to all tools until it is changed or disabled.", "When a ", "multiband raster", " is specified as input, a new multiband raster will be created as output. Each individual band in the input multiband raster will be analyzed accordingly.", "The default output format is an ", "Esri Grid stack", ". Note that the name of an Esri Grid stack cannot start with a number, use spaces, or be more than 9 characters in length.", "If the input is a layer created from a multiband raster with more than three bands, the extraction operation will only consider the bands that were loaded (symbolized) by the layer. As a result, the output multiband raster can only have three bands, corresponding to those used in the display of the input layer.", "If the input mask is raster, the values for non-NoData input cell locations are copied to the output raster. Tools that can create the mask raster include ", "Con", ", ", "Test", ", and other tools in the  extraction toolset.", "When a multiband raster is specified for the input raster mask, only the first band will be used in the operation.", "If the input raster is integer, the output raster will be integer. If the input is floating point, the output will be floating point."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster from which cells will be extracted. ", "dataType": "Raster Layer"}, {"name": "in_mask_data", "isInputFile": true, "isOptional": false, "description": "Input mask data defining areas to extract. This is a raster or feature dataset. When the input mask data is a raster, NoData cells on the mask will be assigned NoData values on the output raster. ", "dataType": "Raster Layer | Feature Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ExtractByCircle (in_raster, center_point, radius, {extraction_area})", "name": "Extract by Circle (Spatial Analyst)", "description": "Extracts the cells of a raster based on a circle.", "example": {"title": "ExtractByCircle example 1 (Python window)", "description": "This example extracts cells within a 500-meter radius around a point location.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outExtCircle = ExtractByCircle ( \"elevation\" , arcpy.Point ( 482838.823 , 222128.982 ), 500 , \"INSIDE\" ) outExtCircle.save ( \"c:/sapyexamples/output/extcircle\" )"}, "usage": ["The center of the cell is used to determine whether a cell is inside or outside a circle. If the center is within the arc of the circle, the cell is considered fully inside even if portions of the cell fall outside the circle.", "Cell locations that are not selected are assigned a value of NoData.", "When a ", "multiband raster", " is specified as input, a new multiband raster will be created as output. Each individual band in the input multiband raster will be analyzed accordingly.", "The default output format is an ", "Esri Grid stack", ". Note that the name of an Esri Grid stack cannot start with a number, use spaces, or be more than 9 characters in length.", "If the input is a layer created from a multiband raster with more than three bands, the extraction operation will only consider the bands that were loaded (symbolized) by the layer. As a result, the output multiband raster can only have three bands, corresponding to those used in the display of the input layer.", "If the input raster is integer, the output raster will be integer. If the input is floating point, the output will be floating point."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster from which cells will be extracted. ", "dataType": "Raster Layer"}, {"name": "center_point", "isOptional": false, "description": "The Point class dictates the center coordinate (x,y) of the circle defining the area to be extracted. The form of the class is: The coordinates are specified in the same map units as the input raster. Point (x, y)", "dataType": "Point"}, {"name": "radius", "isOptional": false, "description": "Radius of the circle defining the area to be extracted. The radius is specified in map units and is in the same units as the input raster. ", "dataType": "Double"}, {"name": "extraction_area", "isOptional": true, "description": "Identifies whether to extract cells inside or outside the input circle. INSIDE \u2014 A keyword specifying that the cells inside the input circle should be selected and written to the output raster. All cells outside the circle will receive NoData values on the output raster. OUTSIDE \u2014 A keyword specifying that the cells outside the input circle should be selected and written to the output raster. All cells inside the circle will receive NoData values on the output raster.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ExtractByAttributes (in_raster, where_clause)", "name": "Extract by Attributes (Spatial Analyst)", "description": "Extracts the cells of a raster based on a logical query.", "example": {"title": "ExtractByAttributes example 1 (Python window)", "description": "This example extracts cells from a raster based on a logical query, where elevation is greater than 1,000 meters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" attExtract = ExtractByAttributes ( \"elevation\" , \"VALUE > 1000\" ) attExtract.save ( \"c:/sapyexamples/output/attextract\" )"}, "usage": ["If the ", "Where clause", " evaluates to true, the original input value is returned for the cell location.  If it evaluates to false, the cell location is assigned NoData.", "Any extra items (other than ", "Value", " and ", "Count", ") of the input raster are dropped for the output raster.", "If an item other than ", "Value", " of input raster is specified in the ", "Where clause", ", the original input value is returned for the cell location.", "When a ", "multiband raster", " is specified as input, a new multiband raster will be created as output. Each individual band in the input multiband raster will be analyzed accordingly.", "The default output format is an ", "Esri Grid stack", ". Note that the name of an Esri Grid stack cannot start with a number, use spaces, or be more than 9 characters in length.", "If the input is a layer created from a multiband raster with more than three bands, the extraction operation will only consider the bands that were loaded (symbolized) by the layer. As a result, the output multiband raster can only have three bands, corresponding to those used in the display of the input layer.", "If the input raster is integer, the output raster will be integer. If the input is floating point, the output will be floating point."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster from which cells will be extracted. ", "dataType": "Raster Layer"}, {"name": "where_clause", "isOptional": false, "description": "A logical expression that selects a subset of raster cells. The expression follows the general form of an SQL expression. Consult the documentation for more information on the SQL reference for query expressions used in ArcGIS and specifying a query in Python . ", "dataType": "SQL Expression"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PathBackLink (in_source_data, {in_cost_raster}, {in_surface_raster}, {maximum_distance}, {out_distance_raster}, {in_horizontal_raster}, {horizontal_factor}, {in_vertical_raster}, {vertical_factor})", "name": "Path Distance Back Link (Spatial Analyst)", "description": "Defines the neighbor that is the next cell on the least accumulative cost path to the nearest source, while accounting for surface distance and horizontal and vertical cost factors. \r\n Learn more about how the path distance tools work", "example": {"title": "PathBackLink example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPathBL = PathBackLink ( \"source.shp\" , \"costraster\" , \"elevation\" , \"backlink2\" , HfForward ( 0.5 , 1.0 ), \"focalcost.tif\" , VfBinary ( 1.0 , - 30 , 30 ), 30000 , \"c:/sapyexamples/output/optbldist\" ) outPathBL.save ( \"c:/sapyexamples/output/pathblink\" )"}, "usage": ["The Path Distance tools are comparable to the Cost Distance tools in that both determine the minimum accumulative travel cost from a source to each location on a raster surface. However, the Path Distance tools ", "add more complexity", " to the analysis by being able to accommodate for the actual surface distance as well as other ", "horizontal and vertical factors", ".", "The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "Cells with NoData act as barriers in the Path Distance tools. The cost distance for cells behind NoData values is calculated by the accumulative cost necessary to move around the NoData barrier. Any cell location that is assigned NoData on any one of the input rasters will receive NoData on all output rasters.", "If the input source data and the cost raster are different extents, the default output extent is the intersection of the two. To get a cost distance surface for the entire extent, choose the ", "Union of Inputs", " option on the output ", "Extent", " environment settings.", "If a ", "Mask", " has been set in the environment, all masked cells will be treated as NoData values.", "When a mask has been defined in the ", "Raster Analysis", " window and the cells to be masked will mask a source, the calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on all outputs (distance, allocation, and back link) rasters.", "The output of the ", "Aspect", " tool can be used as input for the ", "Input horizontal raster", ".", "The ", "Maximum distance", " is specified in the same cost units as those on the cost raster.", "For the output distance raster, the least-cost distance (or minimum accumulative cost distance) of a cell to a set of source locations is the lower bound of the least-cost distances from the cell to all source locations.", "The default values for the Horizontal factor modifiers are:", "The default values for the Vertical factor modifiers are:"], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the least accumulated cost distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "in_cost_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the impedance or cost to move planimetrically through each cell. The value at each cell location represents the cost per unit distance for moving through the cell. Each cell location value is multiplied by the cell resolution while also compensating for diagonal movement to obtain the total cost of passing through the cell. The values of the cost raster can be integer or floating point, but they cannot be negative or zero (you cannot have a negative or zero cost). ", "dataType": "Raster Layer"}, {"name": "in_surface_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the elevation values at each cell location. The values are used to calculate the actual surface distance covered when passing between cells. ", "dataType": "Raster Layer"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative cost values cannot exceed. If an accumulative cost distance value exceeds this value, the output value for the cell location will be NoData. The maximum distance defines the extent for which the accumulative cost distances are calculated. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "out_distance_raster", "isOutputFile": true, "isOptional": true, "description": "The output path distance raster. The output path distance raster identifies, for each cell, the least accumulative cost distance, over a cost surface to the identified source locations, while accounting for surface distance as well as horizontal and vertical surface factors. A source can be a cell, a set of cells, or one or more feature locations. The output raster is of floating point type. ", "dataType": "Raster Dataset"}, {"name": "in_horizontal_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the horizontal direction at each cell. The values on the raster must be integers ranging from 0 to 360 with 0 degrees being north, or toward the top of the screen, and increasing clockwise. Flat areas should be given a value of -1. The values at each location will be used in conjunction with the {horizontal_factor} to determine the horizontal cost incurred when moving from a cell to its neighbors. ", "dataType": "Raster Layer"}, {"name": "horizontal_factor", "isOptional": true, "description": "The Horizontal Factor object defines the relationship between the horizontal cost factor and the horizontal relative moving angle. There are several factors with modifiers from which to select that identify a defined horizontal factor graph. Additionally, a table can be used to create a custom graph. The graphs are used to identify the horizontal factor used in calculating the total cost of moving into a neighboring cell. In the explanations below, two acronyms are used: 'HF' stands for horizontal factor, which defines the horizontal difficulty encountered when moving from one cell to the next; and 'HRMA' stands for horizontal relative moving angle, which identifies the angle between the horizontal direction from a cell and the moving direction. The object comes in the following forms: The definitions and parameters of these are: Indicates that if the HRMA is less than the cut angle, the HF is set to the value associated with the zero factor; otherwise, it is infinity. Establishes that only forward movement is allowed. The HRMA must be greater or equal to 0 and less than 90 (0 <= HRMA < 90). If the HRMA is greater than 0 and less than 45 degrees, the HF for the cell is set to the value associated with the zero factor. If the HRMA is greater than or equal to 45 degrees, then the side value modifier value is used. The HF for any HRMA equal to or greater than 90 degrees is set to infinity. Specifies that the HF is a linear function of the HRMA. Specifies that the HF is an inverse linear function of the HRMA. Identifies that a table file will be used to define the horizontal factor graph used to determine the HFs. The Modifiers to the horizontal keywords are: HfBinary , HfForward , HfLinear , HfInverseLinear , and HfTable . HfBinary({zero_factor},{cut_angle}) Indicates that if the HRMA is less than the cut angle, the HF is set to the value associated with the zero factor; otherwise, it is infinity. HfForward({zero_factor},{side_value}) Establishes that only forward movement is allowed. The HRMA must be greater or equal to 0 and less than 90 (0 <= HRMA < 90). If the HRMA is greater than 0 and less than 45 degrees, the HF for the cell is set to the value associated with the zero factor. If the HRMA is greater than or equal to 45 degrees, then the side value modifier value is used. The HF for any HRMA equal to or greater than 90 degrees is set to infinity. HfLinear({zero_factor},{cut_angle},{slope}) Specifies that the HF is a linear function of the HRMA. HfInverseLinear({zero_factor},{cut_angle},{slope}) Specifies that the HF is an inverse linear function of the HRMA. HfTable(in_table) Identifies that a table file will be used to define the horizontal factor graph used to determine the HFs. {zero_factor} \u2014Establishes the horizontal factor used when the HRMA is 0. This factor positions the y intercept for any of the horizontal factor functions. {cut_angle} \u2014Defines the HRMA angle beyond which the HF will be set to infinity. {slope} \u2014Establishes the slope of the straight line used with the LINEAR and INVERSE_LINEAR horizontal-factor keywords. The slope is specified as a fraction of rise over run (for example, 45 percent slope is 1/45, which is input as 0.02222). {side_value} \u2014Establishes the HF when the HRMA is greater than or equal to 45 degrees and less than 90 degrees when the FORWARD horizontal-factor keyword is specified. in_table \u2014Identifies the name of the table defining the HF.", "dataType": "Horizontal factor"}, {"name": "in_vertical_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the z-values for each cell location. The values are used for calculating the slope used to identify the vertical factor incurred when moving from one cell to another. ", "dataType": "Raster Layer"}, {"name": "vertical_factor", "isOptional": true, "description": "The Vertical factor object defines the relationship between the vertical cost factor and the vertical relative moving angle (VRMA). There are several factors with modifiers from which to select that identify a defined vertical factor graph. Additionally, a table can be used to create a custom graph. The graphs are used to identify the vertical factor used in calculating the total cost for moving into a neighboring cell. In the explanations below, two acronyms are used: 'VF' stands for vertical factor, which defines the vertical difficulty encountered in moving from one cell to the next; and 'VRMA' stands for vertical relative moving angle, which identifies the slope angle between the FROM or processing cell and the TO cell. The object comes in the following forms: The definitions and parameters of these are: Specifies that if the VRMA is greater than the low-cut angle and less than the high-cut angle, the VF is set to the value associated with the zero factor; otherwise, it is infinity. Indicates that the VF is a linear function of the VRMA. Indicates that the VF is an inverse linear function of the VRMA. Specifies that the VF is a linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. Specifies that the VF is an inverse linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. Identifies the VF as the cosine-based function of the VRMA. Identifies the VF as the secant-based function of the VRMA. Indicates that the VF is the cosine-based function of the VRMA when the VRMA is negative and the secant-based function of the VRMA when the VRMA is nonnegative. Specifies that the VF is the secant-based function of the VRMA when the VRMA is negative and the cosine-based function of the VRMA when the VRMA is nonnegative. Identifies that a table file will be used to define the vertical-factor graph used to determine the VFs. The Modifiers to the vertical parameters are: VfBinary , VfLinear , VfInverseLinear , VfSymLinear , VfSymInverseLinear , VfCos , VfSec , VfSec , VfCosSec , VfSecCos , VfTable . VfBinary({zero_factor},{low_cut_angle},{high_cut_angle}) Specifies that if the VRMA is greater than the low-cut angle and less than the high-cut angle, the VF is set to the value associated with the zero factor; otherwise, it is infinity. VfLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Indicates that the VF is a linear function of the VRMA. VfInverseLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Indicates that the VF is an inverse linear function of the VRMA. VfSymLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Specifies that the VF is a linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. VfSymInverseLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Specifies that the VF is an inverse linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. VfCos({low_cut_angle},{high_cut_angle},{cos_power}) Identifies the VF as the cosine-based function of the VRMA. VfSec({low_cut_angle},{high_cut_angle},{sec_power}) Identifies the VF as the secant-based function of the VRMA. VfCosSec({low_cut_angle},{high_cut_angle},{cos_power},{sec_power}) Indicates that the VF is the cosine-based function of the VRMA when the VRMA is negative and the secant-based function of the VRMA when the VRMA is nonnegative. VfSecCos({low_cut_angle},{high_cut_angle},{sec_power},{cos_power}) Specifies that the VF is the secant-based function of the VRMA when the VRMA is negative and the cosine-based function of the VRMA when the VRMA is nonnegative. VfTable(in_table) Identifies that a table file will be used to define the vertical-factor graph used to determine the VFs. {zero_factor} \u2014Establishes the vertical factor used when the VRMA is zero. This factor positions the y-intercept of the specified function. By definition, the zero factor is not applicable to any of the trigonometric vertical functions (Cos, Sec, Cos_Sec, or Sec_Cos). The y-intercept is defined by these functions. {low_cut_angle} \u2014Defines the VRMA angle below which the VF will be set to infinity. {high_cut_angle} \u2014Defines the VRMA angle above which the VF will be set to infinity. {slope} \u2014Establishes the slope of the straight line used with the VfLinear and VfInverseLinear parameters. The slope is specified as a fraction of rise over run (for example, 45 percent slope is 1/45, which is input as 0.02222). in_table \u2014Identifies the name of the table defining the VF.", "dataType": "Vertical factor"}, {"isOutputFile": true, "name": "out_backlink_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PathAllocation (in_source_data, {source_field}, {in_cost_raster}, {in_surface_raster}, {in_horizontal_raster}, {horizontal_factor}, {in_vertical_raster}, {vertical_factor}, {maximum_distance}, {in_value_raster}, {out_distance_raster}, {out_backlink_raster})", "name": "Path Distance Allocation (Spatial Analyst)", "description": "Calculates the nearest source for each cell based on the least accumulative cost over a cost surface, while accounting for surface distance and horizontal and vertical cost factors. \r\n Learn more about how the path distance tools work", "example": {"title": "PathAllocation example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" pathAlloc = PathAllocation ( \"observers.shp\" , \"costraster\" , \"elevation\" , \"backlink2\" , HfForward ( 0.5 , 1.0 ), \"focalcost.tif\" , VfBinary ( 1.0 , - 30 , 30 ), 25000 , \"eucdirout\" , \"FID\" , \"c:/sapyexamples/output/optpathdist\" , \"c:/sapyexamples/output/optpathbl\" ) pathAlloc.save ( \"c:/sapyexamples/output/allocpath\" )"}, "usage": ["The Path Distance tools are comparable to the Cost Distance tools in that both determine the minimum accumulative travel cost from a source to each location on a raster surface. However, the Path Distance tools ", "add more complexity", " to the analysis by being able to accommodate for the actual surface distance as well as other ", "horizontal and vertical factors", ".", "The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", " To calculate allocation, source locations can have an associated value, which can be specified by the ", "Source field", " parameter.   If the input source is an integer raster, the default field is VALUE. If it is a feature, it will be the first integer field in the attribute table.  If the input source data is a floating point  raster an integer value raster parameter must be specified.", "Cells with NoData act as barriers in the Path Distance tools. The cost distance for cells behind NoData values is calculated by the accumulative cost necessary to move around the NoData barrier. Any cell location that is assigned NoData on any one of the input rasters will receive NoData on all output rasters.", "If the input source data and the cost raster are different extents, the default output extent is the intersection of the two. To get a cost distance surface for the entire extent, choose the ", "Union of Inputs", " option on the output ", "Extent", " environment settings.", "If a ", "Mask", " has been set in the environment, all masked cells will be treated as NoData values.", "When a mask has been defined in the ", "Raster Analysis", " window and the cells to be masked will mask a source, the calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on all outputs (distance, allocation, and back link) rasters.", "The output of the ", "Aspect", " tool can be used as input for the ", "Input horizontal raster", ".", "The ", "Maximum distance", " is specified in the same cost units as those on the cost raster.", "For the output distance raster, the least-cost distance (or minimum accumulative cost distance) of a cell to a set of source locations is the lower bound of the least-cost distances from the cell to all source locations.", "The default values for the Horizontal factor modifiers are:", "The default values for the Vertical factor modifiers are:"], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the least accumulated cost distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. If the input source raster is floating point, the {in_value_raster} must be set, and it must be of integer type. The value raster will take precedence over any setting of the {source_field} . ", "dataType": "Raster Layer | Feature Layer"}, {"name": "source_field", "isOptional": true, "description": "The field used to assign values to the source locations. It must be integer type. If the {in_value_raster} has been set, the values in that input will have precedence over any setting for the {source_field} . ", "dataType": "Field"}, {"name": "in_cost_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the impedance or cost to move planimetrically through each cell. The value at each cell location represents the cost per unit distance for moving through the cell. Each cell location value is multiplied by the cell resolution while also compensating for diagonal movement to obtain the total cost of passing through the cell. The values of the cost raster can be integer or floating point, but they cannot be negative or zero (you cannot have a negative or zero cost). ", "dataType": "Raster Layer"}, {"name": "in_surface_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the elevation values at each cell location. The values are used to calculate the actual surface distance covered when passing between cells. ", "dataType": "Raster Layer"}, {"name": "in_horizontal_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the horizontal direction at each cell. The values on the raster must be integers ranging from 0 to 360 with 0 degrees being north, or toward the top of the screen, and increasing clockwise. Flat areas should be given a value of -1. The values at each location will be used in conjunction with the {horizontal_factor} to determine the horizontal cost incurred when moving from a cell to its neighbors. ", "dataType": "Raster Layer"}, {"name": "horizontal_factor", "isOptional": true, "description": "The Horizontal Factor object defines the relationship between the horizontal cost factor and the horizontal relative moving angle. There are several factors with modifiers from which to select that identify a defined horizontal factor graph. Additionally, a table can be used to create a custom graph. The graphs are used to identify the horizontal factor used in calculating the total cost of moving into a neighboring cell. In the explanations below, two acronyms are used: 'HF' stands for horizontal factor, which defines the horizontal difficulty encountered when moving from one cell to the next; and 'HRMA' stands for horizontal relative moving angle, which identifies the angle between the horizontal direction from a cell and the moving direction. The object comes in the following forms: The definitions and parameters of these are: Indicates that if the HRMA is less than the cut angle, the HF is set to the value associated with the zero factor; otherwise, it is infinity. Establishes that only forward movement is allowed. The HRMA must be greater or equal to 0 and less than 90 (0 <= HRMA < 90). If the HRMA is greater than 0 and less than 45 degrees, the HF for the cell is set to the value associated with the zero factor. If the HRMA is greater than or equal to 45 degrees, then the side value modifier value is used. The HF for any HRMA equal to or greater than 90 degrees is set to infinity. Specifies that the HF is a linear function of the HRMA. Specifies that the HF is an inverse linear function of the HRMA. Identifies that a table file will be used to define the horizontal factor graph used to determine the HFs. The Modifiers to the horizontal keywords are: HfBinary , HfForward , HfLinear , HfInverseLinear , and HfTable . HfBinary({zero_factor},{cut_angle}) Indicates that if the HRMA is less than the cut angle, the HF is set to the value associated with the zero factor; otherwise, it is infinity. HfForward({zero_factor},{side_value}) Establishes that only forward movement is allowed. The HRMA must be greater or equal to 0 and less than 90 (0 <= HRMA < 90). If the HRMA is greater than 0 and less than 45 degrees, the HF for the cell is set to the value associated with the zero factor. If the HRMA is greater than or equal to 45 degrees, then the side value modifier value is used. The HF for any HRMA equal to or greater than 90 degrees is set to infinity. HfLinear({zero_factor},{cut_angle},{slope}) Specifies that the HF is a linear function of the HRMA. HfInverseLinear({zero_factor},{cut_angle},{slope}) Specifies that the HF is an inverse linear function of the HRMA. HfTable(in_table) Identifies that a table file will be used to define the horizontal factor graph used to determine the HFs. {zero_factor} \u2014Establishes the horizontal factor used when the HRMA is 0. This factor positions the y intercept for any of the horizontal factor functions. {cut_angle} \u2014Defines the HRMA angle beyond which the HF will be set to infinity. {slope} \u2014Establishes the slope of the straight line used with the LINEAR and INVERSE_LINEAR horizontal-factor keywords. The slope is specified as a fraction of rise over run (for example, 45 percent slope is 1/45, which is input as 0.02222). {side_value} \u2014Establishes the HF when the HRMA is greater than or equal to 45 degrees and less than 90 degrees when the FORWARD horizontal-factor keyword is specified. in_table \u2014Identifies the name of the table defining the HF.", "dataType": "Horizontal factor"}, {"name": "in_vertical_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the z-values for each cell location. The values are used for calculating the slope used to identify the vertical factor incurred when moving from one cell to another. ", "dataType": "Raster Layer"}, {"name": "vertical_factor", "isOptional": true, "description": "The Vertical factor object defines the relationship between the vertical cost factor and the vertical relative moving angle (VRMA). There are several factors with modifiers from which to select that identify a defined vertical factor graph. Additionally, a table can be used to create a custom graph. The graphs are used to identify the vertical factor used in calculating the total cost for moving into a neighboring cell. In the explanations below, two acronyms are used: 'VF' stands for vertical factor, which defines the vertical difficulty encountered in moving from one cell to the next; and 'VRMA' stands for vertical relative moving angle, which identifies the slope angle between the FROM or processing cell and the TO cell. The object comes in the following forms: The definitions and parameters of these are: Specifies that if the VRMA is greater than the low-cut angle and less than the high-cut angle, the VF is set to the value associated with the zero factor; otherwise, it is infinity. Indicates that the VF is a linear function of the VRMA. Indicates that the VF is an inverse linear function of the VRMA. Specifies that the VF is a linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. Specifies that the VF is an inverse linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. Identifies the VF as the cosine-based function of the VRMA. Identifies the VF as the secant-based function of the VRMA. Indicates that the VF is the cosine-based function of the VRMA when the VRMA is negative and the secant-based function of the VRMA when the VRMA is nonnegative. Specifies that the VF is the secant-based function of the VRMA when the VRMA is negative and the cosine-based function of the VRMA when the VRMA is nonnegative. Identifies that a table file will be used to define the vertical-factor graph used to determine the VFs. The Modifiers to the vertical parameters are: VfBinary , VfLinear , VfInverseLinear , VfSymLinear , VfSymInverseLinear , VfCos , VfSec , VfSec , VfCosSec , VfSecCos , VfTable . VfBinary({zero_factor},{low_cut_angle},{high_cut_angle}) Specifies that if the VRMA is greater than the low-cut angle and less than the high-cut angle, the VF is set to the value associated with the zero factor; otherwise, it is infinity. VfLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Indicates that the VF is a linear function of the VRMA. VfInverseLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Indicates that the VF is an inverse linear function of the VRMA. VfSymLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Specifies that the VF is a linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. VfSymInverseLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Specifies that the VF is an inverse linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. VfCos({low_cut_angle},{high_cut_angle},{cos_power}) Identifies the VF as the cosine-based function of the VRMA. VfSec({low_cut_angle},{high_cut_angle},{sec_power}) Identifies the VF as the secant-based function of the VRMA. VfCosSec({low_cut_angle},{high_cut_angle},{cos_power},{sec_power}) Indicates that the VF is the cosine-based function of the VRMA when the VRMA is negative and the secant-based function of the VRMA when the VRMA is nonnegative. VfSecCos({low_cut_angle},{high_cut_angle},{sec_power},{cos_power}) Specifies that the VF is the secant-based function of the VRMA when the VRMA is negative and the cosine-based function of the VRMA when the VRMA is nonnegative. VfTable(in_table) Identifies that a table file will be used to define the vertical-factor graph used to determine the VFs. {zero_factor} \u2014Establishes the vertical factor used when the VRMA is zero. This factor positions the y-intercept of the specified function. By definition, the zero factor is not applicable to any of the trigonometric vertical functions (Cos, Sec, Cos_Sec, or Sec_Cos). The y-intercept is defined by these functions. {low_cut_angle} \u2014Defines the VRMA angle below which the VF will be set to infinity. {high_cut_angle} \u2014Defines the VRMA angle above which the VF will be set to infinity. {slope} \u2014Establishes the slope of the straight line used with the VfLinear and VfInverseLinear parameters. The slope is specified as a fraction of rise over run (for example, 45 percent slope is 1/45, which is input as 0.02222). in_table \u2014Identifies the name of the table defining the VF.", "dataType": "Vertical factor"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative cost values cannot exceed. If an accumulative cost distance value exceeds this value, the output value for the cell location will be NoData. The maximum distance defines the extent for which the accumulative cost distances are calculated. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "in_value_raster", "isInputFile": true, "isOptional": true, "description": "The input integer raster that identifies the zone values that should be used for each input source location. For each source location (cell or feature), the value defined by the {in_value_raster} will be assigned to all cells allocated to the source location for the computation. The value raster will take precedence over any setting for the {source_field} . ", "dataType": "Raster Layer"}, {"name": "out_distance_raster", "isOutputFile": true, "isOptional": true, "description": "The output path distance raster. The output path distance raster identifies, for each cell, the least accumulative cost distance, over a cost surface to the identified source locations, while accounting for surface distance as well as horizontal and vertical surface factors. A source can be a cell, a set of cells, or one or more feature locations. The output raster is of floating point type. ", "dataType": "Raster Dataset"}, {"name": "out_backlink_raster", "isOutputFile": true, "isOptional": true, "description": "The output cost back-link raster. The back-link raster contains values of 0 through 8, which define the direction or identify the next neighboring cell (the succeeding cell) along the least accumulative cost path from a cell to reach its least cost source, while accounting for surface distance as well as horizontal and vertical surface factors. If the path is to pass into the right neighbor, the cell will be assigned the value 1, 2 for the lower right diagonal cell, and continuing clockwise. The value 0 is reserved for source cells. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_allocation_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PathDistance (in_source_data, {in_cost_raster}, {in_surface_raster}, {in_horizontal_raster}, {horizontal_factor}, {in_vertical_raster}, {vertical_factor}, {maximum_distance}, {out_backlink_raster})", "name": "Path Distance (Spatial Analyst)", "description": "Calculates, for each cell, the least accumulative cost distance to the nearest source, while accounting for surface distance and horizontal and vertical cost factors. \r\n Learn more about how the path distance tools work", "example": {"title": "PathDistance example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPathDist = PathDistance ( \"observers.shp\" , \"costraster\" , \"elevation\" , \"backlink2\" , HfForward ( 0.5 , 1.0 ), \"focalcost.tif\" , VfBinary ( 1.0 , - 30 , 30 ), 50000 , \"c:/sapyexamples/output/backlinkpath\" ) outPathDist.save ( \"c:/sapyexamples/output/pathdistout\" )"}, "usage": ["The Path Distance tools are comparable to the Cost Distance tools in that both determine the minimum accumulative travel cost from a source to each location on a raster surface. However, the Path Distance tools ", "add more complexity", " to the analysis by being able to accommodate for the actual surface distance as well as other ", "horizontal and vertical factors", ".", "The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "Cells with NoData act as barriers in the Path Distance tools. The cost distance for cells behind NoData values is calculated by the accumulative cost necessary to move around the NoData barrier. Any cell location that is assigned NoData on any one of the input rasters will receive NoData on all output rasters.", "If the input source data and the cost raster are different extents, the default output extent is the intersection of the two. To get a cost distance surface for the entire extent, choose the ", "Union of Inputs", " option on the output ", "Extent", " environment settings.", "If a ", "Mask", " has been set in the environment, all masked cells will be treated as NoData values.", "When a mask has been defined in the ", "Raster Analysis", " window and the cells to be masked will mask a source, the calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on all outputs (distance, allocation, and back link) rasters.", "The output of the ", "Aspect", " tool can be used as input for the ", "Input horizontal raster", ".", "The ", "Maximum distance", " is specified in the same cost units as those on the cost raster.", "For the output distance raster, the least-cost distance (or minimum accumulative cost distance) of a cell to a set of source locations is the lower bound of the least-cost distances from the cell to all source locations.", "The default values for the Horizontal factor modifiers are:", "The default values for the Vertical factor modifiers are:"], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the least accumulated cost distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "in_cost_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the impedance or cost to move planimetrically through each cell. The value at each cell location represents the cost per unit distance for moving through the cell. Each cell location value is multiplied by the cell resolution while also compensating for diagonal movement to obtain the total cost of passing through the cell. The values of the cost raster can be integer or floating point, but they cannot be negative or zero (you cannot have a negative or zero cost). ", "dataType": "Raster Layer"}, {"name": "in_surface_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the elevation values at each cell location. The values are used to calculate the actual surface distance covered when passing between cells. ", "dataType": "Raster Layer"}, {"name": "in_horizontal_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the horizontal direction at each cell. The values on the raster must be integers ranging from 0 to 360 with 0 degrees being north, or toward the top of the screen, and increasing clockwise. Flat areas should be given a value of -1. The values at each location will be used in conjunction with the {horizontal_factor} to determine the horizontal cost incurred when moving from a cell to its neighbors. ", "dataType": "Raster Layer"}, {"name": "horizontal_factor", "isOptional": true, "description": "The Horizontal Factor object defines the relationship between the horizontal cost factor and the horizontal relative moving angle. There are several factors with modifiers from which to select that identify a defined horizontal factor graph. Additionally, a table can be used to create a custom graph. The graphs are used to identify the horizontal factor used in calculating the total cost of moving into a neighboring cell. In the explanations below, two acronyms are used: 'HF' stands for horizontal factor, which defines the horizontal difficulty encountered when moving from one cell to the next; and 'HRMA' stands for horizontal relative moving angle, which identifies the angle between the horizontal direction from a cell and the moving direction. The object comes in the following forms: The definitions and parameters of these are: Indicates that if the HRMA is less than the cut angle, the HF is set to the value associated with the zero factor; otherwise, it is infinity. Establishes that only forward movement is allowed. The HRMA must be greater or equal to 0 and less than 90 (0 <= HRMA < 90). If the HRMA is greater than 0 and less than 45 degrees, the HF for the cell is set to the value associated with the zero factor. If the HRMA is greater than or equal to 45 degrees, then the side value modifier value is used. The HF for any HRMA equal to or greater than 90 degrees is set to infinity. Specifies that the HF is a linear function of the HRMA. Specifies that the HF is an inverse linear function of the HRMA. Identifies that a table file will be used to define the horizontal factor graph used to determine the HFs. The Modifiers to the horizontal keywords are: HfBinary , HfForward , HfLinear , HfInverseLinear , and HfTable . HfBinary({zero_factor},{cut_angle}) Indicates that if the HRMA is less than the cut angle, the HF is set to the value associated with the zero factor; otherwise, it is infinity. HfForward({zero_factor},{side_value}) Establishes that only forward movement is allowed. The HRMA must be greater or equal to 0 and less than 90 (0 <= HRMA < 90). If the HRMA is greater than 0 and less than 45 degrees, the HF for the cell is set to the value associated with the zero factor. If the HRMA is greater than or equal to 45 degrees, then the side value modifier value is used. The HF for any HRMA equal to or greater than 90 degrees is set to infinity. HfLinear({zero_factor},{cut_angle},{slope}) Specifies that the HF is a linear function of the HRMA. HfInverseLinear({zero_factor},{cut_angle},{slope}) Specifies that the HF is an inverse linear function of the HRMA. HfTable(in_table) Identifies that a table file will be used to define the horizontal factor graph used to determine the HFs. {zero_factor} \u2014Establishes the horizontal factor used when the HRMA is 0. This factor positions the y intercept for any of the horizontal factor functions. {cut_angle} \u2014Defines the HRMA angle beyond which the HF will be set to infinity. {slope} \u2014Establishes the slope of the straight line used with the LINEAR and INVERSE_LINEAR horizontal-factor keywords. The slope is specified as a fraction of rise over run (for example, 45 percent slope is 1/45, which is input as 0.02222). {side_value} \u2014Establishes the HF when the HRMA is greater than or equal to 45 degrees and less than 90 degrees when the FORWARD horizontal-factor keyword is specified. in_table \u2014Identifies the name of the table defining the HF.", "dataType": "Horizontal factor"}, {"name": "in_vertical_raster", "isInputFile": true, "isOptional": true, "description": "A raster defining the z-values for each cell location. The values are used for calculating the slope used to identify the vertical factor incurred when moving from one cell to another. ", "dataType": "Raster Layer"}, {"name": "vertical_factor", "isOptional": true, "description": "The Vertical factor object defines the relationship between the vertical cost factor and the vertical relative moving angle (VRMA). There are several factors with modifiers from which to select that identify a defined vertical factor graph. Additionally, a table can be used to create a custom graph. The graphs are used to identify the vertical factor used in calculating the total cost for moving into a neighboring cell. In the explanations below, two acronyms are used: 'VF' stands for vertical factor, which defines the vertical difficulty encountered in moving from one cell to the next; and 'VRMA' stands for vertical relative moving angle, which identifies the slope angle between the FROM or processing cell and the TO cell. The object comes in the following forms: The definitions and parameters of these are: Specifies that if the VRMA is greater than the low-cut angle and less than the high-cut angle, the VF is set to the value associated with the zero factor; otherwise, it is infinity. Indicates that the VF is a linear function of the VRMA. Indicates that the VF is an inverse linear function of the VRMA. Specifies that the VF is a linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. Specifies that the VF is an inverse linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. Identifies the VF as the cosine-based function of the VRMA. Identifies the VF as the secant-based function of the VRMA. Indicates that the VF is the cosine-based function of the VRMA when the VRMA is negative and the secant-based function of the VRMA when the VRMA is nonnegative. Specifies that the VF is the secant-based function of the VRMA when the VRMA is negative and the cosine-based function of the VRMA when the VRMA is nonnegative. Identifies that a table file will be used to define the vertical-factor graph used to determine the VFs. The Modifiers to the vertical parameters are: VfBinary , VfLinear , VfInverseLinear , VfSymLinear , VfSymInverseLinear , VfCos , VfSec , VfSec , VfCosSec , VfSecCos , VfTable . VfBinary({zero_factor},{low_cut_angle},{high_cut_angle}) Specifies that if the VRMA is greater than the low-cut angle and less than the high-cut angle, the VF is set to the value associated with the zero factor; otherwise, it is infinity. VfLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Indicates that the VF is a linear function of the VRMA. VfInverseLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Indicates that the VF is an inverse linear function of the VRMA. VfSymLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Specifies that the VF is a linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. VfSymInverseLinear({zero_factor},{low_cut_angle},{high_cut_angle},{slope}) Specifies that the VF is an inverse linear function of the VRMA in either the negative or positive side of the VRMA, respectively, and the two linear functions are symmetrical with respect to the VF (y) axis. VfCos({low_cut_angle},{high_cut_angle},{cos_power}) Identifies the VF as the cosine-based function of the VRMA. VfSec({low_cut_angle},{high_cut_angle},{sec_power}) Identifies the VF as the secant-based function of the VRMA. VfCosSec({low_cut_angle},{high_cut_angle},{cos_power},{sec_power}) Indicates that the VF is the cosine-based function of the VRMA when the VRMA is negative and the secant-based function of the VRMA when the VRMA is nonnegative. VfSecCos({low_cut_angle},{high_cut_angle},{sec_power},{cos_power}) Specifies that the VF is the secant-based function of the VRMA when the VRMA is negative and the cosine-based function of the VRMA when the VRMA is nonnegative. VfTable(in_table) Identifies that a table file will be used to define the vertical-factor graph used to determine the VFs. {zero_factor} \u2014Establishes the vertical factor used when the VRMA is zero. This factor positions the y-intercept of the specified function. By definition, the zero factor is not applicable to any of the trigonometric vertical functions (Cos, Sec, Cos_Sec, or Sec_Cos). The y-intercept is defined by these functions. {low_cut_angle} \u2014Defines the VRMA angle below which the VF will be set to infinity. {high_cut_angle} \u2014Defines the VRMA angle above which the VF will be set to infinity. {slope} \u2014Establishes the slope of the straight line used with the VfLinear and VfInverseLinear parameters. The slope is specified as a fraction of rise over run (for example, 45 percent slope is 1/45, which is input as 0.02222). in_table \u2014Identifies the name of the table defining the VF.", "dataType": "Vertical factor"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative cost values cannot exceed. If an accumulative cost distance value exceeds this value, the output value for the cell location will be NoData. The maximum distance defines the extent for which the accumulative cost distances are calculated. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "out_backlink_raster", "isOutputFile": true, "isOptional": true, "description": "The output cost back-link raster. The back-link raster contains values of 0 through 8, which define the direction or identify the next neighboring cell (the succeeding cell) along the least accumulative cost path from a cell to reach its least cost source, while accounting for surface distance as well as horizontal and vertical surface factors. If the path is to pass into the right neighbor, the cell will be assigned the value 1, 2 for the lower right diagonal cell, and continuing clockwise. The value 0 is reserved for source cells. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_distance_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "EucDistance (in_source_data, {maximum_distance}, {cell_size}, {out_direction_raster})", "name": "Euclidean Distance (Spatial Analyst)", "description": "Calculates, for each cell, the Euclidean distance to the closest source. \r\n Learn more about Euclidean distance analysis", "example": {"title": "EucDistance example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outEucDistance = EucDistance ( \"rec_sites.shp\" , 5000 , 5 , \"c:/sapyexamples/output/EucDirOut\" ) outEucDistance.save ( \"C:/sapyexamples/output/eucdist\" )"}, "usage": ["The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "The ", "Maximum distance", " is specified in the same map units as the input source data.", "Allocation is not an available output because there can be no floating-point information in the source data. If allocation output is desired, use ", "Euclidean Allocation", ", which can generate all three outputs (allocation, distance, and direction) at the same time.", "If a ", "Mask", " has been set in the environment and the cells to be masked will mask a source, the Euclidean calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on the output rasters.", "The NoData values created by the masked cells are ignored in the calculations on non-source cell locations. The Euclidean distance for cells behind NoData values is calculated as if the NoData value is not present. Any cell location that is assigned NoData because of the mask on the input surface will receive NoData on all the output rasters."], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the Euclidean distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative distance values cannot exceed. If an accumulative Euclidean distance value exceeds this value, the output value for the cell location will be NoData. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set. If it is not set in the environment, the default cell size will depend on if the input source data is a raster or a feature, as follows: If the source is raster, the output will have that same cell size. If the source is feature, the output will have a cell size determined by the shorter of the width or height of the extent of input feature, in the input spatial reference, divided by 250.", "dataType": "Analysis Cell Size"}, {"name": "out_direction_raster", "isOutputFile": true, "isOptional": true, "description": "The output Euclidean direction raster. The direction raster contains the calculated direction, in degrees, each cell center is from the closest source cell center. The range of values is from 0 degrees to 360 degrees, with 0 reserved for the source cells. Due east (right) is 90, and the values increase clockwise (180 is south, 270 is west, and 360 is north). The output raster is of integer type. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_distance_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "EucDirection (in_source_data, {maximum_distance}, {cell_size}, {out_distance_raster})", "name": "Euclidean Direction (Spatial Analyst)", "description": "Calculates, for each cell, the direction, in degrees, to the nearest source. \r\n Learn more about Euclidean distance analysis", "example": {"title": "EucDirection example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outEucDirect = EucDirection ( \"observers\" , 35000 , 50 , \"c:/sapyexamples/output/optoutdist\" ) outEucDirect.save ( \"c:/sapyexamples/output/eucoutdir\" )"}, "usage": ["The input source data can be a feature class or raster.", "The output values are based on compass directions (90 to the east, 180 to the south, 270 to the west, and 360 to the north), with 0 reserved for the source cells.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "The ", "Maximum distance", " is specified in the same map units as the input source data.", "Allocation is not an available output because there can be no floating-point information in the source data. If allocation output is desired, use ", "Euclidean Allocation", ", which can generate all three outputs (allocation, distance, and direction) at the same time.", "If a ", "Mask", " has been set in the environment and the cells to be masked will mask a source, the Euclidean calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on the output rasters.", "The NoData values created by the masked cells are ignored in the calculations on non-source cell locations. The Euclidean distance for cells behind NoData values is calculated as if the NoData value is not present. Any cell location that is assigned NoData because of the mask on the input surface will receive NoData on all the output rasters."], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the Euclidean distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative distance values cannot exceed. If an accumulative Euclidean distance value exceeds this value, the output value for the cell location will be NoData. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set. If it is not set in the environment, the default cell size will depend on if the input source data is a raster or a feature, as follows: If the source is raster, the output will have that same cell size. If the source is feature, the output will have a cell size determined by the shorter of the width or height of the extent of input feature, in the input spatial reference, divided by 250.", "dataType": "Analysis Cell Size"}, {"name": "out_distance_raster", "isOutputFile": true, "isOptional": true, "description": "The output Euclidean distance raster. The distance raster identifies, for each cell, the Euclidean distance to the closest source cell, set of source cells, or source location. The output raster is of floating point type. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_direction_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "EucAllocation (in_source_data, {maximum_distance}, {in_value_raster}, {cell_size}, {source_field}, {out_distance_raster}, {out_direction_raster})", "name": "Euclidean Allocation (Spatial Analyst)", "description": "Calculates, for each cell, the nearest source based on Euclidean distance. \r\n Learn more about Euclidean distance analysis", "example": {"title": "EucAllocation example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" eucAllocate = EucAllocation ( \"observers\" , 50000 , \"elevation\" , 25 , \"FID\" , \"c:/sapyexamples/output/outeucdist\" , \"c:/sapyexamples/output/outeucdir\" ) eucAllocate.save ( \"c:/sapyexamples/output/eucalloc\" )"}, "usage": ["The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "The ", "Maximum distance", " is specified in the same map units as the input source data.", "The input value raster is useful if the input raster or feature source data is a raster derived from a function that results in either one or zero. These functions lose their original zone values associated with the source cell locations. The input value raster can either restore these values or allow analysis on additional combinations of zone values within the source cells.", "If an input value raster is used, it may change the configuration and results of the Euclidean allocation output. It will not affect the optional Euclidean distance or direction results.", "If a ", "Mask", " has been set in the environment and the cells to be masked will mask a source, the Euclidean calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on the output rasters.", "The NoData values created by the masked cells are ignored in the calculations on non-source cell locations. The Euclidean distance for cells behind NoData values is calculated as if the NoData value is not present. Any cell location that is assigned NoData because of the mask on the input surface will receive NoData on all the output rasters."], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the Euclidean distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. If the input source raster is floating point, the {in_value_raster} must be set, and it must be of integer type. The value raster will take precedence over any setting of the {source_field} . ", "dataType": "Raster Layer | Feature Layer"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative distance values cannot exceed. If an accumulative Euclidean distance value exceeds this value, the output value for the cell location will be NoData. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "in_value_raster", "isInputFile": true, "isOptional": true, "description": "The input integer raster that identifies the zone values that should be used for each input source location. For each source location (cell or feature), the value defined by the {in_value_raster} will be assigned to all cells allocated to the source location for the computation. The value raster will take precedence over any setting for the {source_field} . ", "dataType": "Raster Layer"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set. If it is not set in the environment, the default cell size will depend on if the input source data is a raster or a feature, as follows: If the source is raster, the output will have that same cell size. If the source is feature, the output will have a cell size determined by the shorter of the width or height of the extent of input feature, in the input spatial reference, divided by 250.", "dataType": "Analysis Cell Size"}, {"name": "source_field", "isOptional": true, "description": "The field used to assign values to the source locations. It must be integer type. If the {in_value_raster} has been set, the values in that input will have precedence over any setting for the {source_field} . ", "dataType": "Field"}, {"name": "out_distance_raster", "isOutputFile": true, "isOptional": true, "description": "The output Euclidean distance raster. The distance raster identifies, for each cell, the Euclidean distance to the closest source cell, set of source cells, or source location. The output raster is of floating point type. ", "dataType": "Raster Dataset"}, {"name": "out_direction_raster", "isOutputFile": true, "isOptional": true, "description": "The output Euclidean direction raster. The direction raster contains the calculated direction, in degrees, each cell center is from the closest source cell center. The range of values is from 0 degrees to 360 degrees, with 0 reserved for the source cells. Due east (right) is 90, and the values increase clockwise (180 is south, 270 is west, and 360 is north). The output raster is of integer type. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_allocation_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CostPath (in_destination_data, in_cost_distance_raster, in_cost_backlink_raster, {path_type}, {destination_field})", "name": "Cost Path (Spatial Analyst)", "description": "Calculates the least-cost path from a source to a destination. \r\n Learn more about creating the least cost path", "example": {"title": "CostPath example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCostPath = CostPath ( \"observers\" , \"costraster\" , \"backlink2\" , \"EACH_CELL\" ) outCostPath.save ( \"c:/sapyexamples/output/costpath\" )"}, "usage": ["The ", "Cost Path", " tool produces an output raster that records the least-cost path or paths from selected locations to the closest source cell defined within the accumulative cost surface, in terms of cost distance.", "One or more of the weighted cost tools (", "Cost Distance", ", ", "Cost Back Link", ", or ", "Cost Allocation", ") are generally required to run prior to running ", "Cost Path", " to create the input cost distance and back link rasters. These are mandatory input rasters to ", "Cost Path", ".", "Each least-cost path is assigned a value when encountered in the scanning process. The ending cell on the original source raster (from which the cost distance and back link were derived) of a cost path receives one, the first path receives three, the second four, and so on. The value two is reserved for the merged portion of paths that have portions of a common cost path.", "When the input destination data is a raster, the set of destination cells consists of all cells in the input raster or feature destination data that have valid values. Cells that have NoData values are not included in the source set. The value zero is considered a legitimate destination. A destination raster can be easily created using the ", "extraction tools", ".", "When the source input is a feature, by default, the first valid available field will be used.  If no valid fields exist, the ObjectID field (for example, OID or FID, depending on the type of feature input) will be used.", "When using polygon feature data for the input feature destinations, care must be taken with how the output cell size is handled when it is coarse relative to the detail present in the input.  In the internal rasterization process that employs the  ", "Polygon to Raster", " tool, the default setting of  ", "Cell assignment type", " will be CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized destination output, and so will not be represented in the distance calculations. For example, if your destinations are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost  in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "When multiple paths merge and follow the remaining distance back to a source on the same route, the segment where the two paths travel together is assigned the value 2. The merged portion of the path cannot be assigned the value of one of the paths, since the merged portion belongs to both routes.", "Cost Path", " does not respect the ", "Mask", " environment setting. The analysis extent should not be different from the cost distance and cost back link rasters.", "Cost Path", " will ignore the ", "Cell size", " environment setting and use the cell size of the ", "Input cost backlink raster", " for the output raster. The pattern of the back link raster would be seriously altered if it were resampled to a different resolution. To avoid any confusion, the cell size should not be set when using this tool.", "Cost Path", " can also be used to derive the path of least resistance down a digital elevation model (DEM). In this case, use the DEM for the  ", "Input cost distance raster", " and the output from the ", "Flow Direction", " tool  for the  ", "Input cost backlink raster", ". Valid flow direction raster values are 1, 2, 4, 8, 16, 32, 64, and 128; valid values in the back link raster are 1, 2, 3, 4, 5, 6, 7, and 8. Both of these rasters are acceptable.", "When the input destination data is feature, it must contain at least one valid field."], "parameters": [{"name": "in_destination_data", "isInputFile": true, "isOptional": false, "description": "A raster or feature dataset that identifies those cells from which the least-cost path is determined to the least costly source. If the input is a raster, the input consists of cells that have valid values (zero is a valid value), and the remaining cells must be assigned NoData. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "in_cost_distance_raster", "isInputFile": true, "isOptional": false, "description": "The name of a cost distance raster to be used to determine the least-cost path from the destination locations to a source. The cost distance raster is usually created with the Cost Distance , Cost Allocation or Cost Back Link tools. The cost distance raster stores, for each cell, the minimum accumulative cost distance over a cost surface from each cell to a set of source cells. ", "dataType": "Raster Layer"}, {"name": "in_cost_backlink_raster", "isInputFile": true, "isOptional": false, "description": "The name of a cost back link raster used to determine the path to return to a source via the least-cost path. For each cell in the back link raster, a value identifies the neighbor that is the next cell on the least accumulative cost path from the cell to a single source cell or set of source cells. ", "dataType": "Raster Layer"}, {"name": "path_type", "isOptional": true, "description": "A keyword defining the manner in which the values and zones on the input destination data will be interpreted in the cost path calculations. EACH_CELL \u2014 For each cell with valid values on the input destination data, a least-cost path is determined and saved on the output raster. With this option, each cell of the input destination data is treated separately, and a least-cost path is determined for each from cell. EACH_ZONE \u2014 For each zone on the input destination data, a least-cost path is determined and saved on the output raster. With this option, the least-cost path for each zone begins at the cell with the lowest cost distance weighting in the zone. BEST_SINGLE \u2014 For all cells on the input destination data, the least-cost path is derived from the cell with the minimum of the least-cost paths to source cells. ", "dataType": "String"}, {"name": "destination_field", "isOptional": true, "description": "The field used to obtain values for the destination locations. Input feature data must contain at least one valid field. ", "dataType": "Field"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CostDistance (in_source_data, in_cost_raster, {maximum_distance}, {out_backlink_raster})", "name": "Cost Distance (Spatial Analyst)", "description": "Calculates the least accumulative cost distance for each cell to the nearest source over a cost surface. \r\n Learn more about how Cost distance tools work", "example": {"title": "CostDistance example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCostDist = CostDistance ( \"source.shp\" , \"elevation\" , 200000 , \"backlink\" ) outCostDist.save ( \"C:/sapyexamples/output/costdist\" )"}, "usage": ["The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "When the source input is a feature, by default, the first valid available field will be used.  If no valid fields exist, the ObjectID field (for example, OID or FID, depending on the type of feature input) will be used.", "Cell locations with NoData in the ", "Input cost raster", " act as barriers in the cost surface tools. Any cell location that is assigned NoData on the input cost surface will receive NoData on all output rasters  (cost distance, allocation, and back link).", "If the input source data and the cost raster are different extents, the default output extent is the intersection of the two. To get a cost distance surface for the entire extent, choose the ", "Union of Inputs", " option on the output ", "Extent", " environment settings.", "If a ", "Mask", " has been set in the environment, all masked cells will be treated as NoData values.", "When a mask has been defined in the ", "Raster Analysis", " window and the cells to be masked will mask a source, the calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on all outputs (distance, allocation, and back link) rasters.", "The ", "Maximum distance", " is specified in the same cost units as those on the cost raster.", "For the output distance raster, the least-cost distance (or minimum accumulative cost distance) of a cell to a set of source locations is the lower bound of the least-cost distances from the cell to all source locations.", "The cost raster cannot contain values of zero since the algorithm is a multiplicative process. If your cost raster does contain values of zero, and these values represent areas of lowest cost, change values of zero to a small positive value (such as 0.01) before running ", "Cost Distance", ", by first running the ", "Con", "  tool. If areas with a value of zero represent areas that should be excluded from the analysis, these values should be turned to NoData before running ", "Cost Distance", ", by first running the ", "Set Null", " tool."], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the least accumulated cost distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "in_cost_raster", "isInputFile": true, "isOptional": false, "description": "A raster defining the impedance or cost to move planimetrically through each cell. The value at each cell location represents the cost per unit distance for moving through the cell. Each cell location value is multiplied by the cell resolution while also compensating for diagonal movement to obtain the total cost of passing through the cell. The values of the cost raster can be integer or floating point, but they cannot be negative or zero (you cannot have a negative or zero cost). ", "dataType": "Raster Layer"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative cost values cannot exceed. If an accumulative cost distance value exceeds this value, the output value for the cell location will be NoData. The maximum distance defines the extent for which the accumulative cost distances are calculated. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "out_backlink_raster", "isOutputFile": true, "isOptional": true, "description": "The output cost back-link raster. The back-link raster contains values of 0 through 8, which define the direction or identify the next neighboring cell (the succeeding cell) along the least accumulative cost path from a cell to reach its least cost source. If the path is to pass into the right neighbor, the cell will be assigned the value 1, 2 for the lower right diagonal cell, and continuing clockwise. The value 0 is reserved for source cells. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_distance_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CostBackLink (in_source_data, in_cost_raster, {maximum_distance}, {out_distance_raster})", "name": "Cost Back Link (Spatial Analyst)", "description": "Defines the neighbor that is the next cell on the least accumulative cost path to the nearest source. \r\n Learn more about how Cost distance tools work", "example": {"title": "CostBackLink example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBacklink = CostBackLink ( \"observers\" , \"costraster\" , 100000 , \"c:/sapyexamples/output/distRast\" ) outBacklink.save ( \"c:/sapyexamples/output/backlink\" )"}, "usage": ["The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", "Cell locations with NoData in the ", "Input cost raster", " act as barriers in the cost surface tools. Any cell location that is assigned NoData on the input cost surface will receive NoData on all output rasters  (cost distance, allocation, and back link).", "If the input source data and the cost raster are different extents, the default output extent is the intersection of the two. To get a cost distance surface for the entire extent, choose the ", "Union of Inputs", " option on the output ", "Extent", " environment settings.", "If a ", "Mask", " has been set in the environment, all masked cells will be treated as NoData values.", "When a mask has been defined in the ", "Raster Analysis", " window and the cells to be masked will mask a source, the calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on all outputs (distance, allocation, and back link) rasters.", "The ", "Maximum distance", " is specified in the same cost units as those on the cost raster.", "For the output distance raster, the least-cost distance (or minimum accumulative cost distance) of a cell to a set of source locations is the lower bound of the least-cost distances from the cell to all source locations."], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the least accumulated cost distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "in_cost_raster", "isInputFile": true, "isOptional": false, "description": "A raster defining the impedance or cost to move planimetrically through each cell. The value at each cell location represents the cost per unit distance for moving through the cell. Each cell location value is multiplied by the cell resolution while also compensating for diagonal movement to obtain the total cost of passing through the cell. The values of the cost raster can be integer or floating point, but they cannot be negative or zero (you cannot have a negative or zero cost). ", "dataType": "Raster Layer"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative cost values cannot exceed. If an accumulative cost distance value exceeds this value, the output value for the cell location will be NoData. The maximum distance defines the extent for which the accumulative cost distances are calculated. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "out_distance_raster", "isOutputFile": true, "isOptional": true, "description": "The output cost distance raster. The cost distance raster identifies, for each cell, the least accumulative cost distance over a cost surface to the identified source locations. A source can be a cell, a set of cells, or one or more feature locations. The output raster is of floating point type. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_backlink_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CostAllocation (in_source_data, in_cost_raster, {maximum_distance}, {in_value_raster}, {source_field}, {out_distance_raster}, {out_backlink_raster})", "name": "Cost Allocation (Spatial Analyst)", "description": "Calculates for each cell its nearest source based on the least accumulative cost over a cost surface. \r\n Learn more about how Cost distance tools work", "example": {"title": "CostAllocation example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" out = () costAllocOut = CostAllocation ( \"observers.shp\" , \"costraster\" , 25000 , \"elevation\" , \"FID\" , \"c:/sapyexamples/output/distout\" , \"c:/sapyexamples/output/backlinkout\" ) costAllocOut.save ( \"c:/sapyexamples/output/costalloc\" )"}, "usage": ["The input source data can be a feature class or raster.", "When the input source data is a raster, the set of source cells consists of all cells in the source raster that have valid values. Cells that have NoData values are not included in the source set. The value 0 is considered a legitimate source. A source raster can be easily created using the ", "extraction tools", ".", "When the input source data is a feature class, the source locations are converted internally to a raster before performing the analysis. The resolution of the raster can be controlled with the ", "Output cell size", " parameter or the ", "Cell Size", " environment. By default, the resolution will be  determined by the shorter of the\r\nwidth or height of the extent of input feature, in the input\r\nspatial reference, divided by 250.\r\n", "When using polygon feature data for the input source data, care must be taken with how the output cell size is handled when it is coarse, relative to the detail present in the input.  The internal rasterization process employs the same default ", "Cell assignment type", " method as the ", "Polygon to Raster", " tool, which is CELL_CENTER. This means that data not located at the center of the cell will not be included in the intermediate rasterized source output, and so will not be represented in the distance calculations. For example, if your sources are a series of small polygons, such as building footprints, that are small relative to the output cell size, it is possible that only a few of them will fall under the centers of the output raster cells, seemingly causing most of the others to be lost in the analysis.", "To avoid this situation, as an intermediate step, you could rasterize the input features directly with   the ", "Polygon to Raster", " tool and set a ", "Priority field", ", and use the resulting output as input to the Distance tool. Alternatively, you could select a small enough cell size to capture the appropriate amount of detail from the input features.", " To calculate allocation, source locations can have an associated value, which can be specified by the ", "Source field", " parameter.   If the input source is an integer raster, the default field is VALUE. If it is a feature, it will be the first integer field in the attribute table.  If the input source data is a floating point  raster an integer value raster parameter must be specified.", "When the source input is a feature, by default, the first valid available field will be used.  If no valid fields exist, the ObjectID field (for example, OID or FID, depending on the type of feature input) will be used.", "Cell locations with NoData in the ", "Input cost raster", " act as barriers in the cost surface tools. Any cell location that is assigned NoData on the input cost surface will receive NoData on all output rasters  (cost distance, allocation, and back link).", "If the input source data and the cost raster are different extents, the default output extent is the intersection of the two. To get a cost distance surface for the entire extent, choose the ", "Union of Inputs", " option on the output ", "Extent", " environment settings.", "If a ", "Mask", " has been set in the environment, all masked cells will be treated as NoData values.", "When a mask has been defined in the ", "Raster Analysis", " window and the cells to be masked will mask a source, the calculations will occur on the remaining source cells. The source cells that are masked will not be considered in the computations. These cell locations will be assigned NoData on all outputs (distance, allocation, and back link) rasters.", "The ", "Input value raster", " is useful when alternative values or zones are to be used or if source was derived from a operation that results in a binary result of either 0 or 1, losing  the original zone values that are associated with these locations. The value raster can restore these values or allow for analysis on additional combinations of zone values within the source locations.", "If the value raster is used, it may change the configuration and results of the cost allocation output; it will not affect the optional cost distance or the back link results.", "The ", "Maximum distance", " is specified in the same cost units as those on the cost raster.", "For the output distance raster, the least-cost distance (or minimum accumulative cost distance) of a cell to a set of source locations is the lower bound of the least-cost distances from the cell to all source locations."], "parameters": [{"name": "in_source_data", "isInputFile": true, "isOptional": false, "description": "The input source locations. This is a raster or feature dataset that identifies the cells or locations to which the least accumulated cost distance for every output cell location is calculated. For rasters, the input type can be integer or floating point. If the input source raster is floating point, the {in_value_raster} must be set, and it must be of integer type. The value raster will take precedence over any setting of the {source_field} . ", "dataType": "Raster Layer | Feature Layer"}, {"name": "in_cost_raster", "isInputFile": true, "isOptional": false, "description": "A raster defining the impedance or cost to move planimetrically through each cell. The value at each cell location represents the cost per unit distance for moving through the cell. Each cell location value is multiplied by the cell resolution while also compensating for diagonal movement to obtain the total cost of passing through the cell. The values of the cost raster can be integer or floating point, but they cannot be negative or zero (you cannot have a negative or zero cost). ", "dataType": "Raster Layer"}, {"name": "maximum_distance", "isOptional": true, "description": "Defines the threshold that the accumulative cost values cannot exceed. If an accumulative cost distance value exceeds this value, the output value for the cell location will be NoData. The maximum distance defines the extent for which the accumulative cost distances are calculated. The default distance is to the edge of the output raster. ", "dataType": "Double"}, {"name": "in_value_raster", "isInputFile": true, "isOptional": true, "description": "The input integer raster that identifies the zone values that should be used for each input source location. For each source location (cell or feature), the value defined by the {in_value_raster} will be assigned to all cells allocated to the source location for the computation. The value raster will take precedence over any setting for the {source_field} . ", "dataType": "Raster Layer"}, {"name": "source_field", "isOptional": true, "description": "The field used to assign values to the source locations. It must be integer type. If the {in_value_raster} has been set, the values in that input will have precedence over any setting for the {source_field} . ", "dataType": "Field"}, {"name": "out_distance_raster", "isOutputFile": true, "isOptional": true, "description": "The output cost distance raster. The cost distance raster identifies, for each cell, the least accumulative cost distance over a cost surface to the identified source locations. A source can be a cell, a set of cells, or one or more feature locations. The output raster is of floating point type. ", "dataType": "Raster Dataset"}, {"name": "out_backlink_raster", "isOutputFile": true, "isOptional": true, "description": "The output cost back-link raster. The back-link raster contains values of 0 through 8, which define the direction or identify the next neighboring cell (the succeeding cell) along the least accumulative cost path from a cell to reach its least cost source. If the path is to pass into the right neighbor, the cell will be assigned the value 1, 2 for the lower right diagonal cell, and continuing clockwise. The value 0 is reserved for source cells. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_allocation_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Watershed (in_flow_direction_raster, in_pour_point_data, {pour_point_field})", "name": "Watershed (Spatial Analyst)", "description": " Determines the contributing area above a set of cells in a raster. \r\n Learn more about how Watershed works", "example": {"title": "Watershed example 1 (Python window)", "description": "This example determines the contributing area for selected pour point locations on a flow direction GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outWatershed = Watershed ( \"flowdir\" , \"pourpoint\" ) outWatershed.save ( \"C:/sapyexamples/output/outwtrshd01\" )"}, "usage": ["The value of each watershed will be taken from the value of the source in the input raster or feature pour point data. When the pour point is a raster dataset, the cell values will be used. When the pour point is a point feature dataset, the values will come from the specified field.", "Better results will be obtained if the ", "Snap Pour Point", " tool is used beforehand to help locate the pour points to cells of high accumulated flow.", "When specifying the input pour point locations as  feature data,  the default field will be the first available valid  field.  If no  valid fields exist, the ObjectID field (for example, OID or FID) will be the default."], "parameters": [{"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"name": "in_pour_point_data", "isInputFile": true, "isOptional": false, "description": "The input pour point locations. For a raster, this represents cells above which the contributing area, or catchment, will be determined. All cells that are not NoData will be used as source cells. For a point feature dataset, this represents locations above which the contributing area, or catchment, will be determined. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "pour_point_field", "isOptional": true, "description": "Field used to assign values to the pour point locations. If the pour point dataset is a raster, use Value. If the pour point dataset is a feature, use a numeric field. If the field contains floating-point values, they will be truncated into integers. ", "dataType": "Field"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "StreamToFeature (in_stream_raster, in_flow_direction_raster, out_polyline_features, {simplify})", "name": "Stream to Feature (Spatial Analyst)", "description": " Converts a raster representing a linear network to features representing the linear network. \r\n Learn more about how Stream to Feature works", "example": {"title": "StreamToFeature example 1 (Python window)", "description": "This example converts a raster representing a linear network to features.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" StreamToFeature ( \"stream\" , \"flowdir\" , \"c:/sapyexamples/output/outstrm01.shp\" , \"NO_SIMPLIFY\" )"}, "usage": ["The input stream raster linear network should be represented as values greater than or equal to one on a background of NoData.", "The results of the ", "Flow Accumulation", " tool can be used to create a raster stream network by applying a threshold value to select cells with a high accumulated flow. For example, cells that have more than 100 cells flowing into them are used to define the stream network. Use the ", "Con", " or ", "Set Null", " tool to create a stream network raster where flow accumulation values of 100 or greater go to one, and the remainder are put to the background (NoData). The resulting stream network can be used in ", "Stream Link", " and ", "Stream to Feature.", "An analytical method for determining an appropriate threshold value for stream network delineation is presented in Tarboton, et al. (1991).", "There should be contiguous features with the same value, such as the results of the ", "Stream Order", " or ", "Stream Link", " tool. ", "Stream to Feature", " should not be used on a raster in which there are few adjacent cells of the same value.", "The direction of the output features will point downstream."], "parameters": [{"name": "in_stream_raster", "isInputFile": true, "isOptional": false, "description": "An input raster that represents a linear stream network. ", "dataType": "Raster Layer"}, {"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"name": "out_polyline_features", "isOutputFile": true, "isOptional": false, "description": "Output feature class that will hold the converted streams. ", "dataType": "Feature Class"}, {"name": "simplify", "isOptional": true, "description": "Specifies whether weeding is used. By default, weeding is applied. SIMPLIFY \u2014 The feature is weeded to reduce the number of vertices. The Douglas-Puecker algorithm for line generalization is used with a tolerance of sqrt(0.5) * cell size . NO_SIMPLIFY \u2014 No weeding is applied.", "dataType": "Boolean"}]},
{"syntax": "StreamOrder (in_stream_raster, in_flow_direction_raster, {order_method})", "name": "Stream Order (Spatial Analyst)", "description": " Assigns a numeric order to segments of a raster representing branches of a linear network. \r\n Learn more about how Stream Order works", "example": {"title": "StreamOrder example 1 (Python window)", "description": "This example assigns a numeric order to segments of a raster representing branches of a linear network.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outStreamOrder = StreamOrder ( \"stream\" , \"flowdir\" , \"STRAHLER\" ) outStreamOrder.save ( \"c:/sapyexamples/output/outstrmordr01\" )"}, "usage": ["The output of ", "Stream Order", " will be of higher quality if the input stream raster and input flow direction raster are derived from the same surface. If the stream raster is derived from a rasterized streams dataset, the output may not be usable because, on a cell-by-cell basis, the direction will not correspond with the location of stream cells.", "The results of the ", "Flow Accumulation", " tool can be used to create a raster stream network by applying a threshold value to select cells with a high accumulated flow. For example, cells that have more than 100 cells flowing into them are used to define the stream network. Use the ", "Con", " or ", "Set Null", " tool to create a stream network raster where flow accumulation values of 100 or greater go to one, and the remainder are put to the background (NoData). The resulting stream network can be used in ", "Stream Link", " and ", "Stream to Feature.", "An analytical method for determining an appropriate threshold value for stream network delineation is presented in Tarboton, et al. (1991)."], "parameters": [{"name": "in_stream_raster", "isInputFile": true, "isOptional": false, "description": "An input raster that represents a linear stream network. The input stream raster linear network should be represented as values greater than or equal to one on a background of NoData. ", "dataType": "Raster Layer"}, {"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"name": "order_method", "isOptional": true, "description": "The method used for assigning stream order. STRAHLER \u2014 The method of stream ordering proposed by Strahler in 1952. Stream order only increases when streams of the same order intersect. Therefore, the intersection of a first-order and second-order link will remain a second-order link, rather than creating a third-order link. This is the default. SHREVE \u2014 The method of stream ordering by magnitude, proposed by Shreve in 1967. All links with no tributaries are assigned a magnitude (order) of one. Magnitudes are additive downslope. When two links intersect, their magnitudes are added and assigned to the downslope link.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "StreamLink (in_stream_raster, in_flow_direction_raster)", "name": "Stream Link (Spatial Analyst)", "description": " Assigns unique values to sections of a raster linear network between intersections.", "example": {"title": "StreamLink example 1 (Python window)", "description": "This example assigns unique values to sections of a raster linear network between intersections.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outStreamLink = StreamLink ( \"stream\" , \"flowdir\" ) outStreamLink.save ( \"c:/sapyexamples/output/outstrmlnk01\" )"}, "usage": ["Links are the sections of a stream channel connecting two\r\nsuccessive junctions, a junction and the outlet, or a junction and\r\nthe drainage divide.", "The input stream raster can be created by \"thresholding\" the results of the ", "Flow Accumulation", " tool.", "The stream raster linear network should be represented as values greater than or equal to one on a background of NoData."], "parameters": [{"name": "in_stream_raster", "isInputFile": true, "isOptional": false, "description": "An input raster that represents a linear stream network. ", "dataType": "Raster Layer"}, {"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "SnapPourPoint (in_pour_point_data, in_accumulation_raster, snap_distance, {pour_point_field})", "name": "Snap Pour Point (Spatial Analyst)", "description": " Snaps pour points to the cell of highest flow accumulation within a specified distance.", "example": {"title": "SnapPourPoint example 1 (Python window)", "description": "This example snaps pour points to the cell of highest flow accumulation within a specified distance.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSnapPour = SnapPourPoint ( \"pourpoint\" , \"flowaccumulation.img\" , 5 , \"VALUE\" ) outSnapPour.save ( \"c:/sapyexamples/output/outsnpprpnt01\" )"}, "usage": ["The ", "Snap Pour Point", " tool is used to ensure the selection of points of high accumulated flow when delineating drainage basins using the ", "Watershed", " tool. ", "Snap Pour Point", " will search within a snap distance around the specified pour points for the cell of highest accumulated flow and move the pour point to that location.", "If the input pour point data is a point feature class, it will be converted to a raster internally for processing.", "The output is an integer raster when the original pour point locations have been snapped to locations of higher accumulated flow.", "When there is only one input pour point location, the extent of the output is that of the accumulation raster. If there is more than one pour point location, the extent of the output is determined by the settings in the ", "Output extent", " environment.", "When specifying the input pour point locations as  feature data,  the default field will be the first available valid  field.  If no  valid fields exist, the ObjectID field (for example, OID or FID) will be the default."], "parameters": [{"name": "in_pour_point_data", "isInputFile": true, "isOptional": false, "description": "The input pour point locations that are to be snapped. For a raster input, all cells that are not NoData (i.e., have a value) will be considered pour points and will be snapped. For a point feature input, this specifies the locations of cells that will be snapped. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "in_accumulation_raster", "isInputFile": true, "isOptional": false, "description": "The input flow accumulation raster. This can be created with the Flow Accumulation tool. ", "dataType": "Raster Layer"}, {"name": "snap_distance", "isOptional": false, "description": "Maximum distance, in map units, to search for a cell of higher accumulated flow. ", "dataType": "Double"}, {"name": "pour_point_field", "isOptional": true, "description": "Field used to assign values to the pour point locations. If the pour point dataset is a raster, use Value. If the pour point dataset is a feature, use a numeric field. If the field contains floating-point values, they will be truncated into integers. ", "dataType": "Field"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Sink (in_flow_direction_raster)", "name": "Sink (Spatial Analyst)", "description": " Creates a raster identifying all sinks or areas of internal drainage. \r\n Learn more about how Sink works \r\n", "example": {"title": "Sink example 1 (Python window)", "description": "This example identifies the sinks on an input flow direction GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSink = Sink ( \"flowdir\" ) outSink.save ( \"C:/sapyexamples/output/outsink01\" )"}, "usage": ["A sink is a cell or set of spatially connected cells whose flow\r\ndirection cannot be assigned one of the eight valid values in a\r\nflow direction raster. This can occur when all neighboring cells\r\nare higher than the processing cell or when two cells flow into\r\neach other, creating a two-cell loop.\r\n", "The output of the ", "Sink", " tool is an integer raster with each sink being assigned a unique value. Sinks are numbered between one and the number of sinks."], "parameters": [{"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "FlowLength (in_flow_direction_raster, {direction_measurement}, {in_weight_raster})", "name": "Flow Length (Spatial Analyst)", "description": " Calculates the upstream or downstream distance, or weighted distance, along the flow path for each cell.", "example": {"title": "FlowLength example 1 (Python window)", "description": "This example Calculates the downstream distance along the flow path for each cell.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outFlowLength = FlowLength ( \"flowdir\" , \"DOWNSTREAM\" , \"\" ) outFlowLength.save ( \"c:/sapyexamples/output/outflowlen01\" )"}, "usage": ["The value type for the ", "Flow Length", " output raster is floating point.", "A primary use of the ", "Flow Length", " tool is to calculate the length of the longest flow path within a given basin. This measure is often used to calculate the time of concentration of a basin. This would be done using the UPSTREAM option.", "The tool can also be used to create distance-area diagrams of hypothetical rainfall and runoff events using the weight raster as an impedance to movement downslope."], "parameters": [{"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"name": "direction_measurement", "isOptional": true, "description": "The direction of measurement along the flow path. DOWNSTREAM \u2014 Calculates the downslope distance along the flow path, from each cell to a sink or outlet on the edge of the raster. UPSTREAM \u2014 Calculates the longest upslope distance along the flow path, from each cell to the top of the drainage divide.", "dataType": "String"}, {"name": "in_weight_raster", "isInputFile": true, "isOptional": true, "description": "An optional input raster for applying a weight to each cell. If no weight raster is specified, a default weight of 1 will be applied to each cell. For each cell in the output raster, the result will be the number of cells that flow into it. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "FlowDirection (in_surface_raster, {force_flow}, {out_drop_raster})", "name": "Flow Direction (Spatial Analyst)", "description": " Creates a raster of flow direction from each cell to its steepest downslope neighbor. \r\n Learn more about how Flow Direction works \r\n", "example": {"title": "FlowDirection example 1 (Python window)", "description": "This example creates a flow direction raster from an input GRID elevation surface raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outFlowDirection = FlowDirection ( \"elevation\" , \"NORMAL\" ) outFlowDirection.save ( \"C:/sapyexamples/output/outflowdir01\" )"}, "usage": ["The output of the ", "Flow Direction", " tool is an integer raster whose values range from 1 to 255. The values for each direction from the center are:", "For example, if the direction of steepest drop was to the left of the current processing cell, its flow direction would be coded as 16.", "If a cell is lower than its eight neighbors, that cell is given the value of its lowest neighbor, and flow is defined toward this cell. If multiple neighbors have the lowest value, the cell is still given this value, but flow is defined with one of the two methods explained below. This is used to filter out one-cell sinks, which are considered noise.", "If a cell has the same change in z-value in multiple directions and that cell is part of a sink, the flow direction is referred to as undefined. In such cases, the value for that cell in the output flow direction raster will be the sum of those directions. For example, if the change in z-value is the same both to the right (", "flow direction = 1", ") and down (", "flow direction = 4", "), the flow direction for that cell is ", "1 + 4 = 5", ". Cells with undefined flow direction can be flagged as sinks using the ", "Sink", " tool.", "If a cell has the same change in z-value in multiple directions and is not part of a sink, the flow direction is assigned with a lookup table defining the most likely direction. See Greenlee (1987).", "The output drop raster is calculated as the difference in z-value divided by the path length between the cell centers, expressed in percentages. For adjacent cells, this is analogous to the percent slope between cells. Across a flat area, the distance becomes the distance to the nearest cell of lower elevation. The result is a map of percent rise in the path of steepest descent from each cell.", "When calculating the drop raster in flat areas, the distance to diagonally adjacent cells (", "1.414 * cell size", ") is approximated by ", "1.5 * cell size", " to increase the processing speed by using integer calculations.", "When using the NORMAL option, a cell at the edge of the surface raster will flow toward the inner cell with the steepest drop in z-value. If the drop is less than or equal to zero, the cell will flow out of the surface raster."], "parameters": [{"name": "in_surface_raster", "isInputFile": true, "isOptional": false, "description": "The input raster representing a continuous surface. ", "dataType": "Raster Layer"}, {"name": "force_flow", "isOptional": true, "description": "Specifies if edge cells will always flow outward or follow normal flow rules. NORMAL \u2014 If the maximum drop on the inside of an edge cell is greater than zero, the flow direction will be determined as usual; otherwise, the flow direction will be toward the edge. Cells that should flow from the edge of the surface raster inward will do so. This is the default. FORCE \u2014 All cells at the edge of the surface raster will flow outward from the surface raster.", "dataType": "Boolean"}, {"name": "out_drop_raster", "isOutputFile": true, "isOptional": true, "description": "An optional output drop raster. The drop raster shows the ratio of the maximum change in elevation from each cell along the direction of flow to the path length between centers of cells, expressed in percentages. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_flow_direction_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "FlowAccumulation (in_flow_direction_raster, {in_weight_raster}, {data_type})", "name": "Flow Accumulation (Spatial Analyst)", "description": " Creates a raster of accumulated flow into each cell. A weight factor can optionally be applied. \r\n Learn more about how Flow Accumulation works", "example": {"title": "FlowAccumulation example 1 (Python window)", "description": "This example creates a raster of accumulated flow into each cell of an input flow direction GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outFlowAccumulation = FlowAccumulation ( \"flowdir\" ) outFlowAccumulation.save ( \"C:/sapyexamples/output/outflowacc01\" )"}, "usage": ["The result of ", "Flow Accumulation", " is a raster of accumulated flow to each cell, as determined by accumulating the weight for all cells that flow into each downslope cell.", "Cells of undefined flow direction will only receive flow; they will not contribute to any downstream flow. A cell is considered to have an undefined flow direction if its value in the flow direction raster is anything other than 1, 2, 4, 8, 16, 32, 64, or 128.", "The accumulated flow is based on the number of cells flowing into each cell in the output raster. The current processing cell is not considered in this accumulation.", "Output cells with a high flow accumulation are areas of concentrated flow and can be used to identify stream channels.", "Output cells with a flow accumulation of zero are local topographic highs and can be used to identify ridges.", "If the input flow direction raster is not created with the ", "Flow Direction", " tool, there is a chance that the defined flow could loop. If the flow direction does loop, ", "Flow Accumulation", " will go into an infinite loop and never finish.", " The ", "Flow Accumulation", " tool does not honour the ", "Compression", " environment setting. The output raster will always be uncompressed."], "parameters": [{"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"name": "in_weight_raster", "isInputFile": true, "isOptional": true, "description": "An optional input raster for applying a weight to each cell. If no weight raster is specified, a default weight of 1 will be applied to each cell. For each cell in the output raster, the result will be the number of cells that flow into it. ", "dataType": "Raster Layer"}, {"name": "data_type", "isOptional": true, "description": "The output accumulation raster can be integer or floating point type. FLOAT \u2014 The output raster will be floating point type. This is the default. INTEGER \u2014 The output raster will be integer type.", "dataType": "String"}, {"isOutputFile": true, "name": "out_accumulation_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Fill (in_surface_raster, {z_limit})", "name": "Fill (Spatial Analyst)", "description": " Fills sinks in a surface raster to remove small imperfections in the data. \r\n Learn more about how Fill works", "example": {"title": "Fill example 1 (Python window)", "description": "This example fills the sinks of an input elevation surface GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outFill = Fill ( \"elevation\" ) outFill.save ( \"C:/sapyexamples/output/outfill01\" )"}, "usage": ["A sink is a cell with an undefined drainage direction; no cells surrounding it are lower. The pour point is the boundary cell with the lowest elevation for the contributing area of a sink. If the sink were filled with water, this is the point where water would pour out.", "The z-limit specifies the maximum depth of a sink that will be filled. The z-limit is not the maximum depth to which a sink will be filled. Z-limit must be greater than zero.", "All sinks that are less than the z-limit, lower than their lowest adjacent neighbor, will be filled to the height of their pour points.", "Due to the iterative nature of ", "Fill", ", it can be CPU and disk intensive. It can require up to four times the disk space of the input raster.", "The number of sinks found with the z-limit will determine the length of processing time. The more sinks, the longer the processing time.", "The ", "Sink", " tool can be used to find the number of sinks and help identify their depth. Knowing the depth of the sinks can help in determining an appropriate z-limit for ", "Fill", ".", "Fill", " can also be used to remove peaks. A peak is a cell where no adjacent cells are higher. To remove peaks, the input surface raster must be inverted. This can be performed with the ", "Minus", " tool. Specify the highest value of the surface raster as the first input to ", "Minus", " and surface raster as the second input. Perform a ", "Fill", ". Invert the results to obtain a surface that has original surface raster values with the peaks removed. The z-limit can be applied to this process as well. If nothing is specified for z-limit, then all peaks will be removed. If it is specified, where the difference in z-value between a peak and its highest adjacent neighbor is greater than the z-limit, that peak will not be removed."], "parameters": [{"name": "in_surface_raster", "isInputFile": true, "isOptional": false, "description": "The input raster representing a continuous surface. ", "dataType": "Raster Layer"}, {"name": "z_limit", "isOptional": true, "description": "Maximum elevation difference between a sink and its pour point to be filled. If the difference in z-values between a sink and its pour point is greater than the z_limit that sink will not be filled. The default is to fill all sinks, regardless of depth. ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_surface_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Basin (in_flow_direction_raster)", "name": "Basin (Spatial Analyst)", "description": " Creates a raster delineating all drainage basins.", "example": {"title": "Basin example 1 (Python window)", "description": "This example determines the drainage basins of an input flow direction GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBasin = Basin ( \"flowdir\" ) outBasin.save ( \"C:/sapyexamples/output/outbasin01\" )"}, "usage": ["The drainage basins are delineated within  the analysis window by identifying ridge lines between basins. The input flow direction raster is analyzed to find all sets of connected cells that belong to the same drainage basin. The drainage basins are created by locating the pour points at the edges of the analysis window (where water would pour out of the raster), as well as sinks, then identifying the contributing area above each pour point. This results in a raster of drainage basins.", "The best results will be obtained if when the input ", "Flow Direction", " raster was created, the ", "Force", " option was used.", "All cells in the raster will belong to a basin, even if that basin is only one cell."], "parameters": [{"name": "in_flow_direction_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that shows the direction of flow out of each cell. The flow direction raster can be created using the Flow Direction tool. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PorousPuff (in_track_file, in_porosity_raster, in_thickness_raster, mass, {dispersion_time}, {longitudinal_dispersivity}, {dispersivity_ratio}, {retardation_factor}, {decay_coefficient})", "name": "Porous Puff (Spatial Analyst)", "description": "Calculates the time-dependent, two-dimensional concentration distribution in mass per volume of a solute introduced instantaneously and at a discrete point into a vertically mixed aquifer. \r\n Learn more about how Porous Puff works", "example": {"title": "PorousPuff example 1 (Python window)", "description": "This example executes the tool on the required inputs and outputs a raster of the concentration distribution.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPorousPuff = PorousPuff ( \"trackfile.txt\" , \"gwporo\" , \"gwthick\" , 50 , 10000 , \"\" , 3 , \"\" , \"\" ) outPorousPuff.save ( \"c:/sapyexamples/output/outporpuff\" )"}, "usage": ["The effective porosity field, a physical property of the aquifer, is generally estimated from geological data. It is defined as the volume of void space that contributes to fluid flow divided by the entire volume. Porosity is expressed as a number between 0.0 and 1.0, with typical values around 0.35, and is dimensionless. A value of effective porosity of 0.35 means that 35 percent of the volume of the porous medium contributes to fluid flow. The remaining 65 percent, consisting of solid matrix and unconnected pores, does not contribute to fluid flow.", "No particular system of units is specified by this tool. It is important that all data be consistent, using the same unit for time (seconds, days, years), length (feet, meters), and mass (kilograms, slugs).", "The saturated thickness, measured in units of length, is interpreted from geological information. For a confined aquifer, this measure is the thickness of the formation between the upper and lower confining layers. For an unconfined aquifer, the saturated thickness is the distance between the water table and the lower confining layer.", "The decay coefficient \u03bb is related to the half-life T", "1/2", " as:", "For example, the half-life of Carbon-14 is 5,730 years. Since ln(2) = 0.693, the decay coefficient becomes 0.693/5730 = 1.21x10", "-4", " /year. A stable constituent has a decay coefficient of zero, corresponding to an infinite half-life. Half-lives of radioisotopes are available from several sources, including the CRC Handbook of Chemistry and Physics from ", "CRC Press", ".", "The requested time must not exceed the latest time recorded in the path file. Either a lesser time must be requested in ", "Porous Puff", ", or a new path file with a greater time needs to be generated by ", "Particle Track", ".", "The time requested should not be reached before the completion of the first path step as recorded in the track file. Either a greater time should be requested in ", "Porous Puff", ", or a new track file should be generated by ", "Particle Track", " using a shorter step length.", "The centroid of mass must not migrate to the edge of the raster or beyond. In this case, no data is available on which to base the dispersion, so the tool is aborted. Either a lesser time must be requested, or a larger raster must be generated to accommodate the migration."], "parameters": [{"name": "in_track_file", "isInputFile": true, "isOptional": false, "description": "The input particle track path file. This is an ASCII text file containing information about the position, the local velocity vector, and the cumulative length and time of travel along the path. This file is generated using the Particle Track tool. ", "dataType": "File"}, {"name": "in_porosity_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the effective formation porosity at that location. ", "dataType": "Raster Layer"}, {"name": "in_thickness_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the saturated thickness at that location. The value for the thickness is interpreted from geological properties of the aquifer. ", "dataType": "Raster Layer"}, {"name": "mass", "isOptional": false, "description": "A value for the amount of mass released instantaneously at the source point, in units of mass. ", "dataType": "Double"}, {"name": "dispersion_time", "isOptional": true, "description": "A value representing the time horizon for dispersion of the solute, in units of time. The time must be less than or equal to the maximum time in the track file. If the requested time exceeds the available time from the track file, the tool is aborted. The default time is the latest time (corresponding to the terminal point) in the track file. ", "dataType": "Double"}, {"name": "longitudinal_dispersivity", "isOptional": true, "description": "A value representing the dispersivity parallel to the flow direction. For details on how the default value is determined, and how it relates to the scale of the study, see the How Porous Puff works section in the documentation. ", "dataType": "Double"}, {"name": "dispersivity_ratio", "isOptional": true, "description": "A value representing the ratio of longitudinal dispersivity over transverse dispersivity. Transverse dispersivity is perpendicular to the flow direction in the same horizontal plane. The default value is three. ", "dataType": "Double"}, {"name": "retardation_factor", "isOptional": true, "description": "A dimensionless value representing the retardation of the solute in the aquifer. Retardation varies between one and infinity, with one corresponding to no retardation. The default value is one. ", "dataType": "Double"}, {"name": "decay_coefficient", "isOptional": true, "description": "Decay coefficient for solutes undergoing first-order exponential decay (for example, radionuclides) in units of inverse time. The default is zero, corresponding to no decay. ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ParticleTrack (in_direction_raster, in_magnitude_raster, source_point, out_track_file, {step_length}, {tracking_time}, {out_track_polyline_features})", "name": "Particle Track (Spatial Analyst)", "description": "Calculates the path of a particle through a velocity field, returning an ASCII file of particle tracking data and, optionally, a coverage of track information. \r\n Learn more about how Particle Track works", "example": {"title": "ParticleTrack example 1 (Python window)", "description": "This example executes the tool on the required inputs and outputs an ASCII file of particle tracking data and a shapefile featureclass of the particle track.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" ParticleTrack ( \"gwdir\" , \"gwmag\" , arcpy.Point ( - 200 , - 200 ), \"C:/sapyexamples/output/trackfile.txt\" , 10 , 100000 , \"C:/sapyexamples/output/trackpolyline.shp\" )"}, "usage": ["The input direction and magnitude rasters should be from the same run of the ", "Darcy Flow", " tool.", "The path file generated by this tool is an ASCII text file containing information about position, local velocity direction and magnitude, and cumulative length and time of travel along the path. This file is used for input by ", "Porous Puff", ". The format of this file is as follows:", "No particular system of units is specified by ", "Particle Track", ". It is important that all data be in a consistent set of units, using the same unit for time (seconds, days, years) and length (feet, meters).", "The source location must be within the boundary of the input rasters and cannot be in an area of NoData.", "The track file will end if the track reaches outside the study area and has not met the specified maximum tracking time.", "If the particle being tracked has reached the edge of the study area at the indicated time and the predictor point is outside the study area, the track file will end.", "If the particle being tracked migrates into a depression at the indicated time, the track file will end. A depression can be created by a discharge well or other sink.", "The two outputs from this tool are:"], "parameters": [{"name": "in_direction_raster", "isInputFile": true, "isOptional": false, "description": "An input raster where each cell value represents the direction of the seepage velocity vector (average linear velocity) at the center of the cell. Directions are expressed in compass coordinates, in degrees clockwise from north. This can be created by the Darcy Flow tool. Direction values must be floating point. ", "dataType": "Raster Layer"}, {"name": "in_magnitude_raster", "isInputFile": true, "isOptional": false, "description": "An input raster where each cell value represents the magnitude of the seepage velocity vector (average linear velocity) at the center of the cell. Units are length/time. This can be created by the Darcy Flow tool. ", "dataType": "Raster Layer"}, {"name": "source_point", "isOptional": false, "description": "The location of the source point from which to begin the particle tracking. This is entered as numbers identifying the x,y coordinates of the position in map units. ", "dataType": "Point"}, {"name": "out_track_file", "isOutputFile": true, "isOptional": false, "description": "The output ASCII text file that contains the particle tracking data. ", "dataType": "File"}, {"name": "step_length", "isOptional": true, "description": "The step length to be used for calculating the particle track. The default is one-half the cell size. Units are length. ", "dataType": "Double"}, {"name": "tracking_time", "isOptional": true, "description": "Maximum elapsed time for particle tracking. The algorithm will follow the track until either this time is met or the particle migrates off the raster or into a depression. The default value is infinity. Units are time. ", "dataType": "Double"}, {"name": "out_track_polyline_features", "isOutputFile": true, "isOptional": true, "description": "The optional output line feature class containing the particle track. This feature class contains a series of arcs with attributes for position, local velocity direction and magnitude, and cumulative length and time of travel along the path. ", "dataType": "Feature Class"}]},
{"syntax": "DarcyVelocity (in_head_raster, in_porosity_raster, in_thickness_raster, in_transmissivity_raster, out_magnitude_raster)", "name": "Darcy Velocity (Spatial Analyst)", "description": "Calculates the groundwater seepage velocity vector (direction and magnitude) for steady flow in an aquifer. \r\n Learn more about how Darcy Flow and Darcy Velocity work", "example": {"title": "DarcyVelocity example 1 (Python window)", "description": "Calculates the groundwater seepage velocity (direction and magnitude) for steady flow in an aquifer.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outDarcyVelocity = DarcyVelocity ( \"gwhead\" , \"gwporo\" , \"gwthick\" , \"gwtrans\" , \"C:/sapyexamples/output/outdarcymag\" ) outDarcyVelocity.save ( \"c:/sapyexamples/output/outdarcyvel\" )"}, "usage": ["The differences between ", "Darcy Flow", " and ", "Darcy Velocity", " are:", "All input rasters must have the same extent and cell size.", "All input rasters must be floating point.", "The direction of the velocity vector is recorded in compass coordinates (degrees clockwise from north), the magnitude in units of length over time.", "No particular system of units is specified by this tool. Data should be consistent, using the same unit for time (seconds, days, years) and length (feet, meters) for all data.", "The head elevation raster can come from a variety of sources. It can be interpolated from observation well data by using one of the surface interpolation tools, such as ", "Kriging", " or ", "Spline", ". The head values can also be obtained from the results of a separate modeling program.", "However the head elevation raster is obtained, the head must be consistent with the transmissivity raster; that is, the head must reflect the flow through the transmissivity field. It is not sufficient to use values obtained by measurement and testing in the field\u2014the rasterized values must be analyzed for consistency with the aid of a proper porous medium flow program. Consistency implies that the heads would actually be produced by the modeled transmissivity field. Since the true and modeled transmissivity fields often differ in practice, the true and modeled head fields differ, as well. Check the heads for consistency by examining the residual raster produced by ", "Darcy Flow", ". The residual will reflect the consistency of the dataset. Any analysis using ", "Darcy Velocity", " on inconsistent datasets will produce meaningless results.", "The effective porosity field, a physical property of the aquifer, is generally estimated from geological data. It is defined as the volume of void space that contributes to fluid flow divided by the entire volume. Porosity is expressed as a number between 0 and 1, with typical values around 0.35, and is dimensionless. A value of effective porosity of 0.35 means that 35 percent of the volume of the porous medium contributes to fluid flow. The remaining 65 percent, consisting of solid matrix and unconnected pores, does not contribute to fluid flow.", "The saturated thickness, measured in units of length, is interpreted from geological information. For a confined aquifer, this measure is the thickness of the formation between the upper and lower confining layers. For an unconfined aquifer, the saturated thickness is the distance between the water table and the lower confining layer."], "parameters": [{"name": "in_head_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the groundwater head elevation at that location. The head is typically an elevation above some datum, such as mean sea level. ", "dataType": "Raster Layer"}, {"name": "in_porosity_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the effective formation porosity at that location. ", "dataType": "Raster Layer"}, {"name": "in_thickness_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the saturated thickness at that location. The value for the thickness is interpreted from geological properties of the aquifer. ", "dataType": "Raster Layer"}, {"name": "in_transmissivity_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the formation transmissivity at that location. The transmissivity of an aquifer is defined as the hydraulic conductivity K times the saturated aquifer thickness b, as units of length squared over time. This property is generally estimated from field experimental data such as pumping tests. Tables 1 and 2 in How Darcy Flow and Darcy Velocity work list ranges of hydraulic conductivities for some generalized geologic materials. ", "dataType": "Raster Layer"}, {"name": "out_magnitude_raster", "isOutputFile": true, "isOptional": false, "description": "The output flow direction raster. Each cell value represents the direction of the seepage velocity vector (average linear velocity) at the center of the cell, calculated as the average value of the seepage velocity through the four faces of the cell. It is used with the output magnitude raster to describe the flow vector. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_direction_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "DarcyFlow (in_head_raster, in_porosity_raster, in_thickness_raster, in_transmissivity_raster, {out_direction_raster}, {out_magnitude_raster})", "name": "Darcy Flow (Spatial Analyst)", "description": "Calculates the groundwater volume balance residual and other outputs for steady flow in an aquifer. \r\n Learn more about how Darcy Flow and Darcy Velocity work", "example": {"title": "DarcyFlow example 1 (Python window)", "description": "This example calculates the groundwater volume balance raster as well as the flow direction and seepage velocity of an aquifer.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outDarcyFlow = DarcyFlow ( \"gwhead\" , \"gwporo\" , \"gwthick\" , \"gwtrans\" , \"C:/sapyexamples/output/outdarcydir\" , \"C:/sapyexamples/output/outdarcymag\" ) outDarcyFlow.save ( \"C:/sapyexamples/output/outdarcyflo\" )"}, "usage": ["The differences between ", "Darcy Flow", " and ", "Darcy Velocity", " are:", "All input rasters must have the same extent and cell size.", "All input rasters must be floating point.", "The direction of the velocity vector is recorded in compass coordinates (degrees clockwise from north), the magnitude in units of length over time.", "No particular system of units is specified by this tool. Data should be consistent, using the same unit for time (seconds, days, years) and length (feet, meters) for all data.", "The head elevation raster can come from a variety of sources. It can be interpolated from observation well data by using one of the surface interpolation tools, such as ", "Kriging", " or ", "Spline", ". The head values can also be obtained from the results of a separate modeling program.", "However the head elevation raster is obtained, the head must be consistent with the transmissivity raster; that is, the head must reflect the flow through the transmissivity field. It is not sufficient to use values obtained by measurement and testing in the field\u2014the rasterized values must be analyzed for consistency with the aid of a proper porous medium flow program. Consistency implies that the heads would actually be produced by the modeled transmissivity field. Since the true and modeled transmissivity fields often differ in practice, the true and modeled head fields differ, as well. Check the heads for consistency by examining the residual raster produced by ", "Darcy Flow", ". The residual will reflect the consistency of the dataset. Any analysis using ", "Darcy Velocity", " on inconsistent datasets will produce meaningless results.", "The effective porosity field, a physical property of the aquifer, is generally estimated from geological data. It is defined as the volume of void space that contributes to fluid flow divided by the entire volume. Porosity is expressed as a number between 0 and 1, with typical values around 0.35, and is dimensionless. A value of effective porosity of 0.35 means that 35 percent of the volume of the porous medium contributes to fluid flow. The remaining 65 percent, consisting of solid matrix and unconnected pores, does not contribute to fluid flow.", "The saturated thickness, measured in units of length, is interpreted from geological information. For a confined aquifer, this measure is the thickness of the formation between the upper and lower confining layers. For an unconfined aquifer, the saturated thickness is the distance between the water table and the lower confining layer."], "parameters": [{"name": "in_head_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the groundwater head elevation at that location. The head is typically an elevation above some datum, such as mean sea level. ", "dataType": "Raster Layer"}, {"name": "in_porosity_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the effective formation porosity at that location. ", "dataType": "Raster Layer"}, {"name": "in_thickness_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the saturated thickness at that location. The value for the thickness is interpreted from geological properties of the aquifer. ", "dataType": "Raster Layer"}, {"name": "in_transmissivity_raster", "isInputFile": true, "isOptional": false, "description": "The input raster where each cell value represents the formation transmissivity at that location. The transmissivity of an aquifer is defined as the hydraulic conductivity K times the saturated aquifer thickness b, as units of length squared over time. This property is generally estimated from field experimental data such as pumping tests. Tables 1 and 2 in How Darcy Flow and Darcy Velocity work list ranges of hydraulic conductivities for some generalized geologic materials. ", "dataType": "Raster Layer"}, {"name": "out_direction_raster", "isOutputFile": true, "isOptional": true, "description": "The output flow direction raster. Each cell value represents the direction of the seepage velocity vector (average linear velocity) at the center of the cell, calculated as the average value of the seepage velocity through the four faces of the cell. It is used with the output magnitude raster to describe the flow vector. ", "dataType": "Raster Dataset"}, {"name": "out_magnitude_raster", "isOutputFile": true, "isOptional": true, "description": "An optional output raster where each cell value represents the magnitude of the seepage velocity vector (average linear velocity) at the center of the cell, calculated as the average value of the seepage velocity through the four faces of the cell. It is used with the output direction raster to describe the flow vector. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_volume_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Thin (in_raster, {background_value}, {filter}, {corners}, {maximum_thickness})", "name": "Thin (Spatial Analyst)", "description": "Thins rasterized linear features by reducing the number of cells representing the width of the features.", "example": {"title": "Thin example 1 (Python window)", "description": "This example thins a raster where the background values are NoData, and smooths the boundaries while attempting to preserve corners and junctions.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" thinOut = Thin ( \"land\" , \"NODATA\" , \"FILTER\" , \"SHARP\" , 300 ) thinOut.save ( \"c:/sapyexamples/output/thinout\" )"}, "usage": ["A typical application for the ", "Thin", " tool is for processing a scanned elevation contour map. Because of the resolution of the scanner and the width of the lines on the original map, the contours are respresented in the resulting raster as linear elements from five to ten cells wide.  After running ", "Thin", ", each contour will be represented as a linear feature of a single cell width.", "The ", "FILTER", " option uses the same filtering algorithm as ", "Boundary Clean", " to remove short linear features extending from the major branch. It can also remove features narrower than three cells.", "Specifying the maximum thickness of input linear features is essential for thinning rasters where the thickness of linear features may exceed or stay below the default maximum thickness value. The best results can be expected when the maximum thickness fits the thickest linear features to be thinned."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be thinned. It must be of integer type. ", "dataType": "Raster Layer"}, {"name": "background_value", "isOptional": true, "description": "Specifies the cell value that will identify the background cells. The linear features are formed from the foreground cells. ZERO \u2014 The background is composed of cells of 0 or less, or NoData. All cells whose value is greater than 0 are the foreground. NODATA \u2014 The background is composed of NoData cells. All cells with valid values belong to the foreground.", "dataType": "String"}, {"name": "filter", "isOptional": true, "description": "Specifies whether a filter will be applied as the first phase of thinning. NOFILTER \u2014 No filter will be applied. This is the default. FILTER \u2014 The raster will be filtered to smooth the boundaries between foreground and background cells. This option will eliminate minor irregularities from the output raster.", "dataType": "Boolean"}, {"name": "corners", "isOptional": true, "description": "Specifies whether round or sharp turns will be made at turns or junctions. It is also used during the vector conversion process to spline curves or create sharp intersections and corners. ROUND \u2014 Attempts to smooth corners and junctions. This is best for vectorizing natural features, such as contours or streams. SHARP \u2014 Attempts to preserve rectangular corners and junctions. This is best for vectorizing man-made features such as streets.", "dataType": "String"}, {"name": "maximum_thickness", "isOptional": true, "description": "The maximum thickness, in map units, of linear features in the input raster. The default thickness is ten times the cell size. ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Shrink (in_raster, number_cells, zone_values)", "name": "Shrink (Spatial Analyst)", "description": "Shrinks the selected zones by a specified number of cells by replacing them with the value of the cell that is most frequent in its neighborhood. \r\n  Learn more about how Shrink works", "example": {"title": "Shrink example 1 (Python window)", "description": "This example shrinks the zone specified by a list of values by two cells.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outShrink = Shrink ( \"land\" , 2 , [ 1 , 3 , 7 ]) outShrink.save ( \"c:/sapyexamples/output/shrinkout\" )"}, "usage": ["The specified zone values are considered to be foreground zones, while the remaining zone values are considered to be background zones. With this tool, cells in the foreground zones are allowed to be replaced by cells in the background zones.", "When two adjacent regions are part of the selected set to shrink, there is no change at the boundary between them.", "NoData has the same priority as any valid value to invade areas vacated by shrinking selected values. Therefore, if a selected value is adjacent to NoData, it may become NoData after shrinking."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster for which the identified zones are to be shrunk. It must be of integer type. ", "dataType": "Raster Layer"}, {"name": "number_cells", "isOptional": false, "description": "The number of cells by which to shrink each specified zone. The value must be an integer greater than 0. ", "dataType": "Long"}, {"name": "zone_values", "isOptional": false, "description": "The list of zone values to shrink. The zone values must be integers. They can be in any order. ", "dataType": "Long"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "RegionGroup (in_raster, {number_neighbors}, {zone_connectivity}, {add_link}, {excluded_value})", "name": "Region Group (Spatial Analyst)", "description": "For each cell in the output, the identity of the connected region to which that cell belongs is recorded. A unique number is assigned to each region. Learn more about creating individual zones with Region Group", "example": {"title": "RegionGroup example 1 (Python window)", "description": "This example assigns a unique number to each region of the input raster using eight-way connectivity.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outRgnGrp = RegionGroup ( \"land\" , \"EIGHT\" , \"\" , \"\" , 5 ) outRgnGrp.save ( \"c:/sapyexamples/output/reggrp_ex5\" )"}, "usage": ["The first region scanned  receives the value one, the second two, and so forth, until all regions are assigned a value. The scan moves from left to right, top to bottom. The values assigned to the output zones are based on when they are encountered in the scanning process.", "By default, the ", "Add Link", " option is True. This will create an item called ", "LINK", " in the attribute table of the output raster, which retains the original value for each cell from the input raster.", "The ", "LINK", " field allows you to trace the parentage of each of the newly created regions for queries or analysis.", "For example, the attribute table for the output raster shown in the illustration above is:", "Setting the ", "Add Link", " option to False will significantly speed up processing. This is helpful when the original value of each region is no longer needed.", "If a ", "mask", " has been set in the evironment, those cells that have been masked will receive NoData on the output raster. With a mask, the spatial configuration and the number of regions may be altered on the output raster. If a region was continuous and the imposition of a mask breaks the continuity, the region will be divided into two regions with different values or grouping identifiers on the output raster.", "The mask cannot only create additional regions by dividing a region into two or more separate regions, it can also reduce the number of regions on the output. If a mask totally covers or eliminates a potential region of connected cells, these cells will not be considered as a new zone on the output; they will receive NoData values.", "Region Group", " is especially useful when the analysis is on regions and not on zones. Since the input zone value is maintained, the original zonal class can also be used in the analysis.", "Cell locations that contain the excluded value receive zero on the output so that these zones are not confused with existing NoData cell locations. Since the ", "Region Group", " begins numbering with the value 1, the cells that are excluded from the regroup are considered background. These background cells can be reclassed or manipulated as any other value. The locations containing excluded values can easily be converted to NoData using the ", "Con", " tool."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster whose unique connected regions will be identified. It must be of integer type. ", "dataType": "Raster Layer"}, {"name": "number_neighbors", "isOptional": true, "description": "The number of neighboring cells to use in evaluating connectivity between cells. FOUR \u2014 Defines connectivity between cells of the same value only if the cells are directly to the left, right, above, or below each of the four nearest neighbors. If two cells with the same value are diagonal from one another, they are not considered connected. EIGHT \u2014 Defines connectivity between cells of the same value if they are within the immediate eight-cell neighborhood (eight nearest neighbors) of each other. This includes to the right, left, above, or diagonal to each other.", "dataType": "String"}, {"name": "zone_connectivity", "isOptional": true, "description": "Defines which cell values should be considered when testing for connectivity. WITHIN \u2014 Tests connectivity between input values that are the same within the same zone.The only cells that can be grouped are cells from the same zone (value) that meet the spatial requirements of connectivity specified by the FOUR and EIGHT keywords. CROSS \u2014 Tests connectivity by the spatial requirements specified by the keywords FOUR or EIGHT between cells with any values, except for the value identified to be excluded.When CROSS is used, a value for the {excluded_value} argument must be input.", "dataType": "String"}, {"name": "add_link", "isOptional": true, "description": "Specifies whether a link field is added to the table of the output. ADD_LINK \u2014 An ADD_LINK item will be added to the table of the output raster. This item stores the original values for each newly created zone, from disconnected regions, from the input raster before they are regrouped. This is the default. NO_LINK \u2014 The attribute table for the output raster will only contain the Value and Count items.", "dataType": "Boolean"}, {"name": "excluded_value", "isOptional": true, "description": "Identifies a value such that if a cell location contains the value, no spatial connectivity will be evaluated regardless how the number of neighbors is specified (FOUR or EIGHT). Cells with the excluded value will be treated as NoData and are eliminated from calculations. Cell locations that contain the excluded value will receive 0 on the output raster. The excluded value is similar to the concept of a background value, or setting a mask in the environment for a single run of the tool. A value must be specified for this parameter if the CROSS keyword is specified. ", "dataType": "Long"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Nibble (in_raster, in_mask_raster, {nibble_values})", "name": "Nibble (Spatial Analyst)", "description": "Replaces cells of a raster corresponding to a mask with the values of the nearest neighbors. \r\n  Learn more about how Nibble works", "example": {"title": "Nibble example 1 (Python window)", "description": "This example replaces cells identified by the mask input with values determined by the nearest neighbors of the input raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" nibbleOut = Nibble ( \"land\" , \"snow\" , \"DATA_ONLY\" ) nibbleOut.save ( \"C:/sapyexamples/output/nibbleout\" )"}, "usage": ["Cells in the input raster containing NoData are not nibbled. To nibble NoData, first ", "convert it to another value", "."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that will be nibbled. It must be of integer type. ", "dataType": "Raster Layer"}, {"name": "in_mask_raster", "isInputFile": true, "isOptional": false, "description": "The raster used as the mask. It must be of integer type. Cells with NoData as their value will be nibbled in the in_raster. ", "dataType": "Raster Layer"}, {"name": "nibble_values", "isOptional": true, "description": "Keywords defining if NoData values in the in_raster are allowed to nibble into the area defined by the in_mask_raster. ALL_VALUES \u2014 Specifies that the nearest neighbor value will be used whether it is NoData or another data value in the input raster. NoData values in the input raster are free to nibble into areas defined in the mask if they are the nearest neighbor. DATA_ONLY \u2014 Specifies that only data values are free to nibble into areas defined in the mask raster. NoData values in the input raster are not allowed to nibble into areas defined in the mask raster even if they are the nearest neighbor.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Negate (in_raster_or_constant)", "name": "Negate (Spatial Analyst)", "description": "Changes the sign (multiplies by -1) of the cell values of the input raster on a cell-by-cell basis.", "example": {"title": "Negate example 1 (Python window)", "description": "This example changes the sign of the values in the input raster and outputs a GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outNegate = Negate ( \"degs\" ) outNegate.save ( \"C:/sapyexamples/output/outneg\" )"}, "usage": ["If the input is integer, the output raster will be integer type. If the input is floating point, the output raster will be floating point.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "-", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster to be negated (multiplied by -1). In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Mod (in_raster_or_constant1, in_raster_or_constant2)", "name": "Mod (Spatial Analyst)", "description": "Finds the remainder (modulo) of the first raster when divided by the second raster on a cell-by-cell basis.", "example": {"title": "Mod example 1 (Python window)", "description": "This example returns the value of the remainder (modulus) of dividing the cells in the first raster by the second.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outMod = Mod ( \"degs\" , \"negs\" ) outMod.save ( \"C:/sapyexamples/output/outmod.tif\" )"}, "usage": ["The order of inputs is relevant for this tool.", "Mod", " assumes both its inputs are integers. If any inputs are not integer, those inputs will be converted to integers through truncation. Output values are always integers.", "Any value modulated (divided) by 0 is assigned NoData on the output. Therefore, any location on the second input that is either 0 or NoData will return NoData for that location on the output.", "If the second input value (the divisor) is larger than the first input value (the dividend), the output will be the same value as the first input. For example, if you were to divide a value of 8 by value 10, the integer division calculation will return the input value 8 for the remainder.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"%\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The numerator input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The denominator input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Minus (in_raster_or_constant1, in_raster_or_constant2)", "name": "Minus (Spatial Analyst)", "description": "Subtracts the value of the second input raster from the value of the first input raster on a cell-by-cell basis.", "example": {"title": "Minus example 1 (Python window)", "description": "This example subtracts the values of the second input raster from the first.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outMinus = Minus ( \"degs\" , \"negs\" ) outMinus.save ( \"C:/sapyexamples/output/outminus\" )"}, "usage": ["The order of inputs is relevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "-", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input from which to subtract the values in the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input values to subtract from the values in the first input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Log2 (in_raster_or_constant)", "name": "Log2 (Spatial Analyst)", "description": "Calculates the base 2 logarithm of cells in a raster.", "example": {"title": "Log2 example (Python window)", "description": "This example calculates base 2 logarithm of the input raster values and generates the output as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLog2 = Log2 ( \"elevation\" ) outLog2.save ( \"C:/sapyexamples/output/outlog2.img\" )"}, "usage": ["The input can be of integer or float type. See ", "here", " for some examples of the outputs for floating point input values from the Logarithmic tools.", " Input values that are 0 or negative will be NoData in the output raster.", "The output raster from this tool is always floating point type, regardless of the input value type."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "Input values for which to find the base 2 logarithm. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Log10 (in_raster_or_constant)", "name": "Log10 (Spatial Analyst)", "description": "Calculates the base 10 logarithm of cells in a raster.", "example": {"title": "Log10 example 1 (Python window)", "description": "This example calculates base 10 logarithm of the input raster values and generates the output as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLog10 = Log10 ( \"elevation\" ) outLog10.save ( \"C:/sapyexamples/output/outlog10.img\" )"}, "usage": ["The input can be of integer or float type. See ", "here", " for some examples of the outputs for floating point input values from the Logarithmic tools.", " Input values that are 0 or negative will be NoData in the output raster.", "The output raster from this tool is always floating point type, regardless of the input value type."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "Input values for which to find the base 10 logarithm. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Ln (in_raster_or_constant)", "name": "Ln (Spatial Analyst)", "description": "Calculates the natural logarithm (base e) of cells in a raster.", "example": {"title": "Ln example 1 (Python window)", "description": "This example calculates the natural logarithm (base e) of the input raster values.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLn = Ln ( \"elevation\" ) outLn.save ( \"C:/sapyexamples/output/outln2\" )"}, "usage": ["The input can be of integer or float type. See ", "here", " for some examples of the outputs for floating point input values from the Logarithmic tools.", " Input values that are 0 or negative will be NoData in the output raster.", "The output raster from this tool is always floating point type, regardless of the input value type.", "The natural logarithm (Ln) is the most commonly used logarithmic function."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "Input values for which to find the natural logarithm (Ln). In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Int (in_raster_or_constant)", "name": "Int (Spatial Analyst)", "description": "Converts each cell value of a raster to an integer by truncation.", "example": {"title": "Int example 1 (Python window)", "description": "This example converts the input values to integer by truncation.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outInt = Int ( \"gwhead\" ) outInt.save ( \"C:/sapyexamples/output/outint2\" )"}, "usage": ["The input values can be either positive or negative.", "If rounding is preferred to truncating, add a 0.5 input raster prior to performing the operation.", "There is a difference between the ", "Int", " tool and the ", "Round Down", " tool.   For example, given the following two values ", "Int", " always truncates the number:", "while for the same two values, ", "Round Down", " returns:", "Another difference is that ", "Round Down", " outputs floating-point values, while ", "Int", " only outputs integer values.", "The maximum supported range of integer raster values is  -2,147,483,648 (minimum size determined by -2", "31", ") to 2,147,483,647 (maximum size determined by 2", "31", " \u2013 1).  If ", "Int", " is used on a floating-point raster which has cells with values outside this range, those cells will be NoData in the output raster.", "Storing categorical (discrete) data as an integer raster will use significantly less disk space than the same information stored as a floating-point raster. Whenever possible, it is recommended to convert floating-point rasters to integer with this tool."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster to be converted to integer. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Float (in_raster_or_constant)", "name": "Float (Spatial Analyst)", "description": "Converts each cell value of a raster into a floating-point representation.", "example": {"title": "Float example 1 (Python window)", "description": "This example converts the input raster values to floating point.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outFloat = Float ( \"landuse\" ) outFloat.save ( \"C:/sapyexamples/output/outfloat2\" )"}, "usage": ["The input values can be positive or negative.", "If you execute ", "Float", " on an input that is already floating point, the output values will remain the same as the input values."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster to be converted to floating point. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Exp2 (in_raster_or_constant)", "name": "Exp2 (Spatial Analyst)", "description": "Calculates the base 2 exponential of the cells in a raster.", "example": {"title": "Exp2 example 1 (Python window)", "description": "This example calculates the base 2 exponential of the input raster values, and returns the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outExp2 = Exp2 ( \"degs\" ) outExp2.save ( \"C:/sapyexamples/output/outexp2.img\" )"}, "usage": ["Input values can be integer or float, and can be negative as well as positive.", "See ", "here", " for some examples of the outputs for floating point input values from the Exponential tools.", "Input values less than or equal to -1,075 will be set to NoData in the output, because these values cannot be accurately represented by 32-bit floating-point numbers.", "The output raster from this tool is always floating point type, regardless of the input value type.", "Output values from this tool are always positive."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input values for which to find the base 2 exponential. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Exp10 (in_raster_or_constant)", "name": "Exp10 (Spatial Analyst)", "description": "Calculates the base 10 exponential of the cells in a raster.", "example": {"title": "Exp10 example (Python window)", "description": "This example calculates the base 10 exponential of the input raster values.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outExp10 = Exp10 ( \"degs\" ) outExp10.save ( \"C:/sapyexamples/output/outexp10\" )"}, "usage": ["Input values can be integer or float, and can be negative as well as positive.", "See ", "here", " for some examples of the outputs for floating point input values from the Exponential tools.", "Input values less than or equal to -324 will be set to NoData in the output, because these values cannot be accurately represented by 32-bit floating-point numbers.", "The output raster from this tool is always floating point type, regardless of the input value type.", "Output values from this tool are always positive."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input values for which to find the base 10 exponential. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Exp (in_raster_or_constant)", "name": "Exp (Spatial Analyst)", "description": "Calculates the base e exponential of the cells in a raster.", "example": {"title": "Exp example 1 (Python window)", "description": "This example calculates the base e exponential of the input raster values, and returns the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outExp = Exp ( \"landuse\" ) outExp.save ( \"C:/sapyexamples/output/outexp.tif\" )"}, "usage": ["Input values can be integer or float, and can be negative as well as positive.", "See ", "here", " for some examples of the outputs for floating point input values from the Exponential tools.", "The base e exponential is the most commonly used exponential function.", "Input values less than or equal to -745 will be set to NoData in the output, because these values cannot be accurately represented by 32-bit floating-point numbers.", "The output raster from this tool is always floating point type, regardless of the input value type.", "Output values from this tool are always positive."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input values for which to find the base e exponential. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Divide (in_raster_or_constant1, in_raster_or_constant2)", "name": "Divide (Spatial Analyst)", "description": "Divides the values of two rasters on a cell-by-cell basis.", "example": {"title": "Divide example 1 (Python window)", "description": "This example divides the values of the first input raster by the second.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outDivide = Divide ( \"degs\" , \"negs\" ) outDivide.save ( \"C:/sapyexamples/output/outdivide2\" )"}, "usage": ["The order of inputs is relevant for this tool.", "When a number is divided by zero, the output result is NoData.", "The data types of the inputs to determine the data type of the output:", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "/", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input whose values will be divided by the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input whose values the first input are to be divided by. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Abs (in_raster_or_constant)", "name": "Abs (Spatial Analyst)", "description": "Calculates the absolute value of the cells in a raster.", "example": {"title": "Abs example 1 (Python window)", "description": "This example returns a GRID raster with the absolute value of the input values.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outAbs = Abs ( \"negs\" ) outAbs.save ( \"C:/sapyexamples/output/abs2\" )"}, "usage": ["Input values can be positive or negative and can be either integer or floating point.", "If the input is integer, the output raster will be integer type. If the input is floating point, the output raster will be floating point."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster for which to calculate the absolute values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Rank (in_rank_raster_or_constant, in_rasters)", "name": "Rank (Spatial Analyst)", "description": "The values from the set of input rasters are ranked on a cell-by-cell basis, and which of these gets returned is determined by the value of the rank input raster.", "example": {"title": "Rank example 1 (Python window)", "description": "This example performs a rank operation on several input Grid rasters and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outRank = Rank ( \"cost\" , [ \"degs\" , \"negs\" , \"fourgrd\" ]) outRank.save ( \"C:/sapyexamples/output/outrank.tif\" )"}, "usage": ["In the list of input rasters, the order is irrelevant. However, the Rank input raster must precede these.", "An arbitrary number of rasters can be specified in the input rasters list.", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "If all of the input values are the same for any cell location, regardless of the specified rank, the output for that cell location will be that value.", "If the rank raster value is greater than the number of input rasters, each cell location on the output will be assigned NoData.", "If any of the input rasters are floating point, the output is floating point; otherwise, it is integer."], "parameters": [{"name": "in_rank_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster that defines the rank position to be returned. A number can be used as an input; however, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": "The list of input rasters. The input defines the argument list to identify the value for the rank, defined by the first argument for each cell location. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Popularity (in_popularity_raster_or_constant, in_rasters)", "name": "Popularity (Spatial Analyst)", "description": "Determines the value in an argument list that is at a certain level of popularity on a cell-by-cell basis. The particular level of popularity (the number of occurrences of each value) is specified by the first argument.", "example": {"title": "Popularity example 1 (Python window)", "description": "This example performs a popularity operation on several input rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPopularity = Popularity ( \"cost\" , [ \"degs\" , \"negs\" , \"fourgrd\" ]) outPopularity.save ( \"C:/sapyexamples/output/outpop.img\" )"}, "usage": ["The tool evaluates the number of occurrences of the input raster values for each location and ranks them on an ordinal scale\u2014that is, the most popular, second most popular, and so on. It will return the value of the specified nth most popular value defined by the popularity raster value.", "In the list of input rasters, the order is irrelevant. However, the raster that defines the popularity position must precede these.", "An arbitrary number of rasters can be specified in the input rasters list.", "If the input values are the same for any cell location, regardless of the specified popularity, the output value will be the same as the input for that cell location.", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "If there is no single value found to be the n", "th", " most popular, NoData will be assigned to the location on the output raster. This situation occurs when all input raster values at a location are different or when two or more input raster values have the same number of occurrences and that number is the nth most popular. Returning one of the input raster values, such as the first one encountered in the scan process, would be deceptive. You would not know whether the value is truly the n", "th", " most popular value.", "If the popularity value is greater than the number of input rasters, each cell location on the output will be assigned NoData.", "If 0 is specified as the popularity value, the output value will be NoData.", "A popularity level of 1 is the majority value, similar to the Majority option of ", "Cell Statistics", ".", "If any of the input rasters are floating point, the output is floating point; otherwise, it is integer."], "parameters": [{"name": "in_popularity_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster defining the popularity position to be returned. A number can be used as an input; however, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": "The list of input rasters used to evaluate the popularity of the values for each cell location. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "LowestPosition (in_rasters_or_constants)", "name": "Lowest Position (Spatial Analyst)", "description": "Determines on a cell-by-cell basis the position of the raster with the minimum value in a set of rasters.", "example": {"title": "LowestPosition example 1 (Python window)", "description": "This example evaluates several input rasters and returns as an output value the position in the list of the raster with the minimum value.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLowestPosition = LowestPosition ([ \"degs\" , \"negs\" , \"fourgrd\" ]) outLowestPosition.save ( \"C:/sapyexamples/output/outlp.tif\" )"}, "usage": ["An arbitrary number of rasters can be specified in the input rasters list.", "The order of the input rasters is relevant for this tool.", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "The output raster is always of integer type.", "If two or more input rasters contain the minimum value for a particular cell location, the position of the first one encountered is returned on the output raster."], "parameters": [{"name": "in_rasters_or_constants", "isInputFile": true, "isOptional": false, "description": "The list of input rasters for which the position of the input with the lowest value will be determined. A number can be used as an input; however, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "LessThanFrequency (in_value_raster, in_rasters)", "name": "Less Than Frequency (Spatial Analyst)", "description": "Evaluates on a cell-by-cell basis the number of times a set of rasters is less than another raster.", "example": {"title": "LessThanFrequency example 1 (Python window)", "description": "This example evaluates the number of times a set of input Grid rasters is less than another raster and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLTF = LessThanFrequency ( \"cost\" , [ \"degs\" , \"negs\" , \"fourgrd\" ]) outLTF.save ( \"C:/sapyexamples/output/outltf.img\" )"}, "usage": ["An arbitrary number of rasters can be specified in the input rasters list.", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "The output raster is always of integer type."], "parameters": [{"name": "in_value_raster", "isInputFile": true, "isOptional": false, "description": "For each cell location in the input value raster, the number of occurrences (frequency) where a raster in the input list has a lesser value is counted. ", "dataType": "Raster Layer"}, {"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": " The list of rasters that will be compared against the value raster. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "HighestPosition (in_rasters_or_constants)", "name": "Highest Position (Spatial Analyst)", "description": "Determines on a cell-by-cell basis the position of the raster with the maximum value in a set of rasters.", "example": {"title": "HighestPosition example 1 (Python window)", "description": "This example evaluates several input rasters and returns as an output value the position in the list of the raster with the maximum value.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outHighestPosition = HighestPosition ([ \"degs\" , \"negs\" , \"fourgrd\" ]) outHighestPosition.save ( \"C:/sapyexamples/output/outhp.img\" )"}, "usage": ["An arbitrary number of rasters can be specified in the input rasters list.", "The order of the input rasters is relevant for this tool.", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "The output raster is always of integer type.", "If two or more input rasters contain the maximum value for a particular cell location, the position of the first one is returned on the output raster."], "parameters": [{"name": "in_rasters_or_constants", "isInputFile": true, "isOptional": false, "description": "The list of input rasters for which the position of the input with the highest value will be determined. A number can be used as an input; however, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "GreaterThanFrequency (in_value_raster, in_rasters)", "name": "Greater Than Frequency (Spatial Analyst)", "description": "Evaluates on a cell-by-cell basis the number of times a set of rasters is greater than another raster.", "example": {"title": "GreaterThanFrequency example 1 (Python window)", "description": "This example evaluates the number of times a set of input Grid rasters is greater than another raster and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outGTF = GreaterThanFrequency ( \"cost\" , [ \"degs\" , \"negs\" , \"fourgrd\" ]) outGTF.save ( \"C:/sapyexamples/output/outgtf.tif\" )"}, "usage": ["An arbitrary number of rasters can be specified in the input rasters list.", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "The output raster is always of integer type."], "parameters": [{"name": "in_value_raster", "isInputFile": true, "isOptional": false, "description": "For each cell location in the input value raster, the number of occurrences (frequency) where a raster in the input list has a greater value is counted. ", "dataType": "Raster Layer"}, {"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": " The list of rasters that will be compared against the value raster. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "EqualToFrequency (in_value_raster, in_rasters)", "name": "Equal To Frequency (Spatial Analyst)", "description": "Evaluates on a cell-by-cell basis the number of times the values in a set of rasters are equal to another raster.", "example": {"title": "EqualToFrequency example 1 (Python window)", "description": "This example evaluates the number of times a set of input rasters is equal to another raster and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outETF = EqualToFrequency ( \"cost\" , [ \"degs\" , \"negs\" , \"fourgrd\" ]) outETF.save ( \"C:/sapyexamples/output/outetf.tif\" )"}, "usage": ["An arbitrary number of rasters can be specified in the input rasters list.", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "The output raster is always of integer type."], "parameters": [{"name": "in_value_raster", "isInputFile": true, "isOptional": false, "description": "For each cell location in the input value raster, the number of occurrences (frequency) where a raster in the input list has an equal value is counted. ", "dataType": "Raster Layer"}, {"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": " The list of rasters that will be compared against the value raster. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Combine (in_rasters)", "name": "Combine (Spatial Analyst)", "description": "Combines multiple rasters so that a unique output value is assigned to each unique combination of input values.", "example": {"title": "Combine example 1 (Python window)", "description": "This example takes several input rasters in different formats (Grid, IMG, and TIFF) and outputs the unique combination values as a Grid raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCombine = Combine ([ \"filter\" , \"zone\" , \"source.img\" , \"dec.tif\" ]) outCombine.save ( \"C:/sapyexamples/output/outcombine2\" )"}, "usage": ["Combine", " works on integer values and their associated attribute tables. If the values on the input are floating point, they will be automatically truncated, tested for uniqueness with the other input, and sent to the output attribute table.", "Combine", " is similar to the ", "Combinatorial Or", " tool. They both assign a new number to each unique combination of input values.", "No more than 20 rasters can be used as input to ", "Combine", ".", "If a cell location contains NoData on any of the input rasters, that location will be assigned NoData on the output.", "The output raster is always of integer type.", "For formats other than ESRI GRID, by default, the output raster from this tool can only have a maximum of 65536 unique values.", "You can increase this number by changing a setting in ArcGIS.  On the Main menu, select ", "Customize ", ">", " ArcMap Options", ".  In the ", "ArcMap Options", " dialog box, click on the ", "Raster", " tab and modify the Maximum number of unique values to render field to an appropriate value."], "parameters": [{"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": "The list of input rasters to be combined. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CellStatistics (in_rasters_or_constants, {statistics_type}, {ignore_nodata})", "name": "Cell Statistics (Spatial Analyst)", "description": "Calculates a per-cell statistic from multiple rasters. The available statistics are Majority, Maximum, Mean, Median, Minimum, Minority, Range, Standard Deviation, Sum, and Variety. \r\n Learn more about how Cell Statistics works", "example": {"title": "CellStatistics example 1 (Python window)", "description": "This example calculates the standard deviation per cell on several input Grid rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCellStats = CellStatistics ([ \"degs\" , \"negs\" , \"cost\" ], \"STD\" , \"DATA\" ) outCellStats.save ( \"C:/sapyexamples/output/outcellstats.img\" )"}, "usage": ["The order of the  input rasters is irrelevant for this tool.", "For statistic types Maximum, Minimum, Mean, Median, Majority, Minority and Sum, if a single raster is used as the input, the output cell values will be the same as the input cell values. For Range and STD the output cell values will  all be 0, and for Variety, all 1."], "parameters": [{"name": "in_rasters_or_constants", "isInputFile": true, "isOptional": false, "description": "A list of input rasters for which a statistic will be calculated for each cell within the Analysis window. A number can be used as an input; however, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "statistics_type", "isOptional": true, "description": "Statistic type to be calculated. MEAN \u2014 Calculates the mean (average) of the inputs. MAJORITY \u2014 Determines the majority (value that occurs most often) of the inputs. MAXIMUM \u2014 Determines the maximum (largest value) of the inputs. MEDIAN \u2014 Calculates the median of the inputs. MINIMUM \u2014 Determines the minimum (smallest value) of the inputs. MINORITY \u2014 Determines the minority (value that occurs least often) of the inputs. RANGE \u2014 Calculates the range (difference between largest and smallest value) of the inputs. STD \u2014 Calculates the standard deviation of the inputs. SUM \u2014 Calculates the sum (total of all values) of the inputs. VARIETY \u2014 Calculates the variety (number of unique values) of the inputs.", "dataType": "String"}, {"name": "ignore_nodata", "isOptional": true, "description": "Denotes whether NoData values are ignored by the statistic calculation. DATA \u2014 Only cells that have data values will be used in determining the statistic value.If a NoData value exists at a certain location, the NoData value will be ignored. Only cells that have data values will be used in determining the output. NODATA \u2014 All input cells at each location, including those with a value of NoData, will be used in determining the statistic.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Trend (in_point_features, z_field, {cell_size}, {order}, {regression_type}, {out_rms_file})", "name": "Trend (Spatial Analyst)", "description": "Interpolates a raster surface from points using a trend technique. \r\n Learn more about how Trend works", "example": {"title": "Trend example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outTrend = Trend ( \"ozone_pts.shp\" , \"ozone\" , 2000 , 2 , \"LINEAR\" ) outTrend.save ( \"C:/sapyexamples/output/trendout.tif\" )"}, "usage": ["As the order of the polynomial is increased, the surface being fitted becomes progressively more complex. A higher-order polynomial will not always generate the most accurate surface; it is dependent on the data.", "The optional RMS file output contains information on the RMS (root mean square) error of the interpolation. This information can be used to determine the best value to use for the polynomial order, by changing the order value until you get the lowest RMS error. See ", "How Trend works", " for information on the RMS file.", "For the LOGISTIC option of ", "Type of regression", ", the z-value field of input point features should have codes of zero (0) and one (1).", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. If the regression type is Logistic, the values in the field can only be 0 or 1. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "order", "isOptional": true, "description": "The order of the polynomial. This must be an integer between 1 and 12. A value of 1 will fit a flat plane to the points, and a higher value will fit a more complex surface. The default is 1. ", "dataType": "Long"}, {"name": "regression_type", "isOptional": true, "description": "The type of regression to be performed. LINEAR \u2014 Polynomial regression is performed to fit a least-squares surface to the set of input points. This is applicable for continuous types of data. LOGISTIC \u2014 Logistic trend surface analysis is performed. It generates a continuous probability surface for binary, or dichotomous, types of data.", "dataType": "String"}, {"name": "out_rms_file", "isOutputFile": true, "isOptional": true, "description": "File name for the output text file that contains information about the RMS error and the Chi-Square of the interpolation. The extension must be \" .txt \". ", "dataType": "File"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "TopoToRasterByFile (in_parameter_file, {out_stream_features}, {out_sink_features}, {out_residual_feature}, {out_stream_cliff_error_feature}, {out_contour_error_feature})", "name": "Topo to Raster by File (Spatial Analyst)", "description": "Interpolates a hydrologically correct raster surface from point,\r\nline, and polygon data using parameters specified in a file. \r\n Learn more about how Topo to Raster works", "example": {"title": "TopoToRasterByFile example 1 (Python window)", "description": "This example creates a hydrologically correct TIFF surface raster from a parameter file defining the input point, line, and polygon data.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outTTRByFile = TopoToRasterByFile ( \"topotorasterbyfile.txt\" , \"C:/sapyexamples/output/out_streams.shp\" , \"#\" , \"C:/sapyexamples/output/out_resids.shp\" ) outTTRByFile.save ( \"C:/sapyexamples/output/ttrbyfout.tif\" )"}, "usage": ["The parameter file is structured with the input datasets listed first, followed by the various parameter settings, then the output options.", "The input data identifies the input datasets and, where applicable, fields. There are nine types of input: Contours, Points, Sinks, Streams, Lakes, Boundaries, Cliffs, Exclusion, and Coastal polygons. You can use as many inputs as you want, within reason. The order in which the inputs are entered does not have any bearing on the outcome. ", "<Path>", " indicates a path to a dataset, ", "<Item>", " indicates a field name, and ", "<#>", " indicates a value to be entered.", "The following table lists all of the parameters, the definition of each, and their syntax.", "Parameter", "Definition", "Syntax", "Input datasets:", "Contours", "Contour line dataset with item containing height values.", "Points", "Point dataset with item containing height values.", "Sinks", "Point dataset containing sink locations. If the dataset has elevation values for the sinks, specify that field name as the ", "<Item>", ". If only the locations of the sinks are to be used, use ", "NONE", " for ", "<Item>", ".", "Streams", "Stream line dataset. Height values are not necessary.", "Lakes", "Lake polygon dataset. Height values are not necessary.", "Boundary", "Boundary polygon dataset. Height values are not necessary.", "Cliff", "Line dataset of the cliffs. There is no Field option for Cliff.", "Exclusion", "Exclusion polygon dataset of the areas in which the input data should be ignored. There is no Field option for Exclusion.", "Coast", "Coast polygon dataset containing the outline of a coastal area. There is no Field option for Coast. ", "Parameter settings:", "Enforce", "Controls whether drainage enforcement is applied.", "Datatype", "Primary type of input data.", "Iterations", "The maximum number of iterations the algorithm performs.", "Roughness penalty", "The measure of surface roughness.", "Profile curvature roughness penalty", "The profile curvature roughness penalty is a locally adaptive penalty that can be use to partly replace total curvature.", "Discretisation error factor", "The amount to adjust the data smoothing of the input data into a raster.", "Vertical standard error", "The amount of random error in the z-values of the input data.", "Tolerances", "The first reflects the accuracy of elevation data in relation to surface drainage, and the other prevents drainage clearance through unrealistically high barriers.", "Z-Limits", "Lower and upper height limits.", "Extent", "Minimum x, minimum y, maximum x, and maximum y coordinate limits.", "Cell size", "The resolution of the final output raster.", "Margin", "Distance in cells to interpolate beyond the specified output extent and boundary.", "Outputs:", "Output stream features", "The output line feature class of stream polyline features and ridge line features.", "Output sink features", "The output point feature class of the remaining sink point features.", "Output diagnostics file", "The location and name of the diagnostics file.", "Output residual point features", "The output point feature class of all the large elevation residuals as scaled by the local discretisation error.", "Output stream and cliff point features", "The output point feature class of locations where possible stream and cliff errors occur.", "Output contour  error point features", "The output point feature class of possible errors pertaining to the input contour data.", "Do not specify paths for the optional output feature datasets in the parameter file. Use the ", "Output stream polyline features", " and ", "Output remaining sink point features", " in the tool dialog box to identify these outputs.", "The contents of an example parameter file are:"], "parameters": [{"name": "in_parameter_file", "isInputFile": true, "isOptional": false, "description": "The input ASCII text file containing the inputs and parameters to use for the interpolation. The file is typically created from a previous run of Topo to Raster with the optional output parameter file specified. In order to test the outcome of changing the parameters, it is easier to make edits to this file and rerun the interpolation than to correctly issue the Topo to Raster tool each time. ", "dataType": "File"}, {"name": "out_stream_features", "isOutputFile": true, "isOptional": true, "description": "Output feature class of stream polyline features. The polyline features are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_sink_features", "isOutputFile": true, "isOptional": true, "description": "Output feature class of remaining sink point features. ", "dataType": "Feature Class"}, {"name": "out_residual_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of all the large elevation residuals as scaled by the local discretisation error. All the scaled residuals larger than 10 should be inspected for possible errors in input elevation and stream data. Large-scaled residuals indicate conflicts between input elevation data and streamline data. These may also be associated with poor automatic drainage enforcements. These conflicts can be remedied by providing additional streamline and/or point elevation data after first checking and correcting errors in existing input data. Large unscaled residuals usually indicate input elevation errors. ", "dataType": "Feature Class"}, {"name": "out_stream_cliff_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of locations where possible stream and cliff errors occur. The locations where the streams have closed loops, distributaries and streams over cliffs can be identified from the point feature class. Cliffs with neighboring cells that are inconsistent with the high and low sides of the cliff are also indicated. This can be a good indicator of cliffs with incorrect direction. Points are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_contour_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of possible errors pertaining to the input contour data. Contours with bias in height exceeding five times the standard deviation of the contour values as represented on the output raster are reported to this feature class. Contours that join other contours with a different elevation are flagged in this feature class by the code 1; this is a sure sign of a contour label error. ", "dataType": "Feature Class"}, {"isOutputFile": true, "name": "out_surface_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "TopoToRaster (in_topo_features, {cell_size}, {extent}, {Margin}, {minimum_z_value}, {maximum_z_value}, {enforce}, {data_type}, {maximum_iterations}, {roughness_penalty}, {discrete_error_factor}, {vertical_standard_error}, {tolerance_1}, {tolerance_2}, {out_stream_features}, {out_sink_features}, {out_diagnostic_file}, {out_parameter_file}, {profile_penalty}, {out_residual_feature}, {out_stream_cliff_error_feature}, {out_contour_error_feature})", "name": "Topo to Raster (Spatial Analyst)", "description": "Interpolates a hydrologically correct raster surface from point,\r\nline, and polygon data. \r\n Learn more about how Topo to Raster works", "example": {"title": "TopoToRaster example 1 (Python window)", "description": "This example creates a hydrologically correct TIFF surface raster from point, line, and polygon data.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outTTR = TopoToRaster ([ TopoPointElevation ([[ 'spots' , 'spot_meter' ]]), TopoContour ([[ 'contours' , 'spot_meter' ]]), TopoCliff ([ 'cliff' ])], 60 , \"#\" , \"#\" , \"#\" , \"#\" , \"NO_ENFORCE\" ) outTTR.save ( \"C:/sapyexamples/output/ttrout.tif\" )"}, "usage": ["The best results will be obtained if all input data is stored in the same planar coordinate system and has the same ZUnits. Unprojected data (latitude-longitude) can be used; however, the results may not be as accurate, particularly at high latitudes.", "Topo to Raster", " will only use four input data points for the interpolation of each output cell. All additional points are ignored. If too many points are encountered by the algorithm, an error may occur, indicating the point dataset has too many points. The maximum number of points that can be used is ", "NRows * NCols", ", where ", "NRows", " is the number of rows in the output raster and ", "NCols", " is the number of columns.", "When the input feature type is CONTOUR, the algorithm first generates a generalized morphology of the surface based on the curvature of the contours. The algorithm then implements the contours as a source of elevation information. Contours are best suited for large-scale data where the contours and corners are reliable indicators of streams and ridges. At smaller scales it can be just as effective, and less expensive, to digitize corner points of contours and use them as an input point feature class.", "Representing braided streams or using arcs to represent two sides of a stream may not produce reliable results. Stream data always takes priority over point or contour data; therefore, elevation data points that conflict with descent down each stream are ignored. Stream data is a powerful way of adding topographic information to the interpolation, further ensuring the quality of the output DEM.", "Typical values for the ", "Tolerance 1", " and ", "Tolerance 2", " settings are:", "To make experimentation with the inputs and parameters easier, use the ", "Topo to Raster", " dialog box to create an output parameter file, which can be modified in any text editor and used as input to the ", "Topo to Raster by File", " tool.", "Topo to Raster is a memory-intensive application and it is therefore not possible to create large output rasters. When large output is required, use the MARGIN parameter."], "parameters": [{"name": "in_topo_features", "isInputFile": true, "isOptional": false, "description": "The Topo class specifies the input features containing the z-values to be interpolated into a surface raster. There are nine types of data accepted inputs to the Topo class: TopoPointElevation , TopoContour , TopoStream , TopoSink , TopoBoundary , TopoLake , TopoCliff , TopoExclusion , TopoCoast . A point feature class representing surface elevations. The field stores the elevations of the points. A line feature class that represents elevation contours. The field stores the elevations of the contour lines. A line feature class of stream locations. All arcs must be oriented to point downstream. The feature class should only contain single arc streams. A point feature class that represents known topographic depressions. Topo to Raster will not attempt to remove from the analysis any points explicitly identified as sinks. The field used should be one that stores the elevation of the legitimate sink. If NONE is selected, only the location of the sink is used. A boundary is a feature class containing a single polygon that represents the outer boundary of the output raster. Cells in the output raster outside this boundary will be NoData. This option can be used for clipping out water areas along coastlines before making the final output raster. A polygon feature class that specifies the location of lakes. All output raster cells within a lake will be assigned to the minimum elevation value of all cells along the shoreline. A line feature class of the cliffs. The cliff line features must be oriented so that the left-hand side of the line is on the low side of the cliff and the right-hand side is the high side of the cliff. A polygon feature class of the areas in which the input data should be ignored. These polygons permit removal of elevation data from the interpolation process. This is typically used to remove elevation data associated with dam walls and bridges. This enables interpolation of the underlying valley with connected drainage structure. A polygon feature class containing the outline of a coastal area. Cells in the final output raster that lie outside these polygons are set to a value that is less than the user-specified minimum height limit. The PointElevation, Contour, and Sink types of feature input can have a field specified that contains the z-values. There is no Field option for Boundary, Lake, Cliff, Coast, Exclusion or Stream input types. TopoPointElevation ([[inFeatures,{field}],...]) A point feature class representing surface elevations. The field stores the elevations of the points. TopoContour ([[inFeatures,{field}],...]) A line feature class that represents elevation contours. The field stores the elevations of the contour lines. TopoStream ([inFeatures,...]) A line feature class of stream locations. All arcs must be oriented to point downstream. The feature class should only contain single arc streams. TopoSink ([[inFeatures,{field}],...]) A point feature class that represents known topographic depressions. Topo to Raster will not attempt to remove from the analysis any points explicitly identified as sinks. The field used should be one that stores the elevation of the legitimate sink. If NONE is selected, only the location of the sink is used. TopoBoundary ([inFeatures,...]) A boundary is a feature class containing a single polygon that represents the outer boundary of the output raster. Cells in the output raster outside this boundary will be NoData. This option can be used for clipping out water areas along coastlines before making the final output raster. TopoLake ([inFeatures,...]) A polygon feature class that specifies the location of lakes. All output raster cells within a lake will be assigned to the minimum elevation value of all cells along the shoreline. TopoCliff ([inFeatures,...]) A line feature class of the cliffs. The cliff line features must be oriented so that the left-hand side of the line is on the low side of the cliff and the right-hand side is the high side of the cliff. TopoExclusion ([inFeatures,...]) A polygon feature class of the areas in which the input data should be ignored. These polygons permit removal of elevation data from the interpolation process. This is typically used to remove elevation data associated with dam walls and bridges. This enables interpolation of the underlying valley with connected drainage structure. TopoCoast ([inFeatures,...]) A polygon feature class containing the outline of a coastal area. Cells in the final output raster that lie outside these polygons are set to a value that is less than the user-specified minimum height limit. ", "dataType": "TopoInput"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "extent", "isOptional": true, "description": "The Extent class determines the extent for the output raster dataset. Interpolation will occur out to the x and y limits, and cells outside that extent will be NoData. For best interpolation results along the edges of the output raster, the x and y limits should be smaller than the extent of the input data by at least 10 cells on each side. The form of the Extent class is: where: The default extent is the smallest of all extents of the input feature data. Extent (XMin, YMin, XMax, YMax) where: XMin \u2014The default is the smallest x coordinate of all inputs. YMin \u2014The default is the smallest y coordinate of all inputs. XMax \u2014The default is the largest x coordinate of all inputs. YMax \u2014The default is the largest y coordinate of all inputs. ", "dataType": "Extent"}, {"name": "Margin", "isOptional": false, "description": "Distance in cells to interpolate beyond the specified output extent and boundary. The value must be greater than or equal to 0 (zero). The default value is 20. If the Extent and TopoBoundary feature dataset are the same as the limit of the input data (the default), values interpolated along the edge of the DEM will not match well with adjacent DEM data. This is because they have been interpolated using one-half as much data as the points inside the raster, which are surrounded on all sides by input data. The Margin option allows input data beyond these limits to be used in the interpolation. ", "dataType": "Long"}, {"name": "minimum_z_value", "isOptional": true, "description": "The minimum z-value to be used in the interpolation. The default is 20 percent below the smallest of all the input values. ", "dataType": "Double"}, {"name": "maximum_z_value", "isOptional": true, "description": "The maximum z-value to be used in the interpolation. The default is 20 percent above the largest of all input values. ", "dataType": "Double"}, {"name": "enforce", "isOptional": true, "description": "The type of drainage enforcement to apply. The drainage enforcement option can be set to attempt to remove all sinks or depressions so a hydrologically correct DEM can be created. If sink points have been explicitly identified in the input feature data, these depressions will not be filled. ENFORCE \u2014 The algorithm will attempt to remove all sinks it encounters, whether they are real or spurious. This is the default. NO_ENFORCE \u2014 No sinks will be filled. ENFORCE_WITH_SINK \u2014 Points identified as sinks in Input feature data represent known topographic depressions and will not be altered. Any sink not identified in input feature data is considered spurious, and the algorithm will attempt to fill it.Having more than 8,000 spurious sinks causes the tool to fail.", "dataType": "String"}, {"name": "data_type", "isOptional": true, "description": "The dominant elevation data type of the input feature data. Specifying the relevant selection optimizes the search method used during the generation of streams and ridges. CONTOUR \u2014 The dominant type of input data will be elevation contours. This is the default. SPOT \u2014 The dominant type of input will be point.", "dataType": "String"}, {"name": "maximum_iterations", "isOptional": true, "description": "The maximum number of interpolation iterations. The number of iterations must be greater than zero. A default of 20 is normally adequate for both contour and line data. A value of 30 will clear fewer sinks. Rarely, higher values (45\u201350) may be useful to clear more sinks or to set more ridges and streams. Iteration ceases for each grid resolution when the maximum number of iterations has been reached. ", "dataType": "Long"}, {"name": "roughness_penalty", "isOptional": true, "description": "The integrated squared second derivative as a measure of roughness. The roughness penalty must be zero or greater. If the primary input data type is CONTOUR, the default is zero. If the primary data type is SPOT, the default is 0.5. Larger values are not normally recommended. ", "dataType": "Double"}, {"name": "discrete_error_factor", "isOptional": true, "description": "The discrete error factor is used to adjust the amount of smoothing when converting the input data to a raster. The value must be greater than zero. The normal range of adjustment is 0.5 to 2, and the default is 1. A smaller value results in less data smoothing; a larger value causes greater smoothing. ", "dataType": "Double"}, {"name": "vertical_standard_error", "isOptional": true, "description": "The amount of random error in the z-values of the input data. The value must be zero or greater. The default is zero. The vertical standard error may be set to a small positive value if the data has significant random (non-systematic) vertical errors with uniform variance. In this case, set the vertical standard error to the standard deviation of these errors. For most elevation datasets, the vertical error should be set to zero, but it may be set to a small positive value to stabilize convergence when rasterizing point data with stream line data. ", "dataType": "Double"}, {"name": "tolerance_1", "isOptional": true, "description": "This tolerance reflects the accuracy and density of the elevation points in relation to surface drainage. For point datasets, set the tolerance to the standard error of the data heights. For contour datasets, use one-half the average contour interval. The value must be zero or greater. The default is 2.5 if the data type is CONTOUR and zero if the data type is SPOT. ", "dataType": "Double"}, {"name": "tolerance_2", "isOptional": true, "description": "This tolerance prevents drainage clearance through unrealistically high barriers. The value must be greater than zero. The default is 100 if the data type is CONTOUR and 200 if the data type is SPOT. ", "dataType": "Double"}, {"name": "out_stream_features", "isOutputFile": true, "isOptional": true, "description": "The output line feature class of stream polyline features and ridge line features. The line features are created at the beginning of the interpolation process. It provides the general morphology of the surface for interpolation. It can be used to verify correct drainage and morphology by comparing known stream and ridge data. The polyline features are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_sink_features", "isOutputFile": true, "isOptional": true, "description": "The output point feature class of the remaining sink point features. These are the sinks that were not specified in the sink input feature data and were not cleared during drainage enforcement. Adjusting the values of the tolerances, tolerance_1 and tolerance_2 , can reduce the number of remaining sinks. Remaining sinks often indicate errors in the input data that the drainage enforcement algorithm could not resolve. This can be an efficient way of detecting subtle elevation errors. ", "dataType": "Feature Class"}, {"name": "out_diagnostic_file", "isOutputFile": true, "isOptional": true, "description": "The output diagnostic file listing all inputs and parameters used and the number of sinks cleared at each resolution and iteration. ", "dataType": "File"}, {"name": "out_parameter_file", "isOutputFile": true, "isOptional": true, "description": "The output parameter file listing all inputs and parameters used, which can be used with Topo to Raster by File to run the interpolation again. ", "dataType": "File"}, {"name": "profile_penalty", "isOptional": true, "description": "The profile curvature roughness penalty is a locally adaptive penalty that can be used to partly replace total curvature. It can yield good results with high-quality contour data but can lead to instability in convergence with poor data. Set to 0.0 for no profile curvature (the default), set to 0.5 for moderate profile curvature, and set to 0.8 for maximum profile curvature. Values larger than 0.8 are not recommended and should not be used. ", "dataType": "Double"}, {"name": "out_residual_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of all the large elevation residuals as scaled by the local discretisation error. All the scaled residuals larger than 10 should be inspected for possible errors in input elevation and stream data. Large-scaled residuals indicate conflicts between input elevation data and streamline data. These may also be associated with poor automatic drainage enforcements. These conflicts can be remedied by providing additional streamline and/or point elevation data after first checking and correcting errors in existing input data. Large unscaled residuals usually indicate input elevation errors. ", "dataType": "Feature Class"}, {"name": "out_stream_cliff_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of locations where possible stream and cliff errors occur. The locations where the streams have closed loops, distributaries and streams over cliffs can be identified from the point feature class. Cliffs with neighboring cells that are inconsistent with the high and low sides of the cliff are also indicated. This can be a good indicator of cliffs with incorrect direction. Points are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_contour_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of possible errors pertaining to the input contour data. Contours with bias in height exceeding five times the standard deviation of the contour values as represented on the output raster are reported to this feature class. Contours that join other contours with a different elevation are flagged in this feature class by the code 1; this is a sure sign of a contour label error. ", "dataType": "Feature Class"}, {"isOutputFile": true, "name": "out_surface_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "SplineWithBarriers (Input_point_features, Z_value_field, {Input_barrier_features}, {Output_cell_size}, {Smoothing_Factor})", "name": "Spline with Barriers (Spatial Analyst)", "description": "Interpolates a raster surface, using barriers, from points using a\r\nminimum curvature spline technique. The barriers are entered as\r\neither polygon or polyline features. \r\n Learn more about how Spline with Barriers works", "example": {"title": "SplineWithBarriers example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSplineBarriers = SplineWithBarriers ( \"ca_ozone_pts.shp\" , \"ozone\" , \"ca_ozone_barrier.shp\" , 2000 ) outSplineBarriers.save ( \"C:/sapyexamples/output/splinebarrierout.tif\" )"}, "usage": ["This tool requires the Java runtime environment Version 6, or higher, to be installed. The Java Runtime Environment can be downloaded for free from ", "http://www.java.com/en/download", ".", "If the tool fails to run with an error message stating that a \"more recent version of Java needs to be installed\", and you have multiple versions of Java installed, you need to update the PATH environment variable.", "The resulting smooth surface from is constrained by the input barrier features.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data.", "For the ", "Spline with Barriers", " tool, by default  the values for each set of coincident points will be averaged.", "If a cell size of 0 is entered, the cell size actually used will be the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250.", "The barrier features are rasterized and the cell center is used to decide whether the cell falls within a polygon or whether the cell becomes a barrier for polyline features."], "parameters": [{"name": "Input_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "Z_value_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "Input_barrier_features", "isInputFile": true, "isOptional": true, "description": "The optional input barrier features to constrain the interpolation. ", "dataType": "Feature Layer"}, {"name": "Output_cell_size", "isOutputFile": true, "isOptional": false, "description": "The cell size at which the output raster will be created. If a value of 0 is entered, the shorter of the width or the height of the extent of the input point features in the input spatial reference, divided by 250, will be used as the cell size. ", "dataType": "Analysis Cell Size"}, {"name": "Smoothing_Factor", "isOptional": true, "description": "The parameter that influences the smoothing of the output surface. No smoothing is applied when the value is zero and the maximum amount of smoothing is applied when the factor equals 1. The default is 0.0. ", "dataType": "Double"}, {"isOutputFile": true, "name": "Output_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Spline (in_point_features, z_field, {cell_size}, {spline_type}, {weight}, {number_points})", "name": "Spline (Spatial Analyst)", "description": "Interpolates a raster surface from points using a two-dimensional\r\nminimum curvature spline technique. The resulting smooth surface passes exactly through the input\r\npoints. \r\n Learn more about how Spline works", "example": {"title": "Spline example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSpline = Spline ( \"ozone_pts.shp\" , \"ozone\" , 2000 , \"REGULARIZED\" , 0.1 ) outSpline.save ( \"C:/sapyexamples/output/splineout.tif\" )"}, "usage": ["The REGULARIZED option of ", "Spline type", " usually produces smoother surfaces than those created with the TENSION option.", "With the REGULARIZED option, higher values used for the weight parameter produce smoother surfaces. The values entered for this parameter must be equal to or greater than zero. Typical values used are 0, 0.001, 0.01, 0.1, and 0.5. The ", "Weight", " is the square of the parameter referred to in the literature as tau (t).", "With the TENSION option, higher values entered for the weight parameter result in somewhat coarser surfaces, but surfaces that closely conform to the control points. The values entered must be equal to or greater than zero. Typical values are 0, 1, 5, and 10. The ", "Weight", " is the square of the parameter referred to in the literature as phi (\u03a6).", "The greater the value of ", "Number of Points", ", the smoother the surface of the output raster.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "spline_type", "isOptional": true, "description": "The type of spline to be used. REGULARIZED \u2014 Yields a smooth surface and smooth first derivatives. TENSION \u2014 Tunes the stiffness of the interpolant according to the character of the modeled phenomenon.", "dataType": "String"}, {"name": "weight", "isOptional": true, "description": "Parameter influencing the character of the surface interpolation. When the REGULARIZED option is used, it defines the weight of the third derivatives of the surface in the curvature minimization expression. If the TENSION option is used, it defines the weight of tension. The default weight is 0.1. ", "dataType": "Double"}, {"name": "number_points", "isOptional": true, "description": "The number of points per region used for local approximation. The default is 12. ", "dataType": "Long"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "NaturalNeighbor (in_point_features, z_field, {cell_size})", "name": "Natural Neighbor (Spatial Analyst)", "description": "Interpolates a raster surface from points using a natural neighbor\r\ntechnique. \r\n Learn more about how Natural Neighbor works", "example": {"title": "NaturalNeighbor example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outNaturalNeighbor = NaturalNeighbor ( \"ozone_pts.shp\" , \"ozone\" , 2000 ) outNaturalNeighbor.save ( \"C:/sapyexamples/output/nnout.tif\" )"}, "usage": ["If the cell center of the perimeter cells of the output raster fall outside the convex hull (defined by the input points), then those cells will be assigned NoData values. If an input point falls within one of these perimeter cells and the cell center falls outside the convex hull, the cell will still be assigned a value of NoData.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data.", "This tool has a limit of approximately 15 million input points. If your input feature class contains more than a very large number of points (around 15 million or greater), the tool may fail to create a result.", "You can avoid this limit by processing your study area in several sections and ", "mosaicking the results", " into a single large raster dataset. Ensure that there is some overlap between the sections. Alternatively, you can use a ", "Terrain dataset", " to store and visualize points and surfaces comprised of billions of measurement points.", " It is recommended that the input data be in a projected coordinate system rather than in a geographic coordinate system.", "If the ", "ArcGIS 3D Analyst extension", " is available, an alternative approach is to use a ", "TIN", " dataset.  First, ", "create a TIN", " from your source data. Then, convert the resulting TIN to a raster with the ", "TIN To Raster", " tool, using the Natural Neighbors option. This is particularly useful if you have breaklines or an irregularly shaped data area."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Kriging (in_point_features, z_field, semiVariogram_props, {cell_size}, {search_radius}, {out_variance_prediction_raster})", "name": "Kriging (Spatial Analyst)", "description": "Interpolates a raster surface from points using kriging. \r\n Learn more about how Kriging works", "example": {"title": "Kriging example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a Grid raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outKrig = Kriging ( \"ozone_pts.shp\" , \"OZONE\" , KrigingModelOrdinary ( \"CIRCULAR\" , 2000 , 2.6 , 542 , 0 ), 2000 , RadiusFixed ( 20000 , 1 )) outKrig.save ( \"c:/sapyexamples/output/krigout\" )"}, "usage": ["Kriging is a processor-intensive process. The speed of execution is dependent on the number of points in the input dataset and the size of the search window.", "Low values within the optional output variance of prediction raster indicate a high degree of confidence in the predicted value. High values may indicate a need for more data points.", "The Universal kriging types assume that there is a structural component present and that the local trend varies from one location to another.", "The ", "Advanced Parameters", " allow control of the semivariogram used for kriging. A default value for ", "Lag size", " is initially set to the default output cell size. For ", "Major range", ", ", "Partial sill", ", and ", "Nugget", ", a default value will be calculated internally if nothing is specified.", "The optional output variance of prediction raster contains the kriging variance at each output raster cell. Assuming the kriging errors are normally distributed, there is a 95.5 percent probability that the actual z-value at the cell is the predicted raster value, plus or minus two times the square root of the value in the prediction raster.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "semiVariogram_props", "isOptional": false, "description": "The KrigingModel class defines which kriging model is to be used. There are two types of kriging classes. The KrigingModelOrdinary method has five types of semivariograms available. The KrigingModelUniversal method has two types of semivariograms available. KrigingModelOrdinary ({semivariogramType}, {lagSize}, {majorRange}, {partialSill}, {nugget}) semivariogramType \u2014The semivariogram model to be used. The available models are: SPHERICAL \u2014Spherical semivariogram model. This is the default. CIRCULAR \u2014Circular semivariogram model. EXPONENTIAL \u2014Exponential semivariogram model. GAUSSIAN \u2014Gaussian (or normal distribution) semivariogram model. LINEAR \u2014Linear semivariogram model with a sill. KrigingModelUniversal ({semivariogramType}, {lagSize}, {majorRange}, {partialSill}, {nugget}) semivariogramType \u2014The semivariogram model to be used. The available models are: LINEARDRIFT \u2014Universal Kriging with linear drift. QUADRATICDRIFT \u2014Universal Kriging with quadratic drift. After the {semivariogramType} , the other parameters are common between Ordinary and Universal kriging. lagSize \u2014The default is the output raster cell size. majorRange \u2014Represents a distance beyond which there is little or no correlation. partialSill \u2014The difference between the nugget and the sill. nugget \u2014Represents the error and variation at spatial scales too fine to detect. The nugget effect is seen as a discontinuity at the origin.", "dataType": "KrigingModel"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "search_radius", "isOptional": true, "description": "The Radius class defines which of the input points will be used to interpolate the value for each cell in the output raster. There are two types of radius classes: RadiusVariable and RadiusFixed . A Variable search radius is used to find a specified number of input sample points for the interpolation. The Fixed type uses a specified fixed distance within which all input points will be used for the interpolation. The Variable type is the default. The value of the radius is expressed in map units. The default radius is five times the cell size of the output raster. If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the {minNumberofPoints} fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition. RadiusVariable ({numberofPoints}, {maxDistance}) {numberofPoints} \u2014An integer value specifying the number of nearest input sample points to be used to perform interpolation. The default is 12 points. {maxDistance} \u2014Specifies the distance, in map units, by which to limit the search for the nearest input sample points. The default value is the length of the extent's diagonal. RadiusFixed ({distance}, {minNumberofPoints}) {distance} \u2014Specifies the distance as a radius within which input sample points will be used to perform the interpolation. The value of the radius is expressed in map units. The default radius is five times the cell size of the output raster. {minNumberofPoints} \u2014An integer defining the minimum number of points to be used for interpolation. The default value is 0. If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the {minNumberofPoints} fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition.", "dataType": "Radius"}, {"name": "out_variance_prediction_raster", "isOutputFile": true, "isOptional": true, "description": "Optional output raster where each cell contains the predicted semi-variance values for that location. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_surface_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Idw (in_point_features, z_field, {cell_size}, {power}, {search_radius}, {in_barrier_polyline_features})", "name": "IDW (Spatial Analyst)", "description": "Interpolates a raster surface from points using an inverse distance\r\nweighted (IDW) technique. \r\n Learn more about how IDW works", "example": {"title": "IDW example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outIDW = Idw ( \"ozone_pts.shp\" , \"ozone\" , 2000 , 2 , RadiusVariable ( 10 , 150000 )) outIDW.save ( \"C:/sapyexamples/output/idwout.tif\" )"}, "usage": ["The output value for a cell using inverse distance weighting (IDW) is limited to the range of the values used to interpolate. Because IDW is a weighted distance average, the average cannot be greater than the highest or less than the lowest input. Therefore, it cannot create ridges or valleys if these extremes have not already been sampled (Watson and Philip 1985).", "The best results from IDW are obtained when sampling is sufficiently dense with regard to the local variation you are attempting to simulate. If the sampling of input points is sparse or uneven, the results may not sufficiently represent the desired surface (Watson and Philip 1985).", "The influence of an input point on an interpolated value is isotropic. Since the influence of an input point on an interpolated value is distance related, IDW is not \"ridge preserving\" (Philip and Watson 1982).", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data.", "The barriers option is used to specify the location of linear features known to interrupt the surface continuity. These features do not have z-values. Cliffs, faults, and embankments are typical examples of barriers. Barriers limit the selected set of the input sample points used to interpolate output z-values to those samples on the same side of the barrier as the current processing cell. Separation by a barrier is determined by line-of-sight analysis between each pair of points. This means that topological separation is not required for two points to be excluded from each other's region of influence. Input sample points that lie exactly on the barrier line will be included in the selected sample set for both sides of the barrier.", "Barrier features are input as polyline features. ", "IDW", " only uses the x,y coordinates for the linear feature; therefore, it is not necessary to provide z-values for the left and right sides of the barrier. Any z-values provided will be ignored.", "Using barriers will significantly extend the processing time.", "This tool has a limit of approximately 45 million input points. If your input feature class contains more than 45 million points, the tool may fail to create a result. You can avoid this limit by interpolating your study area in several pieces, making sure there is some overlap in the edges, then mosaicking the results to create a single large raster dataset. Alternatively, you can use a ", "terrain dataset", " to store and visualize points and surfaces comprised of billions of measurement points.", "If you have the Geostatistical Analyst extension, you may be able to process larger datasets.", "The input feature data must contain at least one valid field."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "power", "isOptional": true, "description": "The exponent of distance. Controls the significance of surrounding points on the interpolated value. A higher power results in less influence from distant points. It can be any real number greater than 0, but the most reasonable results will be obtained using values from 0.5 to 3. The default is 2. ", "dataType": "Double"}, {"name": "search_radius", "isOptional": true, "description": "The Radius class defines which of the input points will be used to interpolate the value for each cell in the output raster. There are two types of radius classes: RadiusVariable and RadiusFixed . A Variable search radius is used to find a specified number of input sample points for the interpolation. The Fixed type uses a specified fixed distance within which all input points will be used for the interpolation. The Variable type is the default. The value of the radius is expressed in map units. The default radius is five times the cell size of the output raster. If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the {minNumberofPoints} fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition. RadiusVariable ({numberofPoints}, {maxDistance}) {numberofPoints} \u2014An integer value specifying the number of nearest input sample points to be used to perform interpolation. The default is 12 points. {maxDistance} \u2014Specifies the distance, in map units, by which to limit the search for the nearest input sample points. The default value is the length of the extent's diagonal. RadiusFixed ({distance}, {minNumberofPoints}) {distance} \u2014Specifies the distance as a radius within which input sample points will be used to perform the interpolation. The value of the radius is expressed in map units. The default radius is five times the cell size of the output raster. {minNumberofPoints} \u2014An integer defining the minimum number of points to be used for interpolation. The default value is 0. If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the {minNumberofPoints} fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition.", "dataType": "Radius"}, {"name": "in_barrier_polyline_features", "isInputFile": true, "isOptional": true, "description": "Polyline features to be used as a break or limit in searching for the input sample points. ", "dataType": "Feature Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CosH (in_raster_or_constant)", "name": "CosH (Spatial Analyst)", "description": "Calculates the hyperbolic cosine of cells in a raster.", "example": {"title": "CosH example 1 (Python window)", "description": "This example calculates the hyperbolic cosine of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCosH = CosH ( \"degs\" ) outCosH.save ( \"C:/sapyexamples/output/outcosh\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : 1 \u2264 [out_value] < \u221e ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "Output values are always floating point, regardless of the input data type.", "The input and output values in ", "CosH", " are interpreted as unitless."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the hyperbolic cosine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Cos (in_raster_or_constant)", "name": "Cos (Spatial Analyst)", "description": "Calculates the cosine of cells in a raster.", "example": {"title": "Cos example 1 (Python window)", "description": "This example calculates the cosine of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCos = Cos ( \"degs\" ) outCos.save ( \"C:/sapyexamples/output/outcos\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : -1 \u2264 [out_value] \u2264 1 ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "The input values for this tool are interpreted to be in radians. If the input you wish to use is in degrees, the values must first be divided by the radians-to-degrees conversion factor of 180/pi, or approximately 57.296.", "See ", "here", " for examples of converting input values that are in degrees to the expected units of radians.", "The output values from ", "Cos", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type.", "Due to the range of values, applying a linear stretch renderer can be useful to better see the results."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the cosine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ATanH (in_raster_or_constant)", "name": "ATanH (Spatial Analyst)", "description": "Calculates the inverse hyperbolic tangent of cells in a raster.", "example": {"title": "ATanH example 1 (Python window)", "description": "This example calculates the inverse hyperbolic tangent of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outATanH = ATanH ( \"degs\" ) outATanH.save ( \"C:/sapyexamples/output/outatanh\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -1 < [in_value] < 1 ", "Note that any input value that is outside this domain will receive NoData on the output raster.", "The Range is : -\u221e < [out_value] < \u221e ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "The input and output values in ", "ATanH", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the inverse hyperbolic tangent values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ATan2 (in_raster_or_constant1, in_raster_or_constant2)", "name": "ATan2 (Spatial Analyst)", "description": "Calculates the inverse tangent (based on x,y) of cells in a raster.", "example": {"title": "ATan2 example 1 (Python window)", "description": "This example calculates the inverse tangent for two input GRID rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outATan2 = ATan2 ( \"degs\" , \"negs\" ) outATan2.save ( \"C:/sapyexamples/output/outatan2\" )"}, "usage": ["ATan2", " converts rectangular coordinates (x,y) to polar (r,\u03b8), where r is the distance from the origin and \u03b8 is the angle from the x-axis.", "The equation for determining ", "ATan2", " is: tan\u03b8 = y / x (where \u03b8 is the angle).", "The ", "ATan2", " operation represents all quadrants in a Cartesian matrix (based on sign).", "The values of the first specified input are used as the numerator in the calculation of the tangent angle (y). The values of the second specified input are used as the denominator in the calculation of the angle (x).", "In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "This domain applies to both inputs.", "The Range is : -pi < [out_value] \u2264 pi ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "If both input values are 0, the output will be NoData.", "If first input value is 0, the output will be 0.", "The input values to ", "ATan2", " are interpreted as being in linear units, and to give meaningful results, they should both be in the same unit.", "Output values are always floating point, regardless of the input data type.", "The output values from this tool are in radians. If degrees are desired, the resulting raster must be multiplied by the radians-to-degrees conversion factor of 180/pi, or approximately 57.296.", "See ", "here", " for examples of converting output values from radians to degrees."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input that specifies the numerator, or y value, to use when calculating the inverse tangent. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input that specifies the denominator, or x value, to use when calculating the inverse tangent. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ATan (in_raster_or_constant)", "name": "ATan (Spatial Analyst)", "description": "Calculates the inverse tangent of cells in a raster.", "example": {"title": "ATan example 1 (Python window)", "description": "This example calculates the inverse tangent of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outATan = ATan ( \"degs\" ) outATan.save ( \"C:/sapyexamples/output/outatan\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : -pi/2 \u2264 [out_value] \u2264 pi/2 ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "The input values to ", "ATan", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type.", "The output values from this tool are in radians. If degrees are desired, the resulting raster must be multiplied by the radians-to-degrees conversion factor of 180/pi, or approximately 57.296.", "See ", "here", " for examples of converting output values from radians to degrees."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the inverse tangent values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ASinH (in_raster_or_constant)", "name": "ASinH (Spatial Analyst)", "description": "Calculates the inverse hyperbolic sine of cells in a raster.", "example": {"title": "ASinH example 1 (Python window)", "description": "This example calculates the inverse hyperbolic sine of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outASinH = ASinH ( \"degs\" ) outASinH.save ( \"C:/sapyexamples/output/outasinh\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : -\u221e < [out_value] < \u221e ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "The input and output values in ", "ASinH", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the inverse hyperbolic sine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ASin (in_raster_or_constant)", "name": "ASin (Spatial Analyst)", "description": "Calculates the inverse sine of cells in a raster.", "example": {"title": "ASin example 1 (Python window)", "description": "This example calculates the inverse sine of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outASin = ASin ( \"degs\" ) outASin.save ( \"C:/sapyexamples/output/outasin\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -1 \u2264 [in_value] \u2264 1 ", "Note that any input value that is outside this domain will receive NoData on the output raster.", "The Range is : -pi/2 \u2264 [out_value] \u2264 pi/2 ", "The input values to ", "ASin", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type.", "The output values from this tool are in radians. If degrees are desired, the resulting raster must be multiplied by the radians-to-degrees conversion factor of 180/pi, or approximately 57.296.", "See ", "here", " for examples of converting output values from radians to degrees."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the inverse sine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ACosH (in_raster_or_constant)", "name": "ACosH (Spatial Analyst)", "description": "Calculates the inverse hyperbolic cosine of cells in a raster.", "example": {"title": "ACosH example 1 (Python window)", "description": "This example calculates the inverse hyperbolic cosine of the values in the input GRID raster and outputs an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outACosH = ACosH ( \"degs\" ) outACosH.save ( \"C:/sapyexamples/output/outacosh.img\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : 1 \u2264 [in_value] < \u221e ", "Note that any input value that is outside this domain will receive NoData on the output raster.", "The Range is : -\u221e < [out_value] < \u221e ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "The input and output values in ", "ACosH", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the inverse hyperbolic cosine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ACos (in_raster_or_constant)", "name": "ACos (Spatial Analyst)", "description": "Calculates the inverse cosine of cells in a raster.", "example": {"title": "ACos example 1 (Python window)", "description": "This example calculates the inverse cosine of the values in the input GRID raster and outputs a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outACos = ACos ( \"degs\" ) outACos.save ( \"C:/sapyexamples/output/outacos.tif\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -1 \u2264 [in_value] \u2264 1 ", "Note that any input value that is outside this domain will receive NoData on the output raster.", "The Range is : 0 \u2264 [out_value] \u2264 pi ", "The input values to ", "ACos", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type.", "The output values from this tool are in radians. If degrees are desired, the resulting raster must be multiplied by the radians-to-degrees conversion factor of 180/pi, or approximately 57.296.", "See ", "here", " for examples of converting output values from radians to degrees."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the inverse cosine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Test (in_raster, where_clause)", "name": "Test (Spatial Analyst)", "description": "Performs a Boolean evaluation of the input raster using a logical\r\nexpression. When the expression evaluates to true, the output cell value is 1.\r\nIf the expression is false, the output cell value is 0.", "example": {"title": "Test example 1 (Python window)", "description": "This example uses a Where clause to perform a Boolean operation on an input raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outTest = Test ( \"degs\" , \"VALUE > 100\" ) outTest.save ( \"C:/sapyexamples/output/outest.img\" )"}, "usage": ["The test is specified by an SQL expression in the ", "Where clause", ". "], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster on which the Boolean evaluation is performed, based on a logical expression. ", "dataType": "Raster Layer"}, {"name": "where_clause", "isOptional": false, "description": "A logical expression that selects a subset of raster cells. The expression follows the general form of an SQL expression. Consult the documentation for more information on the SQL reference for query expressions used in ArcGIS and specifying a query in Python . ", "dataType": "SQL Expression"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Over (in_raster_or_constant1, in_raster_or_constant2)", "name": "Over (Spatial Analyst)", "description": "For the cell values in the first input that are not 0, the output\r\nvalue will be that of the first input. Where the cell values are 0,\r\nthe output will be that of the second input raster.", "example": {"title": "Over example 1 (Python window)", "description": "This example performs an Over operation on two GRID rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outOver = Over ( \"degs\" , \"negs\" ) outOver.save ( \"C:/sapyexamples/output/outover2\" )"}, "usage": ["Two inputs are necessary for this logical evaluation to take place.", "The order of inputs is relevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input for which cell values of 0 will be replaced with the value from the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input whose value will be assigned to the output raster cells where the first input value is 0. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "NotEqual (in_raster_or_constant1, in_raster_or_constant2)", "name": "Not Equal (Spatial Analyst)", "description": "Performs a Relational not-equal-to operation on two inputs on a\r\ncell-by-cell basis. Returns 1 for cells where the first raster is not equal to the\r\nsecond raster and 0 for cells where it is equal. Learn more about how the Relational Math tools work", "example": {"title": "NotEqual example 1 (Python window)", "description": "This example performs a Relational not-equal-to operation on two GRID rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outNotEqual = NotEqual ( \"degs\" , \"negs\" ) outNotEqual.save ( \"C:/sapyexamples/output/outne\" )"}, "usage": ["Two inputs are necessary for this relational evaluation to take place.", "The order of inputs is irrelevant for this tool.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "!=", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input that will be compared to for inequality by the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input that will be compared from for inequality by the first input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "LessThanEqual (in_raster_or_constant1, in_raster_or_constant2)", "name": "Less Than Equal (Spatial Analyst)", "description": "Performs a Relational less-than-or-equal-to operation on two inputs\r\non a cell-by-cell basis. Returns 1 for cells where the first raster is less than or equal to\r\nthe second raster and 0 where it is not. Learn more about how the Relational Math tools work", "example": {"title": "LessThanEqual example 1 (Python window)", "description": "This example performs a Relational less-than-or-equal-to operation on two GRID rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLTE = LessThanEqual ( \"degs\" , \"negs\" ) outLTE.save ( \"C:/sapyexamples/output/outlte.img\" )"}, "usage": ["Two inputs are necessary for this relational evaluation to take place.", "The order of inputs is relevant for this tool.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "<=", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input being tested to determine if it is less than or equal to the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input against which the first input is tested to be less than or equal to. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "LessThan (in_raster_or_constant1, in_raster_or_constant2)", "name": "Less Than (Spatial Analyst)", "description": "Performs a Relational less-than operation on two inputs on a\r\ncell-by-cell basis. Returns 1 for cells where the first raster is less than the second\r\nraster and 0 if it is not. Learn more about how the Relational Math tools work", "example": {"title": "LessThan example 1 (Python window)", "description": "This example performs a Relational less-than operation on two GRID rasters and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outLessThan = LessThan ( \"degs\" , \"negs\" ) outLessThan.save ( \"C:/sapyexamples/output/outlt.tif\" )"}, "usage": ["Two inputs are necessary for this relational evaluation to take place.", "The order of inputs is relevant for this tool.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "<", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input being tested to determine if it is less than the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input against which the first input is tested to be less than. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "IsNull (in_raster)", "name": "Is Null (Spatial Analyst)", "description": "Determines which values from the input raster are NoData on a\r\ncell-by-cell basis. Returns a value of 1 if the input value is NoData and 0 for cells\r\nthat are not.", "example": {"title": "IsNull example 1 (Python window)", "description": "This example identifies which cells in the input raster are NoData, and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outIsNull = IsNull ( \"degs\" ) outIsNull.save ( \"C:/sapyexamples/output/outisnull.img\" )"}, "usage": ["Is Null", " can be used along with the ", "Con", " tool to ", "change NoData cells to a value", "."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster being tested to identify the cells that are NoData (null). The input can be either integer or floating point type. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "InList (in_raster_or_constant, in_raster_or_constants)", "name": "InList (Spatial Analyst)", "description": "Determines which values from the first input are contained in a set\r\nof other inputs, on a cell-by-cell basis. For each cell, if the value of the first input raster is found in any of the list of other inputs, that value will be assigned to the output raster. If it is not found, the output cell will be NoData.", "example": {"title": "InList example 1 (Python window)", "description": "This example determines which cell values in the first input are found in the set of other input rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outInList = InList ( \"redlandsc1\" , [ \"redlandsc2\" , \"redlandsc3\" ]) outInList.save ( \"C:/sapyexamples/output/outinlist.tif\" )"}, "usage": [" If all of the inputs are integer, the output raster will be integer. If any of the inputs are floating point, the output will be floating point.", "In the list of input rasters, the order is not relevant to the outcome of this tool."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster whose values will be looked for in the input list. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constants", "isInputFile": true, "isOptional": false, "description": "A list of input rasters in which the cell values from the first input will be looked for. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "GreaterThanEqual (in_raster_or_constant1, in_raster_or_constant2)", "name": "Greater Than Equal (Spatial Analyst)", "description": "Performs a Relational greater-than-or-equal-to operation on two\r\ninputs on a cell-by-cell basis. Returns 1 for cells where the first raster is greater than or equal\r\nto the second raster and 0 if it is not. Learn more about how the Relational Math tools work", "example": {"title": "GreaterThanEqual example 1 (Python window)", "description": "This example performs a Relational greater-than-or-equal-to operation on two GRID rasters and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outGTE = GreaterThanEqual ( \"degs\" , \"negs\" ) outGTE.save ( \"C:/sapyexamples/output/outgte.tif\" )"}, "usage": ["Two inputs are necessary for this relational evaluation to take place.", "The order of inputs is relevant for this tool.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", ">=", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input being tested to determine if it is greater than or equal to the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input against which the first input is tested to be greater than or equal to. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "GreaterThan (in_raster_or_constant1, in_raster_or_constant2)", "name": "Greater Than (Spatial Analyst)", "description": "Performs a Relational greater-than operation on two inputs on a\r\ncell-by-cell basis. Returns 1 for cells where the first raster is greater than the\r\nsecond raster and 0 for cells if it is not. Learn more about how the Relational Math tools work", "example": {"title": "GreaterThan example 1 (Python window)", "description": "This example performs a Relational greater-than operation on two GRID rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outGreaterThan = GreaterThan ( \"degs\" , \"negs\" ) outGreaterThan.save ( \"C:/sapyexamples/output/outgt.img\" )"}, "usage": ["Two inputs are necessary for this relational evaluation to take place.", "The order of inputs is relevant for this tool.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", ">", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input being tested to determine if it is greater than the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input against which the first input is tested to be greater than. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "EqualTo (in_raster_or_constant1, in_raster_or_constant2)", "name": "Equal To (Spatial Analyst)", "description": "Performs a Relational equal-to operation on two inputs on a\r\ncell-by-cell basis. Returns 1 for cells where the first raster equals the second raster\r\nand 0 for cells where it does not. Learn more about how the Relational Math tools work", "example": {"title": "EqualTo example 1 (Python window)", "description": "This example performs a Relational equal-to operation on two GRID rasters and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outEqualTo = EqualTo ( \"degs\" , \"negs\" ) outEqualTo.save ( \"C:/sapyexamples/output/outequalto.tif\" )"}, "usage": ["Two inputs are necessary for this relational evaluation to take place.", "The order of inputs is irrelevant for this tool.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "==", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input that will be compared to for equality by the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input that will be compared from for equality by the first input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Diff (in_raster_or_constant1, in_raster_or_constant2)", "name": "Diff (Spatial Analyst)", "description": "Determines which values from the first input are logically\r\ndifferent from the values of the second input on a cell-by-cell\r\nbasis. If the values on the two inputs are different, the value on the\r\nfirst input is output. If the values on the two inputs are the\r\nsame, the output is 0.", "example": {"title": "Diff example 1 (Python window)", "description": "This example performs a difference operation on two GRID rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outDiff = Diff ( \"degs\" , \"negs\" ) outDiff.save ( \"C:/sapyexamples/output/outdiff.img\" )"}, "usage": ["Two inputs are necessary for this logical evaluation to take place.", "The order of inputs is relevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input to which the second input will be compared. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input to which the first input will be compared. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CombinatorialXOr (in_raster_or_constant1, in_raster_or_constant2)", "name": "Combinatorial XOr (Spatial Analyst)", "description": "Performs a Combinatorial eXclusive Or operation on the cell values\r\nof two input rasters. If one input value is true (non-zero) and the other false (zero), the output is a different value for each unique combination of input values. If both inputs are true or both are false, the output value is 0. \r\n Learn more about how Combinatorial tools work \r\n", "example": {"title": "CombinatorialXOr example 1 (Python window)", "description": "This example performs a Combinatorial XOr operation on two GRID rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCXOr = CombinatorialXOr ( \"degs\" , \"cost\" ) outCXOr.save ( \"C:/sapyexamples/output/outcxor.img\" )"}, "usage": ["The Combinatorial math tools interpret the inputs as Boolean values, where non-zero values are considered true, and zero values are considered false.", "Two inputs are necessary for this Combinatorial evaluation to take place.", "The order of inputs for this tool is only relevant for the output attribute table."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this combinatorial operation. It must be of positive integer type. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this combinatorial operation. It must be of positive integer type. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CombinatorialOr (in_raster_or_constant1, in_raster_or_constant2)", "name": "Combinatorial Or (Spatial Analyst)", "description": "Performs a Combinatorial Or operation on the cell values of two\r\ninput rasters. If either input value is true (non-zero), the output is a different\r\nvalue for each unique combination of input values. If both inputs\r\nare false (zero), the output value is 0. \r\n Learn more about how Combinatorial tools work", "example": {"title": "CombinatorialOr example 1 (Python window)", "description": "This example performs a Combinatorial Or operation on two GRID rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCOr = CombinatorialOr ( \"degs\" , \"cost\" ) outCOr.save ( \"C:/sapyexamples/output/outcor.img\" )"}, "usage": ["The Combinatorial math tools interpret the inputs as Boolean values, where non-zero values are considered true, and zero values are considered false.", "Two inputs are necessary for this Combinatorial evaluation to take place.", "The order of inputs for this tool is only relevant for the output attribute table."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this combinatorial operation. It must be of positive integer type. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this combinatorial operation. It must be of positive integer type. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CombinatorialAnd (in_raster_or_constant1, in_raster_or_constant2)", "name": "Combinatorial And (Spatial Analyst)", "description": "Performs a Combinatorial And operation on the cell values of two\r\ninput rasters. If both input values are true (non-zero), the output is a different\r\nvalue for each unique combination of input values. If one or both\r\ninputs are false (zero), the output value is 0. \r\n Learn more about how Combinatorial tools work", "example": {"title": "CombinatorialAnd example 1 (Python window)", "description": "This example performs a Combinatorial And operation on two GRID rasters and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCAnd = CombinatorialAnd ( \"degs\" , \"cost\" ) outCAnd.save ( \"C:/sapyexamples/output/outcand.tif\" )"}, "usage": ["The Combinatorial math tools interpret the inputs as Boolean values, where non-zero values are considered true, and zero values are considered false.", "Two inputs are necessary for this Combinatorial evaluation to take place.", "The order of inputs for this tool is only relevant for the output attribute table."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this combinatorial operation. It must be of positive integer type. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this combinatorial operation. It must be of positive integer type. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BooleanXOr (in_raster_or_constant1, in_raster_or_constant2)", "name": "Boolean XOr (Spatial Analyst)", "description": "Performs a Boolean eXclusive Or operation on the cell values of two\r\ninput rasters. If one input value is true (non-zero) and the other false (zero),\r\nthe output is 1. If both input values are true or both are false,\r\nthe output is 0. \r\n Learn more about how the Boolean math tools work", "example": {"title": "BooleanXOr example 1 (Python window)", "description": "This example performs a Boolean XOr operation on two GRID rasters and outputs a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBooleanXOr = BooleanXOr ( \"degs\" , \"negs\" ) outBooleanXOr.save ( \"C:/sapyexamples/output/outboolxor.tif\" )"}, "usage": ["The Boolean math tools interpret the inputs as Boolean values, where non-zero values are considered true, and zero is considered false.", "Two inputs are necessary for this Boolean evaluation to take place.", "The order of inputs is irrelevant for this tool.", "If the input values are floating point, they are converted to integer values of either 0 or 1 before the operation is performed. If the input value is a floating point 0.0, it is converted to an integer 0. If the input is any value other than 0.0, it is converted to be an integer 1. For example, input float values of 0.6, 32.22 and -4.2 will all be treated as being 1. The output values are always integer.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "^", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this Boolean operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this Boolean operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BooleanOr (in_raster_or_constant1, in_raster_or_constant2)", "name": "Boolean Or (Spatial Analyst)", "description": "Performs a Boolean Or operation on the cell values of two input\r\nrasters. If one or both input values are true (non-zero), the output value\r\nis 1. If both input values are false (zero), the output is 0. \r\n Learn more about how the Boolean math tools work \r\n", "example": {"title": "BooleanOr example 1 (Python window)", "description": "This example performs a Boolean Or operation on two GRID rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBooleanOr = BooleanOr ( \"degs\" , \"negs\" ) outBooleanOr.save ( \"C:/sapyexamples/output/outboolor2\" )"}, "usage": ["The Boolean math tools interpret the inputs as Boolean values, where non-zero values are considered true, and zero is considered false.", "Two inputs are necessary for this Boolean evaluation to take place.", "The order of inputs is irrelevant for this tool.", "If the input values are floating point, they are converted to integer values of either 0 or 1 before the operation is performed. If the input value is a floating point 0.0, it is converted to an integer 0. If the input is any value other than 0.0, it is converted to be an integer 1. For example, input float values of 0.6, 32.22 and -4.2 will all be treated as being 1. The output values are always integer.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "|", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this Boolean operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this Boolean operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BooleanNot (in_raster_or_constant)", "name": "Boolean Not (Spatial Analyst)", "description": "Performs a Boolean Not (complement) operation on the cell values of\r\nthe input raster. If the input values are true (non-zero), the output value is 1. If\r\nthe input values are false (zero), the output is 0. \r\n Learn more about how the Boolean math tools work", "example": {"title": "BooleanNot example 1 (Python window)", "description": "This example performs a Boolean Not (complement) operation on a GRID raster and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBooleanNot = BooleanNot ( \"degs\" ) outBooleanNot.save ( \"C:/sapyexamples/output/outboolnot.tif\" )"}, "usage": ["The Boolean math tools interpret the inputs as Boolean values, where non-zero values are considered true, and zero is considered false.", "Only a single input is necessary for this Boolean evaluation to take place.", "If the input values are floating point, they are converted to integer values of either 0 or 1 before the operation is performed. If the input value is a floating point 0.0, it is converted to an integer 0. If the input is any value other than 0.0, it is converted to be an integer 1. For example, input float values of 0.6, 32.22 and -4.2 will all be treated as being 1. The output values are always integer.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "~", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input to use in this Boolean operation. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BooleanAnd (in_raster_or_constant1, in_raster_or_constant2)", "name": "Boolean And (Spatial Analyst)", "description": "Performs a Boolean And operation on the cell values of two input\r\nrasters. If both input values are true (non-zero), the output value is 1. If one or both inputs are false (zero), the output is 0. \r\n Learn more about how the Boolean math tools work", "example": {"title": "BooleanAnd example 1 (Python window)", "description": "This example performs a Boolean And operation on two GRID rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBooleanAnd = BooleanAnd ( \"degs\" , \"negs\" ) outBooleanAnd.save ( \"C:/sapyexamples/output/outbooland.img\" )"}, "usage": ["The Boolean math tools interpret the inputs as Boolean values, where non-zero values are considered true, and zero is considered false.", "Two inputs are necessary for this Boolean evaluation to take place.", "The order of inputs is irrelevant for this tool.", "If the input values are floating point, they are converted to integer values of either 0 or 1 before the operation is performed. If the input value is a floating point 0.0, it is converted to an integer 0. If the input is any value other than 0.0, it is converted to be an integer 1. For example, input float values of 0.6, 32.22 and -4.2 will all be treated as being 1. The output values are always integer.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "&", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this Boolean operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this Boolean operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Times (in_raster_or_constant1, in_raster_or_constant2)", "name": "Times (Spatial Analyst)", "description": "Multiplies the values of two rasters on a cell-by-cell basis.", "example": {"title": "Times example 1 (Python window)", "description": "This example multiplies the values of an input elevation raster by a constant value to convert the elevation values from feet to meters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outTimes = Times ( \"elevation\" , \"0.3048\" ) outTimes.save ( \"C:/sapyexamples/output/outtimes\" )"}, "usage": ["The order of inputs is irrelevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "*", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input containing the values to be multiplied. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input containing the values by which the first input will be multiplied. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "SquareRoot (in_raster_or_constant)", "name": "Square Root (Spatial Analyst)", "description": "Calculates the square root of the cell values in a raster.", "example": {"title": "SquareRoot example 1 (Python window)", "description": "This example finds the square root of the values in the input GRID raster and gererates the output as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSquareRoot = SquareRoot ( \"elevation\" ) outSquareRoot.save ( \"C:/sapyexamples/output/outsqrt.img\" )"}, "usage": ["Input values that are less than 0 will be NoData in the output raster.", "The output raster from this tool is always floating point type, regardless of the input value type."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input values to find the square root of. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Square (in_raster_or_constant)", "name": "Square (Spatial Analyst)", "description": "Calculates the square of the cell values in a raster.", "example": {"title": "Square example 1 (Python window)", "description": "This example finds the square of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSquare = Square ( \"degs\" ) outSquare.save ( \"C:/sapyexamples/output/outsq\" )"}, "usage": ["Output values are floating point if the input values are floating point; if the input values are integer, the output values are integer."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input values to find the square of. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "RoundUp (in_raster_or_constant)", "name": "Round Up (Spatial Analyst)", "description": "Returns the next higher whole number for each cell in a raster.", "example": {"title": "RoundUp example 1 (Python window)", "description": "This example rounds the values in the input raster to the next higher whole number.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outRoundUp = RoundUp ( \"gwhead\" ) outRoundUp.save ( \"C:/sapyexamples/output/outru\" )"}, "usage": ["Input values can be positive or negative.", "The output raster from this tool is always floating point type, regardless of the input value type.", "If a number has any values to the right of the decimal point, the output will be assigned the next highest whole value. For example:"], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input values to be rounded up. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "RoundDown (in_raster_or_constant)", "name": "Round Down (Spatial Analyst)", "description": "Returns the next lower whole number for each cell in a raster.", "example": {"title": "RoundDown example 1 (Python window)", "description": "This example rounds the values in the input raster to the next lower whole number.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outRoundDown = RoundDown ( \"gwhead\" ) outRoundDown.save ( \"C:/sapyexamples/output/outrd\" )"}, "usage": ["Input values can be positive or negative.", "The output raster from this tool is always floating point type, regardless of the input value type.", "If a number has any values to the right of the decimal point, the output will be assigned the next lowest whole value. For example:", "There is a difference between the ", "Int", " tool and the ", "Round Down", " tool.   For example, given the following two values ", "Int", " always truncates the number:", "while for the same two values, ", "Round Down", " returns:", "Another difference is that ", "Round Down", " outputs floating-point values, while ", "Int", " only outputs integer values."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input values to be rounded down. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Power (in_raster_or_constant1, in_raster_or_constant2)", "name": "Power (Spatial Analyst)", "description": "Raises the cell values in a raster to the power of the values found in another raster.", "example": {"title": "Power example 1 (Python window)", "description": "This example uses the values in the second input raster as the power to raise the values in the first input raster by, and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPower = Power ( \"degs\" , \"cost\" ) outPower.save ( \"C:/sapyexamples/output/outpower.img\" )"}, "usage": ["The output raster from this tool is always floating point type, regardless of the input value type.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "**", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input values to be raised to the power defined by the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input that determines the power the values in the first input will be raised to. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Plus (in_raster_or_constant1, in_raster_or_constant2)", "name": "Plus (Spatial Analyst)", "description": "Adds (sums) the values of two rasters on a cell-by-cell basis.", "example": {"title": "Plus example 1 (Python window)", "description": "This example adds the values of two GRID rasters and outputs the result as an IMG raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPlus = Plus ( \"degs\" , \"negs\" ) outPlus.save ( \"C:/sapyexamples/output/outplus.img\" )"}, "usage": ["The order of inputs is irrelevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "+", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input whose values will be added to. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input whose values will be added to the first input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "WeightedSum (in_rasters)", "name": "Weighted Sum (Spatial Analyst)", "description": "Overlays several rasters, multiplying each by their given weight and summing them together. Learn more about how Weighted Sum works", "example": {"title": "WeightedSum example 1 (Python window)", "description": "This example creates a suitability raster for locating a ski resort by combining multiple rasters together, and applying appropriate weight factors.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" # Execute WeightedSum outWeightedSum = WeightedSum ( WSTable ([[ \"snow\" , \"VALUE\" , 0.25 ], [ \"land\" , \"VALUE\" , 0.25 ], [ \"soil\" , \"VALUE\" , 0.5 ]])) outWeightedSum.save ( \"C:/sapyexamples/output/outwsum\" )"}, "usage": ["A useful way to add several rasters together is to input multiple rasters and set all weights equal to 1.", "Input rasters can be integer or floating point.", "The weight values can be any positive or negative decimal value. It is not restricted to a relative percentage or equal to 1.0.", "The weight will be applied to the specified field for the input raster. Fields can be of type short or long integer, double or float."], "parameters": [{"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": "The Weighted Sum tool overlays several rasters, multiplying each by their given weight and summing them together. An Overlay class is used to define the table. The WSTable object is used to specify a Python list of input rasters and weight them accordingly. The form of the WSTable object is: WSTable ([[inRaster, field, weight],...])", "dataType": "WSTable"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "WeightedOverlay (in_weighted_overlay_table)", "name": "Weighted Overlay (Spatial Analyst)", "description": "Overlays several rasters using a common measurement scale and weights each according to its importance. Learn more about how Weighted Overlay works", "example": {"title": "WeightedOverlay example 1 (Python window)", "description": "This example create a suitability IMG raster that identifies potential site locations for a ski area.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outsuit = WeightedOverlay ( WOTable ( [ [ \"snow\" , 50 , 'VALUE' , RemapValue ([[ 1 , \"Nodata\" ],[ 5 , 3 ],[ 9 , 10 ],[ \"NODATA\" , \"NODATA\" ]])], [ \"land\" , 20 , '' , RemapValue ([[ \"water\" , \"1\" ],[ \"forest\" , 5 ],[ \"open field\" , 9 ],[ \"NODATA\" , \"NODATA\" ]])], [ \"soil\" , 30 , 'VALUE' , RemapValue ([[ 1 , \"Restricted\" ],[ 5 , 5 ],[ 7 , 7 ],[ 9 , 9 ],[ \"NODATA\" , \"Restricted\" ]])] ],[ 1 , 9 , 1 ])) outsuit.save ( \"C:/sapyexamples/output/outsuit.img\" )"}, "usage": ["All input rasters must be integer. A floating-point raster must first be converted to an integer raster before it can be used in ", "Weighted Overlay", ". The ", "Reclassification", " tools provide an effective way to do the conversion.", "Each value class in an input raster is assigned a new value based on an evaluation scale. These new values are reclassifications of the original input raster values. A restricted value is used for areas you want to exclude from the analysis.", "Each input raster is weighted according to its importance or its percent influence. The weight is a relative percentage, and the sum of the percent influence weights must equal 100.", "Changing the evaluation scales or the percentage influences can change the results of the weighted overlay analysis."], "parameters": [{"name": "in_weighted_overlay_table", "isInputFile": true, "isOptional": false, "description": "The Weighted Overlay tool allows the calculation of a multiple-criteria analysis between several rasters. An Overlay class is used to define the table. The WOTable object is used to specify the criteria rasters and their respective properties. The form of the object is: WOTable([[inRaster, influence, field, remap],...], [from, to, by])", "dataType": "WOTable"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "FuzzyOverlay (in_rasters, {overlay_type}, {gamma})", "name": "Fuzzy Overlay (Spatial Analyst)", "description": " Combine fuzzy membership rasters data together, based on selected overlay type. Learn more about how Fuzzy Overlay works", "example": {"title": "FuzzyOverlay example 1 (Python window)", "description": "This example combines the input membership rasters with the AND overlay type to identify the minium membership value between them.", "code": "import arcpy from arcpy.sa import * from arcpy import env env.workspace = \"c:/sapyexamples/data\" outFzyOverlay = FuzzyOverlay ([ \"fzymembout1\" , \"fzymembout2\" ], \"AND\" ) outFzyOverlay.save ( \"c:/sapexamples/output/fuzzover.tif\" )"}, "usage": [" This tool is recommended for the use with the result of fuzzy membership tool.  It is meant to be applied to rasters with values that range between 0 and 1.", "The following lists the appropriate ", "Overlay type", " to use for certain conditions.", "The GAMMA ", "Overlay type", " is typically used to combine fuzzy combinations of more basic data. When Gamma is 1 the result is the same as Fuzzy Sum. When Gamma is 0 the result is the same as Fuzzy Product. Values in between allow the user to combine evidence between these two extremes and possibly different than Fuzzy And or Fuzzy Or."], "parameters": [{"name": "in_rasters", "isInputFile": true, "isOptional": false, "description": " A list of input membership rasters to be combined in the overlay. ", "dataType": "Raster Layer"}, {"name": "overlay_type", "isOptional": true, "description": " Specifies the method used to combine two or more membership data. AND \u2014 The minimum of the fuzzy memberships from the input fuzzy rasters. OR \u2014 The maximum of the fuzzy memberships from the input rasters. PRODUCT \u2014 A decreasive function. Use this when the combination of multiple evidence is less important or smaller than any of the inputs alone. SUM \u2014 An increasive function. Use this when the combination of multiple evidence is more important or larger than any of the inputs alone. GAMMA \u2014 The algebraic product of the Fuzzy Sum and Fuzzy Product, both raised to the power of gamma.", "dataType": "String"}, {"name": "gamma", "isOptional": true, "description": " The gamma value to be used. This is only when the Overlay type is set to GAMMA. Default value is 0.9. ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "FuzzyMembership (in_raster, {fuzzy_function}, {hedge})", "name": "Fuzzy Membership (Spatial Analyst)", "description": " Transforms  the input raster   into a 0 to 1 scale, indicating the strength of a membership in a set, based on a specified fuzzification algorithm. A value of 1 indicates full membership in the fuzzy set, with membership decreasing to 0, indicating it is not a member of the fuzzy set. Learn more about how Fuzzy Membership works", "example": {"title": "FuzzyMembership example 1 (Python window)", "description": "This example creates a fuzzy membership raster using the Gaussian function where elevation values closer to the midpoint (1,200 ft.) have a higher membership value.", "code": "import arcpy from arcpy.sa import * from arcpy import env env.workspace = \"c:/sapyexamples/data\" outFzyMember = FuzzyMembership ( \"elevation\" , FuzzyGaussian ( 1200 , 0.06 )) outFzyMember.save ( \"c:/sapyexamples/fzymemb\" )"}, "usage": [" This tool does not scale categorical data. To include categorical data into fuzzy overlay analysis, a preprocessing step is necessary.    You can create a model or run the following geoprocessing tools. First, use the ", "Reclassify", " tool to provide a new range of values (for example, 1 to 100).     Then ", "Divide", " the result by a factor  (for example, by 100) to normalize the output values to be between 0.0 and 1.0.", "Spread determines how rapidly the fuzzy membership values\r\ndecrease from 1 to 0. The larger the value, the steeper the\r\nfuzzification around the midpoint.  Said another way, as the spread gets smaller, the fuzzy memberships approach 0 more slowly.  The selection of the appropriate spread value is a subjective process that is dependent on the range of the crisp values. For Gaussian and Near, the\r\ndefault value of 0.1 is a good starting point. Typically, the values\r\nvary within the ranges of [0.01\u20131] or [0.001-1], respectively. For Small\r\nand Large, the default value of 5 is a good starting point where,\r\ntypically, the values vary between 1 and 10.", "You may have a case where none of the input values have a 100 percent possibility of being a member of the specified set.  In other words, no input value has a fuzzy membership of 1. In this situation, you may want to rescale the fuzzy membership values to reflect the new scale. For example, if the highest membership for the input values is .75, you can establish the new scale by multiplying each of the fuzzy membership values by 0.75.", " The hedges implemented are ", "Very", " and ", "Somewhat", ". ", "Very", " is also known as concentration and is defined as the fuzzy membership function squared. ", "Somewhat", " is known as dilation, or More or Less, and is the square root of the fuzzification membership function. The very and somewhat hedges decrease and increase, respectively, the fuzzy membership functions.", "  Negative values are not accepted in the Small and Large membership functions.", " For the Linear membership function, the input raster must be ordered data. The minimum can be less than the maximum to create a positive slope or greater than the maximum to create a negative slope for the transformation.", "If the minimum is less than the maximum, a positive-sloped function is used for the transformation; if the minimum is less than the maximum, a negative-sloped function is used."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": " The input raster whose values will be scaled from 0 to 1. ", "dataType": "Raster Layer"}, {"name": "fuzzy_function", "isOptional": true, "description": "Specifies the algorithm used in fuzzification of the input raster. The fuzzy classes are used to specify the type of membership. The types of membership classes are: The following are the forms of the membership classes: FuzzyGaussian , FuzzyLarge , FuzzyLinear , FuzzyMSLarge , FuzzyMSSmall , FuzzyNear , and FuzzySmall . FuzzyGaussian({midpoint},{spread}) FuzzyLarge({midpoint},{spread}) FuzzyLinear({min},{max}) FuzzyMSLarge({mean_multiplier},{std_multiplier}) FuzzyMSSmall({mean_multiplier},{std_multiplier}) FuzzyNear({midpoint},{spread}) FuzzySmall({midpoint},{spread})", "dataType": "Fuzzy function"}, {"name": "hedge", "isOptional": true, "description": " Defining a hedge increases or decreases the fuzzy membership values which modify the meaning of a fuzzy set. Hedges are useful to help in controlling the criteria or important attributes. NONE \u2014 No hedge is applied. This is the default. SOMEWHAT \u2014 Known as dilation, defined as the square root of the fuzzy membership function. This hedge increases the fuzzy membership functions. VERY \u2014 Also known as concentration, defined as the fuzzy membership function squared. This hedge decreases the fuzzy membership functions.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PointStatistics (in_point_features, field, {cell_size}, {neighborhood}, {statistics_type})", "name": "Point Statistics (Spatial Analyst)", "description": "Calculates a statistic on the points in a neighborhood around each output cell. \r\n Learn more about how Point Statistics works", "example": {"title": "PointStatistics example 1 (Python window)", "description": "This example determines a statistic (the sum) on the input shapefile point features that fall in a circular neighborhood around each output raster cell.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPointStats = PointStatistics ( \"ca_ozone_pts.shp\" , \"OZONE\" , 500 , NbrCircle ( 10000 , \"MAP\" ), \"SUM\" ) outPointStats.save ( \"C:/sapyexamples/output/pointstatsout\" )"}, "usage": ["When the field is integer, the available overlay statistic choices are Mean, Majority, Maximum, Median, Minimum, Minority, Range, STD, Sum, and Variety. When the field is floating point, the only allowed statistics are Mean, Maximum, Minimum, Range, STD, and Sum.", "For statistic types Majority, Maximum, Median, Minimum, Minority, Range, and Sum, the output data type of the raster will be the same as the input field type. For statistic types Mean and STD, the output raster will always be floating point. For Variety, the output raster will always be integer.", "If there aren't any points in the neighborhood of a raster cell, the Variety statistic assigns it a value of 0. For the other statistics, NoData is assigned."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features for which to calculate the statistics in a neighborhood around each output cell. The input can be either a point or multipoint feature class. ", "dataType": "Feature Layer"}, {"name": "field", "isOptional": false, "description": "Field can be any numeric field of the input point features. It can be the Shape field if the input features contain z. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": "Cell size for output raster dataset. This is the value in the environment if specifically set. If not set in the environment, it is the shorter of the width or height of the extent of the input feature dataset, in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "neighborhood", "isOptional": true, "description": "The Neighborhood class dictates the shape of the area around each input point used to calculate the statistic. The different types of neighborhood available are NbrAnnulus , NbrCircle , NbrRectangle , and NbrWedge . The following are the forms of the neighborhoods: The {CELL | MAP} parameter defines the distance units as either being Cell units or Map units. The default neighborhood is a square NbrRectangle with a width and height of 3 cells. NbrAnnulus({innerRadius}, {outerRadius}, {CELL | MAP}) NbrCircle({radius}, {CELL | MAP} NbrRectangle({width}, {height}, {CELL | MAP}) NbrWedge({radius}, {start_angle}, {end_angle}, {CELL | MAP})", "dataType": "Neighborhood"}, {"name": "statistics_type", "isOptional": true, "description": "The statistic type to be calculated. The calculation is performed on the values of the specified field of the point input in the neighborhood of each output raster cell. MEAN \u2014 Calculates the average of the field values in each neighborhood. MAJORITY \u2014 Determines the most frequently occurring field value in each neighborhood. In the case of a tie, the lower value is used. MAXIMUM \u2014 Determines the largest field value in each neighborhood. MEDIAN \u2014 Determines the median field value in each neighborhood. In the case of an even number of points in the neighborhood, the result will be the lower of the two middle values. MINIMUM \u2014 Determines the smallest field value in each neighborhood. MINORITY \u2014 Determines the least frequently occurring field value in each neighborhood. In the case of a tie, the lower value is used. RANGE \u2014 Calculates the range (difference between largest and smallest) of the field values in each neighborhood. STD \u2014 Calculates the standard deviation of the field values in each neighborhood. SUM \u2014 Calculates the total of the field values in each neighborhood. VARIETY \u2014 Calculates the number of unique field values in each neighborhood.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "LineStatistics (in_polyline_features, field, {cell_size}, {search_radius}, {statistics_type})", "name": "Line Statistics (Spatial Analyst)", "description": "Calculates a statistic on the attributes of lines in a circular neighborhood around each output cell. \r\n Learn more about how Line Statistics works", "example": {"title": "LineStatistics example 1 (Python window)", "description": "This example calculates the average length of line segments within a certain radius of each cell in the input raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" lineStatOut = LineStatistics ( \"streams\" , \"LENGTH\" , 50 , 500 , \"MEAN\" ) lineStatOut.save ( \"C:/sapyexamples/output/linestatout\" )"}, "usage": ["Only the part of a line within the neighborhood is considered for the Majority, Mean, Median, Minority, and Length statistics. For the others, it does not matter whether a part or the whole line is used.", "If there are no lines in the neighborhood of a raster cell, then the Variety and Length statistics assign a value of zero. For the other statistics, NoData is assigned.", "The statistic types Majority, Mean, Median, and Minority are weighted according to the length of the lines. For example, if a line is twice as long as another, its value is considered to occur twice as often.", "The values on the output raster will always be integer in the case of Variety. They will always be floating point for Mean and Length. For the other statistics, the output data type is the same as the input item value type.", "When the field is integer, the available overlay statistic choices are Mean, Majority, Maximum, Median, Minimum, Minority, Range, and Variety. When the field is floating point, the only allowed statistics are Mean, Maximum, Minimum, and Range.", "For statistic types Majority, Maximum, Median, Minimum, Minority, and Range, the output data type of the raster will be the same as the input field type. For statistics types Mean and Length, the output raster will always be floating point. For Variety, the output raster will always be integer."], "parameters": [{"name": "in_polyline_features", "isInputFile": true, "isOptional": false, "description": "The input polyline features for which to calculate the Line Statistics. ", "dataType": "Feature Layer"}, {"name": "field", "isOptional": false, "description": "The Field that will be used to calculate the specified statistic on. It can be any numeric field of the input line features. When the Statistics type is set to Length, the Field can be set to None. It can be the Shape field if the input features contain z. ", "dataType": "Field"}, {"name": "cell_size", "isOptional": true, "description": "Cell size for output raster dataset. This is the value in the environment if specifically set. If not set in the environment, it is the shorter of the width or height of the extent of the input feature dataset, in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "search_radius", "isOptional": true, "description": "Search radius to calculate the desired statistic within, in map units. The default radius is five times the output cell size. ", "dataType": "Double"}, {"name": "statistics_type", "isOptional": true, "description": "The Statistic type to be calculated. Statistics are calculated on the value of the specified field for all lines in the neighborhood. MEAN \u2014 Calculates the average field value in each neighborhood, weighted by the length.The form of the calculation is: Mean = (sum of (length * field_value)) / (sum_of_length) Only the part of the length that falls within the neighborhood is used. MAJORITY \u2014 Determines the value having the greatest length of line in the neighborhood. MAXIMUM \u2014 Determines the largest value in the neighborhood. MEDIAN \u2014 Determines the median value, weighted by the length.Conceptually, all line segments in the neighborhood are sorted by value and placed end-to-end in a straight line. The value of the segment at the midpoint of the straight line is the median. MINIMUM \u2014 Calculates smallest value in each neighborhood. MINORITY \u2014 The value having the least length of line in the neighborhood. RANGE \u2014 The range of values (maximum\u2013minimum). VARIETY \u2014 The number of unique values. LENGTH \u2014 The total line length in the neighborhood. If the value of the field is other than 1, the lengths are multiplied by the item value before adding them together. This option can be used when the Field is set to None.", "dataType": "String"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "FocalStatistics (in_raster, {neighborhood}, {statistics_type}, {ignore_nodata})", "name": "Focal Statistics (Spatial Analyst)", "description": "Calculates for each input cell location a statistic of the values within a specified neighborhood around it. \r\n Learn more about how Focal Statistics works", "example": {"title": "FocalStatistics example 1 (Python window)", "description": "This example calculates the least-frequently occuring value in a ring-shaped neighborhood around each cell in the input raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outFocalStat = FocalStatistics ( \"elevation\" , NbrAnnulus ( 5 , 10 , \"CELL\" ), \"MINORITY\" , \"NODATA\" ) outFocalStat.save ( \"C:/sapyexamples/output/focalstat01\" )"}, "usage": ["If the input raster is of floating-point type, only the Mean, Maximum, Minimum, Range, STD, and Sum statistics are available; the Majority, Minority, Median and Variety statistics are not permitted. If the input raster is of integer type, all the statistics types are available.", "When a circular, annulus-shaped, or wedge-shaped neighborhood is specified, some of the outer diagonal cells may not be considered in the calculations since the center of the cell must be encompassed within the neighborhood.", "The Irregular and Weight ", "Neighborhood", " types require a ", "Kernel file", " be specified. Kernel files should have a ", ".txt", " file extension.", "See the Irregular and Weight sections of ", "How Focal Statistics works", " for information on creating and using kernel files.", "Only for the statistics types of Mean, Standard Deviation, or Sum can the ", "Neighborhood", " type can be set to ", "Weight", ".", "Input NoData cells may receive a value in the output if the ", "Ignore NoData in calculations", " option is checked, provided at least one cell within the neighborhood has a valid value."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The raster to perform the focal statistics calculations on. ", "dataType": "Raster Layer"}, {"name": "neighborhood", "isOptional": true, "description": "The Neighborhood class dictates the shape of the area around each cell used to calculate the statistic. The different types of neighborhood available are NbrAnnulus , NbrCircle , NbrRectangle , NbrWedge , NbrIrregular , and NbrWeight . The following are the forms of the neighborhoods: The {CELL | MAP} parameter defines the distance units as either being Cell units or Map units. The default neighborhood is a square NbrRectangle with a width and height of 3 cells. NbrAnnulus({innerRadius}, {outerRadius}, {CELL | MAP}) NbrCircle({radius}, {CELL | MAP} NbrRectangle({width}, {height}, {CELL | MAP}) NbrWedge({radius}, {start_angle}, {end_angle}, {CELL | MAP}) NbrIrregular(kernel_file) NbrWeight(kernel_file)", "dataType": "Neighborhood","available_values": [ "NbrAnnulus" , "NbrCircle" , "NbrRectangle" , "NbrWedge" , "NbrIrregular" , "NbrWeight" ]}, {"name": "statistics_type", "isOptional": true, "description": "The statistic type to be calculated. The default statistic type is MEAN. MEAN \u2014 Calculates the mean (average value) of the cells in the neighborhood. MAJORITY \u2014 Calculates the majority (value that occurs most often) of the cells in the neighborhood. MAXIMUM \u2014 Calculates the maximum (largest value) of the cells in the neighborhood. MEDIAN \u2014 Calculates the median of the cells in the neighborhood. MINIMUM \u2014 Calculates the minimum (smallest value) of the cells in the neighborhood. MINORITY \u2014 Calculates the minority (value that occurs least often) of the cells in the neighborhood. RANGE \u2014 Calculates the range (difference between largest and smallest value) of the cells in the neighborhood. STD \u2014 Calculates the standard deviation of the cells in the neighborhood. SUM \u2014 Calculates the sum (total of all values) of the cells in the neighborhood. VARIETY \u2014 Calculates the variety (the number of unique values) of the cells in the neighborhood.", "dataType": "String"}, {"name": "ignore_nodata", "isOptional": true, "description": "Denotes whether NoData values are ignored by the statistic calculation. DATA \u2014 Specifies that if a NoData value exists within a neighborhood, the NoData value will be ignored. Only cells within the neighborhood that have data values will be used in determining the output value. This is the default. NODATA \u2014 Specifies that if any cell in a neighborhood has a value of NoData, the output for the processing cell will be NoData. With this option, the presence of a NoData value implies that there is insufficient information to determine the statistic value for the neighborhood.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "FocalFlow (in_surface_raster, {threshold_value})", "name": "Focal Flow (Spatial Analyst)", "description": "Determines the flow of the values in the input raster within each cell's immediate neighborhood. \r\n Learn more about how Focal Flow works", "example": {"title": "FocalFlow example 1 (Python window)", "description": "This example determines the binary representation of flow for the input raster with a threshold value of 10.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" focalFlowOut = FocalFlow ( \"elevation\" , 10 ) focalFlowOut.save ( \"C:/sapyexamples/output/flowout\" )"}, "usage": ["Focal Flow", " evaluates the eight immediate neighbors of a cell to determine the flow.", "The resulting values from the tool  measure flow into, not out of, a cell.", "The output values are derived from the binary representation of the results of the analysis."], "parameters": [{"name": "in_surface_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster for which to calculate the focal flow. The eight immediate neighbors of each cell are evaluated to determine the flow. The input raster can be integer or floating point. ", "dataType": "Raster Layer"}, {"name": "threshold_value", "isOptional": true, "description": "Defines a value that constitutes the threshold, which must be equaled or exceeded before flow can occur. The threshold value can be either an integer or floating-point value. If the difference between the value at a neighboring cell location and the value of the processing cell is less than or equal to the threshold value, the output will be 0 (or no flow). ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Filter (in_raster, {filter_type}, {ignore_nodata})", "name": "Filter (Spatial Analyst)", "description": "Performs either a smoothing (Low pass) or edge-enhancing (High pass) filter on a raster. \r\n Learn more about how Filter works", "example": {"title": "Filter example 1 (Python window)", "description": "This example applies a high-pass filter to the input raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" filterOut = Filter ( \"elevation\" , \"HIGH\" , \"DATA\" ) filterOut.save ( \"C:/sapyexamples/output/filtered\" )"}, "usage": ["The LOW filter option is an averaging (smoothing) filter. The HIGH filter option is an edge-enhancement filter.", "Input NoData cells may receive a value in the output if the ", "Ignore NoData in calculations", " option is checked, provided at least one cell within the filter neighborhood has a valid value.", "You can use the ", "Focal Statistics", " tool to create custom filters to your specification."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster on which to perform the filter operation. ", "dataType": "Raster Layer"}, {"name": "filter_type", "isOptional": true, "description": "The type of filter operation to perform. LOW \u2014 Traverses a low pass 3-by-3 filter over the raster. This option smooths the entire input raster and reduces the significance of anomalous cells.This is the default. HIGH \u2014 Traverses a high pass 3-by-3 filter over the raster. This option enhances the edges of subdued features in a raster.", "dataType": "String"}, {"name": "ignore_nodata", "isOptional": true, "description": "Denotes whether NoData values are ignored by the filter calculation. DATA \u2014 If a NoData value exists within the filter, the NoData value will be ignored. Only cells within the filter that have data values will be used in determining the output. NODATA \u2014 If a NoData value exists within the filter, the output for the processing cell will be NoData. With this option, the presence of a NoData value implies that there is insufficient information to determine the statistic value for the neighborhood.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BlockStatistics (in_raster, {neighborhood}, {statistics_type}, {ignore_nodata})", "name": "Block Statistics (Spatial Analyst)", "description": "Partitions the input into non-overlapping blocks and calculates the statistic of the values within each block. The value is assigned to all of the cells in each block in the output. \r\n Learn more about how Block Statistics works", "example": {"title": "BlockStatistics example 1 (Python window)", "description": "This sample calculates the minimum cell value within each non-overlapping annulus (doughnut-shaped) neighborhood in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" nbr = NbrAnnulus ( 1 , 3 , \"MAP\" ) outBlockStat = BlockStatistics ( \"block\" , nbr , \"MINIMUM\" , \"\" ) outBlockStat.save ( \"C:/sapyexamples/output/blockstat\" )"}, "usage": ["If the input raster is of floating-point type, only the Mean, Maximum, Minimum, Range, STD, and Sum statistics are available; the Majority, Minority, Median and Variety statistics are not permitted. If the input raster is of integer type, all the statistics types are available.", "When a circular, annulus-shaped, or wedge-shaped neighborhood is specified, depending on the size of the neighborhood, cells that are not perpendicular to the x- or y-axis may not be considered in the calculations. However, these cell locations will receive the resulting value from the calculations of the neighborhood because they fall within the minimum-bounding rectangle (or the output block) of these circular neighborhood types.", "If the input raster is integer, the output raster will be integer. An exception is for the Mean or STD statistics types, for which the output raster will always be floating point. If the input type is float, the output will be float for all of the available statistics types.", "The Irregular and Weight ", "Neighborhood", " types require a ", "Kernel file", " be specified. Kernel files should have a ", ".txt", " file extension.", "See the Irregular and Weight sections of ", "How Block Statistics works", " for information on creating and using kernel files.", "For statistics type Median, if the number of cells in the block is odd, the values are ranked and the middle value is reported as the median and is an integer. If the number of cells in the block is even, the values are ranked and the middle two values are averaged to the nearest integer.", "For statistics type Majority, cells where there is no single majority value\u2014that is, two or more values within a block are tied as having the most number of cells with the value\u2014will be assigned NoData. For statistics type Minority, cells where there is no single minority value will also be assigned NoData.", "When the Statistic type is Mean, Minority, Standard Deviation, or Sum, the ", "Neighborhood", " type can be set to ", "Weight", "."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The raster on which to perform the Block Statistics calculations. ", "dataType": "Raster Layer"}, {"name": "neighborhood", "isOptional": true, "description": "The Neighborhood class dictates the shape of the area around each cell used to calculate the statistic. The different types of neighborhood available are NbrAnnulus , NbrCircle , NbrRectangle , NbrWedge , NbrIrregular , and NbrWeight . The following are the forms of the neighborhoods: The {CELL | MAP} parameter defines the distance units as either being Cell units or Map units. The default neighborhood is a square NbrRectangle with a width and height of 3 cells. NbrAnnulus({innerRadius}, {outerRadius}, {CELL | MAP}) NbrCircle({radius}, {CELL | MAP} NbrRectangle({width}, {height}, {CELL | MAP}) NbrWedge({radius}, {start_angle}, {end_angle}, {CELL | MAP}) NbrIrregular(kernel_file) NbrWeight(kernel_file)", "dataType": "Neighborhood"}, {"name": "statistics_type", "isOptional": true, "description": "The statistic type to be calculated. The default statistic type is MEAN. MEAN \u2014 Calculates the mean (average value) of the cells in the neighborhood. MAJORITY \u2014 Calculates the majority (value that occurs most often) of the cells in the neighborhood. MAXIMUM \u2014 Calculates the maximum (largest value) of the cells in the neighborhood. MEDIAN \u2014 Calculates the median of the cells in the neighborhood. MINIMUM \u2014 Calculates the minimum (smallest value) of the cells in the neighborhood. MINORITY \u2014 Calculates the minority (value that occurs least often) of the cells in the neighborhood. RANGE \u2014 Calculates the range (difference between largest and smallest value) of the cells in the neighborhood. STD \u2014 Calculates the standard deviation of the cells in the neighborhood. SUM \u2014 Calculates the sum (total of all values) of the cells in the neighborhood. VARIETY \u2014 Calculates the variety (the number of unique values) of the cells in the neighborhood.", "dataType": "String"}, {"name": "ignore_nodata", "isOptional": true, "description": "Denotes whether NoData values are ignored by the statistic calculation. DATA \u2014 Specifies that if a NoData value exists within a block neighborhood, the NoData value will be ignored. Only cells within the neighborhood that have data values will be used in determining the output value. This is the default. NODATA \u2014 Specifies that if any cell in a neighborhood has a value of NoData, the output for each cell in the corresponding block will receive NoData. With this option, the presence of a NoData value implies that there is insufficient information to determine the statistic value for the neighborhood.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PrincipalComponents (in_raster_bands, {number_components}, {out_data_file})", "name": "Principal Components (Spatial Analyst)", "description": "Performs Principal Component Analysis (PCA) on a set of raster bands and generates a single multiband raster as output. \r\n Learn more about how Principal Components works", "example": {"title": "PrincipalComponents example 1 (Python window)", "description": "This example performs Principal Component Analysis (PCA) on an input multiband raster and generates a multiband raster output.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outPrincipalComp = PrincipalComponents ([ \"redlands\" ], 4 , \"pcdata.txt\" ) outPrincipalComp.save ( \"C:/sapyexamples/output/outpc01\" )"}, "usage": ["The value specified for the number of principal components determines the number of principal component bands in the output multiband raster. The number must not be larger than the total number of raster bands in the input.", "The raster bands must have a common intersection. If there is none, an error occurs and no output is created.", "With the output data file name specified, the correlation and covariance matrices, and the eigenvalues and eigenvectors, as well as the percent variance each eigenvalue captures, and the accumulative variance described, will be stored in an ASCII file.", "The percent variance identifies the amount of the variance each eigenvalue captures. This can be useful to help interpret the results of PCA. If a few eigenvalues (each corresponding to bands in the output raster) capture the majority of the variance, then it may be adequate to use this subset of bands in a subsequent analysis since they may capture the majority of the interactions within the original multiband dataset.", "When  determining the percent variance each eigenvalue captures, the sum of eigenvalues is entered into the following formula: (eigenvalue * 100)/Sum. The first eigenvalue (and its associated band) captures the greatest variance, and the subsequent eigenvalues capture sequentially lesser variance. The accumulative percent of variance is a sequential sum of the variance each eigenvalue captures.", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool."], "parameters": [{"name": "in_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands. ", "dataType": "Raster Layer"}, {"name": "number_components", "isOptional": true, "description": "Number of principal components. The number must be greater than zero and less than or equal to the total number of input raster bands. The default is the total number of rasters in the input. ", "dataType": "Long"}, {"name": "out_data_file", "isOutputFile": true, "isOptional": true, "description": "Output ASCII data file storing principal component parameters. The extension for the output file can be .txt or .asc . ", "dataType": "File"}, {"isOutputFile": true, "name": "out_multiband_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "MLClassify (in_raster_bands, in_signature_file, {reject_fraction}, {a_priori_probabilities}, {in_a_priori_file}, {out_confidence_raster})", "name": "Maximum Likelihood Classification (Spatial Analyst)", "description": "Performs a maximum likelihood classification on a set of raster bands and creates a classified raster as output. \r\n Learn more about how Maximum Likelihood Classification works", "example": {"title": "MaximimumLikelihoodClassification example 1 (Python window)", "description": "This example creates an output classified raster containing five classes derived from an input signature file and a multiband raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" mlcOut = MLClassify ( \"redlands\" , \"c:/sapyexamples/data/wedit5.gsg\" , \"0.0\" , \"EQUAL\" , \"\" , \"c:/sapyexamples/output/redmlcconf\" ) mlcOut.save ( \"c:/sapyexamples/output/redmlc\" )"}, "usage": ["Any signature file created by the ", "Create Signature", ", ", "Edit Signature", ", or ", "Iso Cluster", " tools is a valid entry for the input signature file. These will have a ", ".gsg", " extension.", "By default, all cells in the output raster will be classified, with each class having equal probability weights attached to their signatures.", "The input a priori probability file must be an ASCII file consisting of two columns. The values in the left column represent class IDs. The values in the right column represent the a priori probabilities for the respective classes. Valid values for class a priori probabilities must be greater than or equal to zero. If zero is specified as a probability, the class will not appear on the output raster. The sum of the specified a priori probabilities must be less than or equal to one. The format of the file is as follows:", "The classes omitted in the file will receive the average a priori probability of the remaining portion of the value of one. In the above example, all classes from 1 to 8 are represented in the signature file. The a priori probabilities of classes 3 and 6 are missing in the input a priori probability file. Since the sum of all probabilities specified in the above file is equal to 0.8, the remaining portion of the probability (0.2) is divided by the number of classes not specified (2). Therefore, classes 3 and 6 will each be assigned a probability of 0.1.", "A specified reject fraction, which lies between any two valid values, will be assigned to the next upper valid value. For example, 0.02 will become 0.025.", "There is a direct relationship between the number of unclassified cells on the output raster resulting from the reject fraction and the number of cells represented by the sum of levels of confidence smaller than the respective value entered for the reject fraction.", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool.", " If the Class Name in the signature file is different than the Class ID, then an additional field will be added to the output raster attribute table called CLASSNAME. For each class in the output table  CLASSNAME will contain the Class Name associated with the class. For example, if the Class Names for the classes in the signature file are descriptive string names (for example, conifers, water, and urban), these names will be carried to the CLASSNAME field.", "The extension for an input a priori probability file is ", ".txt", "."], "parameters": [{"name": "in_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands. ", "dataType": "Raster Layer"}, {"name": "in_signature_file", "isInputFile": true, "isOptional": false, "description": "The input signature file whose class signatures are used by the maximum likelihood classifier. A .gsg extension is required. ", "dataType": "File"}, {"name": "reject_fraction", "isOptional": true, "description": "Portion of cells that will remain unclassified due to the lowest possibility of correct assignments. The default is 0.0 ; therefore, every cell will be classified. The 14 valid entries are 0.0 , 0.005 , 0.01 , 0.025 , 0.05 , 0.1 , 0.25 , 0.5 , 0.75 , 0.9 , 0.95 , 0.975 , 0.99 , and 0.995 . ", "dataType": "String"}, {"name": "a_priori_probabilities", "isOptional": true, "description": "Specifies how a priori probabilities will be determined. EQUAL \u2014 All classes will have the same a priori probability. SAMPLE \u2014 A priori probabilities will be proportional to the number of cells in each class relative to the total number of cells sampled in all classes in the signature file. FILE \u2014 The a priori probabilities will be assigned to each class from an input ASCII a priori probability file.", "dataType": "String"}, {"name": "in_a_priori_file", "isInputFile": true, "isOptional": true, "description": "A text file containing a priori probabilities for the input signature classes. An input for the a priori probability file is only required when the FILE option is used. The extension for the a priori file can be .txt or .asc . ", "dataType": "File"}, {"name": "out_confidence_raster", "isOutputFile": true, "isOptional": true, "description": "Output confidence raster dataset showing the certainty of the classification in 14 levels of confidence, with the lowest values representing the highest reliability. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_classified_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "IsoClusterUnsupervisedClassification (Input_raster_bands, Number_of_classes, {Minimum_class_size}, {Sample_interval}, {Output_signature_file})", "name": "Iso Cluster Unsupervised Classification (Spatial Analyst)", "description": "Performs unsupervised classification on a series of input raster bands using the  Iso Cluster  and  Maximum Likelihood Classification  tools. Learn more about how the Interactive Supervised Classification tool works", "example": {"title": "IsoClusterUnsupervisedClassification example 1 (Python window)", "description": "This example performs an unsupervised classification classifying the input bands into 5 classes and outputs a classified raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outUnsupervised = IsoClusterUnsupervisedClassification ( \"redlands\" , 5 , 20 , 50 ) outUnsupervised.save ( \"c:/temp/unsup01\" )"}, "usage": ["This tool combines the functionalities of the ", "Iso Cluster", " and ", "Maximum Likelihood Classification", " tools. It outputs a classified raster. It optionally outputs a signature file. ", "The resulting signature file from this tool can be used as the input for another classification tool, such as ", "Maximum Likelihood Classification", ", for greater control over the classification parameters.", "The minimum valid value for the number of classes is two. There is no maximum number of clusters. In general, more clusters require more iterations.", "To provide the sufficient statistics necessary to generate a signature file for a future classification, each cluster should contain enough cells to accurately represent the cluster. The value entered for the minimum class size should be approximately 10 times larger than the number of layers in the input raster bands.", "The value entered for the sample interval indicates one cell out of every n-by-n block of cells is used in the cluster calculations.", "You shouldn't merge or remove classes or change any of the statistics of the ASCII signature file.", "Generally, the more cells contained in the extent of the intersection  of the input bands, the larger the values for minimum class size and sample interval should be specified. Values entered for the sample interval should be small enough that the smallest desirable categories existing in the input data will be appropriately sampled.", "The class ID values on the output signature file start at one and sequentially increase to the number of input classes. The assignment of the class numbers is arbitrary.", "The output signature file's name must have a ", ".gsg", " extension.", "Better results will be obtained if all input bands have the same data ranges. If the bands have vastly different data ranges, the data ranges can be transformed to the same range using Map Algebra to perform the equation.", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool."], "parameters": [{"name": "Input_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands. ", "dataType": "Raster Layer"}, {"name": "Number_of_classes", "isOptional": false, "description": "Number of classes into which to group the cells. ", "dataType": "Long"}, {"name": "Minimum_class_size", "isOptional": false, "description": "Minimum number of cells in a valid class. The default is 20. ", "dataType": "Long"}, {"name": "Sample_interval", "isOptional": false, "description": "The interval to be used for sampling. The default is 10. ", "dataType": "Long"}, {"name": "Output_signature_file", "isOutputFile": true, "isOptional": false, "description": "The output signature file. A .gsg extension must be specified. ", "dataType": "File"}, {"isOutputFile": true, "name": "Output_classified_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "IsoCluster (in_raster_bands, out_signature_file, number_classes, {number_iterations}, {min_class_size}, {sample_interval})", "name": "Iso Cluster (Spatial Analyst)", "description": "Uses an isodata clustering algorithm to determine the characteristics of the natural groupings of cells in multidimensional attribute space and stores the results in an output ASCII signature file. \r\n Learn more about how Iso Cluster works", "example": {"title": "IsoCluster example 1 (Python window)", "description": "This example creates a signature file for classifying the input multiband raster into five classes.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" IsoCluster ( \"redlands\" , \"c:/sapyexamples/output/isosig.gsg\" , 5 , 20 , 50 , 15 )"}, "usage": ["Iso Cluster performs clustering of the multivariate data combined in a list of input bands. The resulting signature file can be used as the input for a classification tool, such as ", "Maximum Likelihood Classification", ", that produces an unsupervised classification raster.", "The minimum valid value for the number of classes is two. There is no maximum number of clusters. In general, more clusters require more iterations.", "To provide the sufficient statistics necessary to generate a signature file for a future classification, each cluster should contain enough cells to accurately represent the cluster. The value entered for the minimum class size should be approximately 10 times larger than the number of layers in the input raster bands.", "The value entered for the sample interval indicates one cell out of every n-by-n block of cells is used in the cluster calculations.", "You shouldn't merge or remove classes or change any of the statistics of the ASCII signature file.", "Generally, the more cells contained in the extent of the intersection  of the input bands, the larger the values for minimum class size and sample interval should be specified. Values entered for the sample interval should be small enough that the smallest desirable categories existing in the input data will be appropriately sampled.", "The class ID values on the output signature file start at one and sequentially increase to the number of input classes. The assignment of the class numbers is arbitrary.", "Better results will be obtained if all input bands have the same data ranges. If the bands have vastly different data ranges, the data ranges can be transformed to the same range using Map Algebra to perform the equation.", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool."], "parameters": [{"name": "in_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands. ", "dataType": "Raster Layer"}, {"name": "out_signature_file", "isOutputFile": true, "isOptional": false, "description": "The output signature file. A .gsg extension must be specified. ", "dataType": "File"}, {"name": "number_classes", "isOptional": false, "description": "Number of classes into which to group the cells. ", "dataType": "Long"}, {"name": "number_iterations", "isOptional": true, "description": "Number of iterations of the clustering process to run. The default is 20. ", "dataType": "Long"}, {"name": "min_class_size", "isOptional": true, "description": "Minimum number of cells in a valid class. The default is 20. ", "dataType": "Long"}, {"name": "sample_interval", "isOptional": true, "description": "The interval to be used for sampling. The default is 10. ", "dataType": "Long"}]},
{"syntax": "EditSignatures (in_raster_bands, in_signature_file, in_signature_remap_file, out_signature_file, {sample_interval})", "name": "Edit Signatures (Spatial Analyst)", "description": "Edits and updates a signature file by merging, renumbering, and deleting class signatures. \r\n Learn more about how Edit Signatures works", "example": {"title": "EditSignatures example 1 (Python window)", "description": "This example will edit the signature file based on the input remap file.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" EditSignatures ( \"redl123\" , \"c:/sapyexamples/data/zsamp12.gsg\" , \"c:/sapyexamples/data/zsamp7.rmp\" , \"c:/sapyexamples/output/redlremap.gsg\" , \"\" )"}, "usage": ["The ", "Edit Signatures", " tool allows the modification of an existing signature file by all or any of the following operations:", "The input signature file must be an ASCII signature file. The file can be the output of any Multivariate tool that produces the file containing the required statistical information\u2014for example, ", "Iso Cluster ", " and ", "Create Signatures", ". The file must have a minimum of two classes. Such a file can be recognized by its ", ".gsg", " extension.", "The input signature remap file is an ASCII file consisting of two columns of values per line, separated by a colon. The first column is the  value of the original class ID.  The second column contains the new class IDs for updating in the signature file. All of the entries in the file must be sorted in ascending order based on the first column", "To merge a set of classes, put the same new class ID for the second value for each class ID of that set. Only classes that need to be edited have to be placed in the signature remap file; any class not present in the remap file will remain unchanged. To delete a class signature, use -9999 as the value for the second column of that class. A class ID can also be renumbered to a value that does not exist in the input signature file. The following is an example of an input signature remap file:", "The example above will merge classes 2 and 9 with 3, merge class 4 with 11, and delete class 5.", "If the input signature file carries names for the class signatures and if the signatures in the input signature remap file are to be merged, the name associated with the value in which to merge will be transferred to the output signature file.", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool."], "parameters": [{"name": "in_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands for which to edit the signatures. ", "dataType": "Raster Layer"}, {"name": "in_signature_file", "isInputFile": true, "isOptional": false, "description": "Input signature file whose class signatures are to be edited. A .gsg extension is required. ", "dataType": "File"}, {"name": "in_signature_remap_file", "isInputFile": true, "isOptional": false, "description": "Input ASCII remap table containing the class IDs to be merged, renumbered, or deleted. The extension can be .rmp , .asc or .txt . The default is .rmp . ", "dataType": "File"}, {"name": "out_signature_file", "isOutputFile": true, "isOptional": false, "description": "The output signature file. A .gsg extension must be specified. ", "dataType": "File"}, {"name": "sample_interval", "isOptional": true, "description": "The interval to be used for sampling. The default is 10. ", "dataType": "Long"}]},
{"syntax": "Dendrogram (in_signature_file, out_dendrogram_file, {distance_calculation}, {line_width})", "name": "Dendrogram (Spatial Analyst)", "description": "Constructs a tree diagram (dendrogram) showing attribute distances between sequentially merged classes in a signature file. \r\n Learn more about how Dendrogram works", "example": {"title": "Dendrogram example 1 (Python window)", "description": "This example takes an input signature file and creates a Dendrogram view.", "code": "import arcpy from arcpy.sa import * Dendrogram ( \"c:/sapyexamples/data/zsamp12.gsg\" , \"c:/sapyexamples/output/z12dendro.txt\" , \"VARIANCE\" , \"\" )"}, "usage": ["The input signature file must in the prescribed signature file format. A signature file can be created with the ", "Iso Cluster", " or ", "Create Signatures", " tools. The file must have a minimum of two classes. A signature file can be recognized by its ", ".gsg", " extension.", "The output of ", "Dendrogram", " is an ASCII text file. The file has two components: a table and a graph.", "The first component is a table of distances between pairs of classes, presented in the   sequence for merging. The second component is a graphical representation using ASCII characters of the classes that demonstrates the relationships and the hierarchy of the merging. The graph illustrates relative distances between pairs of merged classes in the signature file, which are based on statistically determined similarities. The classes themselves represent clusters of cells or cells from training samples extracted from the study site.", "By analyzing the graph and the associate table, you can determine the potential of merging classes.", "The default extension for the output text file is ", ".txt", ". It can also be ", ".asc", ".", "The proximity of a pair of classes within a signature file is measured by the attribute distance.", "The value entered for the line width specifies the width of the graph based on the number of characters. The default value of 78 is also the minimum valid number of characters. If numbers less than this are entered, the default value of 78 will be applied. When specifying values higher than the default, the resolution of the graph will increase, which may provide more accurate interpolation of the distances.", "To make the display of the dendrogram meaningful, the ASCII file should be displayed with a nonproportional font such as Courier."], "parameters": [{"name": "in_signature_file", "isInputFile": true, "isOptional": false, "description": "Input signature file whose class signatures are used to produce a dendrogram. A .gsg extension is required. ", "dataType": "File"}, {"name": "out_dendrogram_file", "isOutputFile": true, "isOptional": false, "description": "The output dendrogram ASCII file. The extension can be .txt or .asc . ", "dataType": "File"}, {"name": "distance_calculation", "isOptional": true, "description": "Specifies the manner in which the distances between classes in multidimensional attribute space are defined. VARIANCE \u2014 The distances between classes will be computed based on the variances and the Euclidean distance between the means of their signatures. MEAN_ONLY \u2014 The distances between classes will be determined by the Euclidean distances between the means of the class signatures only. ", "dataType": "Boolean"}, {"name": "line_width", "isOptional": true, "description": "Sets the width of the dendrogram in number of characters on a line. The default is 78. ", "dataType": "Long"}]},
{"syntax": "CreateSignatures (in_raster_bands, in_sample_data, out_signature_file, {compute_covariance}, {sample_field})", "name": "Create Signatures (Spatial Analyst)", "description": "Creates an ASCII signature file of classes defined by input sample data and a set of raster bands. \r\n Learn more about how Create Signatures works", "example": {"title": "CreateSignatures example 1 (Python window)", "description": "This example creates a signature file for classes defined by sampled training areas and a set of input raster bands.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" CreateSignatures ( \"sb\" , \"sbtrain\" , \"c:/sapyexamples/output/rbsig.gsg\" , \"COVARIANCE\" , \"\" )"}, "usage": ["A ", ".gsg", " extension should be used for the output signature file.", "The input raster bands and the input raster or feature sample data must have overlapping extents. The statistics will be computed for the common area only.", "The minimum valid number of class samples in the sample data is two. There is no maximum number of classes.", "If the signature file is to be used in further multivariate analysis tools that use covariance matrices, such as ", "Maximum Likelihood Classification", " and ", "Class Probability", ", the covariance matrices must be present. This information is generated when the ", "Compute Covariance Matrices", " option in the dialog box is enabled, or the COVARIANCE option is specified in scripting. Note that this is the default setting. See ", "How Create Signatures works", " to compare signature files when the covariance matrices are generated versus only the means.", "You should not change anything in the signature file except to enter the names of classes. The statistics in the file should be created and altered by Multivariate tools only.", "The names of the classes in the output signature file are optional. They're used for reference only. The class names can be entered through the ", "Sample field", " or any text editor can be used on the resulting signature file to input the names. Each class name must consist of a single string no more than 31 alphanumeric characters in length.", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool."], "parameters": [{"name": "in_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands for which to create the signatures. ", "dataType": "Raster Layer"}, {"name": "in_sample_data", "isInputFile": true, "isOptional": false, "description": "The input delineating the set of class samples. The input can be an integer raster or a feature dataset. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "out_signature_file", "isOutputFile": true, "isOptional": false, "description": "The output signature file. A .gsg extension must be specified. ", "dataType": "File"}, {"name": "compute_covariance", "isOptional": true, "description": "Specifies whether covariance matrices in addition to the means are calculated. COVARIANCE \u2014 Both the covariance matrices and the means for all classes of the in_sample_data will be computed. This is the default. MEAN_ONLY \u2014 Only the means for all classes of the in_sample_data will be calculated. ", "dataType": "Boolean"}, {"name": "sample_field", "isOptional": true, "description": "Field of the input raster or feature sample data to assign values to the sampled locations (classes). Only integer or string fields are valid fields. The specified number or string will be used as the Class name in the output signature file. ", "dataType": "Field"}]},
{"syntax": "ClassProbability (in_raster_bands, in_signature_file, {maximum_output_value}, {a_priori_probabilities}, {in_a_priori_file})", "name": "Class Probability (Spatial Analyst)", "description": "Creates a multiband raster of probability bands, with one band being created for each class represented in the input signature file. \r\n Learn more about how Class Probability works", "example": {"title": "ClassProbability example 1 (Python window)", "description": "This example creates a multiband raster of probability bands for each class in a signature file.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outClassProbability = ClassProbability ( \"redlands\" , \"C:/sapyexamples/data/wedit5.gsg\" , 100 , \"EQUAL\" , \"\" ) outClassProbability.save ( \"c:/sapyexamples/output/classprob\" )"}, "usage": ["Any signature file created by the ", "Create Signature", ", ", "Edit Signature", ", or ", "Iso Cluster", " tools is a valid entry for the input signature file. These will have a ", ".gsg", " extension.", "This tool employs Bayesian statistics to estimate class probabilities. Bayesian statistics involves starting with prior information about the data, then updating that information after the data is collected. The prior information about the data values is quantified with a priori probabilities, which are then adjusted by the likelihood function to receive posterior probabilities (the updated information). The likelihood function is defined by the data values for each class/cluster.", "The input a priori probability file must be an ASCII file consisting of two columns. The values in the left column represent class IDs. The values in the right column represent the a priori probabilities for the respective classes. Valid values for class a priori probabilities must be greater than or equal to zero. If zero is specified as a probability, no associated probability band will be created for the class  in the output multiband raster. The sum of the specified a priori probabilities must be less than or equal to one. An example showing the format of the file as follows:", "The classes omitted in the file will receive the average a priori probability of the remaining portion of the value of one. In the above example, all classes from 1 to 8 are represented in the signature file. The a priori probabilities of classes 3 and 6 are missing in the input a priori probability file. Since the sum of all probabilities specified in the above file is equal to 0.8, the remaining portion of the probability (0.2) is divided by the number of classes not specified (2). Therefore, classes 3 and 6 will each be assigned a probability of 0.1.", "The extension for the input a priori probability file can be ", ".txt", " or ", ".asc", ".", "The value entered for maximum output value sets the upper range of the values in the output probability bands. The default value of 100 creates a multiband raster with each band containing integer values ranging from 0 to 100. Any integer value greater than zero is valid for maximum output value. Only the value of one for the maximum output value argument will result in bands having floating-point values.", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool."], "parameters": [{"name": "in_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands. Raster bands can be integer or floating point. ", "dataType": "Raster Layer"}, {"name": "in_signature_file", "isInputFile": true, "isOptional": false, "description": "Input signature file whose class signatures are used to generate the a priori probability bands. A .gsg extension is required. ", "dataType": "File"}, {"name": "maximum_output_value", "isOptional": true, "description": "Factor for scaling the range of values in the output probability bands. By default, the values range from 0 to 100. ", "dataType": "Long"}, {"name": "a_priori_probabilities", "isOptional": true, "description": "Specifies how a priori probabilities will be determined. EQUAL \u2014 All classes will have the same a priori probability. SAMPLE \u2014 A priori probabilities will be proportional to the number of cells in each class relative to the total number of cells sampled in all classes in the signature file. FILE \u2014 The a priori probabilities will be assigned to each class from an input ASCII a priori probability file.", "dataType": "String"}, {"name": "in_a_priori_file", "isInputFile": true, "isOptional": true, "description": "A text file containing a priori probabilities for the input signature classes. An input for the a priori probability file is only required when the FILE option is used. The extension for the a priori file can be .txt or .asc . ", "dataType": "File"}, {"isOutputFile": true, "name": "out_multiband_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BandCollectionStats (in_raster_bands, out_stat_file, {compute_matrices})", "name": "Band Collection Statistics (Spatial Analyst)", "description": "Calculates the statistics for a set of raster bands. \r\n Learn more about how Band Collection Statistics works", "example": {"title": "BandCollectionStatistics example 1 (Python window)", "description": "This example calculates the statistics for a set of raster bands.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" BandCollectionStats ( \"redlands\" , \"c:/sapyexamples/output/redbandstats.txt\" , \"BRIEF\" )"}, "usage": ["The raster bands must have a common intersection. If there is none, an error occurs and no output is created.", "If the extents of the raster bands are not the same, the statistics will be calculated on the common spatial extent of all the input raster bands. The cell size will be that of the maximum of the input rasters, by default; otherwise, it will depend on the ", "Raster Analysis", " environment settings.", "The default setting of the ", "Compute matrices", " parameter (BRIEF in scripting, unchecked in the tool dialog box) is to only compute the minimum, maximum, mean, and standard deviation of the input raster bands. To calculate these statistics and also the covariance and correlation matrices, set the parameter to DETAILED in scripting, or checked in the tool dialog box. A covariance matrix presents the variances of all raster bands along the diagonal from the upper left to lower right and covariances between all raster bands in the remaining entries. The correlation matrix provides the correlation coefficients between each combination of two input bands.", " In the calculation of the covariance matrix, the mean value of the band is used for any input cells that are NoData.", "The statistics are written to the output file in ASCII text format. The extension for the output must be ", ".txt", ".", "If the input is a layer created from a multiband raster with more than three bands, the operation will  consider all the bands associated with the source dataset, not just the three bands that were loaded (symbolized) by the layer.", "There are several ways you can specify a subset of bands from a multiband raster to use as input into the tool."], "parameters": [{"name": "in_raster_bands", "isInputFile": true, "isOptional": false, "description": "The input raster bands. ", "dataType": "Raster Layer"}, {"name": "out_stat_file", "isOutputFile": true, "isOptional": false, "description": "The output ASCII file containing the statistics. A .txt extension is required. ", "dataType": "File"}, {"name": "compute_matrices", "isOptional": true, "description": "Specifies whether covariance and correlation matrices are calculated. BRIEF \u2014 Only the basic statistical measures (minimum, maximum, mean, and standard deviation) will be calculated for every layer. This is the default. DETAILED \u2014 In addition to the standard statistics calculated with {BRIEF}, the covariance and correlation matrices will also be determined. ", "dataType": "Boolean"}]},
{"syntax": "BitwiseXOr (in_raster_or_constant1, in_raster_or_constant2)", "name": "Bitwise XOr (Spatial Analyst)", "description": "Performs a Bitwise eXclusive Or operation on the binary values of two input rasters. \r\n Learn more about how Bitwise Math tools work", "example": {"title": "BitwiseXOr example 1 (Python window)", "description": "This example performs a Bitwise XOr operation on two GRID rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBitwiseXOr = BitwiseXOr ( \"degs\" , \"negs\" ) outBitwiseXOr.save ( \"C:/sapyexamples/output/outbitxor\" )"}, "usage": ["Two inputs are necessary for this bitwise operation to take place.", "The order of inputs is irrelevant for this tool.", "If an input is floating-point, the values are converted to integer values through truncation before the bitwise operation is performed.", "In bitwise operations:", "The Bitwise XOr operation treats the sign bit as it would any other bit. If one or both inputs for a cell location are negative, the output is negative; if both inputs are positive, the output is positive."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this bitwise operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this bitwise operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BitwiseRightShift (in_raster_or_constant1, in_raster_or_constant2)", "name": "Bitwise Right Shift (Spatial Analyst)", "description": "Performs a Bitwise Right Shift operation on the binary values of two input rasters. \r\n Learn more about how Bitwise Math tools work", "example": {"title": "BitwiseRightShift example 1 (Python window)", "description": "This example right-shifts the values of the first input by the number of  bits defined by the second input, and outputs the result as a GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBitwiseRShift = BitwiseRightShift ( \"degs\" , \"negs\" ) outBitwiseRShift.save ( \"C:/sapyexamples/output/outbitrs\" )"}, "usage": ["Two inputs are necessary for this bitwise operation to take place.", "The order of inputs is relevant for this tool.", "If an input is floating-point, the values are converted to integer values through truncation before the bitwise operation is performed.", "In bitwise operations:", "The Bitwise Right Shift operation does no wrapping of bits. The rightmost bit is dropped.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", ">>", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input on which to perform the shift. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input defining the number of positions to shift the bits. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BitwiseOr (in_raster_or_constant1, in_raster_or_constant2)", "name": "Bitwise Or (Spatial Analyst)", "description": "Performs a Bitwise Or operation on the binary values of two input rasters. \r\n Learn more about how Bitwise Math tools work", "example": {"title": "BitwiseOr example 1 (Python window)", "description": "This example performs a Bitwise Or operation on two GRID rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBitwiseOr = BitwiseOr ( \"degs\" , \"negs\" ) outBitwiseOr.save ( \"C:/sapyexamples/output/outbitor\" )"}, "usage": ["Two inputs are necessary for this bitwise operation to take place.", "The order of inputs is irrelevant for this tool.", "If an input is floating-point, the values are converted to integer values through truncation before the bitwise operation is performed.", "In bitwise operations:", "The Bitwise Or operation treats the sign bit as it would any other bit. If one or both inputs for a cell location are negative, the output is negative; if both inputs are positive, the output is positive."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this bitwise operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this bitwise operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BitwiseNot (in_raster_or_constant)", "name": "Bitwise Not (Spatial Analyst)", "description": "Performs a Bitwise Not (complement) operation on the binary value of an input raster. \r\n Learn more about how Bitwise Math tools work", "example": {"title": "BitwiseNot example 1 (Python window)", "description": "This example performs a Bitwise Not operation on an input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBitwiseNot = BitwiseNot ( \"degs\" ) outBitwiseNot.save ( \"C:/sapyexamples/output/outbitn\" )"}, "usage": ["If an input is floating-point, the values are converted to integer values through truncation before the bitwise operation is performed.", "In bitwise operations:", "The Bitwise Not operation treats the sign bit as it would any other bit. If the input for a cell location is negative, the output is negative; if the input is positive, the output is positive."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster on which to perform the Bitwise Not (complement) operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BitwiseLeftShift (in_raster_or_constant1, in_raster_or_constant2)", "name": "Bitwise Left Shift (Spatial Analyst)", "description": "Performs a Bitwise Left Shift operation on the binary values of two input rasters. \r\n Learn more about how Bitwise Math tools work", "example": {"title": "BitwiseLeftShift example 1 (Python window)", "description": "This example left-shifts the values of the first input by the number of  bits defined by the second input, and outputs the result as a TIFF raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBitwiseLS = BitwiseLeftShift ( \"degs\" , \"negs\" ) outBitwiseLS.save ( \"C:/sapyexamples/output/outbitls.tif\" )"}, "usage": ["Two inputs are necessary for this bitwise operation to take place.", "The order of inputs is relevant for this tool.", "If an input is floating-point, the values are converted to integer values through truncation before the bitwise operation is performed.", "In bitwise operations:", "The Bitwise Left Shift operation does no wrapping of bits. The leftmost bit is dropped.", "In Map Algebra, the equivalent ", "operator", " symbol for this tool is \"", "<<", "\" (", "link", ")."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input on which to perform the shift. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input defining the number of positions to shift the bits. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "BitwiseAnd (in_raster_or_constant1, in_raster_or_constant2)", "name": "Bitwise And (Spatial Analyst)", "description": "Performs a Bitwise And operation on the binary values of two input rasters. \r\n Learn more about how Bitwise Math tools work", "example": {"title": "BitwiseAnd example 1 (Python window)", "description": "This example performs a Bitwise And operation on two GRID rasters.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outBitwiseAnd = BitwiseAnd ( \"degs\" , \"negs\" ) outBitwiseAnd.save ( \"C:/sapyexamples/output/bitand\" )"}, "usage": ["Two inputs are necessary for this bitwise operation to take place.", "The order of inputs is irrelevant for this tool.", "If an input is floating-point, the values are converted to integer values through truncation before the bitwise operation is performed.", "In bitwise operations:", "The Bitwise And operation treats the sign bit as it would any other bit. If one or both inputs for a cell location are positive, the output is positive; if both inputs are negative, the output is negative."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The first input to use in this bitwise operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The second input to use in this bitwise operation. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "TanH (in_raster_or_constant)", "name": "TanH (Spatial Analyst)", "description": "Calculates the hyperbolic tangent of cells in a raster.", "example": {"title": "TanH example 1 (Python window)", "description": "This example calculates the hyperbolic tangent of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outTanH = TanH ( \"degs\" ) outTanH.save ( \"C:/sapyexamples/output/outtanh\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : -1 < [out_value] < 1 ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "Output values are always floating point, regardless of the input data type.", "The input and output values of ", "TanH", " are interpreted as unitless."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input to calculate the hyperbolic tangent values for. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Tan (in_raster_or_constant)", "name": "Tan (Spatial Analyst)", "description": "Calculates the tangent of cells in a raster.", "example": {"title": "Tan example 1 (Python window)", "description": "This example calculates the tangent of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outTan = Tan ( \"degs\" ) outTan.save ( \"C:/sapyexamples/output/outtan\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : -\u221e < [out_value] < \u221e ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "The input values for this tool are interpreted to be in radians. If the input you wish to use is in degrees, the values must first be divided by the radians-to-degrees conversion factor of 180/pi, or approximately 57.296.", "See ", "here", " for examples of converting input values that are in degrees to the expected units of radians.", "The output values from ", "Tan", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type.", "Due to the range of values, applying a Histogram Equalized stretch renderer can be useful to best see the results."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the tangent values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "SinH (in_raster_or_constant)", "name": "SinH (Spatial Analyst)", "description": "Calculates the hyperbolic sine of cells in a raster.", "example": {"title": "SinH example 1 (Python window)", "description": "This example calculates the hyperbolic sine of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSinH = SinH ( \"degs\" ) outSinH.save ( \"C:/sapyexamples/output/outsinh\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : -\u221e < [out_value] < \u221e ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "Output values are always floating point, regardless of the input data type.", "The input and output values in ", "SinH", " are interpreted as unitless."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the hyperbolic sine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Sin (in_raster_or_constant)", "name": "Sin (Spatial Analyst)", "description": "Calculates the sine of cells in a raster.", "example": {"title": "Sin example 1 (Python window)", "description": "This example calculates the sine of the values in the input GRID raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSin = Sin ( \"degs\" ) outSin.save ( \"C:/sapyexamples/output/outsin\" )"}, "usage": ["In mathematics, all Trigonometric functions have a defined range of valid input values, called the domain. The output values from each function also has a defined range. For this tool:", "The Domain is : -\u221e < [in_value] < \u221e ", "The Range is : -1 \u2264 [out_value] \u2264 1 ", "Note that here -\u221e and \u221e represent the smallest negative and largest positive value supported by the particular raster format, respectively.", "The input values for this tool are interpreted to be in radians. If the input you wish to use is in degrees, the values must first be divided by the radians-to-degrees conversion factor of 180/pi, or approximately 57.296.", "See ", "here", " for examples of converting input values that are in degrees to the expected units of radians.", "The output values from ", "Sin", " are interpreted as unitless.", "Output values are always floating point, regardless of the input data type.", "Due to the range of values, applying a linear stretch renderer can be useful to better see the results."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input for which to calculate the sine values. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ZonalFill (in_zone_raster, in_weight_raster)", "name": "Zonal Fill (Spatial Analyst)", "description": "Fills zones using the minimum cell value from a weight raster along the zone boundary.", "example": {"title": "ZonalFill example 1 (Python window)", "description": "This example fills the zones in a raster with the minimum value from the weight raster along the zone boundary.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outZonalFill = ZonalFill ( \"inzone\" , \"zoneweight\" ) outZonalFill.save ( \"C:/sapyexamples/output/zonefillout\" )"}, "usage": ["The input zone raster can be integer or floating point. Note that this is an exception to the other zonal tools, which require the zone input to be integer type.", "The data type of the output is the same as that of the input weight raster. If the values on the weight raster are floating point, the resultant output raster will be floating point. If the weight raster is integer, the output will be integer.", "Zonal Fill", " can be used as part of a hydrological analysis to fill sinks to the minimum elevation of their watershed boundary."], "parameters": [{"name": "in_zone_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that defines the zones to be filled. ", "dataType": "Raster Layer"}, {"name": "in_weight_raster", "isInputFile": true, "isOptional": false, "description": "Weight, or value, to be assigned to each zone. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "TabulateArea (in_zone_data, zone_field, in_class_data, class_field, out_table, {processing_cell_size})", "name": "Tabulate Area (Spatial Analyst)", "description": "Calculates cross-tabulated areas between two datasets and outputs a table.", "example": {"title": "TabulateArea example 1 (Python window)", "description": "This example returns a table with the area of each class value that is contained within each zone.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" TabulateArea ( \"zonedata.shp\" , \"IDStr\" , \"valueraster\" , \"VALUE\" , \"C:/sapyexamples/output/areatable.dbf\" , 2 )"}, "usage": ["A zone is defined as all areas in the input that have the same value.  The areas do not have to be contiguous.  Both raster and feature datasets can be used for the zone input.", "When the zone and class inputs are both rasters of the same resolution, they will be used directly.", "If the resolutions are different, an internal resampling is applied to make them match before the zonal operation is performed.", "If the zone input is a raster dataset, it must have an attribute table. The attribute table is usually created automatically for integer rasters, but may not be under certain circumstances. You can use ", "Build Raster Attribute Table", " to create one.", "If the zone input is feature dataset, a vector-to-raster conversion will be internally applied to it.", "To ensure that the results of the conversion will align properly with a class raster input, it is recommended that you check that the extent and snap raster are set appropriately in the environment settings and the raster settings.", "When specifying the input zone or class data,  the default field will be the first available valid  field.  If no other valid fields exist, the ObjectID field (for example, OID or FID) will be the default.", "If a reserved field (for example, OBECTID, FID, or OID) is selected for the ", "Zone field", ", then this may cause some ambiguity in the result.  The result includes the particular reserved field name necessary for the particular output format type, as well as the ", "Zone field", " specified.  If the specified field has the same name as the reserved field for the particular output format, in the output the name for the zone field will be altered in such a way that all field names in the result are unique.", "To make a field of unique values that does not have a reserved name, use the ", "Add Field", " and ", "Calculate Field", " geoprocessing tools.", "If the class input is feature dataset, a vector-to-raster conversion will also be internally applied to it. The conditions listed in the previous tip for a feature zone input also apply to a feature class input.", "It is generally recommended to only use rasters as the zone and class inputs.  If your inputs are feature, consider converting them to rasters first with the ", "To Raster", " conversion tools. This offers you greater control over the vector-to-raster conversion, helping to ensure you consistently get the expected results.", "If a point or line dataset is used as the class data, the area intersected by those features will be reported.", "The output of the ", "Tabulate Area", " tool is a table.", "In this table: ", "See ", "Working with Tabulate Area", " for explanations of certain issues that may be encountered with this tool and suggestions on how to work around them."], "parameters": [{"name": "in_zone_data", "isInputFile": true, "isOptional": false, "description": "Dataset that defines the zones. The zones can be defined by an integer raster or a feature layer. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "zone_field", "isOptional": false, "description": " Field that holds the values that define each zone. It can be an integer or a string field of the zone dataset. ", "dataType": "Field"}, {"name": "in_class_data", "isInputFile": true, "isOptional": false, "description": "The dataset that defines the classes that will have their area summarized within each zone. The class input can be an integer raster layer or a feature layer. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "class_field", "isOptional": false, "description": "The field that holds the class values. It can be an integer or a string field of the input class data. ", "dataType": "Field"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "Output table that will contain the summary of the area of each class in each zone. ", "dataType": "Table"}, {"name": "processing_cell_size", "isOptional": true, "description": "The processing cell size for the zonal operation. This is the value in the environment if specifically set. If the environment is not set, the default for the cell size is determined by the type of the zone data as follows: If the zone dataset is a raster, the cell size is the same as the zone raster. If the zone dataset is a feature, the cell size is the shorter of the width or height of the extent of the zone feature dataset in the output spatial reference, divided by 250.", "dataType": "Analysis Cell Size"}]},
{"syntax": "Viewshed (in_raster, in_observer_features, {z_factor}, {curvature_correction}, {refractivity_coefficient})", "name": "Viewshed (Spatial Analyst)", "description": "Determines the raster surface locations visible to a set of observer features. \r\n\r\n Learn more about how Viewshed works", "example": {"title": "Viewshed example 1 (Python window)", "description": "This example determines the surface locations visible to a set of observers defined in a shapefile.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outViewshed = Viewshed ( \"elevation\" , \"observers.shp\" , 2 , \"CURVED_EARTH\" , 0.15 ) outViewshed.save ( \"C:/sapyexamples/output/outvwshd01\" )"}, "usage": ["Determining observer points is a computer-intensive process. The processing time is dependent on the resolution. For preliminary studies, you may want to use a coarser cell size to reduce the number of cells in the input. Use the full resolution raster when the final results are ready to be generated.", "If the input raster contains undesirable noise caused by sampling errors, you can smooth the raster with a low-pass filter, such as the Mean option of ", "Focal Statistics", ", before running this tool.", "The visibility of each cell center is determined by comparing the altitude angle to the cell center with the altitude angle to the local horizon. The local horizon is computed by considering the intervening terrain between the point of observation and the current cell center. If the point lies above the local horizon, it is considered visible."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "in_observer_features", "isInputFile": true, "isOptional": false, "description": "The feature class that identifies the observer locations. The input can be point or polyline features. ", "dataType": "Feature Layer"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "curvature_correction", "isOptional": true, "description": "Allows correction for the earth's curvature. FLAT_EARTH \u2014 No curvature correction will be applied. This is the default. CURVED_EARTH \u2014 Curvature correction will be applied.", "dataType": "Boolean"}, {"name": "refractivity_coefficient", "isOptional": true, "description": "Coefficient of the refraction of visible light in air. The default value is 0.13. ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Slope (in_raster, {output_measurement}, {z_factor})", "name": "Slope (Spatial Analyst)", "description": "Identifies the slope (gradient, or rate of maximum change in z-value) from each cell of a raster surface. Learn more about how Slope works \r\n", "example": {"title": "Slope example 1 (Python window)", "description": "This example determines the slope values of the input surface raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outSlope = Slope ( \"elevation\" , \"DEGREE\" , 0.3043 ) outSlope.save ( \"C:/sapyexamples/output/outslope01\" )"}, "usage": ["Slope is the rate of maximum change in z-value from each cell.", "The use of a z-factor is essential for correct slope calculations when the surface z units are expressed in units different from the ground x,y units.", "The range of values in the output depends on the type of measurement units. ", "If the center cell in the immediate neighborhood (3 x 3 window) is NoData, the output is NoData.", "If any neighborhood cells are NoData, they are assigned the value of the center cell; then the slope is computed."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "output_measurement", "isOutputFile": true, "isOptional": true, "description": "Determines the measurement units (degrees or percentages) of the output slope data. DEGREE \u2014 The inclination of slope will be calculated in degrees. PERCENT_RISE \u2014 Keyword to output the percent rise, also referred to as the percent slope.", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ObserverPoints (in_raster, in_observer_point_features, {z_factor}, {curvature_correction}, {refractivity_coefficient})", "name": "Observer Points (Spatial Analyst)", "description": "Identifies which observer points are visible from each raster surface location. Learn more about how Observer Points works", "example": {"title": "ObserverPoints example 1 (Python window)", "description": "This example identifies exactly which observer points are visible from each raster surface location.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outObsPoints = ObserverPoints ( \"elevation\" , \"observers.shp\" , 1 , \"CURVED_EARTH\" , 0.13 ) outObsPoints.save ( \"C:/sapyexamples/output/outobspnt01\" )"}, "usage": ["Determining observer points is a computer-intensive process. The processing time is dependent on the resolution. For preliminary studies, you may want to use a coarser cell size to reduce the number of cells in the input. Use the full resolution raster when the final results are ready to be generated.", "If the input raster contains undesirable noise caused by sampling errors, you can smooth the raster with a low-pass filter, such as the Mean option of ", "Focal Statistics", ", before running this tool.", "The visibility of each cell center is determined by comparing the altitude angle to the cell center with the altitude angle to the local horizon. The local horizon is computed by considering the intervening terrain between the point of observation and the current cell center. If the point lies above the local horizon, it is considered visible."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "in_observer_point_features", "isInputFile": true, "isOptional": false, "description": "The point feature class that identifies the observer locations. The maximum number of points allowed is 16. ", "dataType": "Feature Layer"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "curvature_correction", "isOptional": true, "description": "Allows correction for the earth's curvature. FLAT_EARTH \u2014 No curvature correction will be applied. This is the default. CURVED_EARTH \u2014 Curvature correction will be applied.", "dataType": "Boolean"}, {"name": "refractivity_coefficient", "isOptional": true, "description": "Coefficient of the refraction of visible light in air. The default value is 0.13. ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Hillshade (in_raster, {azimuth}, {altitude}, {model_shadows}, {z_factor})", "name": "Hillshade (Spatial Analyst)", "description": "Creates a shaded relief from a surface raster by considering the illumination source angle and shadows. \r\n\r\n Learn more about how Hillshade works", "example": {"title": "Hillshade example 1 (Python window)", "description": "This example generates a hillshade raster that includes shadows. Specific azimuth and altitude angles are set.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outHillShade = HillShade ( \"elevation\" , 180 , 75 , \"SHADOWS\" , 1 ) outHillShade.save ( \"C:/sapyexamples/output/outhillshd01\" )"}, "usage": ["The ", "Hillshade", " tool creates a shaded relief raster from a raster. The illumination source is considered to be at infinity.", "The hillshade raster has an integer value range of 0 to 255.", "Two types of shaded relief rasters can be output. If the ", "Model shadows", " option is disabled (unchecked), the output raster only considers local illumination angle. If it is enabled (checked), the output raster considers the effects of both local illumination angle and shadow.", "The analysis of shadows is done by considering the effects of the local horizon at each cell. Raster cells in shadow are assigned a value of zero.", "To create a raster of the shadow areas only, use the ", "Con", ", ", "Reclassify", ", or ", "Extract by Attributes", " tool to separate the value zero from the other hillshade values. ", "Hillshade", " must have had the ", "Model shadows", " option enabled.", "If the input raster is in a spherical coordinate system, such as decimal degrees, the resulting hillshade may look peculiar. This is due to the difference in measure between the horizontal ground units and the elevation z units. Since the length of a degree of longitude changes with latitude, you will need to specify an appropriate z-factor for that latitude. If your x,y units are decimal degrees and your z units are meters, some appropriate z-factors for particular latitudes are:", "You can create dramatic three-dimensional views of the hillshaded surface by draping the output raster using ArcGIS ArcScene."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "azimuth", "isOptional": true, "description": "Azimuth angle of the light source. The azimuth is expressed in positive degrees from 0 to 360, measured clockwise from north. The default is 315 degrees. ", "dataType": "Double"}, {"name": "altitude", "isOptional": true, "description": "Altitude angle of the light source above the horizon. The altitude is expressed in positive degrees, with 0 degrees at the horizon and 90 degrees directly overhead. The default is 45 degrees. ", "dataType": "Double"}, {"name": "model_shadows", "isOptional": true, "description": "Type of shaded relief to be generated. NO_SHADOWS \u2014 The output raster only considers local illumination angles; the effects of shadows are not considered.The output values can range from 0 to 255, with 0 representing the darkest areas, and 255 the brightest. SHADOWS \u2014 The output shaded raster considers both local illumination angles and shadows.The output values range from 0 to 255, with 0 representing the shadow areas, and 255 the brightest.", "dataType": "Boolean"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CutFill (in_before_surface, in_after_surface, {z_factor})", "name": "Cut Fill (Spatial Analyst)", "description": "Calculates the volume change between two surfaces. This is typically used for cut and fill operations. \r\n Learn more about how Cut Fill works", "example": {"title": "CutFill example 1 (Python window)", "description": "This example calculates the volume and area of cut and fill locations and outputs the result as a Grid raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCutFill = CutFill ( \"elevation01\" , \"elevation02\" , 1 ) outCutFill.save ( \"C:/sapyexamples/output/outcutfill01\" )"}, "usage": ["The ", "Cut Fill", " tool enables you to create a map based on two input surfaces\u2014before and after\u2014displaying the areas and volumes of surface materials that have been modified by the removal or addition of surface material.", "Both the input raster surfaces must be coincident. That is, they must have a common origin, the same number of rows and columns of cells, and the same cell size.", "For accurate results, the z-units should be the same as the x,y ground units. This ensures that the resulting volumes are meaningful cubic measures (i.e., cubic meters). If they are not the same, use a z-factor to convert z units to x,y units. For example, if your x,y units are meters and your z units are feet, you could specify a z-factor of 0.3048 to convert feet to meters.", "Alternatively, use the ", "Times", " math tool to create a surface raster in which the z-values have been adjusted to correspond to the ground units.", "The attribute table of the output raster presents the changes in the surface volumes following the cut/fill operation. Positive values for the volume difference indicate regions of the before raster surface that have been cut (material removed). Negative values indicate areas that have been filled (material added). See ", "How Cut Fill works", " for more details on how the results are calculated.", "When the cut/fill operation is performed from the tool, by default a specialized renderer is applied that highlights the locations of cut and of fill. The renderer draws areas that have been cut in blue, and areas that have been filled in red. Areas that have not changed are displayed in grey."], "parameters": [{"name": "in_before_surface", "isInputFile": true, "isOptional": false, "description": "The input representing the surface before the cut or fill operation. ", "dataType": "Raster Layer"}, {"name": "in_after_surface", "isInputFile": true, "isOptional": false, "description": "The input representing the surface after the cut or fill operation. ", "dataType": "Raster Layer"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Curvature (in_raster, {z_factor}, {out_profile_curve_raster}, {out_plan_curve_raster})", "name": "Curvature (Spatial Analyst)", "description": "Calculates the curvature of a raster surface, optionally including profile and plan curvature. \r\n Learn more about how Curvature works", "example": {"title": "Curvature example 1 (Python window)", "description": "This example creates a curvature raster from an input surface raster and also applies a z-factor.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outCurve = Curvature ( \"elevation\" , 1.094 ) outCurve.save ( \"C:/sapyexamples/output/outcurv01\" )"}, "usage": ["The primary output is the curvature of the surface on a cell-by-cell basis, as fitted through that cell and its eight surrounding neighbors. Curvature is the second derivative of the surface,  or the slope-of-the-slope. Two optional output curvature types are possible: the profile curvature is in the direction of the maximum slope, and the plan curvature is perpendicular to the direction of the maximum slope.", "A positive curvature indicates the surface is upwardly convex at that cell. A negative curvature indicates the surface is upwardly concave at that cell. A value of 0 indicates the surface is flat.", "In the profile output, a negative value indicates the surface is upwardly convex at that cell. A positive profile indicates the surface is upwardly concave at that cell. A value of 0 indicates the surface is flat.", "In the plan output, a positive value indicates the surface is upwardly convex at that cell. A negative plan indicates the surface is upwardly concave at that cell. A value of 0 indicates the surface is flat.", "Units of the curvature output raster, as well as the units for the optional output profile curve raster and output plan curve raster, are one hundredth (1/100) of a z-unit. The reasonably expected values of all three output rasters for a hilly area (moderate relief) can vary from -0.5 to 0.5; while for steep, rugged mountains (extreme relief), the values can vary between -4 and 4. Note that it is possible to exceed this range for certain raster surfaces."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "out_profile_curve_raster", "isOutputFile": true, "isOptional": true, "description": "Output profile curve raster dataset. This is the curvature of the surface in the direction of slope. ", "dataType": "Raster Dataset"}, {"name": "out_plan_curve_raster", "isOutputFile": true, "isOptional": true, "description": "Output plan curve raster dataset. This is the curvature of the surface perpendicular to the slope direction. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_curvature_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ContourWithBarriers (in_raster, out_contour_feature_class, {in_barrier_features}, {in_contour_type}, {in_contour_values_file}, {explicit_only}, {in_base_contour}, {in_contour_interval}, {in_indexed_contour_interval}, {in_contour_list}, {in_z_factor})", "name": "Contour with Barriers (Spatial Analyst)", "description": "Creates contours from a raster surface. The inclusion of barrier features will allow one to independently generate contours on either side of a barrier.", "example": {"title": "ContourWithBarriers example 1 (Python window)", "description": "This example creates contours from an Esri Grid raster with an input barrier feature as well as base and interval parameters specified. The output contours area as polylines in a shapefile.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" ContourWithBarriers ( \"elevation\" , \"C:/sapyexamples/output/outcontourwithbarriers.shp\" , \"elevation_barrier.shp\" , \"POLYLINES\" , \"\" , \"\" , 0 , 300 )"}, "usage": ["The current version of ", "Contour with Barriers", " only supports polyline output. If the polygon output option is used it will be ignored and polyline output will be created.", "Smoother but less accurate contours can be obtained by preprocessing the input raster with a ", "Focal_Statistics", " operation with the MEAN option or the ", "Filter", " tool with the LOW option.", "Contours will extend into the raster's NoData cell by a distance of half the raster's cell size. This will mean that the contours will be generated over single NoData cells. However, a 3-cell-by-3-cell area of NoData will only have the contours extending into this area by half the cell size distance.", "The Type field in the output contour feature class has values:", "An indexed contour interval can be used to generate additional contours and their Type value will be coded as 2 in the output feature class.", "A base contour is used, for example, when you want to create contours every 15 meters, starting at 10 meters. Here, 10 would be used for the base contour, and 15 would be the contour interval. The values to be contoured would be 10, 25, 40, 55, and so on.", "Specifying a base contour does not prevent contours from being created above or below that value.", "A text file containing contour value specifications can include the following:", "For example, if a raster has a minimum value of 102 and a maximum of 500, then a text file with:", "will produce contours at:", "If there are cell values of the raster within a barrier polygon feature the contour lines will be split at the barrier. If the cell values within the polygon feature are to be ignored, change those cell values to NoData.", "If the input raster surface is very large or many output features are requested, a large number of temporary files will be created in the operating system's temporary file location. If any problems are encountered as a result of this do one of the following:", "The output contour features can be labeled by using the ", "Contour Annotation", " tool."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_contour_feature_class", "isOutputFile": true, "isOptional": false, "description": "Output contour features. ", "dataType": "Feature Class"}, {"name": "in_barrier_features", "isInputFile": true, "isOptional": true, "description": "Input barrier features. ", "dataType": "Feature Layer"}, {"name": "in_contour_type", "isInputFile": true, "isOptional": true, "description": "The type of contour to create. The current version of Contour with Barriers only supports polyline output. If the polygon output option is used it will be ignored and polyline output will be created. POLYLINES \u2014 The contour or isoline representation of the input raster. POLYGONS \u2014 Closed polygons representing the contours. ", "dataType": "String"}, {"name": "in_contour_values_file", "isInputFile": true, "isOptional": true, "description": "The base contour, contour interval, indexed contour interval, and explicit contour values can also be specified via a text file. ", "dataType": "File"}, {"name": "explicit_only", "isOptional": true, "description": "Only explicit contour values are used. Base contour, contour interval, and indexed contour intervals are not specified. NO_EXPLICIT_VALUES_ONLY \u2014 The default, contour interval must be specified. EXPLICIT_VALUES_ONLY \u2014 Only explicit contour values are specified.", "dataType": "Boolean"}, {"name": "in_base_contour", "isInputFile": true, "isOptional": true, "description": "Base contour value. Contours are generated above and below this value as needed to cover the entire value range of the input raster. The default is zero. ", "dataType": "Double"}, {"name": "in_contour_interval", "isInputFile": true, "isOptional": true, "description": "The interval, or distance, between contour lines. This can be any positive number. ", "dataType": "Double"}, {"name": "in_indexed_contour_interval", "isInputFile": true, "isOptional": true, "description": "Contours will also be generated for this interval and will be flagged accordingly in the output feature class. ", "dataType": "Double"}, {"name": "in_contour_list", "isInputFile": true, "isOptional": false, "description": "Explicit values at which to create contours. ", "dataType": "Double"}, {"name": "in_z_factor", "isInputFile": true, "isOptional": true, "description": " The unit conversion factor used when generating contours. The default value is 1. The contour lines are generated based on the z-values in the input raster, which are often measured in units of meters or feet. With the default value of 1, the contours will be in the same units as the z-values of the input raster. To create contours in a different unit than that of the z-values, set an appropriate value for the z-factor. Note that it is not necessary to have the ground x,y and surface z-units be consistent for this tool. For example, if the elevation values in your input raster are in feet, but you want the contours to be generated based on units of meters, set the z-factor to 0.3048 (since 1 ft = 0.3048 m). ", "dataType": "Double"}]},
{"syntax": "ContourList (in_raster, out_polyline_features, contour_values)", "name": "Contour List (Spatial Analyst)", "description": "Creates a feature class of selected contour values from a raster surface. Learn more about how Contouring works", "example": {"title": "ContourList example 1 (Python window)", "description": "This example creates contours for three elevation values from an Esri Grid raster and outputs them as a shapefile.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" ContourList ( \"elevation\" , \"C:/sapyexamples/output/outcontourlist.shp\" , [ 600 , 935 , 1237.4 ])"}, "usage": ["Contours do not extend beyond the spatial extent of the raster, and they are not generated in areas of NoData; therefore, adjacent contour inputs should first be edgematched into a continuous feature dataset. As an alternative to edgematching, you can merge the adjacent rasters before computing contours.", "Contours can be generated in areas of negative raster values. The contour values will be negative in such areas. Negative contour intervals are not allowed.", "The contour values do not need to be sorted in order.", "Smoother but less accurate contours can be obtained by preprocessing the input raster with a ", "Focal_Statistics", " operation with the MEAN option or the ", "Filter", " tool with the LOW option."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_polyline_features", "isOutputFile": true, "isOptional": false, "description": "Output contour polyline features. ", "dataType": "Feature Class"}, {"name": "contour_values", "isOptional": false, "description": "List of z-values for which to create contours. ", "dataType": "Double"}]},
{"syntax": "Contour (in_raster, out_polyline_features, contour_interval, {base_contour}, {z_factor})", "name": "Contour (Spatial Analyst)", "description": "Creates a line feature class of contours (isolines) from a raster surface. \r\n Learn more about how Contouring works", "example": {"title": "Contour example 1 (Python window)", "description": "This example creates contours from an Esri Grid raster and outputs them as a shapefile.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" Contour ( \"elevation\" , \"C:/sapyexamples/output/outcontours.shp\" , 200 , 0 )"}, "usage": ["Contours do not extend beyond the spatial extent of the raster, and they are not generated in areas of NoData; therefore, adjacent contour inputs should first be edgematched into a continuous feature dataset. As an alternative to edgematching, you can merge the adjacent rasters before computing contours.", "Contours can be generated in areas of negative raster values. The contour values will be negative in such areas. Negative contour intervals are not allowed.", "Smoother but less accurate contours can be obtained by preprocessing the input raster with a ", "Focal_Statistics", " operation with the MEAN option or the ", "Filter", " tool with the LOW option.", "A base contour is used, for example, when you want to create contours every 15 meters, starting at 10 meters. Here, 10 would be used for the base contour, and 15 would be the contour interval. The values to be contoured would be 10, 25, 40, 55, and so on.", "Specifying a base contour does not prevent contours from being created above or below that value."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_polyline_features", "isOutputFile": true, "isOptional": false, "description": "Output contour polyline features. ", "dataType": "Feature Class"}, {"name": "contour_interval", "isOptional": false, "description": "The interval, or distance, between contour lines. This can be any positive number. ", "dataType": "Double"}, {"name": "base_contour", "isOptional": true, "description": "Base contour value. Contours are generated above and below this value as needed to cover the entire value range of the input raster. The default is zero. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": " The unit conversion factor used when generating contours. The default value is 1. The contour lines are generated based on the z-values in the input raster, which are often measured in units of meters or feet. With the default value of 1, the contours will be in the same units as the z-values of the input raster. To create contours in a different unit than that of the z-values, set an appropriate value for the z-factor. Note that it is not necessary to have the ground x,y and surface z-units be consistent for this tool. For example, if the elevation values in your input raster are in feet, but you want the contours to be generated based on units of meters, set the z-factor to 0.3048 (since 1 ft = 0.3048 m). For another example, consider an input raster in WGS_84 geographic coordinates and elevation units of meters for which you want to generate contour lines every 100 feet with a base of 50 feet (so the contours will be 50 ft, 150 ft, 250 ft, and so on). To do this, set the contour_interval to 100 , the base_contour to 50 , and the z_factor to 3.2808 (since 1m = 3.2808 ft). ", "dataType": "Double"}]},
{"syntax": "Aspect (in_raster)", "name": "Aspect (Spatial Analyst)", "description": "Derives aspect from a raster surface. The aspect identifies the downslope direction of the maximum rate of change in value from each cell to its neighbors. Aspect can be thought of as the slope direction. The values of the output raster will be the compass direction of the aspect. \r\n Learn more about how Aspect works", "example": {"title": "Aspect example 1 (Python window)", "description": "This example creates an aspect raster from an input surface raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outAspect = Aspect ( \"elevation\" ) outAspect.save ( \"C:/sapyexamples/output/outaspect01.img\" )"}, "usage": ["Aspect is the direction of the maximum rate of change in the z-value from each cell in a raster surface.", "Aspect is expressed in positive degrees from 0 to 359.9, measured clockwise from north.", "Cells in the input raster that are flat\u2014with zero slope\u2014are assigned an aspect of -1.", "If the center cell in the immediate neighborhood (3 x 3 window) is NoData, the output is NoData.", "If any neighborhood cells are NoData, they are first assigned the value of the center cell, then the aspect is computed."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "SolarRadiationGraphics (in_surface_raster, {in_points_feature_or_table}, {sky_size}, {height_offset}, {calculation_directions}, {latitude}, {time_configuration}, {day_interval}, {hour_interval}, {out_sunmap_raster}, {zenith_divisions}, {azimuth_divisions}, {out_skymap_raster})", "name": "Solar Radiation Graphics (Spatial Analyst)", "description": "Derives raster representations of a hemispherical viewshed, sunmap, and skymap, which are used in the calculation of direct, diffuse, and global solar radiation.", "example": {"title": "SolarRadiationGraphics example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outViewshedMap = SolarRadiationGraphics ( \"elevation\" , \"observers.shp\" , 200 , 2 , 32 , 52 , TimeMultipleDays ( 2009 , 91 , 212 ), 14 , 0.5 , \"c:/sapyexamples/output/sunmap\" , 8 , 8 , \"c:/sapyexamples/output/skymap\" ) outViewshedMap.save ( \"c:/sapyexamples/output/viewmap\" )"}, "usage": ["Outputs from the ", "Solar Radiation Graphics", " tool are raster representations and are not maps that correspond to the outputs from the area or point solar radiation analysis. Rather, they are representations of directions in a hemisphere of directions looking upward from a given location. In a hemispherical projection, the center is the zenith, the edge of the circular \"map\" is the horizon, and the angle relative to the zenith is proportionate to the radius. Hemispherical projections do not have a geographic coordinate system and have a bottom left corner of (0,0).", "It would not be practical to store viewsheds for all locations in a DEM, so when input locations are not specified, a single viewshed is created for the center of the input surface raster. When input point features or locations file are specified, multiple viewshed rasters are created for each input location. When multiple locations are specified, the default output format is an ESRI GRID stack, which contains multiple bands that correspond to the viewshed for each location.", "The input locations table can be an INFO table, a .dbf file, an Access table, or a text file.", "Output graphic display rasters do not honor extent or cell size environment settings. The output extents are always respective of the sky size/resolution and have a cell size equal to one. However, the underlying analysis will use the environment settings and may affect the results of the viewshed.", "One or two sunmap rasters may be generated, depending on whether the time configuration includes overlapping sun positions throughout the year. When two sunmaps are created, one represents the period between the winter and summer solstice (December 22 to June 22), and the other represents the period between the summer solstice and the winter solstice (June 22 to December 22). When multiple sunmaps are created, the default output is an ESRI GRID stack. When the output is added to ArcMap, only the first band will be displayed.", "The latitude for the site area (units: decimal degree, positive for the north hemisphere and negative for the south hemisphere) is used in calculations such as solar declination and solar position.", "The analysis is designed only for local landscape scales, so it is generally acceptable to use one latitude value for the whole DEM. With larger datasets (i.e., states, countries, or continents), the insolation results will differ significantly at different latitudes (greater than 1 degree). To analyze broader geographic regions, it is necessary to divide the study area into zones with different latitudes.", "For input surface rasters containing a spatial reference, the mean latitude is automatically calculated; otherwise, the latitude will default to 45 degrees. When using an input layer, the spatial reference of the data frame is used.", "Sky size is the resolution of the viewshed, sky map, and sun map rasters which are used in the radiation calculations (units: cells per side). These are upward-looking, hemispherical raster representations of the sky and do not have a geographic coordinate system. These grids are square (equal number of rows and columns).", "Increasing the sky size increases calculation accuracy but also increases calculation time considerably.", "When the \"day interval\" setting is small (e.g., < 14 days) a larger sky size should be used. During analysis the sun map (determined by the sky size) is used to represent sun positions (tracks) for particular time periods to calculate direct radiation. With smaller day intervals, if the sky size resolution is not large enough, sun tracks may overlap, resulting in zero or lower radiation values for that track. Increasing the resolution provides a more accurate result.", "The maximum sky size value is 4000. A value of 200 is default and is sufficient for whole DEMs with large day intervals (e.g., > 14 days). A sky size value of 512 is sufficient for calculations at point locations where calculation time is less of an issue. At smaller day intervals (e.g., < 14 days), it is recommended to use higher values. For example, to calculate insolation for a location at the equator with day interval = 1, it is recommended to use a sky size of 2800 or more.", "Day intervals greater than 3 are recommended as sun tracks within three days typically overlap, depending on sky size and time of year. For calculations of the whole year with monthly interval, day interval is disabled and the program internally uses calendar month intervals. The default value is 14.", "Because the viewshed calculation can be highly intensive, horizon angles are only traced for the number of calculation directions specified. Valid values must be multiples of 8 (8, 16, 24, 32, and so on). Typically, a value of 8 or 16 is adequate for areas with gentle topography, whereas a value of 32 is adequate for complex topography. The default value is 32.", "The number of calculation directions needed is related to the resolution of the input DEM. Natural terrain at 30m resolution is usually quite smooth so fewer directions are sufficient for most situations (16 or 32). With finer DEMs, and particularly with man-made structures incorporated in the DEMs, the number of directions needs to increase. Increasing the number of directions will increase accuracy but will also increase calculation time."], "parameters": [{"name": "in_surface_raster", "isInputFile": true, "isOptional": false, "description": "Input elevation surface raster. ", "dataType": "Raster Layer"}, {"name": "in_points_feature_or_table", "isInputFile": true, "isOptional": true, "description": "The input point feature class or table specifying the locations to analyze solar radiation. ", "dataType": "Feature Layer | Table View"}, {"name": "sky_size", "isOptional": true, "description": "The resolution or sky size for the viewshed, sky map, and sun map grids. The units are cells. The default creates a raster of 200 x 200 cells. ", "dataType": "Long"}, {"name": "height_offset", "isOptional": true, "description": "The height (in meters) above the DEM surface for which calculations are to be performed. The height offset will be applied to all input locations. ", "dataType": "Double"}, {"name": "calculation_directions", "isOptional": true, "description": "The number of azimuth directions used when calculating the viewshed. Valid values must be multiples of 8 (8, 16, 24, 32, and so on). The default value is 32 directions, which is adequate for complex topography. ", "dataType": "Long"}, {"name": "latitude", "isOptional": true, "description": "The latitude for the site area. The units are decimal degrees, with positive values for the northern hemisphere and negative for the southern. For input surface rasters containing a spatial reference, the mean latitude is automatically calculated; otherwise, latitude will default to 45 degrees. ", "dataType": "Double"}, {"name": "time_configuration", "isOptional": true, "description": "Specifies the time configuration (period) used for calculating solar radiation. The Time class objects are used to specify the time configuration. The different types of time configurations available are TimeWithinDay , TimeMultiDays , TimeSpecialDays , and TimeWholeYear . The following are the forms: The default time_configuration is TimeMultiDays with the start_day of 5 and end_day of 160, for the current Julian year. TimeWithinDay({day},{start_time},{end_time}) TimeMultiDays({year},{start_day},{end_day}) TimeSpecialDays() TimeWholeYear({year})", "dataType": "Time configuration"}, {"name": "day_interval", "isOptional": true, "description": "The time interval through the year (units: days) used for calculation of sky sectors for the sun map. The default value is 14 (biweekly). ", "dataType": "Long"}, {"name": "hour_interval", "isOptional": true, "description": "Time interval through the day (units: hours) used for calculation of sky sectors for sun maps. The default value is 0.5. ", "dataType": "Double"}, {"name": "out_sunmap_raster", "isOutputFile": true, "isOptional": true, "description": "The output sunmap raster. The output is a representation that specifies sun tracks, the apparent position of the sun as it varies through time. The output is at the same resolution as the viewshed and skymap. ", "dataType": "Raster Dataset"}, {"name": "zenith_divisions", "isOptional": true, "description": "The number of divisions used to create sky sectors in the sky map. The default is eight divisions (relative to zenith). Values must be greater than zero and less than half the sky size value. ", "dataType": "Long"}, {"name": "azimuth_divisions", "isOptional": true, "description": "The number of divisions used to create sky sectors in the sky map. The default is eight divisions (relative to north). Valid values must be multiples of 8. Values must be greater than zero and less than 160. ", "dataType": "Long"}, {"name": "out_skymap_raster", "isOutputFile": true, "isOptional": true, "description": "The output skymap raster. The output is constructed by dividing the whole sky into a series of sky sectors defined by zenith and azimuth divisions. The output is at the same resolution as the viewshed and sunmap. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_viewshed_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "PointsSolarRadiation (in_surface_raster, in_points_feature_or_table, out_global_radiation_features, {height_offset}, {latitude}, {sky_size}, {time_configuration}, {day_interval}, {hour_interval}, {each_interval}, {z_factor}, {slope_aspect_input_type}, {calculation_directions}, {zenith_divisions}, {azimuth_divisions}, {diffuse_model_type}, {diffuse_proportion}, {transmittivity}, {out_direct_radiation_features}, {out_diffuse_radiation_features}, {out_direct_duration_features})", "name": "Points Solar Radiation (Spatial Analyst)", "description": "Derives incoming solar radiation for specific locations in a point feature class or location table. Learn more about how solar radiation is calculated", "example": {"title": "PointsSolarRadiation example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" PointsSolarRadiation ( \"elevation\" , \"observers.shp\" , \"c:/sapyexamples/output/outglobalrad1.shp\" , \"\" , 35 , 200 , TimeMultipleDays ( 2009 , 91 , 212 ), 14 , 0.5 , \"NOINTERVAL\" , 1 , \"FROM_DEM\" , 32 , 8 , 8 , \"STANDARD_OVERCAST_SKY\" , 0.3 , 0.5 , \"c:/sapyexamples/output/outdirectrad1.shp\" , \"c:/sapyexamples/output/outdiffuserad1.shp\" , \"c:/sapyexamples/output/outduration1.shp\" )"}, "usage": ["The input locations can be a feature class or table. The table can be an INFO table, a .dbf file, an Access table, or a text table file.", "When inputting locations by table, a list of locations must be specified with an x,y coordinate. Using a coordinate file, each line should contain an x,y pair separated by a space or tab. The following is an example:", "Alternatively, you may specify slope (degrees) and aspect in the location table. Along with the x,y coordinate, the file should contain the slope and aspect value for each location. The following is an example:", "For multi-day time configurations, the maximum range of days is a total of one year (365 days, or 366 days for leap years). If the start day is greater than the end day, the time calculations will proceed into the following year.", "For example, [start day, end day] = [365, 31], represents December 31 to January 31 of the following year. The example of [1, 2], the time is inclusive for the first day from 0:00 hours (January 1) to 0:00 (January 2). The start day and end day cannot be equal.", "The year value for time configuration is used to determine a leap\r\nyear. It does not have any other influence on the solar radiation\r\nanalysis as the calculations are a function of the time period\r\ndetermined by Julian days.", "For within-day time configurations, the maximum range of time is one day (24 hours). Calculations will not be performed across days (for instance, from 12:00 p.m. to 12:00 p.m. the next day). The start time must be less than the end time.", "The use of a z-factor is essential for correcting calculations when the surface z units are expressed in units different from the ground x,y units. To get accurate results, the z units should be the same as the x,y ground units. If they are not the same, use a z-factor to convert z units to x,y units. For example, if your x,y units are meters and your z units are feet, you could specify a z-factor of 0.3048 to convert feet to meters.", "It is recommended to have your data in a projection coordinate system (units meters). However, if you choose to run the analysis with a spherical coordinate system you will need to specify an appropriate z-factor for that latitude. If your x,y units are decimal degrees and your z units are meters, some appropriate z-factors for particular latitudes are:", "The height offset should only be specified in meters.", "The latitude for the site area (units: decimal degree, positive for the north hemisphere and negative for the south hemisphere) is used in calculations such as solar declination and solar position. Because the solar analysis is designed for landscape scales and local scales, it is acceptable to use one latitude value for the whole DEM. For broader geographic regions, it is necessary to divide the study area into zones with different latitudes.", "For input surface rasters containing a spatial reference, the mean latitude is automatically calculated; otherwise, latitude will default to 45 degrees. When using an input layer, the spatial reference of the data frame is used.", "Sky size is the resolution of the viewshed, sky map, and sun map rasters which are used in the radiation calculations (units: cells per side). These are upward-looking, hemispherical raster representations of the sky and do not have a geographic coordinate system. These grids are square (equal number of rows and columns).", "Increasing the sky size increases calculation accuracy but also increases calculation time considerably.", "When the \"day interval\" setting is small (e.g., < 14 days) a larger sky size should be used. During analysis the sun map (determined by the sky size) is used to represent sun positions (tracks) for particular time periods to calculate direct radiation. With smaller day intervals, if the sky size resolution is not large enough, sun tracks may overlap, resulting in zero or lower radiation values for that track. Increasing the resolution provides a more accurate result.", "The maximum sky size value is 4000. A value of 200 is default and is sufficient for whole DEMs with large day intervals (e.g., > 14 days). A sky size value of 512 is sufficient for calculations at point locations where calculation time is less of an issue. At smaller day intervals (e.g., < 14 days), it is recommended to use higher values. For example, to calculate insolation for a location at the equator with day interval = 1, it is recommended to use a sky size of 2800 or more.", "Day intervals greater than 3 are recommended as sun tracks within three days typically overlap, depending on sky size and time of year. For calculations of the whole year with monthly interval, day interval is disabled and the program internally uses calendar month intervals. The default value is 14.", "Because the viewshed calculation can be highly intensive, horizon angles are only traced for the number of calculation directions specified. Valid values must be multiples of 8 (8, 16, 24, 32, and so on). Typically, a value of 8 or 16 is adequate for areas with gentle topography, whereas a value of 32 is adequate for complex topography. The default value is 32.", "The number of calculation directions needed is related to the resolution of the input DEM. Natural terrain at 30m resolution is usually quite smooth so fewer directions are sufficient for most situations (16 or 32). With finer DEMs, and particularly with man-made structures incorporated in the DEMs, the number of directions needs to increase. Increasing the number of directions will increase accuracy but will also increase calculation time.", "The ", "Create outputs for each interval", " check box provides the flexibility to calculate insolation integrated over a specified time period or insolation \"for each interval\" in a time series. For example, for the within-day time period with an hour interval of one, checking this box will create hourly insolation values; otherwise, insolation integrated for the entire day is calculated.", "The For each interval check box affects the number of attributes for output features. When checked for point radiation analysis, the output feature class includes additional attributes (t0, t1, t2, and so on), which indicate radiation or duration values for each time interval (hour interval when time configuration is less than one day, or day interval when multiple days).", "The amount of  solar radiation that is received by the surface is only a portion of what would be received outside the atmosphere.  Transmittivity is a property of the atmosphere and is the ratio of the energy received at the upper edge of the atmosphere to that reaching the earths's surface by the shortest path (in the direction of the zenith), averaged over all wavelengths. Values range from 0 (no transmission) to 1 (complete transmission). Typically observed values are 0.6 or 0.7 for very clear sky conditions and 0.5 for only a generally clear sky.", "Because the algorithms  corrects for elevation effects, transmittivity should always be given for sea level. Transmittivity has an inverse relation with the diffuse proportion parameter."], "parameters": [{"name": "in_surface_raster", "isInputFile": true, "isOptional": false, "description": "Input elevation surface raster. ", "dataType": "Raster Layer"}, {"name": "in_points_feature_or_table", "isInputFile": true, "isOptional": false, "description": "The input point feature class or table specifying the locations to analyze solar radiation. ", "dataType": "Feature Layer | Table View"}, {"name": "out_global_radiation_features", "isOutputFile": true, "isOptional": false, "description": "The output feature class representing the global radiation or amount of incoming solar insolation (direct + diffuse) calculated for each location. The output has units of watt hours per square meter (WH/m 2 ). ", "dataType": "Feature Class"}, {"name": "height_offset", "isOptional": true, "description": "The height (in meters) above the DEM surface for which calculations are to be performed. The height offset will be applied to all input locations. ", "dataType": "Double"}, {"name": "latitude", "isOptional": true, "description": "The latitude for the site area. The units are decimal degrees, with positive values for the northern hemisphere and negative for the southern. For input surface rasters containing a spatial reference, the mean latitude is automatically calculated; otherwise, latitude will default to 45 degrees. ", "dataType": "Double"}, {"name": "sky_size", "isOptional": true, "description": "The resolution or sky size for the viewshed, sky map, and sun map grids. The units are cells. The default creates a raster of 200 x 200 cells. ", "dataType": "Long"}, {"name": "time_configuration", "isOptional": true, "description": "Specifies the time configuration (period) used for calculating solar radiation. The Time class objects are used to specify the time configuration. The different types of time configurations available are TimeWithinDay , TimeMultiDays , TimeSpecialDays , and TimeWholeYear . The following are the forms: The default time_configuration is TimeMultiDays with the start_day of 5 and end_day of 160, for the current Julian year. TimeWithinDay({day},{start_time},{end_time}) TimeMultiDays({year},{start_day},{end_day}) TimeSpecialDays() TimeWholeYear({year})", "dataType": "Time configuration"}, {"name": "day_interval", "isOptional": true, "description": "The time interval through the year (units: days) used for calculation of sky sectors for the sun map. The default value is 14 (biweekly). ", "dataType": "Long"}, {"name": "hour_interval", "isOptional": true, "description": "Time interval through the day (units: hours) used for calculation of sky sectors for sun maps. The default value is 0.5. ", "dataType": "Double"}, {"name": "each_interval", "isOptional": true, "description": "Specifies whether to calculate a single total insolation value for all locations or multiple values for the specified hour and day interval. NOINTERVAL \u2014 A single total radiation value will be calculated for the entire time configuration. This is default. INTERVAL \u2014 Multiple radiation values will be calculated for each time interval over the entire time configuration. The number of outputs will depend on the hour or day interval. For example, for a whole year with monthly intervals, the result will contain 12 output radiation values for each location.", "dataType": "Boolean"}, {"name": "z_factor", "isOptional": true, "description": "The number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "slope_aspect_input_type", "isOptional": true, "description": "How slope and aspect information are derived for analysis. FROM_DEM \u2014 The slope and aspect grids are calculated from the input surface raster. This is the default. FLAT_SURFACE \u2014 Constant values of zero are used for slope and aspect. FROM_POINTS_TABLE \u2014 Values for slope and aspect can be specified along with the x,y coordinates in the locations file.", "dataType": "String"}, {"name": "calculation_directions", "isOptional": true, "description": "The number of azimuth directions used when calculating the viewshed. Valid values must be multiples of 8 (8, 16, 24, 32, and so on). The default value is 32 directions, which is adequate for complex topography. ", "dataType": "Long"}, {"name": "zenith_divisions", "isOptional": true, "description": "The number of divisions used to create sky sectors in the sky map. The default is eight divisions (relative to zenith). Values must be greater than zero and less than half the sky size value. ", "dataType": "Long"}, {"name": "azimuth_divisions", "isOptional": true, "description": "The number of divisions used to create sky sectors in the sky map. The default is eight divisions (relative to north). Valid values must be multiples of 8. Values must be greater than zero and less than 160. ", "dataType": "Long"}, {"name": "diffuse_model_type", "isOptional": true, "description": "Type of diffuse radiation model. UNIFORM_SKY \u2014 Uniform diffuse model. The incoming diffuse radiation is the same from all sky directions. This is the default. STANDARD_OVERCAST_SKY \u2014 Standard overcast diffuse model. The incoming diffuse radiation flux varies with zenith angle.", "dataType": "String"}, {"name": "diffuse_proportion", "isOptional": true, "description": "The proportion of global normal radiation flux that is diffuse. Values range from 0 to 1. This value should be set according to atmospheric conditions. The default value is 0.3 for generally clear sky conditions. ", "dataType": "Double"}, {"name": "transmittivity", "isOptional": true, "description": "The fraction of radiation that passes through the atmosphere (averaged over all wavelengths). Values range from 0 (no transmission) to 1 (all transmission). The default is 0.5 for a generally clear sky. ", "dataType": "Double"}, {"name": "out_direct_radiation_features", "isOutputFile": true, "isOptional": true, "description": "The output feature class representing the direct incoming solar radiation for each location. The output has units of watt hours per square meter (WH/m 2 ). ", "dataType": "Feature Class"}, {"name": "out_diffuse_radiation_features", "isOutputFile": true, "isOptional": true, "description": "The output feature class representing the incoming solar radiation for each location that is diffuse. The output has units of watt hours per square meter (WH/m 2 ). ", "dataType": "Feature Class"}, {"name": "out_direct_duration_features", "isOutputFile": true, "isOptional": true, "description": "The output feature class representing the duration of direct incoming solar radiation. The output has units of hours. ", "dataType": "Feature Class"}]},
{"syntax": "AreaSolarRadiation (in_surface_raster, {latitude}, {sky_size}, {time_configuration}, {day_interval}, {hour_interval}, {each_interval}, {z_factor}, {slope_aspect_input_type}, {calculation_directions}, {zenith_divisions}, {azimuth_divisions}, {diffuse_model_type}, {diffuse_proportion}, {transmittivity}, {out_direct_radiation_raster}, {out_diffuse_radiation_raster}, {out_direct_duration_raster})", "name": "Area Solar Radiation (Spatial Analyst)", "description": "Derives incoming solar radiation from a raster surface. Learn more about how solar radiation is calculated", "example": {"title": "AreaSolarRadiation example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcpy from arcpy.sa import * from arcpy import env env.workspace = \"C:/sapyexamples/data\" outGlobalRadiation = AreaSolarRadiation ( \"dem30\" , \"\" , \"400\" , TimeMultipleDays ( 2008 , 91 , 152 )) outGlobalRadiation.save ( \"C:/sapyexamples/output/glob_rad\" )"}, "usage": ["Because insolation calculations can be time consuming, it is important to be sure all parameters are correct. Calculation for a large digital elevation model (DEM) can take hours, and a very large DEM could take days.", "The output radiation rasters will always be floating-point type and have units of watt hours per square meter (WH/m", "2", "). The direct duration raster output will be integer with unit hours.", "The latitude for the site area (units: decimal degree, positive for the north hemisphere and negative for the south hemisphere) is used in calculations such as solar declination and solar position.", "The analysis is designed only for local landscape scales, so it is generally acceptable to use one latitude value for the whole DEM. With larger datasets (i.e., states, countries, or continents), the insolation results will differ significantly at different latitudes (greater than 1 degree). To analyze broader geographic regions, it is necessary to divide the study area into zones with different latitudes.", "For multi-day time configurations, the maximum range of days is a total of one year (365 days, or 366 days for leap years). If the start day is greater than the end day, the time calculations will proceed into the following year.", "For example, [start day, end day] = [365, 31], represents December 31 to January 31 of the following year. The example of [1, 2], the time is inclusive for the first day from 0:00 hours (January 1) to 0:00 (January 2). The start day and end day cannot be equal.", "The year value for time configuration is used to determine a leap\r\nyear. It does not have any other influence on the solar radiation\r\nanalysis as the calculations are a function of the time period\r\ndetermined by Julian days.", "For within-day time configurations, the maximum range of time is one day (24 hours). Calculations will not be performed across days (for instance, from 12:00 p.m. to 12:00 p.m. the next day). The start time must be less than the end time.", "For within-day time configurations, the start and end times are displayed as solar time (units: decimal hours). Use the time conversion dialog box window to convert the local standard time and local solar time (HMS). When converting local standard time to solar time, the program accounts for equation of time.", "The use of a z-factor is essential for correcting calculations when the surface z units are expressed in units different from the ground x,y units. To get accurate results, the z units should be the same as the x,y ground units. If they are not the same, use a z-factor to convert z units to x,y units. For example, if your x,y units are meters and your z units are feet, you could specify a z-factor of 0.3048 to convert feet to meters.", "It is recommended to have your data in a projection coordinate system (units meters). However, if you choose to run the analysis with a spherical coordinate system you will need to specify an appropriate z-factor for that latitude. If your x,y units are decimal degrees and your z units are meters, some appropriate z-factors for particular latitudes are:", "The latitude for the site area (units: decimal degree, positive for the north hemisphere and negative for the south hemisphere) is used in calculations such as solar declination and solar position. Because the solar analysis is designed for landscape scales and local scales, it is acceptable to use one latitude value for the whole DEM. For broader geographic regions, it is necessary to divide the study area into zones with different latitudes.", "For input surface rasters containing a spatial reference, the mean latitude is automatically calculated; otherwise, latitude will default to 45 degrees. When using an input layer, the spatial reference of the data frame is used.", "Sky size is the resolution of the viewshed, sky map, and sun map rasters which are used in the radiation calculations (units: cells per side). These are upward-looking, hemispherical raster representations of the sky and do not have a geographic coordinate system. These grids are square (equal number of rows and columns).", "Increasing the sky size increases calculation accuracy but also increases calculation time considerably.", "When the \"day interval\" setting is small (e.g., < 14 days) a larger sky size should be used. During analysis the sun map (determined by the sky size) is used to represent sun positions (tracks) for particular time periods to calculate direct radiation. With smaller day intervals, if the sky size resolution is not large enough, sun tracks may overlap, resulting in zero or lower radiation values for that track. Increasing the resolution provides a more accurate result.", "The maximum sky size value is 4000. A value of 200 is default and is sufficient for whole DEMs with large day intervals (e.g., > 14 days). A sky size value of 512 is sufficient for calculations at point locations where calculation time is less of an issue. At smaller day intervals (e.g., < 14 days), it is recommended to use higher values. For example, to calculate insolation for a location at the equator with day interval = 1, it is recommended to use a sky size of 2800 or more.", "Day intervals greater than 3 are recommended as sun tracks within three days typically overlap, depending on sky size and time of year. For calculations of the whole year with monthly interval, day interval is disabled and the program internally uses calendar month intervals. The default value is 14.", "Because the viewshed calculation can be highly intensive, horizon angles are only traced for the number of calculation directions specified. Valid values must be multiples of 8 (8, 16, 24, 32, and so on). Typically, a value of 8 or 16 is adequate for areas with gentle topography, whereas a value of 32 is adequate for complex topography. The default value is 32.", "The number of calculation directions needed is related to the resolution of the input DEM. Natural terrain at 30m resolution is usually quite smooth so fewer directions are sufficient for most situations (16 or 32). With finer DEMs, and particularly with man-made structures incorporated in the DEMs, the number of directions needs to increase. Increasing the number of directions will increase accuracy but will also increase calculation time.", "The ", "Create outputs for each interval", " check box provides the flexibility to calculate insolation integrated over a specified time period or insolation \"for each interval\" in a time series. For example, for the within-day time period with an hour interval of one, checking this box will create hourly insolation values; otherwise, insolation integrated for the entire day is calculated.", "The ", "For each interval", " check box parameter affects the format and number of output radiation files. For area analysis, always verify that sufficient disk space is available before initiating calculations, because it creates multiple outputs. When checked, the default output format is an ESRI GRID stack containing multiple bands that correspond to the radiation or duration values for each time interval (hour interval when time configuration is less than one day, or day interval when multiple days).", "The diffuse proportion is the fraction of global normal radiation flux that is diffuse. Values range from 0 to 1. This value should be set according to atmospheric conditions. Typical values are 0.2 for very clear sky conditions and 0.3 for generally clear sky conditions.", "The amount of  solar radiation that is received by the surface is only a portion of what would be received outside the atmosphere.  Transmittivity is a property of the atmosphere and is the ratio of the energy received at the upper edge of the atmosphere to that reaching the earths's surface by the shortest path (in the direction of the zenith), averaged over all wavelengths. Values range from 0 (no transmission) to 1 (complete transmission). Typically observed values are 0.6 or 0.7 for very clear sky conditions and 0.5 for only a generally clear sky.", "Because the algorithms  corrects for elevation effects, transmittivity should always be given for sea level. Transmittivity has an inverse relation with the diffuse proportion parameter."], "parameters": [{"name": "in_surface_raster", "isInputFile": true, "isOptional": false, "description": "Input elevation surface raster. ", "dataType": "Raster Layer"}, {"name": "latitude", "isOptional": true, "description": "The latitude for the site area. The units are decimal degrees, with positive values for the northern hemisphere and negative for the southern. For input surface rasters containing a spatial reference, the mean latitude is automatically calculated; otherwise, latitude will default to 45 degrees. ", "dataType": "Double"}, {"name": "sky_size", "isOptional": true, "description": "The resolution or sky size for the viewshed, sky map, and sun map grids. The units are cells. The default creates a raster of 200 x 200 cells. ", "dataType": "Long"}, {"name": "time_configuration", "isOptional": true, "description": "Specifies the time configuration (period) used for calculating solar radiation. The Time class objects are used to specify the time configuration. The different types of time configurations available are TimeWithinDay , TimeMultiDays , TimeSpecialDays , and TimeWholeYear . The following are the forms: The default time_configuration is TimeMultiDays with the start_day of 5 and end_day of 160, for the current Julian year. TimeWithinDay({day},{start_time},{end_time}) TimeMultiDays({year},{start_day},{end_day}) TimeSpecialDays() TimeWholeYear({year})", "dataType": "Time configuration"}, {"name": "day_interval", "isOptional": true, "description": "The time interval through the year (units: days) used for calculation of sky sectors for the sun map. The default value is 14 (biweekly). ", "dataType": "Long"}, {"name": "hour_interval", "isOptional": true, "description": "Time interval through the day (units: hours) used for calculation of sky sectors for sun maps. The default value is 0.5. ", "dataType": "Double"}, {"name": "each_interval", "isOptional": true, "description": "Specifies whether to calculate a single total insolation value for all locations or multiple values for the specified hour and day interval. NOINTERVAL \u2014 A single total radiation value will be calculated for the entire time configuration. This is default. INTERVAL \u2014 Multiple radiation values will be calculated for each time interval over the entire time configuration. The number of outputs will depend on the hour or day interval. For example, for a whole year with monthly intervals, the result will contain 12 output radiation values for each location.", "dataType": "Boolean"}, {"name": "z_factor", "isOptional": true, "description": "The number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "slope_aspect_input_type", "isOptional": true, "description": "How slope and aspect information are derived for analysis. FROM_DEM \u2014 The slope and aspect grids are calculated from the input surface raster. This is the default. FLAT_SURFACE \u2014 Constant values of zero are used for slope and aspect.", "dataType": "String"}, {"name": "calculation_directions", "isOptional": true, "description": "The number of azimuth directions used when calculating the viewshed. Valid values must be multiples of 8 (8, 16, 24, 32, and so on). The default value is 32 directions, which is adequate for complex topography. ", "dataType": "Long"}, {"name": "zenith_divisions", "isOptional": true, "description": "The number of divisions used to create sky sectors in the sky map. The default is eight divisions (relative to zenith). Values must be greater than zero and less than half the sky size value. ", "dataType": "Long"}, {"name": "azimuth_divisions", "isOptional": true, "description": "The number of divisions used to create sky sectors in the sky map. The default is eight divisions (relative to north). Valid values must be multiples of 8. Values must be greater than zero and less than 160. ", "dataType": "Long"}, {"name": "diffuse_model_type", "isOptional": true, "description": "Type of diffuse radiation model. UNIFORM_SKY \u2014 Uniform diffuse model. The incoming diffuse radiation is the same from all sky directions. This is the default. STANDARD_OVERCAST_SKY \u2014 Standard overcast diffuse model. The incoming diffuse radiation flux varies with zenith angle.", "dataType": "String"}, {"name": "diffuse_proportion", "isOptional": true, "description": "The proportion of global normal radiation flux that is diffuse. Values range from 0 to 1. This value should be set according to atmospheric conditions. The default value is 0.3 for generally clear sky conditions. ", "dataType": "Double"}, {"name": "transmittivity", "isOptional": true, "description": "The fraction of radiation that passes through the atmosphere (averaged over all wavelengths). Values range from 0 (no transmission) to 1 (all transmission). The default is 0.5 for a generally clear sky. ", "dataType": "Double"}, {"name": "out_direct_radiation_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster representing the direct incoming solar radiation for each location. The output has units of watt hours per square meter (WH/m 2 ). ", "dataType": "Raster Dataset"}, {"name": "out_diffuse_radiation_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster representing the diffuse incoming solar radiation for each location. The output has units of watt hours per square meter (WH/m 2 ). ", "dataType": "Raster Dataset"}, {"name": "out_direct_duration_raster", "isOutputFile": true, "isOptional": true, "description": "The output raster representing the duration of direct incoming solar radiation. The output has units of hours. ", "dataType": "Raster Dataset"}, {"isOutputFile": true, "name": "out_global_radiation_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Slice (in_raster, number_zones, {slice_type}, {base_output_zone})", "name": "Slice (Spatial Analyst)", "description": "Slices or reclassifies the range of values of the input cells into zones of equal interval, equal area, or by natural breaks.", "example": {"title": "Slice example 1 (Python window)", "description": "Reclassify the input raster into five classes based on natural groupings inherent in the data.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outslice = Slice ( \"elevation\" , 5 , \"NATURAL_BREAKS\" ) outslice.save ( \"C:/sapyexamples/output/elev_slice\" )"}, "usage": ["Slice", " works best on data that is normally distributed. When using input raster data that is skewed the output result may not contain all of the classes that you had expected or specified.", "If an environment ", "Mask", " has been set, those cells that have been masked will receive NoData on the output slice raster.", "When using the EQUAL_AREA method, sometimes not all of the output zones (classes) will have an equal, or even similar, number of cells (i.e., area). This may be an inherent result based on the nature of the input values and the specified number of zones. If the results are deemed undesirable, you can try using a fewer number of zones or applying a statistics transformation (e.g., logarithm, square root, and so on) to the input dataset."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "number_zones", "isOptional": false, "description": "The number of zones to reclassify the input raster into. When the slice method is EQUAL_AREA, the output raster will have the defined number of zones, with a similar number of cells in each. When EQUAL_INTERVAL is used, the output raster will have the defined number of zones, each containing equal value ranges on the output raster. When NATURAL_BREAKS is used, the output raster will have the defined number of zones, with the number of cells in each determined by the class breaks. ", "dataType": "Long"}, {"name": "slice_type", "isOptional": true, "description": "The manner in which to slice the values in the input raster. EQUAL_INTERVAL \u2014 Determines the range of the input values and divides the range into the specified number of output zones. Each zone on the sliced output raster has the potential of having input cell values that have the same range from the extremes. EQUAL_AREA \u2014 Specifies that the input values will be divided into the specified number of output zones, with each zone having a similar number of cells. Each zone will represent a similar amount of area. NATURAL_BREAKS \u2014 Specifies that the classes will be based on natural groupings inherent in the data. Break points are identified by choosing the class breaks that best group similar values and that maximize the differences between classes. The cell values are divided into classes whose boundaries are set when there are relatively big jumps in the data values.", "dataType": "String"}, {"name": "base_output_zone", "isOptional": true, "description": "Defines the lowest zone value on the output raster dataset. The default value is 1. ", "dataType": "Long"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Reclassify (in_raster, reclass_field, remap, {missing_values})", "name": "Reclassify (Spatial Analyst)", "description": "Reclassifies (or changes) the values in a raster.", "example": {"title": "Reclassify example 1 (Python window)", "description": "The following examples show several ways of several ways reclassifying a raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outReclass1 = Reclassify ( \"landuse\" , \"Value\" , RemapValue ([[ 1 , 9 ],[ 2 , 8 ],[ 3 , 1 ],[ 4 , 6 ],[ 5 , 3 ],[ 6 , 3 ],[ 7 , 1 ]])) outReclass1.save ( \"C:/sapyexamples/output/landuse_rcls\" ) outReclass2 = Reclassify ( \"slope_grd\" , \"Value\" , RemapRange ([[ 0 , 10 , \"NODATA\" ],[ 10 , 20 , 1 ],[ 20 , 30 , 2 ], [ 30 , 40 , 3 ],[ 40 , 50 , 4 ],[ 50 , 60 , 5 ],[ 60 , 75 , 6 ]])) outReclass2.save ( \"C:/sapyexamples/output/slope_rcls\" ) outReclass3 = Reclassify ( \"pop_density\" , \"Value\" , RemapRange ([[ 10 , 10 , 1 ],[ 10 , 20 , 2 ],[ 20 , 25 , 3 ], [ 25 , 50 , 4 ],[ 50 ,]]), \"NODATA\" ) outReclass3.save ( \"C:/sapyexamples/output/popden_rcls\" )"}, "usage": ["The input raster must have valid statistics. If the statistics do\r\nnot exist, they can be created using the ", "Calculate Statistics", " tool in the Data Management Tools toolbox.", "If using the tool dialog box, the remap table can be stored for future use with the ", "Save", " button. Use the ", "Load", " button to open the remap tables you created previously with the ", "Save", " button.", "It is recommended to only load  tables previously saved by the ", "Reclassify", " tool.      The table format is specific and must contain the fields FROM, TO, OUT, and MAPPING.", "By default, the input raster will be classified into nine classes for the reclassification table.", "If the input raster is a layer, the old values of the reclassification will be obtained from the renderer. If the renderer is stretched, the reclassification will default to 255 classes.", "Once the remap table of the reclassification has been modified,  the values will not be updated if a new input raster is selected. If the reclassification is not suitable for the new raster, a default reclassification can be reinitialized by:", "This tool has a precision control that manages how decimal places are treated.", "When using the ", "Reclassify", " tool as part of a model:"], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "reclass_field", "isOptional": false, "description": "Field denoting the values that will be reclassified. ", "dataType": "Field"}, {"name": "remap", "isOptional": false, "description": "The Remap object is used to specify how to reclassify values of the input raster. There are two ways to define how the values will be reclassified in the output raster; RemapRange and RemapValue . Either ranges of input values can be assigned to a new output value, or individual values can be assigned to a new output value. The following are the forms of the remap objects. RemapRange ([[startValue, endValue, newValue],...]) startValue \u2014 The lower boundary of the range of values to be assigned a new output value. endValue \u2014 The upper boundary of the range of values to be assigned a new output value. newValue \u2014 The new value to be assigned to the range of input values defined by the start and end values. RemapValue ([[oldValue, newValue],...]) oldValue \u2014 Represents an original value from the base raster. newValue \u2014 The new reclassified value.", "dataType": "Remap"}, {"name": "missing_values", "isOptional": true, "description": "Denotes whether missing values in the reclass table retain their value or get mapped to NoData. DATA \u2014 Signifies that if any cell location on the input raster contains a value that is not present or reclassed in a remap table, the value should remain intact and be written for that location to the output raster. This is the default. NODATA \u2014 Signifies that if any cell location on the input raster contains a value that is not present or reclassed in a remap table, the value will be reclassed to NoData for that location on the output raster.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ReclassByTable (in_raster, in_remap_table, from_value_field, to_value_field, output_value_field, {missing_values})", "name": "Reclass by Table (Spatial Analyst)", "description": "Reclassifies (or changes) the values of the input cells of a raster using a remap table.", "example": {"title": "ReclassByTable example 1 (Python window)", "description": "This example uses a remap table to reclassify the input raster.", "code": "import arcpy from arcpy.sa import * from arcpy import env env.workspace = \"C:/sapyexamples/data\" outReclass = ReclassByTable ( \"slope\" , \"remapslope\" , \"FROM\" , \"TO\" , \"OUT\" ) outReclass.save ( \"C:/sapyexamples/output/recslope\" )"}, "usage": ["The input raster must have valid statistics. If the statistics do\r\nnot exist, they can be created using the ", "Calculate Statistics", " tool in the Data Management Tools toolbox.", "The ", "From value field", ", ", "To value field", ", and ", "Output value field", " are the field names in the table that define the remapping.", "To reclassify individual values, use a simple remap table of two items. The first item identifies the value to reclassify, and the other item identifies the value to assign to it. Set the 'to value field' the same as the 'from value field'. The value to assign to the output is 'output value field'.", "To reclassify ranges of values, the remap table must have items defining the start and end of each range, along with the value to assign the range. The item defining the start of the range is the ", "From value field", ", and the value defining the end of the range is the ", "To value field", ". The value to assign to the output is ", "Output value field", ".", "The remap table can be an INFO table, a .dbf file, an Access table, or a text file.", "The values in the from and to fields can be any numerical item. The assignment values in the output field must be integers.", "Values in the from field for .dbf, INFO, and Geodatabase tables do not need to be sorted. For text-file based tables, they must be sorted in ascending order. The values should not overlap in either case."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "in_remap_table", "isInputFile": true, "isOptional": false, "description": "Table holding fields defining value ranges to be reclassified and the values they will become. ", "dataType": "Table View"}, {"name": "from_value_field", "isOptional": false, "description": "Field holding the beginning value for each value range to be reclassified. This is a numeric field of the input remap table. ", "dataType": "Field"}, {"name": "to_value_field", "isOptional": false, "description": "Field holding the ending value for each value range to be reclassified. This is a numeric field of the input remap table. ", "dataType": "Field"}, {"name": "output_value_field", "isOutputFile": true, "isOptional": false, "description": "Field holding the integer values to which each range should be changed. This is an integer field of the input remap table. ", "dataType": "Field"}, {"name": "missing_values", "isOptional": true, "description": "Denotes whether missing values in the reclass table retain their value or get mapped to NoData. DATA \u2014 Signifies that if any cell location on the input raster contains a value not present or reclassed in a remap table, the value should remain intact and be written for that location to the output raster. This is the default. NODATA \u2014 Signifies that if any cell location on the input raster contains a value not present or reclassed in a remap table, the value will be reclassed to NoData for that location on the output raster.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ReclassByASCIIFile (in_raster, in_remap_file, {missing_values})", "name": "Reclass by ASCII File (Spatial Analyst)", "description": "Reclassifies (or changes) the values of the input cells of a raster using an ASCII remap file. \r\n Learn more about how Reclass by ASCII File works", "example": {"title": "ReclassByASCIIFile example 1 (Python window)", "description": "example uses an ASCII remap file to reclassify the input raster.", "code": "import arcpy from arcpy.sa import * from arcpy import env env.workspace = \"C:/sapyexamples/data\" outReclass = ReclassByASCIIFile ( \"slope\" , \"remapslope.rmp\" ) outReclass.save ( \"C:/sapyexamples/output/recslope\" )"}, "usage": ["The input raster must have valid statistics. If the statistics do\r\nnot exist, they can be created using the ", "Calculate Statistics", " tool in the Data Management Tools toolbox.", "The output raster will always be of integer type. If the output assignment values in the ASCII file are floating-point values, an error message will be returned and the program will halt."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "in_remap_file", "isInputFile": true, "isOptional": false, "description": "ASCII remap file defining the single values or ranges to be reclassified and the values they will become. Allowed extensions for the ASCII remap files are .rmp , .txt , and .asc . ", "dataType": "File"}, {"name": "missing_values", "isOptional": true, "description": "Denotes whether missing values in the reclass file retain their value or get mapped to NoData. DATA \u2014 Signifies that if any cell location on the input raster contains a value that is not present or reclassed in the remap file, the value should remain intact and be written for that location to the output raster. NODATA \u2014 A keyword signifying that if any cell location on the input raster contains a value that is not present or reclassed in the remap file, the value will be reclassed to NODATA for that location on the output raster.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "Lookup (in_raster, lookup_field)", "name": "Lookup (Spatial Analyst)", "description": "Creates a new raster by looking up values found in another field in the table of the input raster.", "example": {"title": "Lookup example 1 (Python window)", "description": "This example creates a new raster determined by the specified field of the input raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outRaster = Lookup ( \"mycity\" , \"land_code\" ) outRaster.save ( \"C:/sapyexamples/output/mylandcode.img\" )"}, "usage": ["Lookup", " supports fields that are  numeric (integer or floating point) or string.  If the field is  integer or string, the output will be an integer raster; otherwise, the output raster will be a floating-point raster.", "If the lookup field is of  integer type, the values of that field will be written to the output raster attribute table as Value. Other items in the input raster attribute table will not be transferred to the output raster attribute table.", "For example, an attribute table of input raster with numeric field ", "Attr1", ":", "Output attribute table from ", "Lookup", " on ", "Attr1", " field:", "If the lookup field is a string type, the lookup field will appear in the output raster attribute table, and the value field will be the same as for input raster. Any other items in the input raster's attribute table will not be transferred to the output raster's attribute table.", "For example, consider the attribute table of an input raster with string field ", "Text1", ":", "The attribute table of the output raster from running ", "Lookup", " on the ", "Text1", " field would be:"], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that contains a field from which to create a new raster. ", "dataType": "Raster Layer"}, {"name": "lookup_field", "isOptional": false, "description": "Field containing the desired values for the new raster. It can be a numeric or string type. ", "dataType": "Field"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CreateRandomRaster ({seed_value}, {cell_size}, {extent})", "name": "Create Random Raster (Spatial Analyst)", "description": "Creates a raster of random floating point values between 0.0 and 1.0 within the extent and cell size of the analysis window.", "example": {"title": "CreateRandomRaster example 1 (Python window)", "description": "This sample creates an output raster of random values at the defined cell size and extent.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outRandRaster = CreateRandomRaster ( 100 , 2 , Extent ( 0 , 0 , 150 , 150 )) outRandRaster.save ( \"C:/sapyexamples/output/outrandom\" )"}, "usage": ["The ", "Create Random Raster", " tool generates values for every cell in\r\nthe output raster.", "The output raster from this tool is always floating point.", "The cell values will have up to 7 digits of precision after the decimal point.", "Repeatedly using the same seed value or the default will not produce the same raster.", "You can change the seed through a parameter to ensure you obtain different starting points for the random number generator.", "To generate the values, the random number generator accompanying the standard C libraries from Microsoft is used. In Visual Studio 6, the source code is contained in the RAND.c file, which is generally located in  ", "Program Files/Microsoft Visual Studio/VC98/CRT/SRC", ".", "The Data Management toolbox has a ", "Create Random Raster", " tool that offers some more options for the distribution of the values."], "parameters": [{"name": "seed_value", "isOptional": true, "description": "A value to be used to reseed the random number generator. This may be an integer or floating-point number. Rasters are not permitted as input. The random number generator is automatically seeded with the current value of the system clock (seconds since January 1, 1970). The range of permissible values for the seed value is -2 31 +1 to 2 31 (or -2,147,483,647 to 2,147,483,648). ", "dataType": "Double"}, {"name": "cell_size", "isOptional": true, "description": "The cell size for the output raster dataset. This is the value in the environment if specifically set. If not specifically set, it is the shorter of the width or height of the environment extent in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "extent", "isOptional": true, "description": "The extent for the output raster dataset. The Extent is a Python class. In this tool it is in the form of: Extent(XMin, YMin, XMax, YMax) The coordinates are specified in the same map units as the in_raster . The extent will be the value in the environment if specifically set. If not specifically set, the default is 0, 0, 250, 250. where XMin and YMin define the lower-left coordinate of the extent, with XMax and YMax defining the upper-right coordinate.", "dataType": "Extent"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CreateNormalRaster ({cell_size}, {extent})", "name": "Create Normal Raster (Spatial Analyst)", "description": "Creates a raster of random values with a normal (gaussian) distribution within the extent and cell size of the analysis window.", "example": {"title": "CreateNormalRaster example 1 (Python window)", "description": "This sample creates an output raster of normally distributed values at the defined cell size and extent.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outNormalRaster = CreateNormalRaster ( 2 , Extent ( 0 , 0 , 150 , 150 )) outNormalRaster.save ( \"C:/sapyexamples/output/outnormal\" )"}, "usage": ["The ", "Create Normal Raster", " tool generates values for every cell in the output raster.  ", "The output raster from this tool is always floating point.", "The cell values will have up to 7 digits of precision after the decimal point.", "The output values will have a mean of 0.0 and a standard deviation of 1.0. If a different standard deviation is desired, multiply the output raster by that value. If a different mean is desired, add that value to the raster. For example, to create a raster where the values are characterized by a mean of 39 and a standard deviation of 2.5, multiply the results of ", "Create Normal Raster", " by 2.5, then add 39.", "In Map Algebra, you could do something like:", "The random number generator is automatically seeded with the current value of the system clock (seconds since January 1, 1970). Reseeding the ", "Create Random Raster", " tool also reseeds ", "Create Normal Raster", "."], "parameters": [{"name": "cell_size", "isOptional": true, "description": "The cell size for the output raster dataset. This is the value in the environment if specifically set. If not specifically set, it is the shorter of the width or height of the environment extent in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "extent", "isOptional": true, "description": "The extent for the output raster dataset. The Extent is a Python class. In this tool it is in the form of: Extent(XMin, YMin, XMax, YMax) The coordinates are specified in the same map units as the in_raster . The extent will be the value in the environment if specifically set. If not specifically set, the default is 0, 0, 250, 250. where XMin and YMin define the lower-left coordinate of the extent, with XMax and YMax defining the upper-right coordinate.", "dataType": "Extent"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "CreateConstantRaster (constant_value, {data_type}, {cell_size}, {extent})", "name": "Create Constant Raster (Spatial Analyst)", "description": "Creates a raster of a constant value within the extent and cell size of the analysis window.", "example": {"title": "CreateConstantRaster example 1 (Python window)", "description": "This sample creates a float raster of a particular value at the defined cell size and extent.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outConstRaster = CreateConstantRaster ( 12.7 , \"FLOAT\" , 2 , Extent ( 0 , 0 , 250 , 250 )) outConstRaster.save ( \"C:/sapyexamples/output/outconst2\" )"}, "usage": ["The ", "Create Constant Raster", " tool assigns the specified value to every cell in\r\nthe output raster.", "The constant value must be a numeric value. Scientific notation is acceptable (for example, 3.048e", "-4", ").", "Some typical reasons for creating a raster of all the same values include:"], "parameters": [{"name": "constant_value", "isOptional": false, "description": "The constant value from which to create an output raster dataset. ", "dataType": "Double"}, {"name": "data_type", "isOptional": true, "description": "Data type of the output raster dataset. If the specified data type is FLOAT, the input constant value is only accurate to 7 decimal places (single precision). INTEGER\u2014An integer raster will be created. FLOAT\u2014A floating-point raster will be created.", "dataType": "String"}, {"name": "cell_size", "isOptional": true, "description": "The cell size for the output raster dataset. This is the value in the environment if specifically set. If not specifically set, it is the shorter of the width or height of the environment extent in the output spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "extent", "isOptional": true, "description": "The extent for the output raster dataset. The Extent is a Python class. In this tool it is in the form of: Extent(XMin, YMin, XMax, YMax) The coordinates are specified in the same map units as the in_raster . The extent will be the value in the environment if specifically set. If not specifically set, the default is 0, 0, 250, 250. where XMin and YMin define the lower-left coordinate of the extent, with XMax and YMax defining the upper-right coordinate.", "dataType": "Extent"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "TinPolygonTag_3d (in_tin, out_feature_class, {tag_field})", "name": "TIN Polygon Tag (3D Analyst)", "description": "Creates polygon features using tag values in a triangulated irregular network (TIN) dataset.", "example": {"title": "TinPolygonTag example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.TinPolygonTag_3d ( \"tin\" , \"tin_polytag.shp\" , \"Tag_Value\" )"}, "usage": ["Tag values can be assigned using an integer field in a polygon feature class by loading the polygon into the TIN as a ", "valuefill", " surface type. For more information on surface types, please read ", "Fundamentals of editing TIN surfaces", ".", "Triangles that do not have a tag explicitly defined are assigned the default value of 0.", "All contiguous triangles with an identical tag value will be stored in a single polygon feature. ", "The tag value will be denoted as an attribute in the output feature class."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "tag_field", "isOptional": true, "description": "The name of the field storing the tag attribute in the output feature class. The default field name is Tag_Value . ", "dataType": "String"}]},
{"syntax": "SurfaceLength_3d (in_surface, in_feature_class, {out_length_field}, {sample_distance}, {z_factor}, {method}, {pyramid_level_resolution})", "name": "Surface Length (3D Analyst)", "description": "Calculates the surface length for each polyline or polygon feature based on a raster, triangulated irregular network (TIN), or terrain dataset surface. The surface length information is stored as an attribute of the input feature class.", "example": {"title": "SurfaceLength example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcgisscripting gp = arcgisscripting.create () gp.CheckOutExtension ( \"3D\" ) gp.workspace = \"C:/data\" gp.SurfaceLength_3d ( \"elevation_tin\" , \"lines.shp\" , \"Length3D\" )"}, "usage": ["Use a smaller sampling distance to increase the accuracy of the surface length calculations.", "Use {out_length_field} to give the length field a custom name.", "Only polyline and polygon feature classes are valid inputs for this tool."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input raster, TIN, or terrain dataset whose values will be used for interpolation. ", "dataType": "TIN layer; raster layer; terrain lLayer"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The input polygon or polyline feature class. ", "dataType": "Feature layer"}, {"name": "out_length_field", "isOutputFile": true, "isOptional": true, "description": "The name of the attribute field to contain the surface length. ", "dataType": "String"}, {"name": "sample_distance", "isOptional": true, "description": "The surface spacing at which the length is calculated. By default, the sampling distance is the natural densification of a TIN or the cell size of a raster. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": "The factor multiplied by input surface values to store new values in the length field. The Z factor is used to convert z-units to match x,y units. ", "dataType": "Double"}, {"name": "method", "isOptional": true, "description": "The interpolation method. For raster surfaces, the only option is BILINEAR. Select LINEAR or NATURAL_NEIGHBORS. ", "dataType": "String"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "TopoToRaster_3d (in_topo_features, out_surface_raster, {cell_size}, {extent}, {Margin}, {minimum_z_value}, {maximum_z_value}, {enforce}, {data_type}, {maximum_iterations}, {roughness_penalty}, {discrete_error_factor}, {vertical_standard_error}, {tolerance_1}, {tolerance_2}, {out_stream_features}, {out_sink_features}, {out_diagnostic_file}, {out_parameter_file}, {profile_penalty}, {out_residual_feature}, {out_stream_cliff_error_feature}, {out_contour_error_feature})", "name": "Topo to Raster (3D Analyst)", "description": "Interpolates a hydrologically correct raster surface from point,\r\nline, and polygon data. \r\n Learn more about how Topo to Raster works", "example": {"title": "TopoToRaster example 1 (Python window)", "description": "This example creates a hydrologically correct TIFF surface raster from point, line, and polygon data.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.TopoToRaster_3d ( \"C:/data/spots.shp spot_meters PointElevation; C:/data/contours.shp spot_meters Contour; C:/data/cliff.shp # Cliff\" , \"c:/output/toporast\" , 32 , \"740825 4309275 748825 4317275\" , \"20\" , \"\" , \"\" , \"ENFORCE\" , \"CONTOUR\" , \"40\" , \"\" , \"1\" , \"0\" )"}, "usage": ["The best results will be obtained if all input data is stored in the same planar coordinate system and has the same ZUnits. Unprojected data (latitude-longitude) can be used; however, the results may not be as accurate, particularly at high latitudes.", "Topo to Raster", " will only use four input data points for the interpolation of each output cell. All additional points are ignored. If too many points are encountered by the algorithm, an error may occur, indicating the point dataset has too many points. The maximum number of points that can be used is ", "NRows * NCols", ", where ", "NRows", " is the number of rows in the output raster and ", "NCols", " is the number of columns.", "When the input feature type is CONTOUR, the algorithm first generates a generalized morphology of the surface based on the curvature of the contours. The algorithm then implements the contours as a source of elevation information. Contours are best suited for large-scale data where the contours and corners are reliable indicators of streams and ridges. At smaller scales it can be just as effective, and less expensive, to digitize corner points of contours and use them as an input point feature class.", "Representing braided streams or using arcs to represent two sides of a stream may not produce reliable results. Stream data always takes priority over point or contour data; therefore, elevation data points that conflict with descent down each stream are ignored. Stream data is a powerful way of adding topographic information to the interpolation, further ensuring the quality of the output DEM.", "Typical values for the ", "Tolerance 1", " and ", "Tolerance 2", " settings are:", "To make experimentation with the inputs and parameters easier, use the ", "Topo to Raster", " dialog box to create an output parameter file, which can be modified in any text editor and used as input to the ", "Topo to Raster by File", " tool.", "Topo to Raster is a memory-intensive application and it is therefore not possible to create large output rasters. When large output is required, use the MARGIN parameter."], "parameters": [{"name": "in_topo_features", "isInputFile": true, "isOptional": false, "description": "The input features containing the z-values to be interpolated into a surface raster. Each feature input can have a field specified that contains the z-values, and one of six types specified. There are six types of accepted inputs: <Feature Layer>\u2014The input feature dataset. {Field}\u2014The name of the field that stores the attributes, where appropriate. {Type}\u2014The type of input feature dataset. POINTELEVATION\u2014A point feature class representing surface elevations. The Field stores the elevations of the points. CONTOUR\u2014A line feature class that represents elevation contours. The Field stores the elevations of the contour lines. STREAM\u2014A line feature class of stream locations. All arcs must be oriented to point downstream. The feature class should only contain single arc streams. There is no Field option for Stream. SINK\u2014A point feature class that represents known topographic depressions. Topo to Raster will not attempt to remove from the analysis any points explicitly identified as sinks. The Field used should be one that stores the elevation of the legitimate sink. If NONE is selected, only the location of the sink is used. BOUNDARY\u2014A feature class containing a single polygon that represents the outer boundary of the output raster. Cells in the output raster outside this boundary will be NoData. This option can be used for clipping out water areas along coastlines before making the final output raster. There is no Field option for Boundary. LAKE\u2014A polygon feature class that specifies the location of lakes. All output raster cells within a lake will be assigned to the minimum elevation value of all cells along the shoreline. There is no Field option for Lake. Cliff\u2014A line feature class of the cliffs. The cliff line features must be oriented so that the left-hand side of the line is on the low side of the cliff and the right-hand side is the high side of the cliff. There is no Field option for Cliff. Coast\u2014A polygon feature class containing the outline of a coastal area. Cells in the final output raster that lie outside these polygons are set to a value that is less than the user-specified minimum height limit. There is no Field option for Coast. Exclusion\u2014A polygon feature class of the areas in which the input data should be ignored. These polygons permit removal of elevation data from the interpolation process. This is typically used to remove elevation data associated with dam walls and bridges. This enables interpolation of the underlying valley with connected drainage structure. There is no Field option for Exclusion.", "dataType": "TopoInput"}, {"name": "out_surface_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Layer"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "extent", "isOptional": true, "description": "Extent for the output raster dataset. Interpolation will occur out to the x and y limits, and cells outside that extent will be NoData. For best interpolation results along the edges of the output raster, the x and y limits should be smaller than the extent of the input data by at least 10 cells on each side. The default extent is the smallest of all extents of the input feature data. X_Minimum\u2014The default is the smallest x coordinate of all inputs. Y_Minimum\u2014The default is the smallest y coordinate of all inputs. X_Maximum\u2014The default is the largest x coordinate of all inputs. Y_Maximum\u2014The default is the largest y coordinate of all inputs.", "dataType": "Extent"}, {"name": "Margin", "isOptional": false, "description": "Distance in cells to interpolate beyond the specified output extent and boundary. The value must be greater than or equal to 0 (zero). The default value is 20. If the {extent} and Boundary feature dataset are the same as the limit of the input data (the default), values interpolated along the edge of the DEM will not match well with adjacent DEM data. This is because they have been interpolated using one-half as much data as the points inside the raster, which are surrounded on all sides by input data. The {margin} option allows input data beyond these limits to be used in the interpolation. ", "dataType": "Long"}, {"name": "minimum_z_value", "isOptional": true, "description": "The minimum z-value to be used in the interpolation. The default is 20 percent below the smallest of all the input values. ", "dataType": "Double"}, {"name": "maximum_z_value", "isOptional": true, "description": "The maximum z-value to be used in the interpolation. The default is 20 percent above the largest of all input values. ", "dataType": "Double"}, {"name": "enforce", "isOptional": true, "description": "The type of drainage enforcement to apply. The drainage enforcement option can be set to attempt to remove all sinks or depressions so a hydrologically correct DEM can be created. If sink points have been explicitly identified in the input feature data, these depressions will not be filled. ENFORCE \u2014 The algorithm will attempt to remove all sinks it encounters, whether they are real or spurious. This is the default. NO_ENFORCE \u2014 No sinks will be filled. ENFORCE_WITH_SINK \u2014 Points identified as sinks in Input feature data represent known topographic depressions and will not be altered. Any sink not identified in input feature data is considered spurious, and the algorithm will attempt to fill it.Having more than 8,000 spurious sinks causes the tool to fail.", "dataType": "String"}, {"name": "data_type", "isOptional": true, "description": "The dominant elevation data type of the input feature data. Specifying the relevant selection optimizes the search method used during the generation of streams and ridges. CONTOUR \u2014 The dominant type of input data will be elevation contours. This is the default. SPOT \u2014 The dominant type of input will be point.", "dataType": "String"}, {"name": "maximum_iterations", "isOptional": true, "description": "The maximum number of interpolation iterations. The number of iterations must be greater than zero. A default of 20 is normally adequate for both contour and line data. A value of 30 will clear fewer sinks. Rarely, higher values (45\u201350) may be useful to clear more sinks or to set more ridges and streams. Iteration ceases for each grid resolution when the maximum number of iterations has been reached. ", "dataType": "Long"}, {"name": "roughness_penalty", "isOptional": true, "description": "The integrated squared second derivative as a measure of roughness. The roughness penalty must be zero or greater. If the primary input data type is CONTOUR, the default is zero. If the primary data type is SPOT, the default is 0.5. Larger values are not normally recommended. ", "dataType": "Double"}, {"name": "discrete_error_factor", "isOptional": true, "description": "The discrete error factor is used to adjust the amount of smoothing when converting the input data to a raster. The value must be greater than zero. The normal range of adjustment is 0.5 to 2, and the default is 1. A smaller value results in less data smoothing; a larger value causes greater smoothing. ", "dataType": "Double"}, {"name": "vertical_standard_error", "isOptional": true, "description": "The amount of random error in the z-values of the input data. The value must be zero or greater. The default is zero. The vertical standard error may be set to a small positive value if the data has significant random (non-systematic) vertical errors with uniform variance. In this case, set the vertical standard error to the standard deviation of these errors. For most elevation datasets, the vertical error should be set to zero, but it may be set to a small positive value to stabilize convergence when rasterizing point data with stream line data. ", "dataType": "Double"}, {"name": "tolerance_1", "isOptional": true, "description": "This tolerance reflects the accuracy and density of the elevation points in relation to surface drainage. For point datasets, set the tolerance to the standard error of the data heights. For contour datasets, use one-half the average contour interval. The value must be zero or greater. The default is 2.5 if the data type is CONTOUR and zero if the data type is SPOT. ", "dataType": "Double"}, {"name": "tolerance_2", "isOptional": true, "description": "This tolerance prevents drainage clearance through unrealistically high barriers. The value must be greater than zero. The default is 100 if the data type is CONTOUR and 200 if the data type is SPOT. ", "dataType": "Double"}, {"name": "out_stream_features", "isOutputFile": true, "isOptional": true, "description": "The output line feature class of stream polyline features and ridge line features. The line features are created at the beginning of the interpolation process. It provides the general morphology of the surface for interpolation. It can be used to verify correct drainage and morphology by comparing known stream and ridge data. The polyline features are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_sink_features", "isOutputFile": true, "isOptional": true, "description": "The output point feature class of the remaining sink point features. These are the sinks that were not specified in the sink input feature data and were not cleared during drainage enforcement. Adjusting the values of the tolerances, tolerance_1 and tolerance_2 , can reduce the number of remaining sinks. Remaining sinks often indicate errors in the input data that the drainage enforcement algorithm could not resolve. This can be an efficient way of detecting subtle elevation errors. ", "dataType": "Feature Class"}, {"name": "out_diagnostic_file", "isOutputFile": true, "isOptional": true, "description": "The output diagnostic file listing all inputs and parameters used and the number of sinks cleared at each resolution and iteration. ", "dataType": "File"}, {"name": "out_parameter_file", "isOutputFile": true, "isOptional": true, "description": "The output parameter file listing all inputs and parameters used, which can be used with Topo to Raster by File to run the interpolation again. ", "dataType": "File"}, {"name": "profile_penalty", "isOptional": true, "description": "The profile curvature roughness penalty is a locally adaptive penalty that can be used to partly replace total curvature. It can yield good results with high-quality contour data but can lead to instability in convergence with poor data. Set to 0.0 for no profile curvature (the default), set to 0.5 for moderate profile curvature, and set to 0.8 for maximum profile curvature. Values larger than 0.8 are not recommended and should not be used. ", "dataType": "Double"}, {"name": "out_residual_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of all the large elevation residuals as scaled by the local discretisation error. All the scaled residuals larger than 10 should be inspected for possible errors in input elevation and stream data. Large-scaled residuals indicate conflicts between input elevation data and streamline data. These may also be associated with poor automatic drainage enforcements. These conflicts can be remedied by providing additional streamline and/or point elevation data after first checking and correcting errors in existing input data. Large unscaled residuals usually indicate input elevation errors. ", "dataType": "Feature Class"}, {"name": "out_stream_cliff_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of locations where possible stream and cliff errors occur. The locations where the streams have closed loops, distributaries and streams over cliffs can be identified from the point feature class. Cliffs with neighboring cells that are inconsistent with the high and low sides of the cliff are also indicated. This can be a good indicator of cliffs with incorrect direction. Points are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_contour_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of possible errors pertaining to the input contour data. Contours with bias in height exceeding five times the standard deviation of the contour values as represented on the output raster are reported to this feature class. Contours that join other contours with a different elevation are flagged in this feature class by the code 1; this is a sure sign of a contour label error. ", "dataType": "Feature Class"}]},
{"syntax": "ObserverPoints_3d (in_raster, in_observer_point_features, out_raster, {z_factor}, {curvature_correction}, {refractivity_coefficient})", "name": "Observer Points (3D Analyst)", "description": "Identifies which observer points are visible from each raster surface location. Learn more about how Observer Points works", "example": {"title": "ObserverPoints example 1 (Python window)", "description": "This example identifies exactly which observer points are visible from each raster surface location.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.ObserverPoints_3d ( \"elevation\" , \"observers.shp\" , \"C:/output/outobspnt01\" , 1 , \"CURVED_EARTH\" , 0.13 )"}, "usage": ["Determining observer points is a computer-intensive process. The processing time is dependent on the resolution. For preliminary studies, you may want to use a coarser cell size to reduce the number of cells in the input. Use the full resolution raster when the final results are ready to be generated.", "If the input raster contains undesirable noise caused by sampling errors, and you have the ", "ArcGIS Spatial Analyst extension", " available, you can smooth the raster with a low-pass filter, such as the Mean option of ", "Focal Statistics", ", before running this tool.", "The visibility of each cell center is determined by comparing the altitude angle to the cell center with the altitude angle to the local horizon. The local horizon is computed by considering the intervening terrain between the point of observation and the current cell center. If the point lies above the local horizon, it is considered visible."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "in_observer_point_features", "isInputFile": true, "isOptional": false, "description": "The point feature class that identifies the observer locations. The maximum number of points allowed is 16. ", "dataType": "Feature Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The output identifies exactly which observer points are visible from each raster surface location. ", "dataType": "Raster Dataset"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "curvature_correction", "isOptional": true, "description": "Allows correction for the earth's curvature. FLAT_EARTH \u2014 No curvature correction will be applied. This is the default. CURVED_EARTH \u2014 Curvature correction will be applied.", "dataType": "Boolean"}, {"name": "refractivity_coefficient", "isOptional": true, "description": "Coefficient of the refraction of visible light in air. The default value is 0.13. ", "dataType": "Double"}]},
{"syntax": "FeatureClassZToASCII_3d (in_feature_class, output_location, out_file, {format}, {delimiter}, {decimal_format}, {digits_after_decimal}, {decimal_separator})", "name": "Feature Class Z To ASCII (3D Analyst)", "description": "Exports 3D features to ASCII text files storing  GENERATE ,  XYZ , or profile data.", "example": {"title": "FeatureClassZToASCII example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.FeatureClassZToASCII_3d ( \"LidarPts.shp\" , \"\" , \"ASCII_LidarPts.txt\" , \"GENERATE\" , \"COMMA\" , \"FIXED\" , 6 , \"DECIMAL_POINT\" )"}, "usage": ["The ", "PROFILE", " format provides profile information for 3D line features that can be imported into specialized graphing applications.  Each line feature in the source feature class will be written to a separate file whose name is suffixed with the line's unique ID. Each row in the profile table contains the distance from the starting position of the line to the vertex (D) followed by  the elevation of that vertex.", "The ", "XYZ", " format  store x, y, and z coordinates as floating-point values, where each row represents a distinct point record.", "Point and multipoint features are written to the same file, whereas each polygon and polyline feature is written to a separate text file whose name is suffixed with the feature's ID. Each part of features with multiple parts gets written to a separate file and its file name will have its part number appended after the feature's ID.", "The ", "GENERATE", " format does not support  header lines, but it stores all input features in one file. ", "Multipoint features that originate from the same record in the originating feature class will share the same ID.", "The first and last XYZ coordinates for polygon features are always identical."], "parameters": [{"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The 3D point, multipoint, polyline, or polygon feature class that will be exported to an ASCII file. ", "dataType": "Feature Layer"}, {"name": "output_location", "isOutputFile": true, "isOptional": false, "description": "The folder that output files will be written to. ", "dataType": "Folder"}, {"name": "out_file", "isOutputFile": true, "isOptional": false, "description": "The name of the resulting ASCII file. If a line or polygon feature class is exported to XYZ format, the file name is used as a base name. Each feature will have a unique file output since the XYZ format only supports one line or polygon per file. Multipart features will also have each part written to a separate file. The file name will be appended with the OID of each feature, as well as any additional characters needed to make each file name unique. ", "dataType": "String"}, {"name": "format", "isOptional": true, "description": "The format of the ASCII file being created. GENERATE \u2014 Writes output in the GENERATE format. This is the default. XYZ \u2014 Writes XYZ information of input features. One file will be created for each line or polygon in the input feature. PROFILE \u2014 Writes profile information for line features that can be used in external graphing applications.", "dataType": "String"}, {"name": "delimiter", "isOptional": true, "description": "The field delimeter used in the text file. SPACE \u2014 A space will be used to delimit field values. This is the default. COMMA \u2014 A comma will be used to delimit field values. This option is not applicable if the decimal separator is also a comma.", "dataType": "String"}, {"name": "decimal_format", "isOptional": true, "description": "The method used to determine the number of significant digits stored in the output files. AUTOMATIC \u2014 The number of significant digits needed to preserve the available precision, while removing unnecessary trailing zeros, is automatically determined. This is the default. FIXED \u2014 The number of significant digits is defined in the Digits after Decimal parameter.", "dataType": "String"}, {"name": "digits_after_decimal", "isOptional": true, "description": "Used when the Decimal Notation is set to FIXED. This determines how many digits after the decimal are written for floating-point values written to the output files. ", "dataType": "Long"}, {"name": "decimal_separator", "isOptional": true, "description": "The decimal character used in the text file to differentiate the integer of a number from its fractional part. DECIMAL_POINT \u2014 A point is used as the decimal character. This is the default. DECIMAL_COMMA \u2014 A comma is used as the decimal character.", "dataType": "String"}]},
{"syntax": "TinAspect_3d (in_tin, out_feature_class, {class_breaks_table}, {aspect_field})", "name": "TIN Aspect (3D Analyst)", "description": "Extracts aspect information from an input TIN into an output feature class. Produces a polygon feature class whose polygons are categorized by the input TIN's triangle aspect values.", "example": {"title": "TIN Aspect Example (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcgisscripting gp = arcgisscripting.create () gp.CheckOutExtension ( \"3D\" ) gp.workspace = \"C:/data\" gp.TinAspect_3d ( \"tin\" , \"aspect.shp\" )"}, "usage": ["Aspect is expressed in degrees."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "Tin Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "class_breaks_table", "isOptional": true, "description": "An input table containing the classification breaks that will be used to classify the output feature class. ", "dataType": "Table"}, {"name": "aspect_field", "isOptional": true, "description": "The field containing aspect values. ", "dataType": "String"}]},
{"syntax": "Plus_3d (in_raster_or_constant1, in_raster_or_constant2, out_raster)", "name": "Plus (3D Analyst)", "description": "Adds (sums) the values of two rasters on a cell-by-cell basis.", "example": {"title": "Plus example 1 (Python window)", "description": "This example adds the values of two GRID rasters and outputs the result as a GRID raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Plus_3d ( \"elevation\" , 15 , \"C:/output/outplus.img\" )"}, "usage": ["The order of inputs is irrelevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input whose values will be added to. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input whose values will be added to the first input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The cell values are the sum of the first input added to the second. ", "dataType": "Raster Dataset"}]},
{"syntax": "Near3D_3d (in_features, near_features, {search_radius}, {location}, {angle}, {delta})", "name": "Near 3D (3D Analyst)", "description": "Calculates the three-dimensional distance from each input feature to the nearest feature that resides in one or more near feature classes. ", "example": {"title": "Near3D example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.Near3D_3d ( \"points_3D.shp\" , \"buildings_multipatch.shp\" , \"30\" , \"LOCATION\" , \"ANGLE\" , \"DELTA\" )"}, "usage": ["All geometry types are supported, and all input feature classes must have Z values.", " The ", "Near Features", " can include one or more feature classes of different shape types.", "The ", "Near Features", " can be the same dataset as the ", "Input Features", ". When an input feature's nearest ", "Near Feature", " is itself (distance is 0), this feature is ignored and the next nearest feature is found.", "The basic difference between the ", "Near", " tool and the ", "Near 3D", " tool\r\nis that the ", "Near 3D", " tool works with 3D features instead of 2D\r\nfeatures. Furthermore, the distances that are compared by the ", "Near\r\n3D", " tool are the 3D (slope) distances, rather than the horizontal\r\ndistances. A number of additional fields can be output as well.", " The angle fields will only be created and populated if the ", "Angle", " option is enabled.", "The following fields are appended to the input feature's attribute table:", "If the aforementioned fields already exist, their value gets updated.", "The values for NEAR_FID, NEAR_DIST, and NEAR_DIST3 will be -1 if no match is found within the Search Radius.", " The NEAR_DELTX, NEAR_DELTY, and NEAR_DELTZ fields will only be created and populated if the ", "Delta", " option is enabled.", "The NEAR_FROMX, NEAR_FROMY, NEAR_FROMZ, NEAR_X, NEAR_Y, and NEAR_Z fields will only be created and populated if the ", "Location", " check box is checked.", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The input feature class whose features will be attributed with information about the nearest feature. ", "dataType": "Feature Layer"}, {"name": "near_features", "isOptional": false, "description": "The one or more features whose proximity to the input features will be calculated. If multiple feature classes are specified, an additional field named NEAR_FC will be added to the input feature class to identify which near feature class contained the closest feature. ", "dataType": "Feature Layer"}, {"name": "search_radius", "isOptional": true, "description": "The maximum distance between Input Features and Near Features for which distance and FID will be determined. If no Search Radius is specified, all Near Features will be used. ", "dataType": "Linear Unit"}, {"name": "location", "isOptional": true, "description": "Determines whether six coordinate fields (two sets of XYZ values) are added to each input feature. The values are the three coordinates of the input feature (NEAR_FROMX, NEAR_FROMY, NEAR_FROMZ), and the three coordinates of the nearest feature (NEAR_X, NEAR_Y, NEAR_Z). The fields NEAR_FID and NEAR_DIST are always added, regardless of the Location option. NO_LOCATION \u2014 The X,Y, and Z coordinates are not saved. This is the default. LOCATION \u2014 The X,Y, and Z coordinates are saved. Six additional attribute fields are added to each input field.", "dataType": "Boolean"}, {"name": "angle", "isOptional": true, "description": "Determines whether the angles between the input feature and the nearest near feature will be calculated and stored into the NEAR_ANG_H and NEAR_ANG_V fields. Both angle values are in degrees, where one degree represents 1/360 of a circle, and fractions of a degree are represented as decimal points. Horizontal angles are measured from 180\u00b0 to -180\u00b0; 0\u00b0 to the east, 90\u00b0 to the north, 180\u00b0 (-180\u00b0) to the west, and -90\u00b0 to the south. Vertical angles are zero for horizontal, 90\u00b0 for straight up, and -90\u00b0 for straight down. NO_ANGLE \u2014 The angle will not be saved. This is the default. ANGLE \u2014 The angles will be added to the result, and the fields will be created if they don't already exist.", "dataType": "Boolean"}, {"name": "delta", "isOptional": true, "description": "Determines whether the distances along the primary axes between the input feature and the nearest near feature will be calculated and stored into the NEAR_DELTX, NEAR_DELTY, and NEAR_DELTZ fields. NO_DELTA \u2014 No deltas will be calculated. This is the default. DELTA \u2014 Deltas will be calculated. Three additional fields (NEAR_DELTX, NEAR_DELTY, and NEAR_DELTZ) will be added to the result. ", "dataType": "Boolean"}]},
{"syntax": "BatchDefine_samples (input_datasets, {Coordinate_System}, {Template_Dataset})", "name": "Batch Define Coordinate System (Samples)", "description": "Records the coordinate system information for the specified input datasets including any associated projection parameters, datum, or spheroid. It creates or modifies the dataset's projection parameters.", "example": {"title": "BatchDefine Example (Standalone Script)", "description": null, "code": "import arcgisscripting gp = arcgisscripting.create ( 9.3 ) gp.workspace = \"c:/Workspace/canada.mdb\" gp.BatchDefine ( \"rivers;roads\" , \"Coordinate Systems/Geographic Coordinate Systems/North America/North American Datum 1983.prj\" )"}, "usage": ["This command can be used if the input datasets do not have a ", "projection", " defined. If either of the input datasets has a projection defined, a warning will be raised but the tool will execute successfully.", "The input datasets can include feature classes, raster datasets, and layers.", "The ", "coordinate system", " information of the input is created or modified by this tool. No separate output feature class will be created.", "If the input datasets do not have a projection defined, the coordinate system will be listed as Unknown on the dialog box.", "If either of the input datasets has a coordinate system defined, use the Project tool to modify it.", "Use the Spatial Reference Properties dialog box to select a coordinate system definition from an existing definition list or dataset, or create a new coordinate system definition.", "Use the template parameter to select an existing definition from another dataset to create a new coordinate system definition.", "Either a coordinate system or a template dataset must be specified for the tool to execute.", "When using this tool in ModelBuilder, the outputs will be two Boolean parameters. One parameter is named Completed and the other is named Error. These parameters are provided to allow chaining in ModelBuilder. Preconditions can be established with other processes. This will allow branching in models. If the tool fails, the model can stop or run another series of processes. If it completes successfully, the model can continue running."], "parameters": [{"name": "input_datasets", "isInputFile": true, "isOptional": false, "description": "Datasets whose projection is to be defined. The datasets can be feature classes, raster datasets, and layers. ", "dataType": "Geodataset"}, {"name": "Coordinate_System", "isOptional": true, "description": "Name of the coordinate system to be applied to the input datasets. ", "dataType": "Coordinate System"}, {"name": "Template_Dataset", "isOptional": true, "description": "Dataset that will be used to select an existing projection definition to be applied to the input datasets. ", "dataType": "Geodataset"}]},
{"syntax": "WriteFeaturesToTextFile_samples (input_features, output_text_file, decimal_separator_character)", "name": "Write Features to Text File (Samples)", "description": "Writes feature coordinates to a text file.", "example": {"title": null, "description": null, "code": "# Create geoprocessing dispatch object import arcgisscripting gp = arcgisscripting.create () # Set up inputs to tool inFC = r\"C:\\temp\\routes.txt\" TxtCoords = r\"C:\\temp\\RteCoords.shp\" sep = \"locale decimal point\" # Run tool gp.WriteFeaturesToTextFile ( inFC , TxtCoords , sep )"}, "usage": ["Output from this tool is in the same format as described for the input to the Create Features From Text File tool.", "Output coordinates are space delimited with the decimal separator of your choice.", "Outputs with null values for z or m will have the value 1.#QNAN for those positions.", "Selecting the locale decimal point option will result in the output being separated by whatever the system decimal point is set to. If the system is on a German locale, the output will take the form 1234,5. If the system is on a United States locale, the output will take the form 1234.5", "Selecting the default Python output option will result in the output being separated by whatever Python is using for a decimal point. This is usually a period.", "Point features will be output to the following format:", "Multipoint features will be output to the following format:", "Line features will be output to the following format:", "Polygon features will be output to the following format:", "No environment settings affect this tool."], "parameters": [{"name": "input_features", "isInputFile": true, "isOptional": false, "description": "The input features to be written to a text file. ", "dataType": "Feature Layer"}, {"name": "output_text_file", "isOutputFile": true, "isOptional": false, "description": "The output text file. ", "dataType": "Text File"}, {"name": "decimal_separator_character", "isOptional": false, "description": "The character that will separate the whole number from the decimal. locale decimal point \u2014 The system decimal point will be inserted. This is the default. default python output \u2014 The default Python separator will be inserted. comma \u2014 A comma separator will be inserted. period \u2014 A period separator will be inserted. $SEP$ \u2014 The $SEP$ string will be inserted. ", "dataType": "String"}]},
{"syntax": "CreateFeaturesFromTextFile_samples (input_text_file, input_decimal_separator, output_feature_class, {output_feature_class_spatial_reference})", "name": "Create Features from Text File (Samples)", "description": "Creates features using coordinates in text files.", "example": {"title": null, "description": null, "code": "# Create geoprocessing dispatch object import arcgisscripting gp = arcgisscripting.create () # Set up inputs to tool inTxt = r\"C:\\temp\\StreamPoints.txt\" inSep = \".\" strms = r\"C:\\temp\\Streams.shp\" # Run tool gp.CreateFeaturesFromTextFile ( inTxt , inSep , strms , \"#\" ) # Use output from createfeatures tool as input to buffer outFCbuf = r\"C:\\temp\\StreamsBuf.shp\" gp.buffer ( strms , \"10 Unknown\" , \"FULL\" , \"ROUND\" , \"NONE\" , \"#\" )"}, "usage": ["This tool will create a feature class based on coordinates given in a text file. Text files can be the output from the  Write Features To Text File tool or from files you created.", "Text files must be space delimited and will have different formats, depending on the geometry type.", "It is not necessary to provide z- and m-values to point coordinates. Values of 1.#QNAN are given to unprovided z- and m-values.", "If you use a thousands separator, the script will not work correctly. Instead of using 1,023.5, use 1023.5.", "The script is able to handle various decimal separators. For example, data from the United States will often be in the format 1234.5, while data from Europe may be in the format 1234,5. Specify the decimal separator that corresponds to your data. If you have only integers, you may specify any separator you like.", "The spatial reference parameter is optional. If you know the spatial reference of the input text coordinates, you can specify it; however, it is not required. If specified, the output feature class will have the spatial reference you selected.", "Text files that represent points should be in the following format: The first line should contain the word Point to indicate the geometry type and the next lines should have the id and x,y,z,m coordinates of the points delimited by a space. The final line should contain the word END. Generically, it will look like this:", "An example would be:", "Text files that represent multipoints should be in the following format: The first line should contain the word Multipoint to indicate the geometry type and the structure continues with the id number of the first group of points (id x y z m), followed by a zero. The points themselves follow. The final line should contain the word END. Generically, it will look like this:", "An example would be:", "Text files that represent lines should be in the following format: The first line should contain the word Polyline to indicate the geometry type and the structure continues with the id number of the first line, followed by the part number (in case it is a multipart line). The point coordinates follow. The final line should contain the word END. Generically, it will look like this:", "The example below represents a line feature class with two features. Feature zero contains two parts.", "Text files that represent polygons should be in the following format: The first line should contain the word Polygon to indicate the geometry type and the structure continues with the id number of the first line, followed by the part number (in case it is a multipart polygon). Point coordinates for the respective part and feature follow. In the case of an interior ring, the word InteriorRing (no space) is written before the group of coordinates. Polygons should be closed, that is, the first and last points should be the same. The final line should contain the word END. Generically, it will look like this:", "In the example below, there are two polygons. Polygon zero has two parts. The second part has an interior ring. Polygon one is a normal polygon.", "All the examples above will work. Pass them into a text file, save the text file, and use it as input to the tool."], "parameters": [{"name": "input_text_file", "isInputFile": true, "isOptional": false, "description": "A text file containing coordinates. ", "dataType": "Text File"}, {"name": "input_decimal_separator", "isInputFile": true, "isOptional": false, "description": "The character that separates the whole number from the decimal. This may vary depending on the source of your data. If the coordinates take the form \"1.5\", your separator is a period. ", "dataType": "String"}, {"name": "output_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "output_feature_class_spatial_reference", "isOutputFile": true, "isOptional": true, "description": "The spatial reference of the output coordinates. ", "dataType": "Spatial Reference"}]},
{"syntax": "SubModelMemoryLayerWithAllJoins_samples (input_cad_staging_geodatabase, {select_geometry_type})", "name": "Sub Model Memory Layer with All Joins from Staging Geodatabase (Samples)", "description": "Creates a layer of the specified geometry type and joins all the CAD attributes to the selected geometry type.", "example": {"title": null, "description": null, "code": "This tool is intended to be used as a model tool ."}, "usage": ["This model is a submodel in the CAD to Feature Class with all Joins from Staging Geodatabase model.", "This creates in\u2013memory layers, so if you shut down your application, these layers are deleted."], "parameters": [{"name": "input_cad_staging_geodatabase", "isInputFile": true, "isOptional": false, "description": "The staging geodatabase created by IMPORTCAD. ", "dataType": "Folder | Workspace"}, {"name": "select_geometry_type", "isOptional": true, "description": "The geometry type for the output layer. ", "dataType": "String"}]},
{"syntax": "PolygonFeatureClasstoCADLines_samples (input_features, output_file, output_type, {seed_file})", "name": "Polygon Feature Class to CAD Lines (Samples)", "description": "Converts polygon feature classes to a CAD file with bounding lines. The coincident lines where polygons touch will be simplified to a single line.", "example": {"title": null, "description": null, "code": ""}, "usage": ["Output line features will be created for each input line, and polygon boundary. If input lines or polygon boundaries cross, the output lines will be split at those locations.", "Multiple input features will be written to the same output CAD file."], "parameters": [{"name": "input_features", "isInputFile": true, "isOptional": false, "description": "The polygon feature class that will be used to create CAD lines. ", "dataType": "Layer | Table View"}, {"name": "output_file", "isOutputFile": true, "isOptional": false, "description": "The output CAD file that contains the bounding lines from the polygon feature classes. ", "dataType": "String"}, {"name": "output_type", "isOutputFile": true, "isOptional": false, "description": "Specifies the output CAD type and version. ", "dataType": "CAD Drawing Dataset"}, {"name": "seed_file", "isOptional": true, "description": "An existing CAD drawing whose contents, documents, and layer properties will be used for all new output CAD files. ", "dataType": "CAD Drawing Dataset"}]},
{"syntax": "CreateFeatureClasswithAllJoinsfromStagingGeodatabase_samples (input_CAD_staging_geodatabase, output_CAD_text_point_feature_class, output_CAD_point_feature_class, output_CAD_line_feature_class, output_CAD_area_feature_class)", "name": "Create Feature Class with all Joins from Staging Geodatabase (Samples)", "description": "Joins all tables and geometries created by IMPORTCAD. The output feature classes will contain joins for all CAD properties.", "example": {"title": null, "description": null, "code": ""}, "usage": ["Uses the Sub\u2013Model Memory Layer with All Joins from Staging Geodatabase, located in the samples toolbox, to generate layers and generates layer files pointing to the data created by the IMPORTCAD tool.", "This model reassembles the CAD file into feature classes.", "A layer of points for the TEXT TAGS and ATTRIBS is also generated by joining the TEXTPROP table to the results.", "The IMPORTCAD tool must be run first to create a staging geodatabase, which is the input to this model tool."], "parameters": [{"name": "input_CAD_staging_geodatabase", "isInputFile": true, "isOptional": false, "description": "The staging geodatabase created by IMPORTCAD. ", "dataType": "Folder ; Workspace"}, {"name": "output_CAD_text_point_feature_class", "isOutputFile": true, "isOptional": false, "description": "The text point feature class that will be created with all CAD entities joined to it. ", "dataType": "Feature Class"}, {"name": "output_CAD_point_feature_class", "isOutputFile": true, "isOptional": false, "description": "The point feature class that will be created with all CAD entities joined to it. ", "dataType": "Feature Class"}, {"name": "output_CAD_line_feature_class", "isOutputFile": true, "isOptional": false, "description": "The line feature class that will be created with all CAD entities joined to it. ", "dataType": "Feature Class"}, {"name": "output_CAD_area_feature_class", "isOutputFile": true, "isOptional": false, "description": "The area feature class that will be created with all CAD entities joined to it. ", "dataType": "Feature Class"}]},
{"syntax": "CADtoFeatureClasswithAttributeofNearestPoint_samples (input_CAD_file, output_feature_class, {main_features_filter_expression}, {select_near_points_or_annotation}, {near_point_filter_expression}, {search_radius}, {select_output_feature_type})", "name": "CAD To Feature Class with Attribute of Nearest Point (Samples)", "description": "Generate a feature class from CAD objects and point features that are nearest to those CAD features.", "example": {"title": null, "description": null, "code": ""}, "usage": ["The output feature class will include the attributes of the nearest points.", "This tool can be useful in associating text, blocks, or cells and their attributes with lines, points, and polygons."], "parameters": [{"name": "input_CAD_file", "isInputFile": true, "isOptional": false, "description": "The CAD file that will be converted to an ArcGIS feature class. ", "dataType": "CAD Drawing Dataset"}, {"name": "output_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class that will be created from the input CAD file. ", "dataType": "Feature Class"}, {"name": "main_features_filter_expression", "isOptional": true, "description": "The SQL query expression that will be used to select records. ", "dataType": "SQL Expression"}, {"name": "select_near_points_or_annotation", "isOptional": true, "description": "The type of near point objects whose attributes are associated with the output features. ", "dataType": "String"}, {"name": "near_point_filter_expression", "isOptional": true, "description": "The SQL query expression that will be used to select records from the Near feature class. ", "dataType": "SQL Expression"}, {"name": "search_radius", "isOptional": true, "description": "The maximum distance used to attach the point or annotation attributes to output features. ", "dataType": "Linear Unit"}, {"name": "select_output_feature_type", "isOptional": true, "description": "The output feature class type that will be created. ", "dataType": "String"}]},
{"syntax": "CADtoFeatureClass_samples (input_CAD_file, output_feature_class, {select_feature_class_type}, {filter_expression}, {modify_output_fields})", "name": "CAD To Feature Class (Samples)", "description": "Converts objects from a CAD file to a defined feature class type along with the point features that are near those CAD features.", "example": {"title": null, "description": null, "code": ""}, "usage": ["If you change the visibility of a field from the Field control parameter to false, you will omit that field from the output feature class.", "Complex feature classes, such as annotation and dimensions, are not supported by this tool.", "If a SQL expression is used but returns nothing, the output feature layer will be empty."], "parameters": [{"name": "input_CAD_file", "isInputFile": true, "isOptional": false, "description": "The CAD file that will be converted to an ArcGIS feature class. ", "dataType": "CAD Drawing Dataset"}, {"name": "output_feature_class", "isOutputFile": true, "isOptional": false, "description": "The feature class that will be created from the input CAD file. ", "dataType": "Feature Class"}, {"name": "select_feature_class_type", "isOptional": true, "description": "The geometry type of the output feature class or shapefile. ", "dataType": "String"}, {"name": "filter_expression", "isOptional": true, "description": "The SQL query expression that will be used to select records. ", "dataType": "SQL Expression"}, {"name": "modify_output_fields", "isOptional": true, "description": "The field info is used to review and alter the field names for the new layers fields. Fields can be hidden, and a split policy can be specified. ", "dataType": "Field Info"}]},
{"syntax": "CADLinestoPolygonFeatures_samples (input_boundary_lines, input_label_points, output_polygon_feature_class, {boundary_line_filter_expression}, {point_filter_expression})", "name": "CAD Lines To Polygon Features (Samples)", "description": "Converts CAD lines and points to polygon feature classes. This work flow is a starting point to turn CAD lines into ArcGIS polygons.", "example": {"title": null, "description": null, "code": "This tool is intended to be used as a model tool ."}, "usage": ["The label features parameter allows for a feature class of label attributes to be applied to the output polygons.", "This sample model creates polygons from lines and points while adding attributes of the points to the polygons."], "parameters": [{"name": "input_boundary_lines", "isInputFile": true, "isOptional": false, "description": "The bounding line feature class that will be used to create polygons. ", "dataType": "Feature Layer"}, {"name": "input_label_points", "isInputFile": true, "isOptional": false, "description": "Specifies the feature class containing the label points that will be applied to the output polygons. ", "dataType": "Feature Layer"}, {"name": "output_polygon_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class that will contain the newly formed polygon features. ", "dataType": "Feature Class"}, {"name": "boundary_line_filter_expression", "isOptional": true, "description": "The parameter to build a query to select line feature classes to create polygons. ", "dataType": "SQL Expression"}, {"name": "point_filter_expression", "isOptional": true, "description": "The parameter to build a query to only use select point feature classes to create polygons. ", "dataType": "SQL Expression"}]},
{"syntax": "WorkspaceToNewRasterCatalog_samples (input_workspace, output_location, raster_catalog_name, {configuration_keyword}, {management_type})", "name": "Workspace To New Raster Catalog (Samples)", "description": "Creates a new raster catalog and loads all the raster datasets in the input workspace, or geodatabase, to the raster catalog. The spatial reference of both the raster column and the geometry column are taken from the first raster dataset in the input workspace. The x,y domain of the geometry column is calculated with the union extents of all the raster dataset inputs. There must be a spatial reference attached to the raster datasets for the tool to run.", "example": {"title": null, "description": null, "code": "import arcgisscripting gp = arcgisscripting.create () gp.WorkspaceToNewRasterCatalog_samples ( \"D: \\\\ temp \\\\ tempws\" , \"Database Connections \\\\ raster.sde\" , \"wrksp_to_new_rc\" , \"#\" , \"Managed\" )"}, "usage": ["The input workspace, or geodatabase, should contain all the raster datasets you put into the raster catalog.", "There must be a spatial reference attached to the raster datasets for the tool to run.", "Raster datasets within raster catalogs in a geodatabase can be handled in two ways: managed or unmanaged by the geodatabase. Having the raster catalog managed by the geodatabase means that the raster datasets will be stored within the geodatabase. When a row is deleted from the catalog, it is deleted from the geodatabase. When your raster catalog is unmanaged, there will only be a pointer connecting the raster catalog row to the raster dataset. All raster datasets loaded into an unmanaged raster catalog must be a file on disk. Geodatabase raster datasets can only be loaded into raster catalogs that are managed.", "When creating a raster catalog in an ArcSDE geodatabase, the new raster catalog name cannot contain spaces; you can use underscores to separate words.", "This sample tool was created using Python. The code can be viewed and edited to help you write your own scripts.", "The Raster Analysis Environment Settings are not valid for this tool."], "parameters": [{"name": "input_workspace", "isInputFile": true, "isOptional": false, "description": "The input workspace can be a directory, a personal geodatabase, file geodatabase, or an ArcSDE geodatabase. ", "dataType": "Folder | Workspace"}, {"name": "output_location", "isOutputFile": true, "isOptional": false, "description": "The output location is the geodatabase where the new raster catalog will be created. ", "dataType": "Folder | Workspace"}, {"name": "raster_catalog_name", "isOptional": false, "description": "The name of the raster catalog to be created. ", "dataType": "String"}, {"name": "configuration_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}, {"name": "management_type", "isOptional": true, "description": "Raster datasets within raster catalogs can be managed in two ways: managed or unmanaged (by the geodatabase). MANAGED \u2014 With a managed raster catalog, the raster datasets inside the raster catalog will be physically stored within the geodatabase. When a row (or raster) is deleted from the catalog, it is deleted from the geodatabase. UNMANAGED \u2014 With an unmanaged raster catalog, the raster catalog only contains links or pointers connecting a row to a raster dataset stored outside the geodatabase. All raster datasets loaded into an unmanaged raster catalog must be a file on disk. ", "dataType": "String"}]},
{"syntax": "WorkspaceToNewMosaic_samples (input_workspace, output_location, output_raster_name, {config_keyword}, {mosaic_mode}, {colormap_mode}, {pyramid_origin}, {ignore_background_value}, {nodata_value}, {convert_1_bit_data_to_8_bit}, {mosaic_tolerance})", "name": "Workspace To New Mosaic (Samples)", "description": "Creates a new raster dataset in the output location and mosaics all the raster datasets in the input workspace, or geodatabase, into it.", "example": {"title": null, "description": null, "code": "import arcgisscripting gp = arcgisscripting.create () gp.WorkspaceToNewMosaic_samples ( \"D: \\\\ temp \\\\ tempws, \" Database Connections raster.sde \", \" wrksp_to_new_mosaic \", \" #\" \"LAST\", \"FIRST\", \"#\", \"#\", \"#\", \"#\", \"0\")"}, "usage": ["The input workspace should contain all the raster datasets you want to mosaic together. The inputs must have the same number of bands; otherwise, the tool will not run.", "If your output raster dataset is stored in the file system, you must specify the proper extension for the new raster dataset. Valid outputs to a file system raster dataset include ESRI GRID (no extension), ERDAS IMAGINE (.img), and TIFF (.tif).", "This sample tool was created using Python. The code can be viewed and edited to help you write your own scripts.", "The Raster Analysis Environment Settings are not valid for this tool."], "parameters": [{"name": "input_workspace", "isInputFile": true, "isOptional": false, "description": "The location where the raster datasets will be mosaicked into the output raster dataset. It can be a directory, personal geodatabase, file geodatabase, or an enterprise geodatabase. ", "dataType": "Folder | Workspace"}, {"name": "output_location", "isOutputFile": true, "isOptional": false, "description": "The location where the new raster dataset will be created. ", "dataType": "Folder | Workspace"}, {"name": "output_raster_name", "isOutputFile": true, "isOptional": false, "description": "The dataset name of the output raster. ", "dataType": "String"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}, {"name": "mosaic_mode", "isOptional": true, "description": "The method used to mosaic overlapping areas. FIRST \u2014 The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location. LAST \u2014 The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default. BLEND \u2014 The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area. MEAN \u2014 The output cell value of the overlapping areas will be the average value of the overlapping cells. MINIMUM \u2014 The output cell value of the overlapping areas will be the minimum value of the overlapping cells. MAXIMUM \u2014 The output cell value of the overlapping areas will be the maximum value of the overlapping cells. ", "dataType": "String"}, {"name": "colormap_mode", "isOptional": true, "description": "The method used to choose which colormap from the input rasters will be applied to the mosaic output. FIRST \u2014 The colormap from the first raster dataset in the list will be applied to the output raster mosaic. This is the default. LAST \u2014 The colormap from the last raster dataset in the list will be applied to the output raster mosaic. MATCH \u2014 Ensures that all the colors from the input raster datasets are in the final colormap and that they are all unique. REJECT \u2014 Only the raster datasets that do not have a colormap associated with them will be mosaicked.", "dataType": "String"}, {"name": "pyramid_origin", "isOptional": true, "description": "This is the origination location of the raster pyramid. It is recommended that you specify this point if you plan on building large mosaics in a file geodatabase or an ArcSDE geodatabase, especially if you are plan on mosaicking to them over time (for example, for updating). The pyramid reference point should be set to the upper left corner of your raster dataset. In setting this point for a file geodatabase or an ArcSDE geodatabase, partial pyramiding will be used when updating with a new mosaicked raster dataset. Partial pyramiding updated the parts of the pyramid that do not exist due to the new mosaicked datasets. Therefore, it is good to set your pyramid reference point so that your entire raster mosaic will be below and to the right of this point. However, a pyramid reference point should not be set too large either. ", "dataType": "Point"}, {"name": "ignore_background_value", "isOptional": true, "description": "Use this option to remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values within the raster dataset. The pixel value specified will be set to NoData in the output raster dataset. For file-based rasters and personal geodatabase rasters, the Ignore Background Value must be set to the same value as NoData in order for the background value to be ignored. ArcSDE and file geodatabase rasters will work without this extra step. ", "dataType": "Double"}, {"name": "nodata_value", "isOptional": true, "description": "All the pixels with the specified value will be set to NoData in the output raster dataset. ", "dataType": "Double"}, {"name": "convert_1_bit_data_to_8_bit", "isOptional": true, "description": "Choose whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to ArcSDE. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in ArcSDE, 1-bit raster datasets can only have 1-bit pyramid layers, which makes the display unpleasant. By converting the data to 8-bit in ArcSDE, the pyramid layers are built as 8-bit instead of 1-bit, resulting in a proper raster dataset in the display. NONE \u2014 No conversion will be done. This is the default. OneBitTo8Bit \u2014 The input raster will be converted.", "dataType": "Boolean"}, {"name": "mosaic_tolerance", "isOptional": true, "description": "When mosaicking takes place, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, a decision needs to be made whether resampling takes place, or whether the data should be shifted. The mosaicking tolerance controls whether resampling of the pixels take place, or if the pixels should be shifted. If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will take place. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not take place (instead, a shift is performed). The unit of tolerance is a pixel; the valid value range is 0 to 0.9999. The maximum a pixel can be shifted is 0.5, so anything you set that is greater than 0.5 will guarentee a shift takes place. A tolerance of zero guantees resampling, if there is a misalignment in pixels. For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, then resampling will take place since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, then the pixels will be shifted. ", "dataType": "Float"}]},
{"syntax": "WorkspaceToMosaic_samples (input_workspace, output_raster, {mosaic_method}, mosaic_colormap_mode, {ignore_background_value}, {nodata_value}, {convert_1_bit_data_to_8_bit}, {mosaic_tolerance})", "name": "Workspace To Mosaic (Samples)", "description": "Mosaics all the raster datasets in a workspace, or geodatabase, to an output raster dataset. The output raster dataset must already exist.", "example": {"title": null, "description": null, "code": "import arcgisscripting gp = arcgisscripting.create () gp.WorkspaceToMosaic_samples ( \"D: \\\\ temp \\\\ tempws\" , \"Database Connections \\\\ raster.sde \\\\ RASTER.WRKSPTOMOSAIC\" , \"LAST\" , \"FIRST\" , \"#\" , \"#\" , \"#\" , \"0\" )"}, "usage": ["Mosaic is useful when two or more adjacent raster datasets need to be merged into one entity. Some mosaic techniques can help minimize the abrupt changes along the boundaries of the overlapping rasters.", "For mosaicking of discrete data, First, Minimum, or Maximum ", "Mosaic Operator", " options will provide the most meaningful results. The Blend and Mean ", "Mosaic Operator", " options are best suited for continuous data.", "For floating-point input raster datasets of different resolutions or when cells are not aligned, it is recommended to resample all the data using bilinear interpolation or cubic convolution before running ", "Mosaic", "; otherwise, ", "Mosaic", " will automatically resample the raster datasets using nearest neighbor resampling, which is not appropriate for continuous data types.", "The output raster dataset must already exist since the input raster datasets will all be mosaicked into it. This raster dataset cell size determines the cell size for the entire mosaic.", "If an output raster dataset does not already exist, you can use the Mosaic to New Raster tool, or create an empty raster dataset prior to running this tool.", "This sample tool was created using Python. The code can be viewed and edited to help you write your own scripts.", "The Raster Analysis Environment Settings are not valid for this tool."], "parameters": [{"name": "input_workspace", "isInputFile": true, "isOptional": false, "description": "The input workspace can be a directory, a personal geodatabase, file geodatabase, or an ArcSDE geodatabase. ", "dataType": "Folder | Workspace"}, {"name": "output_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster dataset into which the other input raster datasets are mosaicked. The output raster dataset must already exist. ", "dataType": "Raster Dataset"}, {"name": "mosaic_method", "isOptional": true, "description": "The method used to mosaic overlapping areas. FIRST \u2014 The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location. LAST \u2014 The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default. BLEND \u2014 The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area. MEAN \u2014 The output cell value of the overlapping areas will be the average value of the overlapping cells. MINIMUM \u2014 The output cell value of the overlapping areas will be the minimum value of the overlapping cells. MAXIMUM \u2014 The output cell value of the overlapping areas will be the maximum value of the overlapping cells. ", "dataType": "String"}, {"name": "mosaic_colormap_mode", "isOptional": false, "description": "The method used to choose which colormap from the input rasters will be applied to the mosaic output. FIRST \u2014 The colormap from the first raster dataset in the list will be applied to the output raster mosaic. This is the default. LAST \u2014 The colormap from the last raster dataset in the list will be applied to the output raster mosaic. MATCH \u2014 Ensures that all the colors from the input raster datasets are in the final colormap and that they are all unique. REJECT \u2014 Only the raster datasets that do not have a colormap associated with them will be mosaicked.", "dataType": "String"}, {"name": "ignore_background_value", "isOptional": true, "description": "Use this option to remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values within the raster dataset. The pixel value specified will be set to NoData in the output raster dataset. For file-based rasters and personal geodatabase rasters, the Ignore Background Value must be set to the same value as NoData in order for the background value to be ignored. ArcSDE and file geodatabase rasters will work without this extra step. ", "dataType": "Double"}, {"name": "nodata_value", "isOptional": true, "description": "All the pixels with the specified value will be set to NoData in the output raster dataset. ", "dataType": "Double"}, {"name": "convert_1_bit_data_to_8_bit", "isOptional": true, "description": "Choose whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to ArcSDE. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in ArcSDE, 1-bit raster datasets can only have 1-bit pyramid layers, which makes the display unpleasant. By converting the data to 8-bit in ArcSDE, the pyramid layers are built as 8-bit instead of 1-bit, resulting in a proper raster dataset in the display. NONE \u2014 No conversion will be done. This is the default. OneBitTo8Bit \u2014 The input raster will be converted.", "dataType": "Boolean"}, {"name": "mosaic_tolerance", "isOptional": true, "description": "When mosaicking takes place, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, a decision needs to be made whether resampling takes place, or whether the data should be shifted. The mosaicking tolerance controls whether resampling of the pixels take place, or if the pixels should be shifted. If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will take place. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not take place (instead, a shift is performed). The unit of tolerance is a pixel; the valid value range is 0 to 0.9999. The maximum a pixel can be shifted is 0.5, so anything you set that is greater than 0.5 will guarentee a shift takes place. A tolerance of zero guantees resampling, if there is a misalignment in pixels. For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, then resampling will take place since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, then the pixels will be shifted. ", "dataType": "Double"}]},
{"syntax": "WorkspaceToGeodatabase_samples (input_workspace, output_geodatabase, {config_keyword})", "name": "Workspace To Geodatabase (Samples)", "description": "Loads all raster datasets from one workspace or geodatabase to another workspace or geodatabase as individual raster datasets.", "example": {"title": null, "description": null, "code": "import arcgisscripting gp = arcgisscripting.create () gp.WorkspaceToGeodatabase_samples ( \"D: \\\\ temp \\\\ tempws\" , \"Database Connections \\\\ raster.sde\" , \"#\" )"}, "usage": ["Loading and storage parameters can be set in the ", "Environment Settings", " under the ", "Raster Storage Settings", " section.", "This sample tool was created using Python. The code can be viewed and edited to help you write your own scripts.", "The Raster Analysis Environment Settings are not valid for this tool."], "parameters": [{"name": "input_workspace", "isInputFile": true, "isOptional": false, "description": "The input workspace can be a directory, a personal geodatabase, file geodatabase, or an ArcSDE geodatabase. ", "dataType": "Folder | Workspace"}, {"name": "output_geodatabase", "isOutputFile": true, "isOptional": false, "description": "The output geodatabase can be a personal geodatabase, file geodatabase, or an ArcSDE geodatabase. ", "dataType": "Workspace | Feature Dataset"}, {"name": "config_keyword", "isOptional": true, "description": "Specifies the storage parameters (configuration) for a file geodatabase and an ArcSDE geodatabase. Personal geodatabases do not use configuration keywords. ArcSDE configuration keywords are set up by your database administrator. ", "dataType": "String"}]},
{"syntax": "RasterCalculator (expression, output_raster)", "name": "Raster Calculator (Spatial Analyst)", "description": "Builds and executes a single Map Algebra expression using Python syntax in a calculator-like interface. Learn more about how Raster Calculator works", "example": {"title": null, "description": null, "code": ""}, "usage": ["The ", "Raster Calculator", " tool is intended for use in the ", "ArcGIS for Desktop", " application only as a GP tool dialog box or in ModelBuilder. It is not  intended for use in scripting and is not available in the ArcPy Spatial Analyst module.", "The ", "Raster Calculator", " tool allows you to create and execute a ", "Map Algebra", " expression that will  output a raster.", "Use the ", "Layers and variables", " list to select the datasets and variables to use in the expression.  Numerical values and mathematical operators can be added to the expression by clicking the respective buttons in the tool dialog box.  A list of commonly used conditional and mathematical tools is provided, allowing you to easily add them to the expression.", "Full paths to data or data existing in the specified  current workspace environment setting can be entered in quotes (", "\"\"", "). Numbers and scalars can be directly entered into an expression.", "The operators in the Raster Calculator tool dialog box are:", "/", "(", "Division", ")", "==", " (", "Equal To", ")", "!=", "(", "Not Equal", ")", "&", "(", "Boolean And", ")", "*", "(", "Multiplication", ")", ">", "(", "Greater Than", ")", ">=", "(", "Greater Than or Equal to", ")", "|", "(", "Boolean Or", ")", "-", "(", "Subtraction", ")", "(", "Negate", ")", "<", " (", "Less Than", ")", "<=", "(", "Less Than or Equal to", ")", "^", "(", "Boolean XOr", ")", "+", "(", "Addition", ")", "~", "(", "Boolean Not", ")", "Multiple geoprocessing tools and operations can be combined in a map algebra expression using standard Python syntax.", "When typing tool names,  be sure to check  the tool name syntax. If the capitalization is incorrect,  the expression will be invalid and fail to execute because Python is case sensitive.", "An example of the general format of a Map Algebra expression using  using geoprocessing tools is:", "Con(IsNull(\"streams\"), 0, \"streams\")", "The tool supports the standard Map Algebra syntax that is used in Python scripting.  The only differences are the following:", "You can easily clip a raster dataset by setting the ", "extent", "  environment and specifying the input raster  name in the expression.  When the tool is executed, the resulting raster output will be clipped based on the specified extent.", "To create a raster with cells of a constant value, specify the appropriate extent and ", "cell size", " environment settings and enter the numerical value into the expression.", "When multiple operators are used in an expression, they are not necessarily executed in left-to-right order. The operator with the highest precedence value will be executed first. For more information on operator precedence, see ", "operator precedence table", ". You can use parentheses to control the execution order.", "Boolean (", "~", ", ", "&", ", ", "^", ", ", "|", ") operators have a higher precedence level than Relational (", "<", ", ", "<=", ", ", ">", ", ", ">=", ", ", "==", ", ", "!=", ") operators.  Therefore, when Boolean operators are used in the same expression as Relational operators, the Boolean operators will be executed first.  To change the order of execution, use parentheses.", "When multiple Relational and/or Boolean operators are used consecutively in a single expression, in some cases it may fail to execute. To avoid this potential problem, use appropriate parentheses in the expression so that the execution order of the operators is explicitly defined. For more information, see  ", "Complex Statement Rules", ".", "The performance of an operation may be enhanced by the deferred evaluation capabilities of Map Algebra. Deferred evaluation is an optimization technique where individual components of an expression are intelligently processed so as to minimize the creation of intermediate datasets on disk.", "Only operators and tools that process on a per-cell basis can take advantage of this capability.    Operators and tools that support deferred evaluation are included on the raster calculator tool itself either as a button or in the list of tools provided.", "The ", "Raster Calculator", " tool can be used in ModelBuilder, but keep the following points in mind:", "Example: ", "\"inlayer\"", "Example: ", "%scale_factor%", "For example, ", "inraster", " when selected from the variable list will become ", "\"%inraster%\"", " in the expression."], "parameters": [{"name": "expression", "isOptional": false, "description": "In Python, Map Algebra expressions should be created and executed with the Spatial Analyst module, which is an extension of the ArcPy Python site package. See Map Algebra in Spatial Analyst to learn about how to perform your analysis in Python. ", "dataType": "String"}, {"name": "output_raster", "isOutputFile": true, "isOptional": false, "description": "See Creating output from Map Algebra for information on producing output from Map Algebra expressions in Python. ", "dataType": "Raster Dataset"}]},
{"syntax": "ZonalStatisticsAsTable (in_zone_data, zone_field, in_value_raster, out_table, {ignore_nodata}, {statistics_type})", "name": "Zonal Statistics as Table (Spatial Analyst)", "description": "Summarizes the values of a raster within the zones of another dataset and reports the results to a table. \r\n Learn more about how Zonal Statistics works", "example": {"title": "ZonalStatisticsAsTable example 1 (Python window)", "description": "This example summarizes the values of a raster within the zones defined by a polygon shapefile and records the results in a table.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outZSaT = ZonalStatisticsAsTable ( \"zones.shp\" , \"Classes\" , \"valueforzone\" , \"zonalstattblout\" , \"NODATA\" , \"SUM\" )"}, "usage": ["A zone is defined as all areas in the input that have the same value.  The areas do not have to be contiguous.  Both raster and feature datasets can be used for the zone input.", "When the zone and value inputs are both rasters of the same resolution, they will be used directly.", "If the resolutions are different, an internal resampling is applied to make them match before the zonal operation is performed.", "Should there be any NoData cells in the inputs, the resampling may cause there to be larger areas of NoData in your output than you might have expected. To avoid this situation, either ", "Resample", " the coarser input rasters to the resolution of the finer input raster, or set the ", "Cell size", " to ", "Minimum Of Inputs", " in the Raster Analysis ", "environment", ".", "If the zone input is a raster dataset, it must have an attribute table. The attribute table is usually created automatically for integer rasters, but may not be under certain circumstances. You can use ", "Build Raster Attribute Table", " to create one.", "If the zone input is a feature dataset, a vector-to-raster conversion will be internally applied to it. To ensure that the results of the conversion will align properly with the value raster, it is recommended that you check that the extent and snap raster are set appropriately in the environment settings and the raster settings.", "Since the internal raster must have an attribute table, an error will occur if one was not created in the conversion. If this happens, convert your feature dataset directly with ", "Feature To Raster", ", ", "Polygon To Raster", ", ", "Point To Raster", ", or ", "Polyline To Raster", ". Generate an attribute table for it as described in the previous tip and use the resulting raster as your Zone input.", "If the zone input is a feature dataset with relatively small features, keep in mind that the resolution of the information needs to be appropriate relative to the resolution of the value raster. If the areas of single features are similar to or smaller than the area of single cells in the value raster, in the feature-to-raster conversion some of these zones may not be represented.", "To demonstrate this, try converting the feature dataset to a raster with the appropriate feature-to-raster conversion tool and specify the resolution to be that of the Value raster. The result from this conversion will give an indication about what the default output of the zonal operation will be.", "If you have fewer results in the output than you may have expected, you need to determine an appropriate raster resolution that will represent the detail of your feature input, and use this resolution as the ", "Cell Size", " of the Raster Analysis Settings of the Environment.", "If the zone input is a point feature dataset, it is possible to have more than one point contained within any particular cell of the value input raster. For such cells, the zone value is determined by the point with the highest feature ID.", "If the zone feature input has overlapping polygons, the zonal analysis will not be performed for each individual polygon. Since the feature input is converted to a raster, each location can only have one value.", "An alternative method is to process the zonal statistics iteratively for each of the polygon zones and collate the results.", "It is recommended to only use rasters as the zone input, as it offers you greater control over the vector-to-raster conversion. This will help ensure you consistently get the expected results.", "When specifying the input zone data,  the default zone field will be the first available valid  field.  If no other valid fields exist, the ObjectID field (for example, OID or FID) will be the default.", "If a reserved field (for example, OBECTID, FID, or OID) is selected for the ", "Zone field", ", then this may cause some ambiguity in the result.  The result includes the particular reserved field name necessary for the particular output format type, as well as the ", "Zone field", " specified.  If the specified field has the same name as the reserved field for the particular output format, in the output the name for the zone field will be altered in such a way that all field names in the result are unique.", "To make a field of unique values that does not have a reserved name, use the ", "Add Field", " and ", "Calculate Field", " geoprocessing tools.", "The input value raster can be either integer or floating point. However, when it is floating-point type, the zonal calculations for majority, median, minority, and variety will not be computed.", "For majority and minority calculations, when there is a tie, the output for the zone is based on the lowest of the tied values.", "A field or series of fields will be created in the output table, depending on the setting of the ", "Statistics type", ".    When the Value input is integer, all of the statistics (Area, Min, Max, Range, Mean, STD, Sum, Variety, Majority, Minority, and Median) are available to be calculated.  If the Value input is floating point, the Majority, Minority, Median, and Variety statistics will not be calculated.", "The data type for each value under the items in the output table is dependent on the zonal calculation being performed. See ", "how Zonal Statistics works", " for the specific behavior of any statistic.", "The number of rows in the output table is the number of zones."], "parameters": [{"name": "in_zone_data", "isInputFile": true, "isOptional": false, "description": "Dataset that defines the zones. The zones can be defined by an integer raster or a feature layer. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "zone_field", "isOptional": false, "description": " Field that holds the values that define each zone. It can be an integer or a string field of the zone dataset. ", "dataType": "Field"}, {"name": "in_value_raster", "isInputFile": true, "isOptional": false, "description": "Raster that contains the values on which to calculate a statistic. ", "dataType": "Raster Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "Output table that will contain the summary of the values in each zone. ", "dataType": "Table"}, {"name": "ignore_nodata", "isOptional": true, "description": "Denotes whether NoData values in the Value input will influence the results of the zone that they fall within. DATA \u2014 Within any particular zone, only cells that have a value in the input Value raster will be used in determining the output value for that zone. NoData cells in the Value raster will be ignored in the statistic calculation. NODATA \u2014 Within any particular zone, if any NoData cells exist in the Value raster, it is deemed that there is insufficient information to perform statistical calculations for all the cells in that zone; therefore, the entire zone will receive the NoData value on the output raster.", "dataType": "Boolean"}, {"name": "statistics_type", "isOptional": true, "description": "Statistic type to be calculated. ALL \u2014 All of the statistics will be calculated. This is the default. MEAN \u2014 Calculates the average of all cells in the value raster that belong to the same zone as the output cell. MAJORITY \u2014 Determines the value that occurs most often of all cells in the value raster that belong to the same zone as the output cell. MAXIMUM \u2014 Determines the largest value of all cells in the value raster that belong to the same zone as the output cell. MEDIAN \u2014 Determines the median value of all cells in the value raster that belong to the same zone as the output cell. MINIMUM \u2014 Determines the smallest value of all cells in the value raster that belong to the same zone as the output cell. MINORITY \u2014 Determines the value that occurs least often of all cells in the value raster that belong to the same zone as the output cell. RANGE \u2014 Calculates the difference between the largest and smallest value of all cells in the value raster that belong to the same zone as the output cell. STD \u2014 Calculates the standard deviation of all cells in the value raster that belong to the same zone as the output cell. SUM \u2014 Calculates the total value of all cells in the value raster that belong to the same zone as the output cell. VARIETY \u2014 Calculates the number of unique values for all cells in the value raster that belong to the same zone as the output cell. MIN_MAX \u2014 Both the Minimum and Maximum statistics are calculated. MEAN_STD \u2014 Both the Mean and STD statistics are calculated. MIN_MAX_MEAN \u2014 The Minimum, Maximum and Mean statistics are calculated.", "dataType": "String"}]},
{"syntax": "ZonalStatistics (in_zone_data, zone_field, in_value_raster, {statistics_type}, {ignore_nodata})", "name": "Zonal Statistics (Spatial Analyst)", "description": "Calculates statistics on values of a raster within the zones of another dataset. \r\n Learn more about how Zonal Statistics works", "example": {"title": "ZonalStatistics example 1 (Python window)", "description": "This example determines for each zone the range of cell values in the Value input raster.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outZonalStats = ZonalStatistics ( \"zone\" , \"value\" , \"valueraster\" , \"RANGE\" , \"NODATA\" ) outZonalStats.save ( \"C:/sapyexamples/output/zonestatout\" )"}, "usage": ["A zone is defined as all areas in the input that have the same value.  The areas do not have to be contiguous.  Both raster and feature datasets can be used for the zone input.", "When the zone and value inputs are both rasters of the same resolution, they will be used directly.", "If the resolutions are different, an internal resampling is applied to make them match before the zonal operation is performed.", "Should there be any NoData cells in the inputs, the resampling may cause there to be larger areas of NoData in your output than you might have expected. To avoid this situation, either ", "Resample", " the coarser input rasters to the resolution of the finer input raster, or set the ", "Cell size", " to ", "Minimum Of Inputs", " in the Raster Analysis ", "environment", ".", "If the zone input is a raster dataset, it must have an attribute table. The attribute table is usually created automatically for integer rasters, but may not be under certain circumstances. You can use ", "Build Raster Attribute Table", " to create one.", "If the zone input is a feature dataset, a vector-to-raster conversion will be internally applied to it. To ensure that the results of the conversion will align properly with the value raster, it is recommended that you check that the extent and snap raster are set appropriately in the environment settings and the raster settings.", "Since the internal raster must have an attribute table, an error will occur if one was not created in the conversion. If this happens, convert your feature dataset directly with ", "Feature To Raster", ", ", "Polygon To Raster", ", ", "Point To Raster", ", or ", "Polyline To Raster", ". Generate an attribute table for it as described in the previous tip and use the resulting raster as your Zone input.", "If the zone input is a feature dataset with relatively small features, keep in mind that the resolution of the information needs to be appropriate relative to the resolution of the value raster. If the areas of single features are similar to or smaller than the area of single cells in the value raster, in the feature-to-raster conversion some of these zones may not be represented.", "To demonstrate this, try converting the feature dataset to a raster with the appropriate feature-to-raster conversion tool and specify the resolution to be that of the Value raster. The result from this conversion will give an indication about what the default output of the zonal operation will be.", "If you have fewer results in the output than you may have expected, you need to determine an appropriate raster resolution that will represent the detail of your feature input, and use this resolution as the ", "Cell Size", " of the Raster Analysis Settings of the Environment.", "If the zone input is a point feature dataset, it is possible to have more than one point contained within any particular cell of the value input raster. For such cells, the zone value is determined by the point with the highest feature ID.", "If the zone feature input has overlapping polygons, the zonal analysis will not be performed for each individual polygon. Since the feature input is converted to a raster, each location can only have one value.", "An alternative method is to process the zonal statistics iteratively for each of the polygon zones and collate the results.", "It is recommended to only use rasters as the zone input, as it offers you greater control over the vector-to-raster conversion. This will help ensure you consistently get the expected results.", "When specifying the input zone data,  the default zone field will be the first available valid  field.  If no other valid fields exist, the ObjectID field (for example, OID or FID) will be the default.", "If a reserved field (for example, OBECTID, FID, or OID) is selected for the ", "Zone field", ", then this may cause some ambiguity in the result.  The result includes the particular reserved field name necessary for the particular output format type, as well as the ", "Zone field", " specified.  If the specified field has the same name as the reserved field for the particular output format, in the output the name for the zone field will be altered in such a way that all field names in the result are unique.", "To make a field of unique values that does not have a reserved name, use the ", "Add Field", " and ", "Calculate Field", " geoprocessing tools.", "The input value raster can be either integer or floating point. However, when it is floating-point type, the zonal calculations for majority, median, minority, and variety will not be computed.", "For majority and minority calculations, when there is a tie, the output for the zone is based on the lowest of the tied values.", "The data type of the output is dependent on the zonal calculation being performed and the input value raster type. See ", "how Zonal Statistics works", " for more information."], "parameters": [{"name": "in_zone_data", "isInputFile": true, "isOptional": false, "description": "Dataset that defines the zones. The zones can be defined by an integer raster or a feature layer. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "zone_field", "isOptional": false, "description": " Field that holds the values that define each zone. It can be an integer or a string field of the zone dataset. ", "dataType": "Field"}, {"name": "in_value_raster", "isInputFile": true, "isOptional": false, "description": "Raster that contains the values on which to calculate a statistic. ", "dataType": "Raster Layer"}, {"name": "statistics_type", "isOptional": true, "description": "Statistic type to be calculated. MEAN \u2014 Calculates the average of all cells in the value raster that belong to the same zone as the output cell. MAJORITY \u2014 Determines the value that occurs most often of all cells in the value raster that belong to the same zone as the output cell. MAXIMUM \u2014 Determines the largest value of all cells in the value raster that belong to the same zone as the output cell. MEDIAN \u2014 Determines the median value of all cells in the value raster that belong to the same zone as the output cell. MINIMUM \u2014 Determines the smallest value of all cells in the value raster that belong to the same zone as the output cell. MINORITY \u2014 Determines the value that occurs least often of all cells in the value raster that belong to the same zone as the output cell. RANGE \u2014 Calculates the difference between the largest and smallest value of all cells in the value raster that belong to the same zone as the output cell. STD \u2014 Calculates the standard deviation of all cells in the value raster that belong to the same zone as the output cell. SUM \u2014 Calculates the total value of all cells in the value raster that belong to the same zone as the output cell. VARIETY \u2014 Calculates the number of unique values for all cells in the value raster that belong to the same zone as the output cell.", "dataType": "String"}, {"name": "ignore_nodata", "isOptional": true, "description": "Denotes whether NoData values in the Value input will influence the results of the zone that they fall within. DATA \u2014 Within any particular zone, only cells that have a value in the input Value raster will be used in determining the output value for that zone. NoData cells in the Value raster will be ignored in the statistic calculation. NODATA \u2014 Within any particular zone, if any NoData cells exist in the Value raster, it is deemed that there is insufficient information to perform statistical calculations for all the cells in that zone; therefore, the entire zone will receive the NoData value on the output raster.", "dataType": "Boolean"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ZonalHistogram (in_zone_data, zone_field, in_value_raster, out_table, {out_graph})", "name": "Zonal Histogram (Spatial Analyst)", "description": " Creates a table and a histogram graph that show the frequency distribution of cell values on the Value input for each unique Zone.", "example": {"title": "ZonalHistogram example 1 (Python window)", "description": "This example creates a zonal histogram .dbf table.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outZonHisto = ZonalHistogram ( \"zoneras\" , \"zonfield\" , \"valueras\" , \"znhist_tbl.dbf\" )"}, "usage": ["A zonal histogram enables you to investigate the frequency distribution of values in  one dataset within classes of another dataset. Examples include slope distribution within land use classes, rainfall distribution within elevation classes, or crime distribution by police beat.", "A zone is defined as all areas in the input that have the same value.  The areas do not have to be contiguous.  Both raster and feature datasets can be used for the zone input.", "When the zone and value inputs are both rasters of the same resolution, they will be used directly.", "If the resolutions are different, an internal resampling is applied to make them match before the zonal operation is performed.", "Should there be any NoData cells in the inputs, the resampling may cause there to be larger areas of NoData in your output than you might have expected. To avoid this situation, either ", "Resample", " the coarser input rasters to the resolution of the finer input raster, or set the ", "Cell size", " to ", "Minimum Of Inputs", " in the Raster Analysis ", "environment", ".", "If the zone input is a raster dataset, it must have an attribute table. The attribute table is usually created automatically for integer rasters, but may not be under certain circumstances. You can use ", "Build Raster Attribute Table", " to create one.", "It is recommended to only use rasters as the zone input, as it offers you greater control over the vector-to-raster conversion. This will help ensure you consistently get the expected results.", "If the zone input is a feature dataset, a vector-to-raster conversion will be internally applied to it. To ensure that the results of the conversion will align properly with the value raster, it is recommended that you check that the extent and snap raster are set appropriately in the environment settings and the raster settings.", "Since the internal raster must have an attribute table, an error will occur if one was not created in the conversion. If this happens, convert your feature dataset directly with ", "Feature To Raster", ", ", "Polygon To Raster", ", ", "Point To Raster", ", or ", "Polyline To Raster", ". Generate an attribute table for it as described in the previous tip and use the resulting raster as your Zone input.", "If the zone input is a feature dataset with relatively small features, keep in mind that the resolution of the information needs to be appropriate relative to the resolution of the value raster. If the areas of single features are similar to or smaller than the area of single cells in the value raster, in the feature-to-raster conversion some of these zones may not be represented.", "To demonstrate this, try converting the feature dataset to a raster with the appropriate feature-to-raster conversion tool and specify the resolution to be that of the Value raster. The result from this conversion will give an indication about what the default output of the zonal operation will be.", "If you have fewer results in the output than you may have expected, you need to determine an appropriate raster resolution that will represent the detail of your feature input, and use this resolution as the ", "Cell Size", " of the Raster Analysis Settings of the Environment.", "If the zone input is a point feature dataset, it is possible to have more than one point contained within any particular cell of the value input raster. For such cells, the zone value is determined by the point with the highest feature ID.", "If the zone feature input has overlapping polygons, the zonal analysis will not be performed for each individual polygon. Since the feature input is converted to a raster, each location can only have one value.", "An alternative method is to process the zonal statistics iteratively for each of the polygon zones and collate the results.", "The ", "Zone field", " must be either integer or string type.", "When specifying the input zone data,  the default zone field will be the first available valid  field.  If no other valid fields exist, the ObjectID field (for example, OID or FID) will be the default.", "The cells on the ", "input value raster", " belong to the zone that their cell centers fall within. In this case the zones are the zones after any necessary conversion to raster and resampling has taken place.", "  In the histogram graph, the number of classes (bins) for each zone is determined by the ", "Input value raster", ".  ", "A zonal histogram graph is not generated by default.  To have it be created when the tool is run, specify the ", "Output graph name", ".", "The graph is temporary (in-memory) only.  To make a permanent version of it, use the ", "Save Graph", " tool to create a .grf graph file, or  one of the other output formats available in that tool."], "parameters": [{"name": "in_zone_data", "isInputFile": true, "isOptional": false, "description": "Dataset that defines the zones. The zones can be defined by an integer raster or a feature layer. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "zone_field", "isOptional": false, "description": " Field that holds the values that define each zone. It can be an integer or a string field of the zone dataset. ", "dataType": "Field"}, {"name": "in_value_raster", "isInputFile": true, "isOptional": false, "description": " The raster values to create the histograms. ", "dataType": "Raster Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": " The output table file. The optional graph is created from the information in the table. ", "dataType": "Table"}, {"name": "out_graph", "isOutputFile": true, "isOptional": true, "description": " The name of the output graph for display. The graph is temporary. To persist it, use the Save Graph tool. ", "dataType": "Graph"}]},
{"syntax": "ZonalGeometryAsTable (in_zone_data, zone_field, out_table, {processing_cell_size})", "name": "Zonal Geometry as Table (Spatial Analyst)", "description": "Calculates for each zone in a dataset the geometry measures (area, perimeter, thickness, and the characteristics of ellipse) and reports the results as a table. \r\n Learn more about how Zonal Geometry works", "example": {"title": "ZonalGeometryAsTable example 1 (Python window)", "description": "This example determines the geometry measures for each zone defined by the input polygon shape file.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outZonalGeometryAsTable = ZonalGeometryAsTable ( \"zones.shp\" , \"Classes\" , \"zonalgeomout\" , 0.2 )"}, "usage": ["A zone is defined as all areas in the input that have the same value.  The areas do not have to be contiguous.  Both raster and feature datasets can be used for the zone input.", "If the input zone data is a feature dataset, a cell size must be set either by the ", "Processing cell size", " or in the ", "Cell Size", " environment.", "The calculations for each zone are recorded in the output table.", "When specifying the input zone data,  the default zone field will be the first available valid  field.  If no other valid fields exist, the ObjectID field (for example, OID or FID) will be the default.", "If a reserved field (for example, OBECTID, FID, or OID) is selected for the ", "Zone field", ", then this may cause some ambiguity in the result.  The result includes the particular reserved field name necessary for the particular output format type, as well as the ", "Zone field", " specified.  If the specified field has the same name as the reserved field for the particular output format, in the output the name for the zone field will be altered in such a way that all field names in the result are unique.", "To make a field of unique values that does not have a reserved name, use the ", "Add Field", " and ", "Calculate Field", " geoprocessing tools.", "In the output table, the value field always precedes the fields containing the zonal output calculations. The value field contains the values of the zones defined by the zone dataset.", "The values for the zonal calculations will be floating point.", "Other than the ORIENTATION item, all the results in the output table are presented in map units. The ORIENTATION item values are in degrees, with a possible range of 0 to 180. The ORIENTATION is defined as an angle between the x-axis and the major axis of the ellipse. The values of the orientation angle increase counterclockwise, starting at 0 in the east (horizontal, to the right) and going through 90 when the major axis is vertical.", "If a particular zone consists of only one cell or if the zone is a single square block of cells, the orientation of the ellipse (which, in this case, is a circle) is set to 90 degrees."], "parameters": [{"name": "in_zone_data", "isInputFile": true, "isOptional": false, "description": "Dataset that defines the zones. The zones can be defined by an integer raster or a feature layer. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "zone_field", "isOptional": false, "description": " Field that holds the values that define each zone. It must be an integer field of the zone dataset. ", "dataType": "Field"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "Output table that will contain the summary of the values in each zone. ", "dataType": "Table"}, {"name": "processing_cell_size", "isOptional": true, "description": "The processing cell size for the zonal operation. This is the value in the environment if specifically set. If the environment is not set, the default for the cell size is determined by the type of the zone data as follows: If the zone dataset is a raster, the cell size is the same as the zone raster. If the zone dataset is a feature, the cell size is the shorter of the width or height of the extent of the zone feature dataset in the output spatial reference, divided by 250.", "dataType": "Analysis Cell Size"}]},
{"syntax": "ZonalGeometry (in_zone_data, zone_field, {geometry_type}, {cell_size})", "name": "Zonal Geometry (Spatial Analyst)", "description": "Calculates for each zone in a dataset the specified geometry measure (area, perimeter, thickness, or the characteristics of ellipse). \r\n Learn more about how Zonal Geometry works", "example": {"title": "ZonalGeometry example 1 (Python window)", "description": "This example determines the area for each zone defined by the input polygon shape file.", "code": "import arcpy from arcpy import env from arcpy.sa import * env.workspace = \"C:/sapyexamples/data\" outZonalGeometry = ZonalGeometry ( \"zones.shp\" , \"Classes\" , \"AREA\" , 0.2 ) outZonalGeometry.save ( \"C:/sapyexamples/output/zonegeomout3\" )"}, "usage": ["A zone is defined as all areas in the input that have the same value.  The areas do not have to be contiguous.  Both raster and feature datasets can be used for the zone input.", "When specifying the input zone data,  the default zone field will be the first available valid  field.  If no other valid fields exist, the ObjectID field (for example, OID or FID) will be the default.", "If a reserved field (for example, OBECTID, FID, or OID) is selected for the ", "Zone field", ", then this may cause some ambiguity in the result.  The result includes the particular reserved field name necessary for the particular output format type, as well as the ", "Zone field", " specified.  If the specified field has the same name as the reserved field for the particular output format, in the output the name for the zone field will be altered in such a way that all field names in the result are unique.", "To make a field of unique values that does not have a reserved name, use the ", "Add Field", " and ", "Calculate Field", " geoprocessing tools.", "The data type of the output raster for each of the geometry types will be floating point.", "If a particular zone consists of only one cell or if the zone is a single square block of cells, the orientation of the ellipse (which, in this case, is a circle) is set to 90 degrees."], "parameters": [{"name": "in_zone_data", "isInputFile": true, "isOptional": false, "description": "Dataset that defines the zones. The zones can be defined by an integer raster or a feature layer. ", "dataType": "Raster Layer | Feature Layer"}, {"name": "zone_field", "isOptional": false, "description": " Field that holds the values that define each zone. It must be an integer field of the zone dataset. ", "dataType": "Field"}, {"name": "geometry_type", "isOptional": true, "description": "Geometry type to be calculated. AREA \u2014 The area for each zone. PERIMETER \u2014 The perimeter for each zone. THICKNESS \u2014 The deepest (or thickest) point within the zone from its surrounding cells. CENTROID \u2014 Locates the centroids of each zone.", "dataType": "String"}, {"name": "cell_size", "isOptional": true, "description": "The processing cell size for the zonal operation. This is the value in the environment if specifically set. If the environment is not set, the default for the cell size is determined by the type of the zone data as follows: If the zone dataset is a raster, the cell size is the same as the zone raster. If the zone dataset is a feature, the cell size is the shorter of the width or height of the extent of the zone feature dataset in the output spatial reference, divided by 250.", "dataType": "Analysis Cell Size"}, {"isOutputFile": true, "name": "out_raster", "isOptional": false, "description": " ", "dataType": null}]},
{"syntax": "ASCII3DToFeatureClass_3d (input, in_file_type, out_feature_class, out_geometry_type, {z_factor}, {input_coordinate_system}, {average_point_spacing}, {file_suffix}, {decimal_separator})", "name": "ASCII 3D To Feature Class (3D Analyst)", "description": "Imports 3D features from one or more ASCII files stored in XYZ, XYZI, or GENERATE formats into a new feature class.", "example": {"title": "ASCII3DToFeatureClass example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.Ascii3DToFeatureClass_3d ( \"masspnts.txt\" , \"GENERATE\" , \"masspnts.shp\" , \"POINT\" , 1 , \"Coordinate Systems/Projected Coordinate Systems/State Plane/NAD 1983 (Feet)/NAD 1983 StatePlane Massachusetts Mainland FIPS 2001 (Feet).prj\" )"}, "usage": ["All input files must be of the same format and geometry type.", " ", "XYZ", " and ", "XYZI", " formats support header lines and treat the first row that starts with three consecutive numbers as the beginning of the point records. Both formats store all supported geometry types in the same manner. and only supports one, single-part line or polygon feature per file.", "Only one single-part line or polygon feature can be stored in an ", "XYZ", " or ", "XYZI", " files. Polygons must close, with the coordinates of the last vertex being identical to that of the first, and must not self-intersect.", "XYZ", " files store x, y, and z coordinates as floating-point values, where each row represents a distinct point record. The XYZ coordinates can be followed by alphanumeric entries, but this information will not be transferred to the resulting feature class.", "XYZI", " files store x, y, z, and intensity measures. Intensity values are stored in a binary large object (BLOB) field for multipatch outputs.", "If the ASCII file is in ", "XYZI", " format but the intensity measures are not desired, choose ", "XYZ", ". This will skip the intensity value when reading the files.", "The ", "GENERATE", " format does not support  header lines, but it provides an ID for each point along with XYZ coordinates, and the last line of the file is optionally noted using the ", "END", " keyword:", "The ", "GENERATE", " format supports multiple features per file. For lines and polygons,  the ", "END", " keyword signals the end of a feature, and each feature must be a single-part feature. Two END keywords in a row indicates the end of the file. ", "Polygons should be oriented clockwise, have no self-intersections, and be closed (that is, the last vertex is equal to the first). If one of these conditions is not met, the output polygon will not be valid. The ", "Check Geometry", " tool can be used to validate the resulting features, and the ", "Repair Geometry", " tool can be used to correct the errors."], "parameters": [{"name": "input", "isOptional": false, "description": "The ASCII files or folders containing data in XYZ, XYZI (with lidar intensity), or 3-D GENERATE format. If a folder is specified, the File Suffix parameter becomes required, and all the files that have the same extension as the specified suffix will be used. If multiple files are involved, they need to be in the same format. ", "dataType": "Folder; File"}, {"name": "in_file_type", "isInputFile": true, "isOptional": false, "description": " ", "dataType": "String"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "out_geometry_type", "isOutputFile": true, "isOptional": false, "description": "The geometry type of the output feature class. MULTIPOINT \u2014Only valid when points are input. Multipoints are recommended when you don\u2019t intend to add any attribution on a per-point basis and especially when many thousands or millions of points are involved. This is the default. POINT \u2014Only valid when points are input. POLYLINES \u2014Only valid when polylines or polygons are input. If inputting polygons, you can use POLYLINES as output. POLYGONS \u2014Only valid when polygons are input.", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}, {"name": "input_coordinate_system", "isInputFile": true, "isOptional": true, "description": "The coordinate system of the input data. The default is an Unknown Coordinate System. If specified, the output may or may not be projected into a different coordinate system. This depends the whether the geoprocessing environment has a coordinate system defined for the location of the target feature class. ", "dataType": "Coordinate System"}, {"name": "average_point_spacing", "isOptional": true, "description": "The average planimetric distance between points of the input. This parameter is only used when the output geometry is set to MULTIPOINT , and its function is to provide a means for grouping the points together. This value is used in conjunction with the points per shape limit to construct a virtual tile system used to group the points. The tile system's origin is based on the domain of the target feature class. Specify the spacing in the horizontal units of the target feature class. ", "dataType": "Double"}, {"name": "file_suffix", "isOptional": true, "description": "The suffix of the files to import from an input folder. This parameter is required when a folder is specified as input. ", "dataType": "String"}, {"name": "decimal_separator", "isOptional": true, "description": "The decimal character used in the text file to differentiate the integer of a number from its fractional part. DECIMAL_POINT \u2014 A point is used as the decimal character. This is the default. DECIMAL_COMMA \u2014 A comma is used as the decimal character.", "dataType": "String"}]},
{"syntax": "Kriging_3d (in_point_features, z_field, out_surface_raster, semiVariogram_props, {cell_size}, {search_radius}, {out_variance_prediction_raster})", "name": "Kriging (3D Analyst)", "description": "Interpolates a raster surface from points using kriging. \r\n Learn more about how Kriging works", "example": {"title": "Kriging example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a Grid raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Kriging_3d ( \"ca_ozone_pts.shp\" , \"OZONE\" , \"c:/output/krigout\" , \"Spherical\" , 2000 , \"Variable 12\" )"}, "usage": ["Kriging is a processor-intensive process. The speed of execution is dependent on the number of points in the input dataset and the size of the search window.", "Low values within the optional output variance of prediction raster indicate a high degree of confidence in the predicted value. High values may indicate a need for more data points.", "The Universal kriging types assume that there is a structural component present and that the local trend varies from one location to another.", "The ", "Advanced Parameters", " allow control of the semivariogram used for kriging. A default value for ", "Lag size", " is initially set to the default output cell size. For ", "Major range", ", ", "Partial sill", ", and ", "Nugget", ", a default value will be calculated internally if nothing is specified.", "The optional output variance of prediction raster contains the kriging variance at each output raster cell. Assuming the kriging errors are normally distributed, there is a 95.5 percent probability that the actual z-value at the cell is the predicted raster value, plus or minus two times the square root of the value in the prediction raster.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "out_surface_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Dataset"}, {"name": "semiVariogram_props", "isOptional": false, "description": "The Semivariogram model to be used. There are two models for kriging, Ordinary and Universal. The Ordinary model has five types of semivariograms available. The Universal model has two types of semivariograms available. Each semivariogram has several optional parameters that can also be set. The form of the semivariogram is a text string: For example: Ordinary model semivariograms: Spherical \u2014Spherical semivariogram model. This is the default. Circular \u2014Circular semivariogram model. Exponential \u2014Exponential semivariogram model. Gaussian \u2014Gaussian (or normal distribution) semivariogram model. Linear \u2014Linear semivariogram model with a sill. Universal model semivariograms: LinearDrift \u2014Universal Kriging with linear drift. QuadraticDrift \u2014Universal Kriging with quadratic drift. After the semivariogram model is defined, the remaining parameters are common between Ordinary and Universal kriging. These are: Lag size\u2014The default is the output raster cell size. MajorRange\u2014Represents a distance beyond which there is little or no correlation. PartialSill\u2014The difference between the nugget and the sill. Nugget\u2014Represents the error and variation at spatial scales too fine to detect. The nugget effect is seen as a discontinuity at the origin.", "dataType": "KrigingModel"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "search_radius", "isOptional": true, "description": "Defines which of the input points will be used to interpolate the value for each cell in the output raster. There are two ways to specify the specify the searching neighborhood: Variable and Fixed . Variable uses a variable search radius in order to find a specified number of input sample points for the interpolation. Fixed uses a specified fixed distance within which all input points will be used. Variable is the default. The syntax for these parameters are: If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the minimum_number_of_points fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition. Variable, number_of_points, maximum_distance , where: number_of_points \u2014An integer value specifying the number of nearest input sample points to be used to perform interpolation. The default is 12 points. maximum_distance \u2014Specifies the distance, in map units, by which to limit the search for the nearest input sample points. The default value is the length of the extent's diagonal. Fixed, distance, minimum_number_of_points , where: distance \u2014Specifies the distance as a radius within which input sample points will be used to perform the interpolation. The value of the radius is expressed in map units. The default radius is five times the cell size of the output raster. minimum_number_of_points \u2014An integer defining the minimum number of points to be used for interpolation. The default value is 0. If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the minimum_number_of_points fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition. ", "dataType": "Radius"}, {"name": "out_variance_prediction_raster", "isOutputFile": true, "isOptional": true, "description": "Optional output raster where each cell contains the predicted semi-variance values for that location. ", "dataType": "Raster Dataset"}]},
{"syntax": "Curvature_3d (in_raster, out_curvature_raster, {z_factor}, {out_profile_curve_raster}, {out_plan_curve_raster})", "name": "Curvature (3D Analyst)", "description": "Calculates the curvature of a raster surface, optionally including profile and plan curvature. \r\n Learn more about how Curvature works", "example": {"title": "Curvature example 1 (Python window)", "description": "This example creates a curvature raster from an input surface raster and also applies a z-factor.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Curvature_3d ( \"elevation\" , \"c:/output/outcurv01\" , 1.094 )"}, "usage": ["The primary output is the curvature of the surface on a cell-by-cell basis, as fitted through that cell and its eight surrounding neighbors. Curvature is the second derivative of the surface,  or the slope-of-the-slope. Two optional output curvature types are possible: the profile curvature is in the direction of the maximum slope, and the plan curvature is perpendicular to the direction of the maximum slope.", "A positive curvature indicates the surface is upwardly convex at that cell. A negative curvature indicates the surface is upwardly concave at that cell. A value of 0 indicates the surface is flat.", "In the profile output, a negative value indicates the surface is upwardly convex at that cell. A positive profile indicates the surface is upwardly concave at that cell. A value of 0 indicates the surface is flat.", "In the plan output, a positive value indicates the surface is upwardly convex at that cell. A negative plan indicates the surface is upwardly concave at that cell. A value of 0 indicates the surface is flat.", "Units of the curvature output raster, as well as the units for the optional output profile curve raster and output plan curve raster, are one hundredth (1/100) of a z-unit. The reasonably expected values of all three output rasters for a hilly area (moderate relief) can vary from -0.5 to 0.5; while for steep, rugged mountains (extreme relief), the values can vary between -4 and 4. Note that it is possible to exceed this range for certain raster surfaces."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_curvature_raster", "isOutputFile": true, "isOptional": false, "description": "The output curvature raster. ", "dataType": "Raster Dataset"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "out_profile_curve_raster", "isOutputFile": true, "isOptional": true, "description": "Output profile curve raster dataset. This is the curvature of the surface in the direction of slope. ", "dataType": "Raster Dataset"}, {"name": "out_plan_curve_raster", "isOutputFile": true, "isOptional": true, "description": "Output plan curve raster dataset. This is the curvature of the surface perpendicular to the slope direction. ", "dataType": "Raster Dataset"}]},
{"syntax": "ContourWithBarriers_3d (in_raster, out_contour_feature_class, {in_barrier_features}, {in_contour_type}, {in_contour_values_file}, {explicit_only}, {in_base_contour}, {in_contour_interval}, {in_indexed_contour_interval}, {in_contour_list}, {in_z_factor})", "name": "Contour with Barriers (3D Analyst)", "description": "Creates contours from a raster surface. The inclusion of barrier features will allow one to independently generate contours on either side of a barrier.", "example": {"title": "ContourWithBarriers example 1 (Python window)", "description": "This example creates contours from an Esri Grid raster with an input barrier feature as well as base and interval parameters specified. The output contours area as polylines in a shapefile.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.ContourWithBarriers_3d ( \"elevation\" , \"C:/output/outcwb.shp\" , \"elevation_barrier.shp\" , \"POLYLINES\" , \"\" , \"\" , 0 , 300 )"}, "usage": ["The current version of ", "Contour with Barriers", " only supports polyline output. If the polygon output option is used it will be ignored and polyline output will be created.", "If you have the ", "ArcGIS Spatial Analyst extension", " available, smoother but less accurate contours can be obtained by preprocessing the  input raster with a ", "Focal_Statistics", " operation with the MEAN option or the ", "Filter", " tool with the LOW option.", "Contours will extend into the raster's NoData cell by a distance of half the raster's cell size. This will mean that the contours will be generated over single NoData cells. However, a 3-cell-by-3-cell area of NoData will only have the contours extending into this area by half the cell size distance.", "The Type field in the output contour feature class has values:", "An indexed contour interval can be used to generate additional contours and their Type value will be coded as 2 in the output feature class.", "A base contour is used, for example, when you want to create contours every 15 meters, starting at 10 meters. Here, 10 would be used for the base contour, and 15 would be the contour interval. The values to be contoured would be 10, 25, 40, 55, and so on.", "Specifying a base contour does not prevent contours from being created above or below that value.", "A text file containing contour value specifications can include the following:", "For example, if a raster has a minimum value of 102 and a maximum of 500, then a text file with:", "will produce contours at:", "If there are cell values of the raster within a barrier polygon feature the contour lines will be split at the barrier. If the cell values within the polygon feature are to be ignored, change those cell values to NoData.", "If the input raster surface is very large or many output features are requested, a large number of temporary files will be created in the operating system's temporary file location. If any problems are encountered as a result of this do one of the following:", "The output contour features can be labeled by using the ", "Contour Annotation", " tool."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_contour_feature_class", "isOutputFile": true, "isOptional": false, "description": "Output contour features. ", "dataType": "Feature Class"}, {"name": "in_barrier_features", "isInputFile": true, "isOptional": true, "description": "Input barrier features. ", "dataType": "Feature Layer"}, {"name": "in_contour_type", "isInputFile": true, "isOptional": true, "description": "The type of contour to create. The current version of Contour with Barriers only supports polyline output. If the polygon output option is used it will be ignored and polyline output will be created. POLYLINES \u2014 The contour or isoline representation of the input raster. POLYGONS \u2014 Closed polygons representing the contours. ", "dataType": "String"}, {"name": "in_contour_values_file", "isInputFile": true, "isOptional": true, "description": "The base contour, contour interval, indexed contour interval, and explicit contour values can also be specified via a text file. ", "dataType": "File"}, {"name": "explicit_only", "isOptional": true, "description": "Only explicit contour values are used. Base contour, contour interval, and indexed contour intervals are not specified. NO_EXPLICIT_VALUES_ONLY \u2014 The default, contour interval must be specified. EXPLICIT_VALUES_ONLY \u2014 Only explicit contour values are specified.", "dataType": "Boolean"}, {"name": "in_base_contour", "isInputFile": true, "isOptional": true, "description": "Base contour value. Contours are generated above and below this value as needed to cover the entire value range of the input raster. The default is zero. ", "dataType": "Double"}, {"name": "in_contour_interval", "isInputFile": true, "isOptional": true, "description": "The interval, or distance, between contour lines. This can be any positive number. ", "dataType": "Double"}, {"name": "in_indexed_contour_interval", "isInputFile": true, "isOptional": true, "description": "Contours will also be generated for this interval and will be flagged accordingly in the output feature class. ", "dataType": "Double"}, {"name": "in_contour_list", "isInputFile": true, "isOptional": false, "description": "Explicit values at which to create contours. ", "dataType": "Double"}, {"name": "in_z_factor", "isInputFile": true, "isOptional": true, "description": " The unit conversion factor used when generating contours. The default value is 1. The contour lines are generated based on the z-values in the input raster, which are often measured in units of meters or feet. With the default value of 1, the contours will be in the same units as the z-values of the input raster. To create contours in a different unit than that of the z-values, set an appropriate value for the z-factor. Note that it is not necessary to have the ground x,y and surface z-units be consistent for this tool. For example, if the elevation values in your input raster are in feet, but you want the contours to be generated based on units of meters, set the z-factor to 0.3048 (since 1 ft = 0.3048 m). ", "dataType": "Double"}]},
{"syntax": "PolygonVolume_3d (in_surface, in_feature_class, in_height_field, {reference_plane}, {out_volume_field}, {surface_area_field}, {pyramid_level_resolution})", "name": "Polygon Volume (3D Analyst)", "description": "Calculates the volume and surface area between a polygon and terrain or TIN surface.", "example": {"title": "PolygonVolume example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.PolygonVolume_3d ( \"sample.gdb/featuredataset/terrain\" , \"polygon.shp\" , \"<None>\" , \"ABOVE\" , \"Volume\" , \"SArea\" , \"5\" )"}, "usage": ["Calculations will only be made for the portions of the input polygons and the TIN or terrain dataset surface that overlap.", "Each polygon boundary is first intersected with the interpolation zone of the surface. This identifies the area in common between the two. Then, volume and surface area gets calculated for all triangles and portions thereof that fall within the intersected polygon.", "The volume represents the cubic area between the selected portion of the surface and a horizontal plane located at the height specified in the ", "Height Field", " parameter:"], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input terrain or TIN surface. ", "dataType": "Tin Layer; Terrain Layer"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The input polygon feature class. ", "dataType": "Feature Layer"}, {"name": "in_height_field", "isInputFile": true, "isOptional": false, "description": "The field in the polygon's attribute table that defines the height of the reference plane used in determining volumetric calculations. ", "dataType": "String"}, {"name": "reference_plane", "isOptional": true, "description": "Determines how volume and surface area are calculated. ABOVE \u2014 Volume and surface area are calculated above the reference plane height of the polygons. BELOW \u2014 Volume and surface area are calculated below the reference plane height of the polygons. This is the default.", "dataType": "String"}, {"name": "out_volume_field", "isOutputFile": true, "isOptional": true, "description": "The name of the field in the output that will contain the volume calculated in the analysis. The default is Volume. ", "dataType": "String"}, {"name": "surface_area_field", "isOptional": true, "description": "The name of the field in the output that will contain the surface area calculated in the analysis. The default is SArea. ", "dataType": "String"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "RasterTin_3d (in_raster, out_tin, {z_tolerance}, {max_points}, {z_factor})", "name": "Raster To TIN (3D Analyst)", "description": "Converts a raster to a triangulated  irregular network (TIN) dataset. \r\n Learn more about how Raster To TIN works \r\n", "example": {"title": "RasterToTIN example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.RasterTin_3d ( \"vermont_ele.tif\" , \"C:/output/TIN_VT\" , \"2\" , \"1000\" , \"1\" )"}, "usage": ["Converting a raster to a TIN will not, in and of itself, produce a better surface. You need ancillary data that's compatible with, and improves, the surface definition. Such data could be added to the TIN using the ", "Edit TIN", " tool.", "The default maximum allowable difference between the height of the input raster and the height of the output TIN is 1/10 of the z range of the input raster.", "While the maximum size of a TIN that can be used under Win32 is between 15 to 20 million nodes, it's recommended to cap the size at a few million. Large input rasters, and small z-tolerance settings, may exceed this. If size is an issue, consider processing subsets."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster. ", "dataType": "Raster Layer"}, {"name": "out_tin", "isOutputFile": true, "isOptional": false, "description": "The output TIN dataset. ", "dataType": "TIN"}, {"name": "z_tolerance", "isOptional": true, "description": "The maximum allowable difference in (z units) between the height of the input raster and the height of the output TIN. By default, the z tolerance is 1/10 of the z range of the input raster. ", "dataType": "Double"}, {"name": "max_points", "isOptional": true, "description": "The maximum number of points that will be added to the TIN before the process is terminated. By default, the process will continue until all the points are added. ", "dataType": "Long"}, {"name": "z_factor", "isOptional": true, "description": "The factor that the height values of the raster will be multiplied by in the resulting TIN dataset. This is typically used to convert Z units to match XY units. ", "dataType": "Double"}]},
{"syntax": "DeleteTerrainPoints_3d (in_terrain, data_source, polygon_features_or_extent)", "name": "Delete Terrain Points (3D Analyst)", "description": "Deletes points within a specified area of interest from one or more features that participate in a terrain dataset.", "example": {"title": "DeleteTerrainPoints example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.DeleteTerrainPoints_3d ( \"sample.gdb/featuredataset/terrain\" , \"mass_pts_embed\" , \"1379938 235633 1382756 237681\" )"}, "usage": ["Deleting points from an embedded feature class will invalidate the terrain. Run the ", "Build Terrain", " tool after deleting points.", "If the terrain is in SDE, it must be registered as versioned."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "data_source", "isOptional": false, "description": "One or more feature classes from which points will be removed. ", "dataType": "String"}, {"name": "polygon_features_or_extent", "isOptional": false, "description": "Specifies the area from which points will be removed. A polygon feature class or an extent can be used. If extent values are desired, use an arcpy.Extent object. ", "dataType": "Feature Layer; Extent"}]},
{"syntax": "Hillshade_3d (in_raster, out_raster, {azimuth}, {altitude}, {model_shadows}, {z_factor})", "name": "Hillshade (3D Analyst)", "description": "Creates a shaded relief from a surface raster by considering the illumination source angle and shadows. \r\n\r\n Learn more about how Hillshade works", "example": {"title": "Hillshade example 1 (Python window)", "description": "This example generates a hillshade raster that includes shadows. Specific azimuth and altitude angles are set.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.HillShade_3d ( \"elevation\" , \"C:/output/outhillshd01\" , 180 , 75 , \"SHADOWS\" , 1 )"}, "usage": ["The ", "Hillshade", " tool creates a shaded relief raster from a raster. The illumination source is considered to be at infinity.", "The hillshade raster has an integer value range of 0 to 255.", "Two types of shaded relief rasters can be output. If the ", "Model shadows", " option is disabled (unchecked), the output raster only considers local illumination angle. If it is enabled (checked), the output raster considers the effects of both local illumination angle and shadow.", "The analysis of shadows is done by considering the effects of the local horizon at each cell. Raster cells in shadow are assigned a value of zero.", "To create a raster of the shadow areas only, use the ", "Reclassify", " tool to separate the value zero from the other hillshade values. ", "Hillshade", " must have had the ", "Model shadows", " option enabled.", "If the input raster is in a spherical coordinate system, such as decimal degrees, the resulting hillshade may look peculiar. This is due to the difference in measure between the horizontal ground units and the elevation z units. Since the length of a degree of longitude changes with latitude, you will need to specify an appropriate z-factor for that latitude. If your x,y units are decimal degrees and your z units are meters, some appropriate z-factors for particular latitudes are:", "You can create dramatic three-dimensional views of the hillshaded surface by draping the output raster using ArcGIS ArcScene."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output hillshade raster. The hillshade raster has an integer value range of 0 to 255. ", "dataType": "Raster Dataset"}, {"name": "azimuth", "isOptional": true, "description": "Azimuth angle of the light source. The azimuth is expressed in positive degrees from 0 to 360, measured clockwise from north. The default is 315 degrees. ", "dataType": "Double"}, {"name": "altitude", "isOptional": true, "description": "Altitude angle of the light source above the horizon. The altitude is expressed in positive degrees, with 0 degrees at the horizon and 90 degrees directly overhead. The default is 45 degrees. ", "dataType": "Double"}, {"name": "model_shadows", "isOptional": true, "description": "Type of shaded relief to be generated. NO_SHADOWS \u2014 The output raster only considers local illumination angles; the effects of shadows are not considered.The output values can range from 0 to 255, with 0 representing the darkest areas, and 255 the brightest. SHADOWS \u2014 The output shaded raster considers both local illumination angles and shadows.The output values range from 0 to 255, with 0 representing the shadow areas, and 255 the brightest.", "dataType": "Boolean"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}]},
{"syntax": "TinSlope_3d (in_tin, out_feature_class, {units}, {class_breaks_table}, {slope_field}, {z_factor})", "name": "TIN Slope (3D Analyst)", "description": "Extracts slope information from an input TIN into an output feature class. Produces a polygon feature class whose polygons are determined by the input TIN's triangle slope values.", "example": {"title": "TIN Slope Example (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcgisscripting gp = arcgisscripting.create () gp.CheckOutExtension ( \"3D\" ) gp.workspace = \"C:/data\" gp.TinSlope_3d ( \"tin\" , \"slope.shp\" )"}, "usage": ["Use {class_breaks_table} to constrain slope information into specific break intervals of the output feature class.", "Use a comma-delimited TXT or DBF file for a class breaks table. The table requires one column for class break values and, optionally, a second column of class codes. Also, the table needs a header row so that the first row of true data is not skipped. An example is illustrated in the following link: ", "Learn more about how TIN Slope (3D Analyst) works", ".", "Units are only honored when using a class breaks table."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "Tin Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "units", "isOptional": true, "description": "The units of measure for the slope values. Units are honored when a class breaks table is used. ", "dataType": "String"}, {"name": "class_breaks_table", "isOptional": true, "description": "An input table containing the classification breaks that will be used to classify the output feature class. ", "dataType": "Table"}, {"name": "slope_field", "isOptional": true, "description": "The field containing slope values. ", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The factor applied to the slope calculation to convert the TIN's z units to x and y units. By default, the z-factor is 1. ", "dataType": "Double"}]},
{"syntax": "Viewshed_3d (in_raster, in_observer_features, out_raster, {z_factor}, {curvature_correction}, {refractivity_coefficient})", "name": "Viewshed (3D Analyst)", "description": "Determines the raster surface locations visible to a set of observer features. \r\n\r\n Learn more about how Viewshed works", "example": {"title": "Viewshed example 1 (Python window)", "description": "This example determines the surface locations visible to a set of observers defined in a shapefile.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Viewshed_3d ( \"elevation\" , \"observers.shp\" , \"C:/output/outvwshd01\" , 2 , \"CURVED_EARTH\" , 0.15 )"}, "usage": ["Determining observer points is a computer-intensive process. The processing time is dependent on the resolution. For preliminary studies, you may want to use a coarser cell size to reduce the number of cells in the input. Use the full resolution raster when the final results are ready to be generated.", "If the input raster contains undesirable noise caused by sampling errors, and you have the ", "ArcGIS Spatial Analyst extension", " available, you can smooth the raster with a low-pass filter, such as the Mean option of ", "Focal Statistics", ", before running this tool.", "The visibility of each cell center is determined by comparing the altitude angle to the cell center with the altitude angle to the local horizon. The local horizon is computed by considering the intervening terrain between the point of observation and the current cell center. If the point lies above the local horizon, it is considered visible."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "in_observer_features", "isInputFile": true, "isOptional": false, "description": "The feature class that identifies the observer locations. The input can be point or polyline features. ", "dataType": "Feature Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The output will only record the number of times that each cell location in the input surface raster can be seen by the input observation points (or vertices for polylines). The observation frequency will be recorded in the VALUE item in the output raster's attribute table. ", "dataType": "Raster Dataset"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}, {"name": "curvature_correction", "isOptional": true, "description": "Allows correction for the earth's curvature. FLAT_EARTH \u2014 No curvature correction will be applied. This is the default. CURVED_EARTH \u2014 Curvature correction will be applied.", "dataType": "Boolean"}, {"name": "refractivity_coefficient", "isOptional": true, "description": "Coefficient of the refraction of visible light in air. The default value is 0.13. ", "dataType": "Double"}]},
{"syntax": "ExtrudeBetween_3d (in_tin1, in_tin2, in_feature_class, out_feature_class)", "name": "Extrude Between (3D Analyst)", "description": "Creates 3D features by extruding each input feature between two triangulated irregular network (TIN) datasets.", "example": {"title": "ExtrudeBetween example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.ExtrudeBetween_3d ( \"tin1\" , \"tin2\" , \"study_area.shp\" , \"extrusion.shp\" )"}, "usage": ["Only the portion of input features that intersect with the horizontal extent of both TINs will be represented in the output.", "The geometry of the extruded features depends on the geometry of the input features: "], "parameters": [{"name": "in_tin1", "isInputFile": true, "isOptional": false, "description": "The first input TIN. ", "dataType": "TIN Layer"}, {"name": "in_tin2", "isInputFile": true, "isOptional": false, "description": "The second input TIN. ", "dataType": "TIN Layer"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The features that will be extruded between the TINs. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output that will store the extruded features. ", "dataType": "Feature Class"}]},
{"syntax": "RemoveTerrainPyramidLevel_3d (in_terrain, pyramid_level_resolution)", "name": "Remove Terrain Pyramid Level (3D Analyst)", "description": "Removes a pyramid level from a terrain dataset.", "example": {"title": "RemoveTerrainPyramidLevel example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.RemoveTerrainPyramidLevel_3d ( \"sample.gdb/featuredataset/terrain\" , 10 )"}, "usage": ["Any pyramid level can be removed except for level 0, which represents the full resolution pyramid.", "When used in an ", "ArcSDE", " database, the input terrain cannot be registered as versioned."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "pyramid_level_resolution", "isOptional": false, "description": "The pyramid level to be removed as specified by its resolution. ", "dataType": "Double"}]},
{"syntax": "FeatureTo3DByAttribute_3d (in_features, out_feature_class, height_field, {to_height_field})", "name": "Feature To 3D By Attribute (3D Analyst)", "description": "Creates 3D features using height values derived from the attribute of the input features.", "example": {"title": "FeatureTo3DByAttribute example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.FeatureTo3DByAttribute_3d ( 'Points2D.shp' , 'Points3D.shp' , 'Elevation' )"}, "usage": [" Supports points, multipoints, lines, and polygon geometries. ", "Each feature's elevation will be derived from the value obtained in  the specified height field. ", "Line features can optionally provide a second height field. Using two height fields will result in  each line feature starting from the Z-value obtained in the first height field and ending at the Z-value from the second height field. The heights for any intermediate vertices will be interpolated based on the slope of the line between the two endpoints."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The features that will be used to create 3D features. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "height_field", "isOptional": false, "description": "The field whose values will define the height of the resulting 3D features. ", "dataType": "Field"}, {"name": "to_height_field", "isOptional": true, "description": "An optional second height field used for lines. When using two height fields, each line will start at the first height and end at the second (sloped). ", "dataType": "Field"}]},
{"syntax": "NaturalNeighbor_3d (in_point_features, z_field, out_raster, {cell_size})", "name": "Natural Neighbor (3D Analyst)", "description": "Interpolates a raster surface from points using a natural neighbor\r\ntechnique. \r\n Learn more about how Natural Neighbor works", "example": {"title": "NaturalNeighbor example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.NaturalNeighbor_3d ( \"ca_ozone_pts.shp\" , \"ozone\" , \"C:/output/nnout.tif\" , 2000 )"}, "usage": ["If the cell center of the perimeter cells of the output raster fall outside the convex hull (defined by the input points), then those cells will be assigned NoData values. If an input point falls within one of these perimeter cells and the cell center falls outside the convex hull, the cell will still be assigned a value of NoData.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data.", "This tool has a limit of approximately 15 million input points. If your input feature class contains more than a very large number of points (around 15 million or greater), the tool may fail to create a result.", "You can avoid this limit by processing your study area in several sections and ", "mosaicking the results", " into a single large raster dataset. Ensure that there is some overlap between the sections. Alternatively, you can use a ", "Terrain dataset", " to store and visualize points and surfaces comprised of billions of measurement points.", " It is recommended that the input data be in a projected coordinate system rather than in a geographic coordinate system.", "An alternative approach is to use a ", "TIN", " dataset.  First, ", "create a TIN", " from your source data. Then, convert the resulting TIN to a raster with the ", "TIN To Raster", " tool, using the Natural Neighbors option. This is particularly useful if you have breaklines or an irregularly shaped data area."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Layer"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}]},
{"syntax": "TerrainToRaster_3d (in_terrain, out_raster, {data_type}, {method}, {sample_distance}, {pyramid_level_resolution})", "name": "Terrain To Raster (3D Analyst)", "description": "Converts a terrain dataset into a raster.", "example": {"title": "TerrainToRaster example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.TerrainToRaster_3d ( \"sample.gdb/featuredataset/terrain\" , \"terrain.img\" , \"INT\" , \"LINEAR\" , \"CELLSIZE 10\" , 2.5 )"}, "usage": ["The resolution parameter indicates which pyramid level of the terrain to use for conversion. Pyramid levels are defined using the z-tolerance or window-size  pyramid types.  For more information on terrain pyramids, see ", "Terrain Pyramids", ".", "To extract a subset of the terrain, define the extent using the geoprocessing environment settings.", "The available interpolation options are LINEAR and NATURAL_NEIGHBORS. These are TIN-based methods applied through the triangulated terrain surface. The linear option finds the triangle encompassing each cell center and applies a weighted average of the triangle's nodes to interpolate a value. The ", "natural neighbors", " option uses area-based weights on Voronoi neighbors.", "The output raster may be file based or created as a raster dataset in a geodatabase. A file based raster format is determined by the extension given to the raster. For example, use of the extensions .img and .tif will produce IMAGINE or TIFF files. If the raster does not include a file extension, an ", "Esri", " GRID will be produced. "], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The location and name of the output raster. When storing a raster dataset in a geodatabase or in a folder such as an Esri Grid, no file extension should be added to the name of the raster dataset. A file extension can be provided to define the raster's format when storing it in a folder: If the raster is stored as a TIFF file or in a geodatabase, its raster compression type and quality can be specified using geoprocessing environment settings. .bil\u2014Esri BIL .bip\u2014Esri BIP .bsq\u2014Esri BSQ .dat\u2014ENVI DAT .img\u2014ERDAS IMAGINE .png\u2014PNG .tif\u2014TIFF", "dataType": "Raster Dataset"}, {"name": "data_type", "isOptional": true, "description": "The data type of the output raster can be defined by the following keywords: FLOAT \u2014 Output raster will use 32-bit floating point, which supports values ranging from -3.402823466e+38 to 3.402823466e+38. This is the default. INT \u2014 Output raster will use an appropriate integer bit depth. This option will round Z-values to the nearest whole number and write an integer to each raster cell value.", "dataType": "String"}, {"name": "method", "isOptional": true, "description": "Choose an interpolation method. By default, cell values are calculated using the LINEAR method. LINEAR \u2014 Calculates cell values by using linear interpolation of the TIN triangles NATURAL_NEIGHBORS \u2014 Calculates cell values by using natural neighbors interpolation of TIN triangles", "dataType": "String"}, {"name": "sample_distance", "isOptional": false, "description": "The sampling method and distance used to define the cell size of the output raster. OBSERVATIONS \u2014Defines the number of cells on the longest side of the output raster. This method is used by default with a distance of 250. CELLSIZE \u2014Defines the cell size of the output raster.", "dataType": "String"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "ConstructSightLines_3d (in_observer_points, in_target_features, out_line_feature_class, {observer_height_field}, {target_height_field}, {join_field}, {sample_distance})", "name": "Construct Sight Lines (3D Analyst)", "description": " Creates line features that represent sight lines from one or more observer points to features in a  target feature class.", "example": {"title": "ConstructSightLines example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.ConstructSightLines_3d ( 'observer_pt.shp' , 'target.shp' , 'sightlines.shp' , 'BASEHEIGHT' , 'TOP_HEIGHT' , 'NAME' )"}, "usage": ["The perimeter is sampled uniformly if the target features are lines or polygons.", " The input feature class must be a point feature class. Multipoints are not valid. ", " A three-dimensional output will be generated if a height source is specified for both observer and target features.", "A join field is used to specify one or more targets for a given observer. If no join field is used, all points will be connected to all targets.", "The height source of the observer and target features defaults to the first field name encountered in this list: ", "If no suitable height field exists, the <None> keyword will be used by default to indicate the features have no Z-values.", "If  the desired height field does not have a higher priority in the default field selection, the desired field will need to be explicitly specified.   Similarly, if a height field is not desired but the feature class contains one of the fields in the default selection list, the <None> keyword will need to be specified.", "The ", "Sampling Distance", " units  should be given in x,y units of output feature class.", "The following fields will be added to the output feature class\r\nthat contains the sight lines:\r\n"], "parameters": [{"name": "in_observer_points", "isInputFile": true, "isOptional": false, "description": "The single-point features that represent observer points. Multipoint features are not supported. ", "dataType": "Feature Layer"}, {"name": "in_target_features", "isInputFile": true, "isOptional": false, "description": " The target features (points, multipoints, lines, and polygons). ", "dataType": "Feature Layer"}, {"name": "out_line_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output feature class containing the sight lines. ", "dataType": "Feature Class"}, {"name": "observer_height_field", "isOptional": true, "description": " The source of the height values for the observer points obtained from its attribute table. A default Observer Height Field field is selected from among the options listed below by order of priority. If multiple fields exist, and the desired field does not have a higher priority in the default field selection, the desired field will need to be specified. If no suitable height field exists, the <None> keyword will be used. Similarly, if a height field is not desired but the feature class has one of the fields listed below, the <None> keyword will need to be specified. ", "dataType": "String"}, {"name": "target_height_field", "isOptional": true, "description": " The height field for the target. A default Target Height Field field is selected from among the options listed below by order of priority. If multiple fields exist, and the desired field does not have a higher priority in the default field selection, the desired field will need to be specified. If no suitable height field exists, the <None> keyword will be used. Similarly, if a height field is not desired but the feature class has one of the fields listed below, the <None> keyword will need to be specified. If no suitable height field exists, the <None> keyword will be used by default. ", "dataType": "String"}, {"name": "join_field", "isOptional": true, "description": " The join field is used to match observers to specific targets. ", "dataType": "String"}, {"name": "sample_distance", "isOptional": true, "description": " The distance between samples when the target is either a line or polygon feature class. The Sampling Distance units are interpreted in the XY units of the output feature class. ", "dataType": "Double"}]},
{"syntax": "CopyTIN_3d (in_tin, out_tin, {version})", "name": "Copy TIN (3D Analyst)", "description": "Creates a copy of a triangulated irregular network (TIN) dataset.", "example": {"title": "CopyTIN example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.CopyTin_3d ( \"elevation\" , \"elevation_copy\" , \"CURRENT\" )"}, "usage": ["Specify the PRE_10.0 option to create a backward-compatible copy of the TIN that can be used in versions of ArcGIS released prior to 10.0.", "Specify the CURRENT option to create a copy of the TIN using the 10.0 specifications, which offer the following enhancements:"], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_tin", "isOutputFile": true, "isOptional": false, "description": "The output TIN dataset. ", "dataType": "TIN"}, {"name": "version", "isOptional": true, "description": " The version of the output TIN. CURRENT \u2014 The current TIN version at ArcGIS 10.0, which supports constrained Delaunay triangulation. This is the default. PRE_10.0 \u2014 The TIN version supported prior to ArcGIS 10.0, which only supports conforming Delaunay triangulation.", "dataType": "String"}]},
{"syntax": "TopoToRasterByFile_3d (in_parameter_file, out_surface_raster, {out_stream_features}, {out_sink_features}, {out_residual_feature}, {out_stream_cliff_error_feature}, {out_contour_error_feature})", "name": "Topo to Raster by File (3D Analyst)", "description": "Interpolates a hydrologically correct raster surface from point,\r\nline, and polygon data using parameters specified in a file. \r\n Learn more about how Topo to Raster works", "example": {"title": "TopoToRasterByFile example 1 (Python window)", "description": "This example creates a hydrologically correct TIFF surface raster from a parameter file defining the input point, line, and polygon data.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.TopoToRasterByFile_3d ( \"topotorasbyfile.txt\" , \"c:/output/ttrbf_out.tif\" , \"c:/ouput/outstreams.shp\" , \"#\" , \"c:/ouput/outresid.shp\" )"}, "usage": ["The parameter file is structured with the input datasets listed first, followed by the various parameter settings, then the output options.", "The input data identifies the input datasets and, where applicable, fields. There are nine types of input: Contours, Points, Sinks, Streams, Lakes, Boundaries, Cliffs, Exclusion, and Coastal polygons. You can use as many inputs as you want, within reason. The order in which the inputs are entered does not have any bearing on the outcome. ", "<Path>", " indicates a path to a dataset, ", "<Item>", " indicates a field name, and ", "<#>", " indicates a value to be entered.", "The following table lists all of the parameters, the definition of each, and their syntax.", "Parameter", "Definition", "Syntax", "Input datasets:", "Contours", "Contour line dataset with item containing height values.", "Points", "Point dataset with item containing height values.", "Sinks", "Point dataset containing sink locations. If the dataset has elevation values for the sinks, specify that field name as the ", "<Item>", ". If only the locations of the sinks are to be used, use ", "NONE", " for ", "<Item>", ".", "Streams", "Stream line dataset. Height values are not necessary.", "Lakes", "Lake polygon dataset. Height values are not necessary.", "Boundary", "Boundary polygon dataset. Height values are not necessary.", "Cliff", "Line dataset of the cliffs. There is no Field option for Cliff.", "Exclusion", "Exclusion polygon dataset of the areas in which the input data should be ignored. There is no Field option for Exclusion.", "Coast", "Coast polygon dataset containing the outline of a coastal area. There is no Field option for Coast. ", "Parameter settings:", "Enforce", "Controls whether drainage enforcement is applied.", "Datatype", "Primary type of input data.", "Iterations", "The maximum number of iterations the algorithm performs.", "Roughness penalty", "The measure of surface roughness.", "Profile curvature roughness penalty", "The profile curvature roughness penalty is a locally adaptive penalty that can be use to partly replace total curvature.", "Discretisation error factor", "The amount to adjust the data smoothing of the input data into a raster.", "Vertical standard error", "The amount of random error in the z-values of the input data.", "Tolerances", "The first reflects the accuracy of elevation data in relation to surface drainage, and the other prevents drainage clearance through unrealistically high barriers.", "Z-Limits", "Lower and upper height limits.", "Extent", "Minimum x, minimum y, maximum x, and maximum y coordinate limits.", "Cell size", "The resolution of the final output raster.", "Margin", "Distance in cells to interpolate beyond the specified output extent and boundary.", "Outputs:", "Output stream features", "The output line feature class of stream polyline features and ridge line features.", "Output sink features", "The output point feature class of the remaining sink point features.", "Output diagnostics file", "The location and name of the diagnostics file.", "Output residual point features", "The output point feature class of all the large elevation residuals as scaled by the local discretisation error.", "Output stream and cliff point features", "The output point feature class of locations where possible stream and cliff errors occur.", "Output contour  error point features", "The output point feature class of possible errors pertaining to the input contour data.", "Do not specify paths for the optional output feature datasets in the parameter file. Use the ", "Output stream polyline features", " and ", "Output remaining sink point features", " in the tool dialog box to identify these outputs.", "The contents of an example parameter file are:"], "parameters": [{"name": "in_parameter_file", "isInputFile": true, "isOptional": false, "description": "The input ASCII text file containing the inputs and parameters to use for the interpolation. The file is typically created from a previous run of Topo to Raster with the optional output parameter file specified. In order to test the outcome of changing the parameters, it is easier to make edits to this file and rerun the interpolation than to correctly issue the Topo to Raster tool each time. ", "dataType": "File"}, {"name": "out_surface_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Layer"}, {"name": "out_stream_features", "isOutputFile": true, "isOptional": true, "description": "Output feature class of stream polyline features. The polyline features are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_sink_features", "isOutputFile": true, "isOptional": true, "description": "Output feature class of remaining sink point features. ", "dataType": "Feature Class"}, {"name": "out_residual_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of all the large elevation residuals as scaled by the local discretisation error. All the scaled residuals larger than 10 should be inspected for possible errors in input elevation and stream data. Large-scaled residuals indicate conflicts between input elevation data and streamline data. These may also be associated with poor automatic drainage enforcements. These conflicts can be remedied by providing additional streamline and/or point elevation data after first checking and correcting errors in existing input data. Large unscaled residuals usually indicate input elevation errors. ", "dataType": "Feature Class"}, {"name": "out_stream_cliff_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of locations where possible stream and cliff errors occur. The locations where the streams have closed loops, distributaries and streams over cliffs can be identified from the point feature class. Cliffs with neighboring cells that are inconsistent with the high and low sides of the cliff are also indicated. This can be a good indicator of cliffs with incorrect direction. Points are coded as follows: ", "dataType": "Feature Class"}, {"name": "out_contour_error_feature", "isOutputFile": true, "isOptional": true, "description": " The output point feature class of possible errors pertaining to the input contour data. Contours with bias in height exceeding five times the standard deviation of the contour values as represented on the output raster are reported to this feature class. Contours that join other contours with a different elevation are flagged in this feature class by the code 1; this is a sure sign of a contour label error. ", "dataType": "Feature Class"}]},
{"syntax": "SurfaceVolume_3d (in_surface, {out_text_file}, {reference_plane}, {base_z}, {z_factor}, {pyramid_level_resolution})", "name": "Surface Volume (3D Analyst)", "description": "Calculates the area and volume of a raster, triangulated irregular network (TIN), or terrain dataset surface above or below a given reference plane. Learn more about how Surface Volume works", "example": {"title": "SurfaceVolume example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.SurfaceVolume_3d ( \"sample.gdb/featuredataset/terrain\" , \"surf_vol.txt\" , \"ABOVE\" , 300 , 1 , 5 )"}, "usage": ["Consider using the ", "Polygon Volume", " tool to determine the volume of a specified portion of a TIN or terrain dataset  surface.", "Using a ", "Z Factor", " is essential for correcting volume calculations when the surface z-units are expressed in a different unit of measure than the ground units. Using a ", "Z Factor", " does not modify the original data.", " A raster DEM is comprised of regularly spaced points and not cells such as an image.  The area calculation reported by the ", "Surface Volume", " tool is based on the extent of the regularly spaced points of the DEM and not the extent of the cells. Because this calculation is based on points, not cells, the data area for the raster DEM is decreased by half a cell relative to the data area displayed as a raster image."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input raster, TIN, or terrain dataset surface used for calculating area and volume. ", "dataType": "Raster Layer; Terrain Layer; TIN Layer"}, {"name": "out_text_file", "isOutputFile": true, "isOptional": true, "description": "The optional output text file containing the results. ", "dataType": "File"}, {"name": "reference_plane", "isOptional": true, "description": "Choose whether to calculate above or below a given height. ABOVE \u2014 Volume and area will be calculated above the Plane Height . This is the default. BELOW \u2014 Volume and area will be calculated below the Plane Height .", "dataType": "String"}, {"name": "base_z", "isOptional": true, "description": "The elevation of the plane that will be used to calculate area and volume. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which the heights of the input surface will be multiplied to calculate surface volume; used for converting Z-units to match XY units. ", "dataType": "Double"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The resolution of the terrain dataset pyramid level to use for geoprocessing. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "SplineWithBarriers_3d (Input_point_features, Z_value_field, {Input_barrier_features}, {Output_cell_size}, Output_raster, {Smoothing_Factor})", "name": "Spline with Barriers (3D Analyst)", "description": "Interpolates a raster surface, using barriers, from points using a\r\nminimum curvature spline technique. The barriers are entered as\r\neither polygon or polyline features. \r\n Learn more about how Spline with Barriers works", "example": {"title": "SplineWithBarriers example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.SplineWithBarriers_3d ( \"ozone_pts.shp\" , \"ozone\" , \"ozone_barrier.shp\" , 2000 , \"C:/output/splinebarrierout.tif\" )"}, "usage": ["This tool requires the Java runtime environment Version 6, or higher, to be installed. The Java Runtime Environment can be downloaded for free from ", "http://www.java.com/en/download", ".", "If the tool fails to run with an error message stating that a \"more recent version of Java needs to be installed\", and you have multiple versions of Java installed, you need to update the PATH environment variable.", "The resulting smooth surface from is constrained by the input barrier features.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data.", "For the ", "Spline with Barriers", " tool, by default  the values for each set of coincident points will be averaged.", "If a cell size of 0 is entered, the cell size actually used will be the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250.", "The barrier features are rasterized and the cell center is used to decide whether the cell falls within a polygon or whether the cell becomes a barrier for polyline features."], "parameters": [{"name": "Input_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "Z_value_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "Input_barrier_features", "isInputFile": true, "isOptional": true, "description": "The optional input barrier features to constrain the interpolation. ", "dataType": "Feature Layer"}, {"name": "Output_cell_size", "isOutputFile": true, "isOptional": false, "description": "The cell size at which the output raster will be created. If a value of 0 is entered, the shorter of the width or the height of the extent of the input point features in the input spatial reference, divided by 250, will be used as the cell size. ", "dataType": "Analysis Cell Size"}, {"name": "Output_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Layer"}, {"name": "Smoothing_Factor", "isOptional": true, "description": "The parameter that influences the smoothing of the output surface. No smoothing is applied when the value is zero and the maximum amount of smoothing is applied when the factor equals 1. The default is 0.0. ", "dataType": "Double"}]},
{"syntax": "PointFileInformation_3d (input, out_feature_class, in_file_type, {file_suffix}, {input_coordinate_system}, {folder_recursion}, {extrude_geometry}, {decimal_separator}, {summarize_by_class_code}, {improve_las_point_spacing})", "name": "Point File Information (3D Analyst)", "description": "Generates statistical information about one or more point files in a polygon or multipatch output.", "example": {"title": "PointFileInformation example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.PointFileInformation_3d ( env.workspace , \"Test.gdb/two_las\" , \"LAS\" , \"las\" , \"Coordinate Systems/Projected Coordinate Systems/UTM/NAD 1983/NAD 1983 UTM Zone 17N.prj\" , True , True , \"DECIMAL_POINT\" , True )"}, "usage": ["Input file(s) must be in either an  XYZ, XYZI, LAS, or GENERATE format.", "When a folder containing point data files is selected as input, the ", "File Suffix", " must be entered.  However, this is not needed for file inputs.", "When the summarize option is not used, the statistical information presented in the feature attribute table consists of the point count, average point spacing, z minimum, and z maximum of each point file entered. A separate row is created for each input file encountered. The point spacing is an estimate that assumes the points within the input file are evenly spaced over the XY extent of each input file.", "Each resulting feature will encompass the XY extent of an input file. The features can be created as 2D polygons or extruded multipatch features that provide a 3D bounding box whose z-values at the base and top reflect the range of elevation values found in the file. The multipatch can be visualized in 3D using ArcScene or ArcGlobe.", "The summarize option is useful to statistically summarize  information for each class code in the input file, but is  expensive, as  each file must be scanned and analyzed.  ", " The point spacing reported by ", "Point File Information", " is not  exact; it is an estimate. The point spacing given is a  summary when looking at trends for collections of files.  The tool uses a rough estimate that simply compares the area of the file's bounding box with the point count. It  is most accurate when the rectangular extent of the file being examined is filled with data. Files with points over large water bodies or on the perimeter of a study area, only partially occupied with data, will not produce accurate estimates.  "], "parameters": [{"name": "input", "isOptional": false, "description": "One or more point data files or folders that will be analyzed. ", "dataType": "Folder; File"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "in_file_type", "isInputFile": true, "isOptional": false, "description": "The format of the input files. LAS \u2014 Airborne lidar format defined by the American Society of Photogrammetry and Remote Sensing (ASPRS). XYZ \u2014 XYZ file. XYZI \u2014 XYZI file. GENERATE \u2014 GENERATE file.", "dataType": "String"}, {"name": "file_suffix", "isOptional": true, "description": "The suffix of the files to import when a folder is specified in the input. This parameter is required if an input folder is provided. ", "dataType": "String"}, {"name": "input_coordinate_system", "isInputFile": true, "isOptional": true, "description": "The coordinate system of the input data. ", "dataType": "Coordinate System"}, {"name": "folder_recursion", "isOptional": true, "description": "Scans through subfolders when an input folder is selected containing data in a subfolders directory. The output feature class will be generated with a row for each file encountered in the directory structure. NO_RECURSION \u2014 Only the data found in the input folder will be used to generate the results. This is the default. RECURSION \u2014 Any data found in the input folder and its subdirectories will be used to generate results.", "dataType": "Boolean"}, {"name": "extrude_geometry", "isOptional": true, "description": "Specifies whether to create a 2D polygon or multipatch feature class with extruded features that reflect the elevation range found in each file. NO_EXTRUSION \u2014 The output will be created as a 2D polygon feature class. This is the default. EXTRUSION \u2014 The output will be created as a multipatch feature class.", "dataType": "Boolean"}, {"name": "decimal_separator", "isOptional": true, "description": "The decimal character used in the text file to differentiate the integer of a number from its fractional part. DECIMAL_POINT \u2014 A point is used as the decimal character. This is the default. DECIMAL_COMMA \u2014 A comma is used as the decimal character.", "dataType": "String"}, {"name": "summarize_by_class_code", "isOptional": true, "description": "Specifies if the results will summarize LAS files per class code or LAS file. NO_ SUMMARIZE \u2014 Each output feature will represent all the class codes found in a lidar file. This is the default. SUMMARIZE \u2014 Each output feature will represent a single class code found in a lidar file.", "dataType": "Boolean"}, {"name": "improve_las_point_spacing", "isOptional": true, "description": " Provides enhanced assessment of the point spacing in LAS files that can reduce over-estimation caused by irregular data distribution. LAS_SPACING \u2014 Regular point spacing estimate is used for LAS files, where the extent is equally divided by the number of points. This is the default. NO_LAS_SPACING \u2014 Binning will be used to obtain a more precise point spacing estimate for LAS files. This may increase the tool's execution time.", "dataType": "Boolean"}]},
{"syntax": "Spline_3d (in_point_features, z_field, out_raster, {cell_size}, {spline_type}, {weight}, {number_points})", "name": "Spline (3D Analyst)", "description": "Interpolates a raster surface from points using a two-dimensional\r\nminimum curvature spline technique. The resulting smooth surface passes exactly through the input\r\npoints. \r\n Learn more about how Spline works", "example": {"title": "Spline example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Spline_3d ( \"ozone_pts.shp\" , \"ozone\" , \"C:/output/splineout.tif\" , 2000 , \"REGULARIZED\" , 0.1 )"}, "usage": ["The REGULARIZED option of ", "Spline type", " usually produces smoother surfaces than those created with the TENSION option.", "With the REGULARIZED option, higher values used for the weight parameter produce smoother surfaces. The values entered for this parameter must be equal to or greater than zero. Typical values used are 0, 0.001, 0.01, 0.1, and 0.5. The ", "Weight", " is the square of the parameter referred to in the literature as tau (t).", "With the TENSION option, higher values entered for the weight parameter result in somewhat coarser surfaces, but surfaces that closely conform to the control points. The values entered must be equal to or greater than zero. Typical values are 0, 1, 5, and 10. The ", "Weight", " is the square of the parameter referred to in the literature as phi (\u03a6).", "The greater the value of ", "Number of Points", ", the smoother the surface of the output raster.", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Layer"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "spline_type", "isOptional": true, "description": "The type of spline to be used. REGULARIZED \u2014 Yields a smooth surface and smooth first derivatives. TENSION \u2014 Tunes the stiffness of the interpolant according to the character of the modeled phenomenon.", "dataType": "String"}, {"name": "weight", "isOptional": true, "description": "Parameter influencing the character of the surface interpolation. When the REGULARIZED option is used, it defines the weight of the third derivatives of the surface in the curvature minimization expression. If the TENSION option is used, it defines the weight of tension. The default weight is 0.1. ", "dataType": "Double"}, {"name": "number_points", "isOptional": true, "description": "The number of points per region used for local approximation. The default is 12. ", "dataType": "Long"}]},
{"syntax": "LandXMLToTin_3d (in_landxml_path, out_tin_folder, tin_basename, {tinnames})", "name": "LandXML To TIN (3D Analyst)", "description": "This tool imports one or more triangulated irregular network (TIN) surfaces from a LandXML file to output  Esri  TINs. ", "example": {"title": "LandXMLToTin example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.LandXMLToTin_3d ( \"surfaces.xml\" , \"TINs\" , \"_\" , \"1;2\" )"}, "usage": [" When the input LandXML file is selected, the TINs to Import parameter is populated with all the TIN surfaces found in the LandXML file.", "A constrained Delaunay TIN from the LandXML file is created as a constrained Delaunay TIN.", "When several TINs will be exported from the LandXML file, the basename would be iterated to define the name of the output TINs in the following manner: <basename>, <basename>2, <basename>3, and so on. If <basename> already exists, the tool will not write anything. If <basename> does not exist but <basename>2 exists, the tool will create <basename> and <basename>2_1, instead of <basename>2.", "In script mode, the names inside the ", "tinnames", " parameter can be\r\nspecified in short form (only number or only name) for \r\nconvenience.\r\n  Instead of ", "\"1. Site0445; 2. <unnamed>; 3. <unnamed>;\r\n4. Site_09\"", " , you  can specify ", "\"1;2;3;4\"", " or ", "\"Site0445;\r\nSite_09;2;3\"", ". The ", "<unnamed>", " keyword cannot be used by itself because the TIN must be uniquely identified.\r\n"], "parameters": [{"name": "in_landxml_path", "isInputFile": true, "isOptional": false, "description": " The input LandXML file. ", "dataType": "File"}, {"name": "out_tin_folder", "isOutputFile": true, "isOptional": false, "description": " The folder that the TINs will be created in. ", "dataType": "Folder"}, {"name": "tin_basename", "isOptional": false, "description": "The prefix name attached to the output TIN. When multiple TINs will be exported from the LandXML file, the base name will be followed by an integer value reflecting the order of the TIN outputs. ", "dataType": "String"}, {"name": "tinnames", "isOptional": false, "description": "Each TIN can be specified by either a name (for example, \"Tin01\" ) or its position in the list of available LandXML TINs (for example 1 to specify the first TIN). The list of TINs to import can be entered as a semicolon-delimited string (for example, \"1. Tin01; 2. Tin02\" ), a list of strings (for example, [\"1. Tin01\", \"2. Tin02\"] ), or a list of numeric values that denote the position of the desired TINs (for example, [1, 2, 3] ). ", "dataType": "String"}]},
{"syntax": "Contour_3d (in_raster, out_polyline_features, contour_interval, {base_contour}, {z_factor})", "name": "Contour (3D Analyst)", "description": "Creates a line feature class of contours (isolines) from a raster surface. \r\n Learn more about how Contouring works", "example": {"title": "Contour example 1 (Python window)", "description": "This sample creates contours from an Esri Grid raster and outputs them as a shapefile.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Contour_3d ( \"elevation\" , \"C:/output/outcontours.shp\" , 200 , 0 )"}, "usage": ["Contours do not extend beyond the spatial extent of the raster, and they are not generated in areas of NoData; therefore, adjacent contour inputs should first be edgematched into a continuous feature dataset. As an alternative to edgematching, you can merge the adjacent rasters before computing contours.", "Contours can be generated in areas of negative raster values. The contour values will be negative in such areas. Negative contour intervals are not allowed.", "If you have the ", "ArcGIS Spatial Analyst extension", " available, smoother but less accurate contours can be obtained by preprocessing the  input raster with a ", "Focal_Statistics", " operation with the MEAN option or the ", "Filter", " tool with the LOW option.", "A base contour is used, for example, when you want to create contours every 15 meters, starting at 10 meters. Here, 10 would be used for the base contour, and 15 would be the contour interval. The values to be contoured would be 10, 25, 40, 55, and so on.", "Specifying a base contour does not prevent contours from being created above or below that value."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_polyline_features", "isOutputFile": true, "isOptional": false, "description": "Output contour polyline features. ", "dataType": "Feature Class"}, {"name": "contour_interval", "isOptional": false, "description": "The interval, or distance, between contour lines. This can be any positive number. ", "dataType": "Double"}, {"name": "base_contour", "isOptional": true, "description": "Base contour value. Contours are generated above and below this value as needed to cover the entire value range of the input raster. The default is zero. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": " The unit conversion factor used when generating contours. The default value is 1. The contour lines are generated based on the z-values in the input raster, which are often measured in units of meters or feet. With the default value of 1, the contours will be in the same units as the z-values of the input raster. To create contours in a different unit than that of the z-values, set an appropriate value for the z-factor. Note that it is not necessary to have the ground x,y and surface z-units be consistent for this tool. For example, if the elevation values in your input raster are in feet, but you want the contours to be generated based on units of meters, set the z-factor to 0.3048 (since 1 ft = 0.3048 m). For another example, consider an input raster in WGS_84 geographic coordinates and elevation units of meters for which you want to generate contour lines every 100 feet with a base of 50 feet (so the contours will be 50 ft, 150 ft, 250 ft, and so on). To do this, set the contour_interval to 100 , the base_contour to 50 , and the z_factor to 3.2808 (since 1m = 3.2808 ft). ", "dataType": "Double"}]},
{"syntax": "SurfaceDifference_3d (in_surface, in_reference_surface, out_feature_class, {pyramid_level_resolution}, {reference_pyramid_level_resolution}, {raster_cell_size}, {out_raster}, {out_tin_folder}, {out_tin_basename})", "name": "Surface Difference (3D Analyst)", "description": "Calculates the volumetric difference between two surface models stored as either a triangulated irregular networks (TIN) or terrain dataset. Learn more about how Surface Difference works", "example": {"title": "SurfaceDifference example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.SurfaceDifference_3d ( \"sample.gdb/featuredataset/terrain\" , \"sample.gdb/featuredataset/terrain2\" , \"surface_diff.shp\" )"}, "usage": ["The triangles from the first surface are classified as either\r\ncompletely above, below, or intersecting the second (reference)\r\nsurface.", " When an output difference raster is requested,\r\nthe tool converts the integrated difference TIN that's been\r\ncalculated during the geometric comparison to the raster using\r\nlinear interpolation."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input terrain or TIN dataset. ", "dataType": "Terrain Layer; TIN Layer"}, {"name": "in_reference_surface", "isInputFile": true, "isOptional": false, "description": "The reference terrain or TIN dataset. ", "dataType": "Terrain Layer; TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class containing contiguous triangles and triangle parts that have the same classification grouped into polygons. The volume enclosed by each region of difference is listed in the attribute table. ", "dataType": "Feature Class"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The pyramid-level resolution of the input terrain dataset. The default is 0, or full resolution. ", "dataType": "Double"}, {"name": "reference_pyramid_level_resolution", "isOptional": true, "description": "The pyramid-level resolution of the reference terrain dataset. The default is 0, or full resolution. ", "dataType": "Double"}, {"name": "raster_cell_size", "isOptional": true, "description": " The cell size of the output raster dataset. ", "dataType": "Double"}, {"name": "out_raster", "isOutputFile": true, "isOptional": true, "description": " The output difference raster dataset. The raster will be converted from the integrated difference TIN using a linear interpolation method. ", "dataType": "Raster Dataset"}, {"name": "out_tin_folder", "isOutputFile": true, "isOptional": true, "description": " The folder location to write the TIN or TINs. ", "dataType": "Folder"}, {"name": "out_tin_basename", "isOutputFile": true, "isOptional": true, "description": " The base name given to each output TIN surface. If one TIN dataset is not sufficient to represent the data, multiple TINs will be created with the same base name. ", "dataType": "String"}]},
{"syntax": "CutFill_3d (in_before_surface, in_after_surface, out_raster, {z_factor})", "name": "Cut Fill (3D Analyst)", "description": "Calculates the volume change between two surfaces. This is typically used for cut and fill operations. \r\n Learn more about how Cut Fill works", "example": {"title": "CutFill example 1 (Python window)", "description": "This example calculates the volume and area of cut and fill locations and outputs the result as a Grid raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.CutFill_3d ( \"elevation01\" , \"elevation02\" , \"c:/output/outcutfill01\" , 1 )"}, "usage": ["The ", "Cut Fill", " tool enables you to create a map based on two input surfaces\u2014before and after\u2014displaying the areas and volumes of surface materials that have been modified by the removal or addition of surface material.", "Both the input raster surfaces must be coincident. That is, they must have a common origin, the same number of rows and columns of cells, and the same cell size.", "For accurate results, the z-units should be the same as the x,y ground units. This ensures that the resulting volumes are meaningful cubic measures (i.e., cubic meters). If they are not the same, use a z-factor to convert z units to x,y units. For example, if your x,y units are meters and your z units are feet, you could specify a z-factor of 0.3048 to convert feet to meters.", "Alternatively, use the ", "Times", " math tool to create a surface raster in which the z-values have been adjusted to correspond to the ground units.", "The attribute table of the output raster presents the changes in the surface volumes following the cut/fill operation. Positive values for the volume difference indicate regions of the before raster surface that have been cut (material removed). Negative values indicate areas that have been filled (material added). See ", "How Cut Fill works", " for more details on how the results are calculated.", "When the cut/fill operation is performed from the tool, by default a specialized renderer is applied that highlights the locations of cut and of fill. The renderer draws areas that have been cut in blue, and areas that have been filled in red. Areas that have not changed are displayed in grey."], "parameters": [{"name": "in_before_surface", "isInputFile": true, "isOptional": false, "description": "The input representing the surface before the cut or fill operation. ", "dataType": "Raster Layer"}, {"name": "in_after_surface", "isInputFile": true, "isOptional": false, "description": "The input representing the surface after the cut or fill operation. ", "dataType": "Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster defining regions of cut and of fill. The values show the locations and amounts where the surface has been added to or removed from. ", "dataType": "Raster Dataset"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}]},
{"syntax": "CreateTin_3d (out_tin, {spatial_reference}, {in_features}, {constrained_delaunay})", "name": "Create TIN (3D Analyst)", "description": "Creates a triangulated irregular network (TIN) dataset.", "example": {"title": "CreateTin example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.CreateTin_3d ( \"NewTIN\" , \"Coordinate Systems/Projected Coordinate Systems/State Plane/NAD 1983 (Feet)/NAD 1983 StatePlane California II FIPS 0402 (Feet).prj\" , \"points.shp Shape.Z masspoints\" , \"constrained_delaunay\" )"}, "usage": ["TINs used for surface modeling should be constructed using projected coordinate systems. Geographic coordinate systems are not recommended because Delaunay triangulation cannot be guaranteed when the XY coordinates are expressed in angular units, and distance-based calculations, such as slope, volume, and line-of-sight, can produce misleading or incorrect results.", "Consider resizing the tool's dialog box if the ", "Input Feature Class", " parameter does not display legibly.", "Consider capping the number of nodes loaded into the TIN from the input features at a few million to maintain adequate usability and display performance. The maximum number of nodes supported by a TIN varies relative to free, contiguous, memory resources on the system. Ten to fifteen million nodes generally represent the largest size achievable under normal operating conditions on  32-bit Windows platforms. Larger datasets are best represented using a terrain."], "parameters": [{"name": "out_tin", "isOutputFile": true, "isOptional": false, "description": "The output TIN dataset. ", "dataType": "TIN"}, {"name": "spatial_reference", "isOptional": true, "description": "The spatial reference of the output TIN. ", "dataType": "Coordinate System"}, {"name": "in_features", "isInputFile": true, "isOptional": false, "description": "Add references to one or more feature classes that will be included in the TIN. For each feature class you'll need to set properties that indicate how it's used to define the surface. in_feature_class : The feature class whose features will be imported into the TIN. height_field : The field that specifies the source of elevation values for the features. Any numeric field in the feature's attribute table can be used. If the feature supports z-values, the feature geometry can be read by selecting the Shape.Z option. If no height is desired, specify the keyword <None> to create Z-less features whose elevation would be interpolated from the surface. SF_type : The surface feature type defines how the geometry imported from the features are incorporated into the triangulation for the surface. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change when the triangulated surface gets converted to a raster. The following keywords are available: tag_value : The integer field from the attribute table of the feature class that will be used when the surface feature type is set to a value fill option. Tag fill is used as a basic form of triangle attribution whose boundaries are enforced in the triangulation as breaklines. The default option is set to <none>. masspoints \u2014 Elevation points that will be imported as nodes hardline or softline \u2014 Breaklines that enforce a height value hardclip or softclip \u2014 Polygon dataset that defines the boundary of the TIN harderase or softerase \u2014 Polygon dataset that defines holes in the interior portions of the TIN hardreplace or softreplace \u2014 Polygon dataset that defines areas of constant height hardvaluefill or softvaluefill \u2014 Polygon dataset that defines tag values for the triangles based on the integer field specified in the tag_value column", "dataType": "Value Table"}, {"name": "constrained_delaunay", "isOptional": true, "description": "Specifies the triangulation technique used along the breaklines of the TIN. DELAUNAY \u2014 The TIN will use Delaunay conforming triangulation, which may densify each segment of the breaklines to produce multiple triangle edges. This is the default. CONSTRAINED_DELAUNAY \u2014 The TIN will use constrained Delaunay triangulation, which will add each segment as a single edge. Delaunay triangulation rules are honored everywhere except along breaklines, which will not get densified.", "dataType": "Boolean"}]},
{"syntax": "Idw_3d (in_point_features, z_field, out_raster, {cell_size}, {power}, {search_radius}, {in_barrier_polyline_features})", "name": "IDW (3D Analyst)", "description": "Interpolates a raster surface from points using an inverse distance\r\nweighted (IDW) technique. \r\n Learn more about how IDW works", "example": {"title": "IDW example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Idw_3d ( \"ozone_pts.shp\" , \"ozone\" , \"C:/output/idwout.tif\" , 2000 , 2 , 10 )"}, "usage": ["The output value for a cell using inverse distance weighting (IDW) is limited to the range of the values used to interpolate. Because IDW is a weighted distance average, the average cannot be greater than the highest or less than the lowest input. Therefore, it cannot create ridges or valleys if these extremes have not already been sampled (Watson and Philip 1985).", "The best results from IDW are obtained when sampling is sufficiently dense with regard to the local variation you are attempting to simulate. If the sampling of input points is sparse or uneven, the results may not sufficiently represent the desired surface (Watson and Philip 1985).", "The influence of an input point on an interpolated value is isotropic. Since the influence of an input point on an interpolated value is distance related, IDW is not \"ridge preserving\" (Philip and Watson 1982).", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data.", "The barriers option is used to specify the location of linear features known to interrupt the surface continuity. These features do not have z-values. Cliffs, faults, and embankments are typical examples of barriers. Barriers limit the selected set of the input sample points used to interpolate output z-values to those samples on the same side of the barrier as the current processing cell. Separation by a barrier is determined by line-of-sight analysis between each pair of points. This means that topological separation is not required for two points to be excluded from each other's region of influence. Input sample points that lie exactly on the barrier line will be included in the selected sample set for both sides of the barrier.", "Barrier features are input as polyline features. ", "IDW", " only uses the x,y coordinates for the linear feature; therefore, it is not necessary to provide z-values for the left and right sides of the barrier. Any z-values provided will be ignored.", "Using barriers will significantly extend the processing time.", "This tool has a limit of approximately 45 million input points. If your input feature class contains more than 45 million points, the tool may fail to create a result. You can avoid this limit by interpolating your study area in several pieces, making sure there is some overlap in the edges, then mosaicking the results to create a single large raster dataset. Alternatively, you can use a ", "terrain dataset", " to store and visualize points and surfaces comprised of billions of measurement points.", "If you have the Geostatistical Analyst extension, you may be able to process larger datasets.", "The input feature data must contain at least one valid field."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. ", "dataType": "Field"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Layer"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "power", "isOptional": true, "description": "The exponent of distance. Controls the significance of surrounding points on the interpolated value. A higher power results in less influence from distant points. It can be any real number greater than 0, but the most reasonable results will be obtained using values from 0.5 to 3. The default is 2. ", "dataType": "Double"}, {"name": "search_radius", "isOptional": true, "description": "Defines which of the input points will be used to interpolate the value for each cell in the output raster. There are two ways to specify the specify the searching neighborhood: Variable and Fixed . Variable uses a variable search radius in order to find a specified number of input sample points for the interpolation. Fixed uses a specified fixed distance within which all input points will be used. Variable is the default. The syntax for these parameters are: If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the minimum_number_of_points fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition. Variable, number_of_points, maximum_distance , where: number_of_points \u2014An integer value specifying the number of nearest input sample points to be used to perform interpolation. The default is 12 points. maximum_distance \u2014Specifies the distance, in map units, by which to limit the search for the nearest input sample points. The default value is the length of the extent's diagonal. Fixed, distance, minimum_number_of_points , where: distance \u2014Specifies the distance as a radius within which input sample points will be used to perform the interpolation. The value of the radius is expressed in map units. The default radius is five times the cell size of the output raster. minimum_number_of_points \u2014An integer defining the minimum number of points to be used for interpolation. The default value is 0. If the required number of points is not found within the specified distance, the search distance will be increased until the specified minimum number of points is found. When the search radius needs to be increased it is done so until the minimum_number_of_points fall within that radius, or the extent of the radius crosses the lower (southern) and/or upper (northern) extent of the output raster. NoData is assigned to all locations that do not satisfy the above condition. ", "dataType": "Radius"}, {"name": "in_barrier_polyline_features", "isInputFile": true, "isOptional": true, "description": "Polyline features to be used as a break or limit in searching for the input sample points. ", "dataType": "Feature Layer"}]},
{"syntax": "MultipatchFootprint_3d (in_feature_class, out_feature_class)", "name": "Multipatch Footprint (3D Analyst)", "description": "Creates polygon footprints representing the two-dimensional  area occupied by a multipatch feature class.", "example": {"title": "MultipatchFootprint example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.MultiPatchFootprint_3d ( \"multipatch.shp\" , \"multipatch_footprint.shp\" )"}, "usage": ["The range of elevation values in the multipatch feature would be attributed to the Z_MIN and Z_MAX fields in the output feature class."], "parameters": [{"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The multipatch feature whose footprint will be generated. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The resulting footprint polygon feature class. ", "dataType": "Feature Class"}]},
{"syntax": "CreateTerrain_3d (in_feature_dataset, out_terrain_name, average_point_spacing, {max_overview_size}, {config_keyword}, {pyramid_type}, {windowsize_method}, {secondary_thinning_method}, {secondary_thinning_threshold})", "name": "Create Terrain (3D Analyst)", "description": "Creates a new terrain dataset.", "example": {"title": "CreateTerrain example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.CreateTerrain_3d ( 'source.gdb/Redlands' , 'Redlands_terrain' , 5 , 50000 , '' , 'WINDOWSIZE' , 'ZMIN' , 'NONE' , 1 )"}, "usage": ["The terrain must reside in a feature dataset.", "The ", "Average Point Spacing", " parameter should be based on the data that will be used in the terrain. While this value does not need to be exact, it should represent a good approximation. If the data has been gathered at significantly different densities from one location to another, give more weight to the smaller spacing in determining this value.", "The point spacing is used to determine the terrain tile size, which is used to optimize data analysis and display performance. Each tile is approximated to contain no more than 200,000 source elevation points.", "Use ", "Add Terrain Pyramid Level", ", and ", "Add Feature Class To Terrain", ", followed by ", "Build Terrain", " to complete the terrain definition and construct a usable terrain.", "Geoprocessing tools for terrain construction are geared toward data automation procedures in Python scripts and ModelBuilder. Consider using the ", "Terrain Wizard", " in ArcCatalog or the ", "Catalog", " window for interactively creating a new terrain. To access the ", "Terrain Wizard", ", right-click a feature dataset and click ", "New ", ">", " Terrain", "."], "parameters": [{"name": "in_feature_dataset", "isInputFile": true, "isOptional": false, "description": "The feature dataset where the terrain dataset will be created. ", "dataType": "Feature Dataset"}, {"name": "out_terrain_name", "isOutputFile": true, "isOptional": false, "description": "The output terrain dataset. ", "dataType": "String"}, {"name": "average_point_spacing", "isOptional": false, "description": "The average, or nominal, horizontal distance between points for the data being used to build the terrain. Data collected for photogrammetric, lidar, and sonar surveys typically have a known spacing. This is the value that should be used. If you're unsure of the spacing you should go back and check the data rather than guess. The spacing is given in the horizontal units of the feature dataset's coordinate system. ", "dataType": "Double"}, {"name": "max_overview_size", "isOptional": true, "description": "The terrain overview is the coarsest representation of the terrain dataset, and is similar to the image thumbnail concept. The maximum size represents the upper limit of the number of measurement points sampled to create the overview. ", "dataType": "Long"}, {"name": "config_keyword", "isOptional": true, "description": "Configuration keyword for ArcSDE . A configuration keyword is used to optimize database storage and is typically configured by the database administrator. ", "dataType": "String"}, {"name": "pyramid_type", "isOptional": true, "description": "The point thinning method used to construct the terrain pyramids. WINDOWSIZE \u2014 Thinning is performed by selecting data points in the area defined by a given window size for each pyramid level using the criterion specified in the Window Size Method parameter. ZTOLERANCE \u2014 Thinning is performed by specifying the vertical accuracy of each pyramid level relative to the full resolution of the data points.", "dataType": "String"}, {"name": "windowsize_method", "isOptional": true, "description": "The criterion used for selecting points in the area defined by the window size. This parameter is only applicable when WINDOWSIZE is specified in the Pyramid Type parameter. ZMIN \u2014 The point with the smallest elevation value. ZMAX \u2014 The point with the largest elevation value. ZMEAN \u2014 The point with the elevation value closest to the average of all values. ZMINMAX \u2014 The points with the smallest and largest elevation values.", "dataType": "String"}, {"name": "secondary_thinning_method", "isOptional": true, "description": "Specifies additional thinning options to reduce the number of points used over flat areas when Window Size pyramids are being used. An area is considered flat if the heights of points in an area are within the value supplied for the Secondary Thinning Threshold parameter. Its effect is more evident at higher-resolution pyramid levels, since smaller areas are more likely to be flat than larger areas. NONE \u2014 No secondary thinning will be performed. This is the default. MILD \u2014 Works best to preserve linear discontinuities (for example, building sides and forest boundaries). It is recommended for lidar that includes both ground and nonground points. It will thin the fewest points. MODERATE \u2014 Provides a good trade-off between performance and accuracy. It does not preserve as much detail as mild thinning but comes nearly as close while eliminating more points overall. STRONG \u2014 Removes the most points but is less likely to preserve sharply delineated features. Its use should be limited to surfaces where slope tends to change gradually. For example, strong thinning would be efficient for bare-earth lidar and bathymetry. ", "dataType": "String"}, {"name": "secondary_thinning_threshold", "isOptional": true, "description": "The vertical threshold used to activate secondary thinning with the Window Size filter. The value should be set equal to or larger than the vertical accuracy of the data. ", "dataType": "Double"}]},
{"syntax": "Float_3d (in_raster_or_constant, out_raster)", "name": "Float (3D Analyst)", "description": "Converts each cell value of a raster into a floating-point representation.", "example": {"title": "Float example 1 (Python window)", "description": "This example converts the input raster values to floating point.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Float_3d ( \"elevation\" , \"C:/output/outfloat2\" )"}, "usage": ["The input values can be positive or negative.", "If you execute ", "Float", " on an input that is already floating point, the output values will remain the same as the input values."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster to be converted to floating point. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The cell values are the floating-point representation of the input values. ", "dataType": "Raster Dataset"}]},
{"syntax": "SurfaceSpot_3d (in_surface, in_feature_class, {out_spot_field}, {z_factor}, {method}, {pyramid_level_resolution})", "name": "Surface Spot (3D Analyst)", "description": "Calculates surface values for each point of a point feature class by interpolating from a raster, TIN, or terrain dataset surface. Calculated values are added to the input feature class attributes.", "example": {"title": "Surface Spot example 1 (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcgisscripting gp = arcgisscripting.create () gp.CheckOutExtension ( \"3D\" ) gp.workspace = \"C:/data\" gp.SurfaceSpot_3d ( \"elevation_tin\" , \"points.shp\" , \"SPOT\" , 3.28 )"}, "usage": ["Ensure the input surface and the input feature class have overlapping extents.", "Use the {z_factor} argument to convert the output field to the desired units."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input raster, TIN, or Terrain dataset surface to be used for interpolation. ", "dataType": "Terrain Layer; TIN Layer; Raster Layer"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The input point feature class to which the interpolated values will be added. ", "dataType": "Feature Layer"}, {"name": "out_spot_field", "isOutputFile": true, "isOptional": true, "description": "The name of the attribute field to be added to the input feature class. By default, the name is Spot. ", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The factor multiplied by the surface values to convert them to new values added to the input feature class. Used to convert z units to match x,y units. ", "dataType": "Double"}, {"name": "method", "isOptional": true, "description": "Algorithm used to calculate surface values. ", "dataType": "String"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The resolution of the terrain dataset pyramid level to use for geoprocessing. The default is 0, full resolution. ", "dataType": "Double"}]},
{"syntax": "AddSurfaceInformation_3d (in_feature_class, in_surface, out_property, {method}, {sample_distance}, {z_factor}, {pyramid_level_resolution}, {noise_filtering})", "name": "Add Surface Information (3D Analyst)", "description": "Adds surface elevation information to the attribute table of point, line, or polygon features.", "example": {"title": "AddSurfaceInformation example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.AddSurfaceInformation_3d ( \"points.shp\" , \"my_tin\" , \"Z\" , \"LINEAR\" )"}, "usage": ["All features are treated as two-dimensional; Z values from 3D feature classes are ignored. ", " The selected ", "Output Property", " options get written to the attribute table of the input feature class. Each feature defines the location of the surface properties being assessed, and the type of property that can be reported depends on the feature's geometry:", "Feature Geometry", "Surface Properties", "Point", "Spot elevation interpolated from the point's XY coordinate on the surface.", "Multipoint", "Minimum, maximum, and mean of the spot elevation derived for all points in the multipoint record.", "Polyline", "3D distance of the line along the surface. ", "Minimum, maximum, and mean of the elevation and slope obtained from the line's path on the surface.", "Polygon", "3D area of the surface portions defined by the polygon.", "Minimum, maximum, and mean of the elevation and slope from the surface.", "Slope values are measured in percentage units (grade) and, for line features, get calculated at each segment along the line."], "parameters": [{"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The point, multipoint, polyline, or polygon features that define the locations for determining one or more surface properties. ", "dataType": "Feature Layer"}, {"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The LAS dataset, raster, terrain, or TIN surface used for interpolating z-values. ", "dataType": "LAS Dataset Layer; Raster Layer; Terrain Layer; TIN Layer"}, {"name": "out_property", "isOutputFile": true, "isOptional": false, "description": "The surface elevation property that will be added to the attribute table of the input feature class. The following list summarizes the available property keywords and their supported geometry types: Z \u2014 The interpolated elevation value for single-point features. Z_MIN \u2014 The lowest interpolated elevation value in the multipoint record, along the polyline, or within the area defined by the polygon. Z_MAX \u2014 The highest interpolated elevation value in the multipoint record, along the polyline, or within the area defined by the polygon. Z_MEAN \u2014 The average interpolated elevation value in the multipoint record, along the polyline, or within the area defined by the polygon. SURFACE_AREA \u2014 The 3D area of the polygon when draped over the input surface. SURFACE_LENGTH \u2014 The 3D length of the polyline feature when draped along the input surface. MIN_SLOPE \u2014 The slope value closest to zero along the polyline or within the area defined by the polygon. MAX_SLOPE \u2014 The highest interpolated slope value along the polyline or within the area defined by the polygon. AVG_SLOPE \u2014 The average of the interpolated slope values along the polyline or within the area defined by the polygon.", "dataType": "String"}, {"name": "method", "isOptional": true, "description": " Interpolation method to be used in determining Z values for input features. Available options are: LINEAR \u2014 Default interpolation method. Estimates z from the plane defined by the TIN or terrain triangle that contains the XY location of a query point. NATURAL_NEIGHBORS \u2014 Estimates z by applying area-based weights to the TIN or terrain's natural neighbors of a query point. CONFLATE_ZMIN \u2014 Obtains z from one of the TIN or terrain's natural neighbors of a query point. The z of the neighbor with the minimum height is used. CONFLATE_ZMAX \u2014 Obtains z from one of the TIN or terrain's natural neighbors of a query point. The z of the neighbor with the maximum height is used. CONFLATE_NEAREST \u2014 Obtains z from one of the TIN or terrain's natural neighbors of a query point. The z of the neighbor closest in XY to the query point is used. CONFLATE_CLOSEST_TO_MEAN \u2014 Obtains z from one of the TIN or terrain's natural neighbors of a query point. The z of the neighbor which is closest to the average height of all the neighbors is used.", "dataType": "String"}, {"name": "sample_distance", "isOptional": true, "description": "The spacing at which z-values will be interpolated. By default, the raster cell size is used when the input surface is a raster, and the natural densification of the triangulated surface is used when the input is a terrain or TIN dataset. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}, {"name": "noise_filtering", "isOptional": true, "description": " Provides the option to exclude small line segments that exist after the line is densifiied from interpolation or small surface triangles within the polygon areas from contributing to the slope calculations. This option is useful for obtaining sound estimates, as small portions often exhibit extreme slopes which may bias the statistical results. The values given in either the Area or Length options will be used to exclude these portions of features and are evaluated in the linear units of the input feature's coordinate system. This parameter does not apply to point and multipoint features. NO_FILTER \u2014 No noise filter will be used and all line segments or surface triangles will be factored into slope calculations. This is the default. AREA <\u2026> \u2014 An area filter will be applied to surface triangles within the areas defined by the polygon features. An AREA value of 2.5 indicates that surface triangles whose area measures less than 2.5 will be ignored from contributing to slope calculations. LENGTH <\u2026> \u2014 A length filter will be applied to line segments that are created after interpolating the input line feature on the surface. A LENGTH value of 2.5 indicates that segments whose length is less than 2.5 will be ignored from contributing to slope calculations.", "dataType": "String"}]},
{"syntax": "LocateOutliers_3d (in_surface, out_feature_class, {apply_hard_limit}, {absolute_z_min}, {absolute_z_max}, {apply_comparison_filter}, {z_tolerance}, {slope_tolerance}, {exceed_tolerance_ratio}, {outlier_cap})", "name": "Locate Outliers (3D Analyst)", "description": "Identifies anomalous elevation measurements from terrain, TIN, or LAS datasets that are beyond a defined range of elevation values or are inconsistent with the surrounding surface.", "example": {"title": "LocateOutliers example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.LocateOutliers_3d ( \"tin\" , \"outliers.shp\" , \"NO_APPLY_HARD_LIMIT\" , 0 , 0 , \"APPLY_COMPARISON_FILTER\" , 0 , 150 , 0.5 , 2500 )"}, "usage": ["Outliers represent elevation measurements which might be extremely inconsistent with the surrounding surface or produce a high slope variance in their localized sector.  ", "Consider using the ", "Apply Hard Limit", " option when the range of valid elevation values for the region represented by the surface are known. Points which fall outside of the range specified in   ", "Absolute Z Minimum", " and ", "Absolute Z Maximum", " will be created in the output.", "To eliminate the outlier points from a terrain dataset, consider using the ", "Delete Terrain Points", " tool with the outlier points specified in the ", "Area of Interest", " parameter.", "Consider reclassifying outlier points identified in a LAS dataset as noise by using the ", "Set LAS Class Codes Using Features", " tool."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The terrain, TIN, or LAS dataset that will be analyzed. ", "dataType": "LAS Dataset Layer; Terrain Layer; TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "apply_hard_limit", "isOptional": true, "description": "Specifies use of absolute Z minimum and maximum to find outliers. APPLY_HARD_LIMIT \u2014 Use the absolute Z minimum and maximum to find outliers. NO_APPLY_HARD_LIMIT \u2014 Do not use the absolute Z minimum and maximum to find outliers. This is the default.", "dataType": "Boolean"}, {"name": "absolute_z_min", "isOptional": true, "description": " If hard limits are applied, then any point with an elevation below this value will be considered as being an outlier. The default is 0. ", "dataType": "Double"}, {"name": "absolute_z_max", "isOptional": true, "description": " If hard limits are applied, then any point with an elevation above this value will be considered as being an outlier. The default is 0. ", "dataType": "Double"}, {"name": "apply_comparison_filter", "isOptional": true, "description": " The comparison filter consists of three parameters for determining outliers ( z_tolerance , slope_tolerance , and exceed_tolerance_ratio ). APPLY_COMPARISON_FILTER \u2014 Use the three comparison parameters (Z tolerance, slope tolerance, and exceed tolerance ratio) in assessing points. This is the default. NO_APPLY_COMPARISON_FILTER \u2014 Do not use the three comparison parameters (Z tolerance, slope tolerance, and exceed tolerance ratio) in assessing points.", "dataType": "Boolean"}, {"name": "z_tolerance", "isOptional": true, "description": " Used to compare Z values of neighboring points if the comparison filter is applied. The default is 0. ", "dataType": "Double"}, {"name": "slope_tolerance", "isOptional": true, "description": " The threshold of slope variance between consecutive points that would be used to identify outlier points. Slope is expressed as a percentage, with the default being 150. ", "dataType": "Double"}, {"name": "exceed_tolerance_ratio", "isOptional": true, "description": " The maximum tolerance. The default is 0.5. ", "dataType": "Double"}, {"name": "outlier_cap", "isOptional": true, "description": " The maximum number of outlier points that can be written to the output. Once this value is reached, no further outliers are sought. The default is 2,500. ", "dataType": "Long"}]},
{"syntax": "AddFeatureClassToTerrain_3d (in_terrain, in_features)", "name": "Add Feature Class To Terrain (3D Analyst)", "description": "Adds one or more feature classes to a terrain dataset.", "example": {"title": "AddFeatureClassToTerrain example 1 (Python window)", "description": "The following sample demonstrates how to use this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" params = \"terrain.gdb/terrainFDS/points2 SHAPE masspoints 2 0 10 true false \" \"points_embed <None> false\" arcpy.AddFeatureClassToTerrain_3d ( \"test.gdb/featuredataset/terrain\" , params )"}, "usage": ["Input feature classes must reside in the same feature dataset as the terrain dataset.", "The terrain dataset must already have one or more pyramid levels created.", "Depending on the surface type associated with features added to a terrain, the terrain dataset may need to be rebuilt using ", "Build Terrain", ". The terrain dataset's ", "Properties", " dialog box in ArcCatalog and the terrain layer's ", " Properties", " dialog box in ArcMap both provide an indication as to whether the dataset needs to be rebuilt.", "For terrain datasets stored in SDE:"], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The terrain to which feature classes will be added. The terrain dataset must already have one or more pyramid levels created. ", "dataType": "Terrain Layer"}, {"name": "in_features", "isInputFile": true, "isOptional": false, "description": "The feature classes being added to the terrain. Each feature class must reside in the same feature dataset as the terrain, and properties that define the feature's role in the terrain must be specified. The dialog box can be resized to increase its width in the event the default column width is too small. Feature class properties: in_feature_class\u2014The name of the input feature class being added to the terrain dataset. height_source\u2014The field supplying height values for the features. All numeric fields will be listed as available options. If the feature class is z-enabled, the feature's geometry field is included in the list and selected as the default value. If the feature class is not z-enabled, the keyword <None> will be provided by default. Z-less features have no heights and their values are interpolated from the surface before they get added. SF_type\u2014This represents the Surface Feature Type. It defines how the features are treated when triangulating the surface. The options include mass points, breaklines, and several polygon types. Breaklines and polygons also have hard and soft qualifiers, which indicate to an interpolator whether the surface crosses over the features smoothly (soft) or with a potentially sharp discontinuity (hard). group\u2014Thematically similar data, representing the same geographic features, but at different levels of detail, are placed into groups. Feature classes belonging to the same group are assigned the same value. For example, if there are two study area boundary features where one contains a very detailed boundary used for large-scale applications and another with a coarse boundary, both can be assigned to the same group to ensure there is no overlap in their respective display scales. This parameter only applies to breaklines and polygon surface types. min_resolution and max_resolution\u2014Defines the pyramid resolution ranges that the features will be enforced at in the terrain dataset. These parameters only apply to breaklines and polygon surface types. overview\u2014Indicates whether the feature class is enforced at the overview display of the terrain dataset, which is the coarsest representation that gets drawn by default when viewed at full extent. To maximize display performance, make sure that feature classes represented in the overview contain simplified geometry. For example, breaklines may not be visible enough to matter at an overview display, whereas a clip polygon would be useful. If the boundary feature you have is detailed, consider generalizing it and use the coarser representation for the overview. The detailed version should be used in more detailed pyramid resolution levels. This parameter only applies to breaklines and polygon surface types. embedded\u2014Indicates whether the feature class will be embedded in the terrain dataset. Setting this option to true creates a copy of the feature's points. The embedded feature class will only be accessible through terrain dataset-related tools, but it will not be directly visible in ArcCatalog or selectable through the Add Data browser. This option only applies to multipoint feature classes. embedded_fields\u2014Identifies the lidar attributes to be stored with the embedded feature class. The LAS To Multipoint tool allows the storage of lidar attributes in a multipoint feature class. anchored\u2014Specifies whether the point feature class will be anchored through all terrain pyramid levels. Anchor points are never filtered or thinned away to ensure they persist in the terrain surface. This option only applies to single-point feature classes. ", "dataType": "Value Table"}]},
{"syntax": "TinContour_3d (in_tin, out_feature_class, interval, {base_contour}, {contour_field}, {contour_field_precision}, {index_interval}, {index_interval_field}, {z_factor})", "name": "TIN Contour (3D Analyst)", "description": "Creates a feature class containing a set of contours generated from a functional surface. The output feature class is 2D and contains an attribute with contour values.", "example": {"title": "TIN Contour example (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "Missing source code file"}, "usage": ["Use the interval and base contour options to tailor the extent and resolution of the output feature class.", "Use the out contour field data to convert the feature class to 3D.", "In certain instances the last valid contour line may not be produced when creating contours using TIN surfaces. This is an algorithmic limitation common to computer contouring software. To ensure that all valid contours are generated, add a very small negative value to the base contour field to slightly shift the data."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The surface from which the contours will be interpolated. ", "dataType": "Tin Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "interval", "isOptional": false, "description": "The interval between the contours. ", "dataType": "Double"}, {"name": "base_contour", "isOptional": true, "description": "Along with the index interval, the base height is used to determine what contours are produced. The base height is a starting point from which the index interval is either added or subtracted. By default, the base contour is 0.0. ", "dataType": "Double"}, {"name": "contour_field", "isOptional": true, "description": "The field containing contour values. ", "dataType": "String"}, {"name": "contour_field_precision", "isOptional": true, "description": "The precision of the contour field. Zero specifies an integer, and the numbers 1\u20139 indicate how many decimal places the field will contain. By default, the field will be an integer (0). ", "dataType": "Long"}, {"name": "index_interval", "isOptional": true, "description": "The difference, in Z units, between index contours. The value specified should be evenly divisible by the contour interval. Typically, it\u2019s five times greater. Use of this parameter adds an attribute field to the output feature class that\u2019s used to differentiate index contours from regular contours. ", "dataType": "Double"}, {"name": "index_interval_field", "isOptional": true, "description": "The name of the field used to record whether a contour is a regular or an index contour. By default, the value is \u2018Index\u2019. ", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "Specifies a factor by which to multiply the surface heights. Used to convert z units to x and y units. ", "dataType": "Double"}]},
{"syntax": "AddTerrainPoints_3d (in_terrain, terrain_feature_class, in_feature_class, {method})", "name": "Add Terrain Points (3D Analyst)", "description": "Adds multipoints to an embedded terrain feature class by appending them or by using them to replace existing multipoints that fall within the same extent.", "example": {"title": "Add Terrain Points example (Python window)", "description": "The following sample demonstrates how to use this tool in the Python window:", "code": "import arcgisscripting gp = arcgisscripting.create () gp.CheckOutExtension ( \"3D\" ) gp.workspace = \"C:/data\" gp.AddTerrainPoints_3d ( \"sample.gdb/featuredataset/terrain\" , \"mass_points\" , \"new_points.shp\" , \"APPEND\" )"}, "usage": ["This tool adds multipoints to a feature class embedded in a terrain. The terrain and the embedded feature class must already exist.", "Only multipoints can be embedded in a terrain. Therefore, the input feature class must be of type multipoint.", "This tool performs two types of operations: APPEND and REPLACE. Use the method argument to indicate which mode of operation you want to use.", "Either operation uses a rectangular Area Of Interest (AOI). By default, the AOI is equal to the extent of the input feature class. The Geoprocessing Environment can be used to define a specific extent if a subset of the input feature class is desired. In the case of APPEND, the vertices of the input feature class that fall within the AOI will be added. REPLACE works in two phases. First, vertices of the embedded feature class that fall within the AOI are deleted. Then, those within the AOI of the source feature class are added. If a multipoint crosses the AOI boundary only those vertices within the boundary are operated on.", "If you want to add points to a regular feature class, potentially referenced by a terrain but not embedded in it, use the Append tool.", "Add Terrain Points looks for an existing edit session to piggy-back on. This supports undo if the edit session was initialized to support undo (e.g., using the Editor inside ArcMap\u2122). If there's no edit session, it starts and stops one itself, in which case there is no chance for an undo.", "In order to use this tool on a terrain inside an SDE\u2122 database, the terrain needs to be registered as versioned. You do this by registering the feature dataset the terrain lives in as versioned.", "Adding points to an embedded feature class will invalidate the terrain. Run ", "Build Terrain", " after adding points."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The terrain dataset to be modified. ", "dataType": "Terrain Layer"}, {"name": "terrain_feature_class", "isOptional": false, "description": "The embedded feature class to which points will be added. ", "dataType": "String"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The feature class containing the points to be added. ", "dataType": "Feature Layer"}, {"name": "method", "isOptional": true, "description": "Indicates whether points are appended to the target or used to replace those in the target. ", "dataType": "String"}]},
{"syntax": "Union3D_3d (in_feature_class, {group_field}, out_feature_class, {out_table}, {disable_optimization}, {output_all})", "name": "Union 3D (3D Analyst)", "description": "Merges closed, overlapping multipatch features from an input feature class.", "example": {"title": "Union3D example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.Union3D_3d ( 'multipatch.shp' , 'union_output.shp' , 'GROUP_FIELD' , 'DISABLE' , 'ENABLE' , 'UnionTable.dbf' )"}, "usage": ["Closed multipatch geometry is required for this analysis. The ", "Is Closed 3D", " tool can be used to determine if a multipatch feature class contains closed features, and the ", "Enclose Multipatch", " tool can be used to eliminate gaps in multipatch features.", "Multipatch features encompassing overlapping volumes are combined by intersecting the triangles and rings that constitute the shells of the features and removing redundant interior portions. This process occurs iteratively until all multipatch features in the feature class have been processed.\r\n", " A grouping field can be used to identify features to be unioned, such as when the multipatches represent building parts in a city where multiple features represent one building. This can significantly improve performance by reducing the number of times the tool must iterate through the dataset. Rather than comparing a feature against all features, it is only compared against those that participate in its group.", "When optimization is enabled, the tool attempts to automatically subset the\r\nfeatures into groups by analyzing the bounding box for each feature. Disabling optimization can increase the tool's performance if a grouping field has been specified.  Optimization can also be disabled in the absence of a grouping field if the desired output is to union all overlapping features into a single  multipatch.\r\n ", "Use caution when deciding how many features to aggregate, as very large and  complex features may be created in the output feature class  which may exhibit poor display performance. ", "A warning stating that the resulting feature is not simple and could not be created is raised if two or more multipatch features share only an edge or a vertex.  The same message is returned if a group contains multipatches that share no volume or space.", "An optional table can be created to identify the attributes of the source features that were unioned to create each unioned output. ", "A relationship between the table and the output feature class can be established to query the attributes associated with the source features. Please see ", "Relating the attributes in one table to another", " for more information.", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them."], "parameters": [{"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": " The closed multipatch features to be intersected and aggregated. ", "dataType": "Feature Layer"}, {"name": "group_field", "isOptional": true, "description": " The field used to identify the features that should be grouped together. ", "dataType": "Field"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output multipatch feature class that will store the aggregated features. ", "dataType": "Feature Class"}, {"name": "out_table", "isOutputFile": true, "isOptional": true, "description": " A many-to-one table representing the relationship between input features and their aggregated counterparts. ", "dataType": "Table"}, {"name": "disable_optimization", "isOptional": true, "description": "Specifies whether optimization will automatically take place or is disabled: ENABLED \u2014 Optimization is performed on the input data. The tool will do some preprocessing to detect which features may overlap, grouping them together to improve performance and create unique outputs for each set of overlapping features. This is the default. DISABLED \u2014 No optimization is performed on the input data. Feature will be union according to their grouping field, or all features will be unioned into a single output feature.", "dataType": "Boolean"}, {"name": "output_all", "isOutputFile": true, "isOptional": true, "description": "Determines if the output feature class contains all features or only the overlapping ones that were unioned. DISABLED \u2014 Only unioned features are written to the output. ENABLED \u2014 All input features are written to the output. This is the default.", "dataType": "Boolean"}]},
{"syntax": "RasterDomain_3d (in_raster, out_feature_class, out_geometry_type)", "name": "Raster Domain (3D Analyst)", "description": "Creates a polygon or polyline footprint of the data portions of a raster dataset.", "example": {"title": "RasterDomain example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.RasterDomain_3d ( \"dtm_grd\" , \"raster_domain.shp\" , \"POLYGON\" )"}, "usage": ["Single-band or multiband rasters can be provided as input.", "The domain feature will extend to the center of the perimeter cells in the contiguous data blocks of the raster.  The cell center defines  interpolation zone of a raster surface. NoData cells are ignored in the output feature.", "Output geometry is placed in one feature record and may comprise of multipart feature if the raster has discontinuous data cells separated by NoData cells.", "3D polygons only contain elevation values along the perimeter of the features, as interior portions of the polygon will not contain any vertices. When drawn in 3D with an area fill, the boundary vertices are arbitrarily connected into triangles for rendering. Unless the polygon is planar, either sloped or horizontal, it's unlikely the fill will accurately represent the interior surface. For this reason, it is recommended that nonplanar 3D polygons are  drawn without fill symbology."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster. ", "dataType": "Raster Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "out_geometry_type", "isOutputFile": true, "isOptional": false, "description": "The geometry of the output feature class. LINE \u2014 The output will be a z-enabled line feature class. POLYGON \u2014 The output will be a z-enabled polygon feature class.", "dataType": "String"}]},
{"syntax": "ChangeTerrainResolutionBounds_3d (in_terrain, feature_class, {lower_pyramid_resolution}, {upper_pyramid_resolution}, {overview})", "name": "Change Terrain Resolution Bounds (3D Analyst)", "description": "Changes the pyramid levels at which a feature class will be enforced for a given terrain dataset.", "example": {"title": "ChangeTerrainResolutionBounds example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.ChangeTerrainResolutionBounds_3d ( 'sample.gdb/featuredataset/terrain' , 'breaklines' , 2.5 , 7.5 )"}, "usage": ["Pyramid resolution bounds can only be assigned for surface types that are not designated as ", "masspoints", ".", "Consider adjusting the resolution bounds for a terrain feature when new pyramids are introduced or its current range provides  undesirable display performance.", "The terrain dataset will not be invalidated from this operation and will not need to be rebuilt with the ", "Build Terrain", " tool."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "feature_class", "isOptional": false, "description": "The feature class referenced by the terrain that will have its pyramid-level resolutions modified. ", "dataType": "String"}, {"name": "lower_pyramid_resolution", "isOptional": true, "description": "The new lower pyramid-level resolution for the chosen feature class. ", "dataType": "Double"}, {"name": "upper_pyramid_resolution", "isOptional": true, "description": "The new upper pyramid-level resolution for the chosen feature class. ", "dataType": "Double"}, {"name": "overview", "isOptional": true, "description": "Specifies whether the feature class will contribute to the overview of the terrain dataset. OVERVIEW \u2014 Enforces the feature class at the overview display of the terrain dataset. This is the default. NO_OVERVIEW \u2014 Omits the feature class from the overview display of the terrain dataset.", "dataType": "Boolean"}]},
{"syntax": "Reclassify_3d (in_raster, reclass_field, remap, out_raster, {missing_values})", "name": "Reclassify (3D Analyst)", "description": "Reclassifies (or changes) the values in a raster.", "example": {"title": "Reclassify example 1 (Python window)", "description": "The following example shows how to reclassify a raster into seven classes.", "code": "import arcpy from arcpy import env env.workspace = \"C:/sapyexamples/data\" arcpy.Reclassify_3d ( \"C:/data/landuse\" , \"VALUE\" , \"1 9;2 8;3 1;4 6;5 3;6 2;7 1\" , \"C:/output/outremap\" , \"DATA\" )"}, "usage": ["The input raster must have valid statistics. If the statistics do\r\nnot exist, they can be created using the ", "Calculate Statistics", " tool in the Data Management Tools toolbox.", "If using the tool dialog box, the remap table can be stored for future use with the ", "Save", " button. Use the ", "Load", " button to open the remap tables you created previously with the ", "Save", " button.", "It is recommended to only load  tables previously saved by the ", "Reclassify", " tool.      The table format is specific and must contain the fields FROM, TO, OUT, and MAPPING.", "By default, the input raster will be classified into nine classes for the reclassification table.", "If the input raster is a layer, the old values of the reclassification will be obtained from the renderer. If the renderer is stretched, the reclassification will default to 255 classes.", "Once the remap table of the reclassification has been modified,  the values will not be updated if a new input raster is selected. If the reclassification is not suitable for the new raster, a default reclassification can be reinitialized by:", "This tool has a precision control that manages how decimal places are treated.", "When using the ", "Reclassify", " tool as part of a model:"], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "reclass_field", "isOptional": false, "description": "Field denoting the values that will be reclassified. ", "dataType": "Field"}, {"name": "remap", "isOptional": false, "description": "A remap list that defines how the values will be reclassified. The remap list is composed of three components: From, To, and New values. Each row in the remap list is separated by a semicolon, and the three components are separated by spaces. For example: \" 0 5 1;5.01 7.5 2;7.5 10 3 \" ", "dataType": "Remap"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output reclassified raster. The output will always be of integer type. ", "dataType": "Raster Dataset"}, {"name": "missing_values", "isOptional": true, "description": "Denotes whether missing values in the reclass table retain their value or get mapped to NoData. DATA \u2014 Signifies that if any cell location on the input raster contains a value that is not present or reclassed in a remap table, the value should remain intact and be written for that location to the output raster. This is the default. NODATA \u2014 Signifies that if any cell location on the input raster contains a value that is not present or reclassed in a remap table, the value will be reclassed to NoData for that location on the output raster.", "dataType": "Boolean"}]},
{"syntax": "Intersect3D_3d (in_feature_class_1, {in_feature_class_2}, out_feature_class, {output_geometry_type})", "name": "Intersect 3D (3D Analyst)", "description": " Computes the geometric intersection of closed multipatch features to produce multipatches from the intersecting volumes, polygons from the intersecting planes, or lines from the intersecting edges.", "example": {"title": "Intersect3D example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = 'C:/data' arcpy.Intersect3D_3d ( 'inMultipatch1.shp' , 'outMultipatch.shp' , 'inMultipatch2.shp' )"}, "usage": ["Closed multipatch geometry is required for this analysis. The ", "Is Closed 3D", " tool can be used to determine if a multipatch feature class contains closed features, and the ", "Enclose Multipatch", " tool can be used to eliminate gaps in multipatch features.", "Exercise caution when determining the data used for this analysis. Highly detailed features may produce extremely complex geometries that could exhibit display performance issues on account of their total number of vertices and orientation.", "If one input is given, the intersection of features in that multipatch dataset will be computed, whereas if two were given, the intersection of features from both datasets will be determined and intersections found in only one input get ignored.", "When using two input features, attributes from both features get concatenated in the output. ", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them."], "parameters": [{"name": "in_feature_class_1", "isInputFile": true, "isOptional": false, "description": " The multipatch features that will be intersected. When only one input feature ", "dataType": "Feature Layer"}, {"name": "in_feature_class_2", "isInputFile": true, "isOptional": true, "description": " The optional second multipatch feature class to be intersected with the first. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "output_geometry_type", "isOutputFile": true, "isOptional": true, "description": " Determines the type of intersection geometry created. SOLID \u2014 Creates a closed multipatch representing the overlapping volumes between input features. This is the default. SURFACE \u2014 Creates a multipatch surface representing shared faces between input features. POLYLINE \u2014 Creates a polyline representing shared edges between input features.", "dataType": "String"}]},
{"syntax": "RemoveTerrainPoints_3d (in_terrain, data_source, aoi_extents)", "name": "Remove Terrain Points (3D Analyst)", "description": "This tool removes points within an area of interest from one or more embedded feature classes.", "example": {"title": "Remove Terrain Points example (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcgisscripting gp = arcgisscripting.create () gp.CheckOutExtension ( \"3D\" ) gp.workspace = \"C:/data\" extent = \"6442927 2053828 6452927 2065858\" gp.RemoveTerrainPoints_3d ( \"sample.gdb/featuredataset/terrain\" , \"napa_points\" , extent )"}, "usage": ["This tool functions only on feature classes embedded in a terrain. The terrain and at least one embedded feature class must already exist.", "A rectangular area of interest (AOI) is required. This defines the area in the embedded feature class where multipoint vertices will be deleted. If a multipoint crosses the AOI boundary, only those vertices within the boundary will be deleted.", "Remove Terrain Points looks for an existing edit session to piggyback on. This supports undo if the edit session was initialized to support undo (for example, using Editor inside ArcMap). If there's no edit session, it starts and stops one itself, in which case there is no chance for an undo.", "To use this tool on a terrain inside an ArcSDE database, the terrain needs to be registered as versioned. You do this by registering the feature dataset the terrain resides in as versioned.", "Removing points from an embedded feature class will invalidate the terrain. Run ", "Build Terrain", " after adding points."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The terrain dataset to be modified ", "dataType": "Terrain layer"}, {"name": "data_source", "isOptional": false, "description": "One or more embedded feature classes from which points will be removed ", "dataType": "String"}, {"name": "aoi_extents", "isOptional": false, "description": "The XY extent defining the area from which points will be removed ", "dataType": "Extent"}]},
{"syntax": "TinRaster_3d (in_tin, out_raster, {data_type}, {method}, {sample_distance}, {z_factor})", "name": "TIN To Raster (3D Analyst)", "description": "Creates a raster by interpolating its cell values from the elevation of the input TIN at the specified sampling distance. Learn more about how TIN To Raster works", "example": {"title": "TinRaster example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.TinRaster_3d ( \"tin\" , \"raster.img\" , \"INT\" , \"LINEAR\" , \"OBSERVATIONS 250\" , 1 )"}, "usage": ["Because interpolation of the input TIN surface occurs at regular intervals, some loss of information in the output raster should be expected. How well the raster represents the TIN is dependent on the resolution of the raster and the degree and interval of TIN surface variation. Generally, as the resolution is increased, the output raster more closely represents the TIN surface. Because the raster is a cell structure, it cannot maintain the hard and soft breakline edges that may be present in the TIN.", "You can accept the TIN's XMIN, YMIN as the default origin and the XMAX, YMAX as the upper right corner of the extent to generate a raster covering the full extent of the TIN. Alternatively, you can specify the output raster's origin, extent, and resolution or cell spacing to create a raster covering only a portion of the TIN.", "When exporting a large raster, consider specifying the ", "Output Data Type", " as an integer to save on disk space if the accuracy requirements of your z-values are such that they can be represented by integer data.", "The output raster may be file based or created as a raster dataset in a geodatabase. Supported file-based formats include ", "Esri", " Grid, ERDAS IMAGINE, and TIFF. The format is determined based on the output name. If the output path references a geodatabase, a geodatabase raster will be created. If the output resides in a normal file folder and does not include a file extension, an ", "Esri", " Grid will be produced. Use of the extensions .img and .tif will produce IMAGINE or TIFF files."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The location and name of the output raster. When storing a raster dataset in a geodatabase or in a folder such as an Esri Grid, no file extension should be added to the name of the raster dataset. A file extension can be provided to define the raster's format when storing it in a folder: If the raster is stored as a TIFF file or in a geodatabase, its raster compression type and quality can be specified using geoprocessing environment settings. .bil\u2014Esri BIL .bip\u2014Esri BIP .bsq\u2014Esri BSQ .dat\u2014ENVI DAT .img\u2014ERDAS IMAGINE .png\u2014PNG .tif\u2014TIFF", "dataType": "Raster Dataset"}, {"name": "data_type", "isOptional": true, "description": "The data type of the output raster can be defined by the following keywords: FLOAT \u2014 Output raster will use 32-bit floating point, which supports values ranging from -3.402823466e+38 to 3.402823466e+38. This is the default. INT \u2014 Output raster will use an appropriate integer bit depth. This option will round Z-values to the nearest whole number and write an integer to each raster cell value.", "dataType": "String"}, {"name": "method", "isOptional": true, "description": "The interpolation method used to create the raster. LINEAR \u2014 Calculates cell values by applying linear interpolation to the TIN triangles. This is the default. NATURAL_NEIGHBORS \u2014 Calculates cell values by using natural neighbors interpolation of TIN triangles", "dataType": "String"}, {"name": "sample_distance", "isOptional": false, "description": "The sampling method and distance used to define the cell size of the output raster. OBSERVATIONS \u2014Defines the number of cells on the longest side of the output raster. This method is used by default with a distance of 250. CELLSIZE \u2014Defines the cell size of the output raster.", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}]},
{"syntax": "SurfaceSlope_3d (in_surface, out_feature_class, {units}, {class_breaks_table}, {slope_field}, {z_factor}, {pyramid_level_resolution})", "name": "Surface Slope (3D Analyst)", "description": "Creates polygon features from the triangle slope values of a TIN or terrain dataset. \r\n Learn more about how Surface Slope works \r\n", "example": {"title": "SurfaceSlope example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.SurfaceSlope_3d ( \"sample.gdb/featuredataset/terrain\" , \"s_slope.shp\" , \"PERCENT\" )"}, "usage": ["Use the ", "Class Breaks Table", " parameter to constrain slope information into specific break intervals of the output feature class.", "Customized slope classifications can be provided through a ", "Class Breaks Table", " consisting of up to two columns. The first column will always indicate the classification break value. If the second column is available, it will be used to define the code that corresponds with the slope class. Consider the following example:", "CLASS_BREAK", "CODE", "10.0", "1", "25.0", "2", "40.0", "3", "70.0", "4", "The table can be in any supported format (", ".dbf", ", ", ".txt", ", or geodatabase table). The name of the fields are irrelevant, as the first will always be used for the class breaks and the second for the aspect codes.", "Units are only honored when using a class breaks table."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input terrain or TIN dataset. ", "dataType": "Tin Layer; Terrain Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "units", "isOptional": true, "description": "The units of measure to be used in calculating slope. PERCENT \u2014 Slope is expressed as a percentage value. This is the default. DEGREE \u2014 Slope is expressed as the angle of inclination from a horizontal plane.", "dataType": "String"}, {"name": "class_breaks_table", "isOptional": true, "description": "A table containing the classification breaks that will be used to classify the output features. The first column of this table will indicate the break point, whereas the second will provide the classification code. ", "dataType": "Table"}, {"name": "slope_field", "isOptional": true, "description": "The field containing slope values. ", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "Inside3D_3d (in_target_feature_class, in_container_feature_class, out_table, {complex_output})", "name": "Inside 3D (3D Analyst)", "description": " Determines if 3D features from an input feature class are contained inside a closed multipatch, and writes an output table recording the features that are partially or fully inside the multipatch.", "example": {"title": "Inside3D example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.Inside3D_3d ( 'inFeature.shp' , 'sample.gdb/multipatch' , 'sample.gdb/output_table' )"}, "usage": ["All input features must have Z information stored as part of their geometry. If field based height measurements are present in a 2D feature class, the ", "Feature To 3D By Attribute", " tool can be used to create a 3D feature class.", "Closed multipatch geometry is required for this analysis. The ", "Is Closed 3D", " tool can be used to determine if a multipatch feature class contains closed features, and the ", "Enclose Multipatch", " tool can be used to eliminate gaps in multipatch features.", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them.", "If the ", "Complex Output Table", " option is selected, a record is created for each multipatch that a given input feature intersects.  A feature may fall within multiple closed multipatch features and have multiple entries in the output table. ", "The following fields are present in the output table:"], "parameters": [{"name": "in_target_feature_class", "isInputFile": true, "isOptional": false, "description": "The input multipatch or 3D point, line, or polygon feature class. ", "dataType": "Feature Layer"}, {"name": "in_container_feature_class", "isInputFile": true, "isOptional": false, "description": "The closed multipatch features that will be used as the containers for the input features. ", "dataType": "Feature Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output table providing a list of 3D Input Features that are inside or partially inside the Input Multipatch Features which are closed. The output table contains an OBJECTID (object ID), Target_ID, and Status. The Status will state if the input feature (Target_ID) is inside or partially inside a multipatch. ", "dataType": "Table"}, {"name": "complex_output", "isOptional": true, "description": "Specifies if the output table will identify the relationship between the Input Features and the Input Multipatch Features through the creation of a Contain_ID field that identifies the multipatch feature that contains the input feature. Specifies if the output table will identify the relationship between the Input Features and the Input Multipatch Features through the creation of a Contain_ID field that identifies the multipatch feature that contains the input feature. Checked\u2014The multipatch feature that contains an input feature will be identified. Unchecked\u2014The multipatch feature that contains an input feature will not be identified. This is the default. COMPLEX \u2014 The multipatch feature that contains an input feature will be identified. SIMPLE \u2014 The multipatch feature that contains an input feature will not be identified. This is the default.", "dataType": "Boolean"}]},
{"syntax": "ContourList_3d (in_raster, out_polyline_features, contour_values)", "name": "Contour List (3D Analyst)", "description": "Creates a feature class of selected contour values from a raster surface. Learn more about how Contouring works", "example": {"title": "ContourList example 1 (Python window)", "description": "This example creates contours for three elevation values from an Esri Grid raster and outputs them as a shapefile.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.ContourList_3d ( \"elevation\" , \"C:/sapyexamples/output/outcontourlist.shp\" , \"600; 935; 1237.4\" )"}, "usage": ["Contours do not extend beyond the spatial extent of the raster, and they are not generated in areas of NoData; therefore, adjacent contour inputs should first be edgematched into a continuous feature dataset. As an alternative to edgematching, you can merge the adjacent rasters before computing contours.", "Contours can be generated in areas of negative raster values. The contour values will be negative in such areas. Negative contour intervals are not allowed.", "The contour values do not need to be sorted in order.", "If you have the ", "ArcGIS Spatial Analyst extension", " available, smoother but less accurate contours can be obtained by preprocessing the  input raster with a ", "Focal_Statistics", " operation with the MEAN option or the ", "Filter", " tool with the LOW option."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_polyline_features", "isOutputFile": true, "isOptional": false, "description": "Output contour polyline features. ", "dataType": "Feature Class"}, {"name": "contour_values", "isOptional": false, "description": "List of z-values for which to create contours. ", "dataType": "Double"}]},
{"syntax": "DecimateTinNodes_3d (in_tin, out_tin, method, {copy_breaklines})", "name": "Decimate TIN Nodes (3D Analyst)", "description": "Creates a triangulated irregular network (TIN) dataset using a subset of nodes from a source TIN.", "example": {"title": "DecimateTinNodes example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.DecimateTinNodes_3d ( \"tin\" , \"tin_simple\" , \"COUNT 5000\" \"BREAKLINES\" )"}, "usage": ["This tool provides a form of generalization that is useful for thinning oversampled data and subsequently improving visualization. The following thinning options are provided:", "This method also supports the enforcement of an optional node limit, which will cause the tool to stop processing if the Z tolerance value causes the resulting TIN to exceed the maximum number of nodes. If this occurs, the TIN will be produced, but a warning will be returned.", "If the", "Copy Breaklines", " option is selected, breaklines in the source TIN will be stored in the output without any generalization.  Nodes contributed by breaklines are not factored  toward any maximum node count limit. ", "The data boundary of the resulting TIN will be enforced as breakline edges regardless of whether the option to copy breaklines has been used or even whether the input TIN data boundary is enforced with breakline edges. This may increase the size of the output TIN and operates independently of any specified maximum node count limit."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_tin", "isOutputFile": true, "isOptional": false, "description": "The output TIN dataset. ", "dataType": "TIN"}, {"name": "method", "isOptional": false, "description": "Specifies the decimation method for selecting a subset of nodes from the input TIN. ZTOLERANCE <z_tolerance_value> <max_node_value> \u2014 Generalizes TIN within a specified vertical accuracy. An optional node limit can also be specified. This parameter is supplied as a string, so \"ZTOLERANCE 0.5 5500\" would represent a Z-tolerance value of 0.5 and a max node value of 5,500. COUNT <max_node_value> \u2014 Generalizes TIN by constraining its size to a specified node limit. This parameter is supplied as a string, so \"COUNT 5500\" would represent a maximum node count of 5,500.", "dataType": "Decimate"}, {"name": "copy_breaklines", "isOptional": true, "description": "Indicates whether breaklines from the input TIN are copied over to the output. BREAKLINES \u2014 Breaklines will be copied. NO_BREAKLINES \u2014 Breaklines will not be copied. This is the default.", "dataType": "Boolean"}]},
{"syntax": "Slice_3d (in_raster, out_raster, number_zones, {slice_type}, {base_output_zone})", "name": "Slice (3D Analyst)", "description": "Slices or reclassifies the range of values of the input cells into zones of equal interval, equal area, or by natural breaks.", "example": {"title": "Slice example 1 (Python window)", "description": "Reclassify the input raster into five classes based on natural groupings inherent in the data.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Slice_3d ( \"elevation\" , \"c:/output/elevslice\" , 5 , \"NATURAL_BREAKS\" )"}, "usage": ["Slice", " works best on data that is normally distributed. When using input raster data that is skewed the output result may not contain all of the classes that you had expected or specified.", "If an environment ", "Mask", " has been set, those cells that have been masked will receive NoData on the output slice raster.", "When using the EQUAL_AREA method, sometimes not all of the output zones (classes) will have an equal, or even similar, number of cells (i.e., area). This may be an inherent result based on the nature of the input values and the specified number of zones. If the results are deemed undesirable, you can try using a fewer number of zones or applying a statistics transformation (e.g., logarithm, square root, and so on) to the input dataset."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output reclassified raster. The output will always be of integer type. ", "dataType": "Raster Dataset"}, {"name": "number_zones", "isOptional": false, "description": "The number of zones to reclassify the input raster into. When the slice method is EQUAL_AREA, the output raster will have the defined number of zones, with a similar number of cells in each. When EQUAL_INTERVAL is used, the output raster will have the defined number of zones, each containing equal value ranges on the output raster. When NATURAL_BREAKS is used, the output raster will have the defined number of zones, with the number of cells in each determined by the class breaks. ", "dataType": "Long"}, {"name": "slice_type", "isOptional": true, "description": "The manner in which to slice the values in the input raster. EQUAL_INTERVAL \u2014 Determines the range of the input values and divides the range into the specified number of output zones. Each zone on the sliced output raster has the potential of having input cell values that have the same range from the extremes. EQUAL_AREA \u2014 Specifies that the input values will be divided into the specified number of output zones, with each zone having a similar number of cells. Each zone will represent a similar amount of area. NATURAL_BREAKS \u2014 Specifies that the classes will be based on natural groupings inherent in the data. Break points are identified by choosing the class breaks that best group similar values and that maximize the differences between classes. The cell values are divided into classes whose boundaries are set when there are relatively big jumps in the data values.", "dataType": "String"}, {"name": "base_output_zone", "isOptional": true, "description": "Defines the lowest zone value on the output raster dataset. The default value is 1. ", "dataType": "Long"}]},
{"syntax": "Int_3d (in_raster_or_constant, out_raster)", "name": "Int (3D Analyst)", "description": "Converts each cell value of a raster to an integer by truncation.", "example": {"title": "Int example 1 (Python window)", "description": "This example converts the input values to integer by truncation.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Int_3d ( \"gwhead\" , \"C:/output/outint\" )"}, "usage": ["The input values can be either positive or negative.", "If rounding is preferred to truncating, add a 0.5 input raster prior to performing the operation.", "The maximum supported range of integer raster values is  -2,147,483,648 (minimum size determined by -2", "31", ") to 2,147,483,647 (maximum size determined by 2", "31", " \u2013 1).  If ", "Int", " is used on a floating-point raster which has cells with values outside this range, those cells will be NoData in the output raster.", "Storing categorical (discrete) data as an integer raster will use significantly less disk space than the same information stored as a floating-point raster. Whenever possible, it is recommended to convert floating-point rasters to integer with this tool."], "parameters": [{"name": "in_raster_or_constant", "isInputFile": true, "isOptional": false, "description": "The input raster to be converted to integer. In order to use a number as an input for this parameter, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The cell values are the input values converted to integers by truncation. ", "dataType": "Raster Dataset"}]},
{"syntax": "Trend_3d (in_point_features, z_field, out_raster, {cell_size}, {order}, {regression_type}, {out_rms_file})", "name": "Trend (3D Analyst)", "description": "Interpolates a raster surface from points using a trend technique. \r\n Learn more about how Trend works", "example": {"title": "Trend example 1 (Python window)", "description": "This example inputs a point shapefile and interpolates the output surface as a TIFF raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Trend_3d ( \"ca_ozone_pts.shp\" , \"ozone\" , \"C:/output/trendout.tif\" , 2000 , 2 , \"LINEAR\" )"}, "usage": ["As the order of the polynomial is increased, the surface being fitted becomes progressively more complex. A higher-order polynomial will not always generate the most accurate surface; it is dependent on the data.", "The optional RMS file output contains information on the RMS (root mean square) error of the interpolation. This information can be used to determine the best value to use for the polynomial order, by changing the order value until you get the lowest RMS error. See ", "How Trend works", " for information on the RMS file.", "For the LOGISTIC option of ", "Type of regression", ", the z-value field of input point features should have codes of zero (0) and one (1).", "Some input datasets may have several points with the same x,y coordinates. If the values of the points at the common location are the same, they are considered duplicates and have no affect on the output. If the values are different, they are considered coincident points.", "The various interpolation tools may handle this data condition differently. For example, in some cases the first coincident point encountered is used for the calculation; in other cases the last point encountered is used.  This may cause some locations in the output raster to have different values than what you might expect. The solution is to prepare your data by removing these coincident points. The ", "Collect Events", " tool in the Spatial Statistics toolbox is useful for identifying any coincident points in your data."], "parameters": [{"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": "The input point features containing the z-values to be interpolated into a surface raster. ", "dataType": "Feature Layer"}, {"name": "z_field", "isOptional": false, "description": "The field that holds a height or magnitude value for each point. This can be a numeric field or the Shape field if the input point features contain z-values. If the regression type is Logistic, the values in the field can only be 0 or 1. ", "dataType": "Field"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output interpolated surface raster. ", "dataType": "Raster Layer"}, {"name": "cell_size", "isOptional": true, "description": "The cell size at which the output raster will be created. This will be the value in the environment if it is explicitly set; otherwise, it is the shorter of the width or the height of the extent of the input point features, in the input spatial reference, divided by 250. ", "dataType": "Analysis Cell Size"}, {"name": "order", "isOptional": true, "description": "The order of the polynomial. This must be an integer between 1 and 12. A value of 1 will fit a flat plane to the points, and a higher value will fit a more complex surface. The default is 1. ", "dataType": "Long"}, {"name": "regression_type", "isOptional": true, "description": "The type of regression to be performed. LINEAR \u2014 Polynomial regression is performed to fit a least-squares surface to the set of input points. This is applicable for continuous types of data. LOGISTIC \u2014 Logistic trend surface analysis is performed. It generates a continuous probability surface for binary, or dichotomous, types of data.", "dataType": "String"}, {"name": "out_rms_file", "isOutputFile": true, "isOptional": true, "description": "File name for the output text file that contains information about the RMS error and the Chi-Square of the interpolation. The extension must be \" .txt \". ", "dataType": "File"}]},
{"syntax": "ChangeTerrainReferenceScale_3d (in_terrain, old_refscale, new_refscale)", "name": "Change Terrain Reference Scale (3D Analyst)", "description": "Changes the reference scale associated with a terrain pyramid level.", "example": {"title": "ChangeTerrainReferenceScale example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.ChangeTerrainReferenceScale_3d ( 'terrain.gdb/terrainFDS/terrain1' , 1000 , 2000 )"}, "usage": ["Consider changing a terrain pyramid's reference scale if you experience undesirable draw speed performance or require greater densification of data points in the pyramid's display scale range.", "It may also be necessary to add or remove a pyramid level when changing the resolution of an existing one, which can be done by using ", "Add Terrain Pyramid", " or ", "Remove Terrain Pyramid", ".", "When used in an SDE database, the input terrain cannot be registered as versioned."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "old_refscale", "isOptional": false, "description": "The reference scale of an existing pyramid level. ", "dataType": "Long"}, {"name": "new_refscale", "isOptional": false, "description": "The new reference scale for the pyramid level. ", "dataType": "Long"}]},
{"syntax": "SkylineBarrier_3d (in_observer_point_features, in_features, out_feature_class, {min_radius_value_or_field}, {max_radius_value_or_field}, {closed}, {base_elevation}, {project_to_plane})", "name": "Skyline Barrier (3D Analyst)", "description": "Generates a multipatch feature class representing a skyline barrier or shadow volume.  Learn more about how Skyline Barrier works \r\n", "example": {"title": "SkylineBarrier example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.SkylineBarrier_3d ( \"observers.shp\" , \"skyline_outline.shp\" , \"barrier_output.shp\" )"}, "usage": ["The barrier resembles a triangle fan formed by drawing a line from the observer point to the first vertex of the skyline, then  sweeping the line through all of the vertices of the skyline. Consider using this tool to determine whether features, such as multipatches representing buildings, violate the barrier by protruding up through it, or whether a proposed building would alter the skyline.", "Use the ", "Skyline", " tool first to generate a skyline or silhouette. A silhouette would produce a volumetric representation of the shadow cast by light coming from the observation point. ", "Learn more about creating shadow volumes", "The ", "Minimum Radius", " and ", "Maximum Radius", " parameters define the length of the triangle edges emanating from the observation point.  If the default value of 0 is specified for the ", "Minimum Radius", " or ", "Maximum Radius", ", then no minimum or maximum length is used in the analysis. ", " If you choose to create a closed multipatch, then the output will be extruded to the height defined in the ", " Base Elevation", " parameter, and a horizontal ring will be created to form the bottom of the closed geometry. If the specified base elevation is greater than the highest vertex in the skyline barrier, then the base will actually be a ceiling.", "The new multipatch feature class will have the following\r\nfields:"], "parameters": [{"name": "in_observer_point_features", "isInputFile": true, "isOptional": false, "description": " The point feature class containing the observer points. ", "dataType": "Feature Layer"}, {"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The input line feature class which represents the skylines, or the input multipatch feature class which represents the silhouettes. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class into which the skyline barrier or shadow volume is placed. ", "dataType": "Feature Class"}, {"name": "min_radius_value_or_field", "isOptional": true, "description": "The minimum radius to which triangle edges should be extended from the observer point. The default is 0, meaning no minimum. ", "dataType": "Linear Unit; Field"}, {"name": "max_radius_value_or_field", "isOptional": true, "description": "The maximum radius to which triangle edges should be extended from the observer point. The default is 0, meaning no maximum. ", "dataType": "Linear Unit; Field"}, {"name": "closed", "isOptional": true, "description": "Whether to close the skyline barrier with a skirt and a base so that the resulting multipatch will appear to be a solid. NO_CLOSED \u2014 No skirt or base is added to the multipatch; just the multipatch representing the surface going from the observer to the skyline is represented. This is the default. CLOSED \u2014 A skirt and a base are added to the multipatch so as to form what appears to be a closed solid.", "dataType": "Boolean"}, {"name": "base_elevation", "isOptional": true, "description": " The elevation of the base of the closed multipatch; it is ignored if the barrier is not to be closed. The default is 0. ", "dataType": "Linear Unit; Field"}, {"name": "project_to_plane", "isOptional": true, "description": "Whether the front (nearer to the observer) and back (farther from the observer) ends of the barrier should each be projected onto a vertical plane. This is typically checked (turned on) in order to create a shadow volume. NO_PROJECT_TO_PLANE \u2014 The barrier will extend from the observer point to the skyline (or nearer or farther if nonzero values are provided for minimum and maximum radius). This is the default. PROJECT_TO_PLANE \u2014 The barrier will extend from a vertical plane to a vertical plane.", "dataType": "Boolean"}]},
{"syntax": "IsClosed3D_3d (in_feature_class)", "name": "Is Closed 3D (3D Analyst)", "description": "Evaluates multipatch features to determine whether each feature completely encloses a volume of space.", "example": {"title": "IsClosed example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env env.workspace = 'C:/data' arcpy.IsClosed3D_3d ( 'sample_multipatch.shp' )"}, "usage": ["A new field that specifies whether the multipatch is closed is added to the input multipatch feature. ", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them."], "parameters": [{"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": " The multipatch features to be tested. ", "dataType": "Feature Layer"}]},
{"syntax": "InterpolateShape_3d (in_surface, in_feature_class, out_feature_class, {sample_distance}, {z_factor}, {method}, {vertices_only}, {pyramid_level_resolution})", "name": "Interpolate Shape (3D Analyst)", "description": "Interpolates z-values for a feature class based on elevation derived from a raster, triangulated irregular network (TIN), or terrain dataset.  Learn more about how Interpolate Shape works", "example": {"title": "InterpolateShape example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.InterpolateShape_3d ( \"my_tin\" , \"roads.shp\" , \"roads_interp.shp\" )"}, "usage": ["When using the ", "natural neighbors", " interpolation option, make sure to specify a reasonable sample distance. This usually falls between 0.5 and 1.0 times the average point spacing of the data used to build the TIN or terrain dataset.", "When using the", " Interpolate Vertices Only", " option, input features with vertices that fall outside the data area of the surface will be ignored and not output. Clip features prior to running ", "Interpolate Shape", " to ensure the features are completely on the surface."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The LAS dataset, raster, TIN, or terrain surface used for interpolating z-values. ", "dataType": "LAS Dataset Layer, Raster Layer; Terrain Layer; TIN Layer"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The input feature class. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "sample_distance", "isOptional": true, "description": "The spacing at which z-values will be interpolated. By default, this is a raster's cell size or a TIN's natural densification. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}, {"name": "method", "isOptional": true, "description": " Interpolation method used to determine elevation values for the output features. The available options depend on the surface type being used. BILINEAR interpolation is available for a raster surface, where a query point obtains its elevation from the values found in the four nearest cells. Terrain and TIN datasets provide the following options: LINEAR \u2014 Default interpolation method. Obtains elevation from the plane defined by the triangle that contains the XY location of a query point. NATURAL_NEIGHBORS \u2014 Obtains elevation by applying area-based weights to the natural neighbors of a query point. CONFLATE_ZMIN \u2014 Obtains elevation from the smallest Z value found among the natural neighbors of a query point. CONFLATE_ZMAX \u2014 Obtains elevation from the largest Z value found among the natural neighbors of a query point. CONFLATE_NEAREST \u2014 Obtains elevation from the nearest value among the natural neighbors of a query point. CONFLATE_CLOSEST_TO_MEAN \u2014 Obtains elevation from the Z value that is closest to the average of all the natural neighbors of a query point.", "dataType": "String"}, {"name": "vertices_only", "isOptional": true, "description": "Specifies whether the interpolation will only occur along the vertices of an input feature, thereby ignoring the sample distance option. DENSIFY \u2014 Interpolates using the sampling distance. This is the default. VERTICES_ONLY \u2014 Interpolates along the vertices.", "dataType": "Boolean"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "RasterToMultipoint_3d (in_raster, out_feature_class, {out_vip_table}, {method}, {kernel_method}, {z_factor})", "name": "Raster To Multipoint (3D Analyst)", "description": "Converts raster cell centers into multipoint features whose Z values reflect the raster cell value.", "example": {"title": "RasterToMultipoint example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.RasterToMultipoint_3d ( \"elevation.tif\" , \"\" , \"elev_VIP.dbf\" , \"VIP_HISTOGRAM\" , \"\" , \"1\" )"}, "usage": ["This tool is intended for raster datasets that model surface properties, such as elevation.", "This tool is useful for incorporating a raster DEM into points that can be used as ancillary data for creating a ", "Terrain dataset", " or ", "TIN", ".", "All the cell centers can be converted, using the ", "NO_THIN", " option, or one of several filters can be applied to exclude those cells that are less significant. The thinning options include ", "ZTOLERANCE", ", ", "KERNEL", ", and ", "VIP", ".", "Use the ", "ZTOLERANCE", " thinning method when it is important to preserve vertical accuracy. Use the ", "KERNEL", " thinning method when it is important to control the horizontal sample distance.", "Consider applying a thinning method when converting from raster to multipoints. There are two thinning methods used to generate a new multipoint feature class: ", "ZTOLERANCE", " and ", "KERNEL", ". If ", "NO_THIN", " is selected, the full resolution data will be output into a new multipoint feature class.", "The ", "VIP", " method is relatively fast, outputs a predictable number of points, and is good at selecting local peaks and pits. However, it is sensitive to noise and insensitive to topographic features that are larger than the 3 by 3 window."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster. ", "dataType": "Raster Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "out_vip_table", "isOutputFile": true, "isOptional": true, "description": " The histogram table to be produced when VIP Histogram is specified for the Method parameter. ", "dataType": "Table"}, {"name": "method", "isOptional": true, "description": "The thinning method applied to generate the multipoint feature class. NO_THIN \u2014 The full resolution data will be output into a new multipoint feature class. ZTOLERANCE \u2014 The maximum allowable difference in (z units) between the height of the input raster and the height of the output multipoint feature class. By default, the z tolerance is 1/10 of the z range of the input raster. The larger the tolerance, the more thinning, the fewer points output. KERNEL \u2014 Defines the number of cells for a window. The default is 3, which translates into a 3 by 3 set of cells in the input raster. The individual cell values in each of these windows are evaluated. Then just one or two cells are chosen, depending on the KERNEL selection method. The larger the kernel size, the more thinning will be carried out, and the fewer points output. VIP \u2014 Selects a percentage of points from the input raster based on their significance. The significance is assessed using a roving 3 by 3 window. VIP_HISTOGRAM \u2014 Creates a table to view the actual significance values and the corresponding number of points associated with those values.", "dataType": "String"}, {"name": "kernel_method", "isOptional": true, "description": "The selection method used for creating points when kernel thinning is specified in the Method parameter. MIN \u2014 A point is created at the cell with the smallest elevation value found in the kernel neighborhood. This is the default. MAX \u2014 A point is created at the cell with the largest elevation value found in the kernel neighborhood. MINMAX \u2014 Two points are created at the cells with the smallest and largest elevation values found in the kernel neighborhood. MEAN \u2014 A point is created at the cell whose elevation value is closest to the average of the cells in the kernel neighborhood.", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": " The factor used for multiplying the elevation of the raster. Generally used to convert units between feet and meters. ", "dataType": "Double"}]},
{"syntax": "TerrainToTin_3d (in_terrain, out_tin, {pyramid_level_resolution}, {max_nodes}, {clip_to_extent})", "name": "Terrain To TIN (3D Analyst)", "description": "Converts a terrain dataset to a triangulated irregular network (TIN) dataset.", "example": {"title": "TerrainToTIN example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" #arcpy.TerrainToTin_3d(\"sample.gdb/featuredataset/terrain\", \"tin\", 6, 5000000, False)"}, "usage": ["Define the extent of the output TIN using the geoprocessing ", "extent environment", "  setting.", "Use an extent and pyramid level that will not exceed the node limit for a TIN. While the maximum number of TIN nodes in a 32-bit Windows platform is estimated to be between 15 to 20 million, a cap of a few million is recommended to maintain optimal display performance. Triangulation of a larger surface is best handled by the terrain dataset."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "out_tin", "isOutputFile": true, "isOptional": false, "description": "The output TIN dataset. ", "dataType": "TIN"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}, {"name": "max_nodes", "isOptional": true, "description": "The maximum number of nodes permitted in the output TIN. The tool will return an error if the analysis extent and pyramid level would produce a TIN that exceeds this size. The default is 5 million. ", "dataType": "Long"}, {"name": "clip_to_extent", "isOptional": true, "description": "Specifies whether the resulting TIN will be clipped against the analysis extent. This only has an effect if the analysis extent is defined and it's smaller than the extent of the input terrain. CLIP \u2014 Clips the output TIN against the analysis extent. This is the default. NO_CLIP \u2014 Does not clip the output TIN against the analysis extent.", "dataType": "Boolean"}]},
{"syntax": "TinPolygonVolume_3d (in_tin, in_feature_class, in_height_field, {reference_plane}, {out_volume_field}, {surface_area_field})", "name": "TIN Polygon Volume (3D Analyst)", "description": "Calculates the volumetric and surface area between polygons of an input feature class and a TIN surface.", "example": {"title": "TIN Polygon Volume example (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "import arcgisscripting gp = arcgisscripting.create () gp.CheckOutExtension ( \"3D\" ) gp.workspace = \"C:/data\" gp.TinPolygonVolume_3d ( \"tin\" , \"PolyVol.shp\" , \"Height\" , \"ABOVE\" , \"Volume\" , \"S_Area\" )"}, "usage": ["The input polygons and TIN need to overlap in horizontal extent in order for surface area or volume to be calculated."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The input polygon feature class. ", "dataType": "Feature Layer"}, {"name": "in_height_field", "isInputFile": true, "isOptional": false, "description": "The name of the field containing polygon reference plane heights. ", "dataType": "String"}, {"name": "reference_plane", "isOptional": true, "description": "The keyword used to indicate whether volume and surface area are calculated ABOVE the reference plane height of the polygons, or BELOW. The default is BELOW. ", "dataType": "String"}, {"name": "out_volume_field", "isOutputFile": true, "isOptional": true, "description": "The name of the output field used to store the volume result. The default is Volume. ", "dataType": "String"}, {"name": "surface_area_field", "isOptional": true, "description": "The name of the output field used to store the surface area result. The default is SArea. ", "dataType": "String"}]},
{"syntax": "AddZInformation_3d (in_feature_class, out_property, {noise_filtering})", "name": "Add Z Information (3D Analyst)", "description": "Adds information about elevation properties of features in a Z-enabled feature class. Each 3D shape is examined and the selected properties are appended to the attribute table of the input feature class. The output options vary based on the feature's geometry.", "example": {"title": "AddZInformation example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.AddZInformation_3d ( 'lines_3D.shp' , 'Z_MEAN; LENGTH_3D; AVG_SLOPE' , 'NO_FILTER' )"}, "usage": ["The following list summarizes Z property options by geometry:", " Slope is returned as a percentage  value, or grade, and is calculated differently for each geometry type that supports this property.", "Volume can only be computed for closed multipatches. An open multipatch feature will return a value of 0.0.\r\nOn the Solaris platform, a design limitation currently prevents the tool from determining if a multipatch is closed, resulting in volume measurements being calculated for all multipatches under the assumption they are closed."], "parameters": [{"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The input feature class. ", "dataType": "Feature Layer"}, {"name": "out_property", "isOutputFile": true, "isOptional": false, "description": "The output Z properties that will be added to the attribute table of the input feature class. The following options are available: Z \u2014 The elevation of each single-point feature. POINT_COUNT \u2014 The number of points in each multipoint array. Z_MIN \u2014 The lowest elevation found in each multipoint, polyline, polygon, or multipatch feature. Z_MAX \u2014 The highest elevation found in each multipoint, polyline, polygon, or multipatch feature. Z_MEAN \u2014 The average elevation found in each multipoint, polyline, polygon, or multipatch feature. LENGTH_3D \u2014 The 3-dimensional length of each polyline or polygon feature. VERTEX_COUNT \u2014 The total number of vertices in each polyline or polygon feature. MIN_SLOPE \u2014 The lowest slope value calculated for each polyline, polygon, or multipatch feature. MAX_SLOPE \u2014 The highest slope value calculated for each polyline, polygon, or multipatch feature. AVG_SLOPE \u2014 The average slope value calculated for each polyline, polygon, or multipatch feature. VOLUME \u2014 The volume determined for each closed multipatch feature.", "dataType": "String"}, {"name": "noise_filtering", "isOptional": true, "description": " Provides the option to exclude small portions of features from statistical calculations. This option is useful for obtaining good maximum slope estimates, as small portions often exhibit extreme slopes, which may bias the statistical results. The values given in either the Area or Length options will be used to exclude these portions of features. This parameter does not apply to point and multipoint features. NO_FILTER \u2014 No noise filter will be used. This is the default. AREA <\u2026> \u2014 An area filter will be applied to portions of features in multipatch feature classes. An AREA value of 0.001 indicates that subparts of multipatches with an area less than 0.001 will be ignored. LENGTH <\u2026> \u2014 A length filter will be applied to portions of features in a line or polygon feature class. A LENGTH value of 0.001 indicates that portions of features with a length less than 0.001 will be ignored.", "dataType": "String"}]},
{"syntax": "TinEdge_3d (in_tin, out_feature_class, {edge_type})", "name": "TIN Edge (3D Analyst)", "description": "Creates 3D line features using the triangle edges of a triangulated irregular network (TIN) dataset.", "example": {"title": "TinEdge example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.TinEdge_3d ( 'tin' , 'tin_edge.shp' , 'ENFORCED' )"}, "usage": ["Use the ", "Edge Type", " parameter to extract a specific type of triangle edge.", "Output feature contains a field named EdgeType that uses integer values to represent the type of edge that each line represents:"], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "edge_type", "isOptional": true, "description": "The type of triangle edge to be extracted. DATA \u2014 Edges representing the TIN's interpolation zone. This is the default. SOFT \u2014 Edges defined as representing gradual breaks in slope. HARD \u2014 Edges defined as representing distinct breaks in slope. ENFORCED \u2014 Edges representing breaklines that were not introduced by the triangulation of the TIN. REGULAR \u2014 Edges that were not classified as soft or hard. OUTSIDE \u2014 Edges that are not drawn. ALL \u2014 All edges in the TIN dataset.", "dataType": "String"}]},
{"syntax": "LineOfSight_3d (in_surface, in_line_feature_class, out_los_feature_class, {out_obstruction_feature_class}, {use_curvature}, {use_refraction}, {refraction_factor}, {pyramid_level_resolution}, {in_features})", "name": "Line Of Sight (3D Analyst)", "description": "Determines the visibility of sight lines over a surface defined by a multipatch, raster, TIN, terrain, or LAS dataset.  \r\n Learn more about how Line Of Sight works \r\n", "example": {"title": "LineOfSight example 1 (Python window)", "description": "The following sample demonstrates how to use this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.LineOfSight_3d ( \"tin\" , \"line.shp\" , \"los.shp\" , \"buldings_multipatch.shp\" , \"obstruction.shp\" )"}, "usage": ["Only the endpoints of the input line are used to define the observer and target. Ideally, sight lines should be simple, straight lines comprised of two vertices representing the observation point and the target location to which visibility is determined.", "If the observation location is identified by point features and the target locations reside in a different feature class, use ", "Construct Sight Lines", " to generate input for this tool.", "Consider specifying the optional obstruction point feature to identify the location of the first obstacle blocking the target's visibility for each line.", "The output line feature's attribute table contains the following fields:"], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The raster, TIN, terrain, or LAS dataset that defines the surface used in determining visibility. ", "dataType": "Raster Layer; Terrain Layer; TIN Layer"}, {"name": "in_line_feature_class", "isInputFile": true, "isOptional": false, "description": "The line features whose first vertex defines the observation point and last vertex identifies the target location. Height of the observation and target locations are obtained from the z-values of 3D features and interpolated from the surface for 2D features. 2D lines also have a default offset of 1 added to their elevation to raise the points above the surface. If the feature has an OffsetA field, its value will be added to the height of the observation point. If the OffsetB field is present, its value will be added to the height of the target position. ", "dataType": "Feature Layer"}, {"name": "out_los_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output line feature class along which visibility has been determined. Two attribute fields are created. VisCode indicates visibility along the line, 1 being visible and 2 not visible. TarIsVis indicates the target visibility, 0 being not visible and 1 being visible. ", "dataType": "Feature Class"}, {"name": "out_obstruction_feature_class", "isOutputFile": true, "isOptional": true, "description": "An optional point feature class identifying the location of the first obstruction on the observer's sight line to its target. ", "dataType": "Feature Class"}, {"name": "use_curvature", "isOptional": true, "description": "Indicates whether the earth's curvature should be taken into consideration for the line-of-sight analysis. For this option to be enabled, the surface needs to have a defined spatial reference in projected coordinates with defined z-units. CURVATURE \u2014 The earth's curvature will be taken into consideration. NO_CURVATURE \u2014 The earth's curvature will not be taken into consideration. This is the default.", "dataType": "Boolean"}, {"name": "use_refraction", "isOptional": true, "description": "Indicates whether atmospheric refraction should be taken into consideration when generating a line of sight from a functional surface. REFRACTION \u2014 Atmospheric refraction will be taken into consideration. NO_REFRACTION \u2014 Atmospheric refraction will not be taken into consideration. This is the default.", "dataType": "Boolean"}, {"name": "refraction_factor", "isOptional": true, "description": "Provides a value to be used in the refraction factor. The default refraction factor is 0.13. ", "dataType": "Double"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The resolution of the terrain dataset pyramid level to use for geoprocessing. The default is 0, or full resolution. ", "dataType": "Double"}, {"name": "in_features", "isInputFile": true, "isOptional": true, "description": " An optional multipatch feature class that may define additional obstructing elements, such as buildings. ", "dataType": "Feature Layer"}]},
{"syntax": "RemoveFeatureClassFromTerrain_3d (in_terrain, feature_class)", "name": "Remove Feature Class From Terrain (3D Analyst)", "description": "Removes reference to a feature class participating in a terrain dataset.", "example": {"title": "RemoveFeatureClassFromTerrain example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.RemoveFeatureClassFromTerrain_3d ( \"sample.gdb/featuredataset/terrain\" , \"points_1995\" )"}, "usage": ["This tool will only delete embedded features referenced by a terrain dataset.", "The terrain may need to be rebuilt using ", "Build Terrain", " if the features being removed were referenced as ", "masspoints", " surface type. Both the terrain dataset's Properties dialog box in ArcCatalog and the terrain layer's ", "Properties", " dialog box provide an indication as to whether the dataset needs to be rebuilt.", "When used in an SDE database, the input terrain cannot be registered as versioned."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "feature_class", "isOptional": false, "description": "The feature class to be removed. ", "dataType": "String"}]},
{"syntax": "TinNode_3d (in_tin, out_feature_class, {spot_field}, {tag_field})", "name": "TIN Node (3D Analyst)", "description": "Exports the nodes of a triangulated irregular network (TIN) dataset to a point feature class.", "example": {"title": "TinNode example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.TinNode_3d ( 'tin' , 'elevation_node.shp' , '' , 'Tag_Value' )"}, "usage": ["Populating a name for the ", "Spot Field", " parameter results in the creation of 2D point feature class.  Omitting the name creates 3D points."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "spot_field", "isOptional": true, "description": "The name of the elevation attribute field of the output feature class. If a name is given, the feature class will be 2D; otherwise, it will be 3D. No name is provided by default, which results in the creation of 3D point features. ", "dataType": "String"}, {"name": "tag_field", "isOptional": true, "description": "The name of the field storing the tag attribute in the output feature class. By default, no tag value field is created. ", "dataType": "String"}]},
{"syntax": "ReclassByASCIIFile_3d (in_raster, in_remap_file, out_raster, {missing_values})", "name": "Reclass by ASCII File (3D Analyst)", "description": "Reclassifies (or changes) the values of the input cells of a raster using an ASCII remap file. \r\n Learn more about how Reclass by ASCII File works", "example": {"title": "ReclassByASCIIFile example 1 (Python window)", "description": "This example uses an ASCII remap file to reclassify the input raster.", "code": "import arcpy from arcpy.sa import * from arcpy import env env.workspace = \"C:/data\" arcpy.ReclassByASCIIFile_3d ( \"slope\" , \"remapslope.rmp\" , \"C:/output/recslope\" )"}, "usage": ["The input raster must have valid statistics. If the statistics do\r\nnot exist, they can be created using the ", "Calculate Statistics", " tool in the Data Management Tools toolbox.", "The output raster will always be of integer type. If the output assignment values in the ASCII file are floating-point values, an error message will be returned and the program will halt."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "in_remap_file", "isInputFile": true, "isOptional": false, "description": "ASCII remap file defining the single values or ranges to be reclassified and the values they will become. Allowed extensions for the ASCII remap files are .rmp , .txt , and .asc . ", "dataType": "File"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output reclassified raster. The output will always be of integer type. ", "dataType": "Raster Dataset"}, {"name": "missing_values", "isOptional": true, "description": "Denotes whether missing values in the reclass file retain their value or get mapped to NoData. DATA \u2014 Signifies that if any cell location on the input raster contains a value that is not present or reclassed in the remap file, the value should remain intact and be written for that location to the output raster. NODATA \u2014 A keyword signifying that if any cell location on the input raster contains a value that is not present or reclassed in the remap file, the value will be reclassed to NODATA for that location on the output raster.", "dataType": "Boolean"}]},
{"syntax": "Slope_3d (in_raster, out_raster, {output_measurement}, {z_factor})", "name": "Slope (3D Analyst)", "description": "Identifies the slope (gradient, or rate of maximum change in z-value) from each cell of a raster surface. \r\n Learn more about how Slope works", "example": {"title": "Slope example 1 (Python window)", "description": "This example determines the slope values of the input surface raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Slope_3d ( \"elevation\" , \"C:/output/outslope01\" , \"DEGREE\" , 0.3043 )"}, "usage": ["Slope is the rate of maximum change in z-value from each cell.", "The use of a z-factor is essential for correct slope calculations when the surface z units are expressed in units different from the ground x,y units.", "The range of values in the output depends on the type of measurement units. ", "If the center cell in the immediate neighborhood (3 x 3 window) is NoData, the output is NoData.", "If any neighborhood cells are NoData, they are assigned the value of the center cell; then the slope is computed."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output slope raster. ", "dataType": "Raster Dataset"}, {"name": "output_measurement", "isOutputFile": true, "isOptional": true, "description": "Determines the measurement units (degrees or percentages) of the output slope data. DEGREE \u2014 The inclination of slope will be calculated in degrees. PERCENT_RISE \u2014 Keyword to output the percent rise, also referred to as the percent slope.", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "Number of ground x,y units in one surface z unit. The z-factor adjusts the units of measure for the z units when they are different from the x,y units of the input surface. The z-values of the input surface are multiplied by the z-factor when calculating the final output surface. If the x,y units and z units are in the same units of measure, the z-factor is 1. This is the default. If the x,y units and z units are in different units of measure, the z-factor must be set to the appropriate factor, or the results will be incorrect. For example, if your z units are feet and your x,y units are meters, you would use a z-factor of 0.3048 to convert your z units from feet to meters (1 foot = 0.3048 meter). ", "dataType": "Double"}]},
{"syntax": "Skyline_3d (in_observer_point_features, out_feature_class, {in_surface}, {virtual_surface_radius}, {virtual_surface_elevation}, {in_features}, {feature_lod}, {from_azimuth_value_or_field}, {to_azimuth_value_or_field}, {azimuth_increment_value_or_field}, {max_horizon_radius}, {segment_skyline}, {scale_to_percent}, {scale_according_to}, {scale_method}, {use_curvature}, {use_refraction}, {refraction_factor}, {pyramid_level_resolution}, {create_silhouettes})", "name": "Skyline (3D Analyst)", "description": " Generates a line or multipatch feature class containing the results from a skyline silhouette analysis.  \r\n Learn more about how Skyline works \r\n", "example": {"title": "Skyline example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.Skyline_3d ( \"observers.shp\" , \"skyline_output.shp\" , \"sample.gdb/featuredataset/terrain\" )"}, "usage": ["The analysis is conducted from observer points above a functional or virtual surface and will also consider features that are encountered during the analysis.  When used in conjunction with other tools, especially the ", "Skyline Barrier", " tool, shadow volumes and other such features can be created.", "The ", "Skyline ", " tool is often used in conjunction with the ", "Skyline Barrier ", " tool and sometimes with the ", "Skyline Graph", " tool. Other applications of the ", "Skyline ", " tool are described  in ", "Analyzing threats to 3D flight paths and corridors", ".", " The ", "Skyline ", " tool can be used to create silhouettes of features, and these silhouettes can be extruded into shadow volumes with the ", "Skyline Barrier ", " tool. See further information in ", "How Skyline works", ".", " If feature silhouettes are generated instead of a skyline, then the usage and description of some of the parameters will be different.", "If no features are specified, then the skyline will consist\r\nsolely of a ridgeline (horizon line).\r\n\r\n\r\n", "If no functional surface is specified, then a virtual surface\r\nwill be used, generated from the radius and elevation provided.", "The only required parameters are the input observer points and\r\nthe output feature class (the skyline).\r\n\r\n", "If one or more features are selected, then only the selected\r\nfeatures will be considered; otherwise, they are all\r\nconsidered.\r\n\r\n", "A separate skyline analysis is done for each observer point.\r\nOne or more lines are generated to represent the skyline as seen\r\nfrom each observer point.\r\n", "The azimuth increment value affects only the granularity of\r\nany ridgeline portion of the skyline, not the portion of the\r\nskyline that runs along any features.\r\n\r\n\r\n", "Each output line has a value indicating the FID of the\r\nobserver point feature used to create the skyline that the line\r\nrepresents.\r\n", " To create a shadow volume of one or more features, it is usually recommended to check the ", "Segment Skyline", " check box so that each feature that participates in the skyline will have its own polyline.", " When creating a shadow volume, you would usually run the ", "Skyline ", " tool with only one feature selected per run, then run the  ", "Skyline Barrier", " tool on the specific polyline that represents the feature.", "The output geometry is a 3D polyline.", "The following fields will be added to the output feature class\r\nthat contains the skylines:", " The following fields will be added to the output feature class that contains the silhouettes:"], "parameters": [{"name": "in_observer_point_features", "isInputFile": true, "isOptional": false, "description": "The 3D points representing observers; a separate skyline is generated for each. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The line feature class into which the skylines will be placed. Alternatively, this could be a multipatch feature class to contain feature silhouettes. ", "dataType": "Feature Class"}, {"name": "in_surface", "isInputFile": true, "isOptional": true, "description": " The functional surface for determining the horizon. ", "dataType": "LAS Dataset Layer; Raster Layer; TIN Layer; Terrain Layer"}, {"name": "virtual_surface_radius", "isOptional": true, "description": " The radius of the virtual surface for defining the horizon in lieu of an actual surface. Must be a positive value. Ignored if an actual surface is provided. The default is 1,000. ", "dataType": "Linear Unit"}, {"name": "virtual_surface_elevation", "isOptional": true, "description": "The elevation of the virtual surface for defining the horizon in lieu of an actual surface. It is ignored if an actual surface is provided. The default is 0. ", "dataType": "Linear Unit"}, {"name": "in_features", "isInputFile": true, "isOptional": true, "description": "The feature class to use in the skyline analysis. The features typically represent obstructions such as city buildings. ", "dataType": "Feature Layer"}, {"name": "feature_lod", "isOptional": true, "description": " The level of detail at which each feature should be examined in the skyline analysis. FULL_DETAIL \u2014 Every edge within the feature is considered in the skyline analysis (only edges of triangles and exterior rings are considered). This is the slowest but most precise. This is the default. CONVEX_FOOTPRINT \u2014 The contribution to the skyline analysis is generated by the upper perimeter of the convex polygon representing the convex hull of the footprint of the feature, raised to the elevation of the highest vertex within the feature. ENVELOPE \u2014 The contribution to the skyline analysis is generated by the upper horizontal rectangular perimeter of the rectangular solid enveloping the feature (vertical faces perpendicular to the X and Y axes).", "dataType": "String"}, {"name": "from_azimuth_value_or_field", "isOptional": true, "description": " The azimuth, in degrees, from which the skyline analysis should be started. The analysis starts from the observer point and goes to the right, from the From Azimuth until the To Azimuth is reached. Must be greater than minus 360 and less than 360. The default is 0. ", "dataType": "Double; Field"}, {"name": "to_azimuth_value_or_field", "isOptional": true, "description": " The direction, in degrees, at which the skyline analysis should be completed. The analysis starts from the observer point and goes to the right, from the From Azimuth until the To Azimuth is reached. Must be no more than 360 greater than the From Azimuth. The default is 360. ", "dataType": "Double; Field"}, {"name": "azimuth_increment_value_or_field", "isOptional": true, "description": "The angular interval, in degrees, at which the horizon should be evaluated while conducting the skyline analysis between the From Azimuth and the To Azimuth. Must be no greater than the To Azimuth minus the From Azimuth. The default is 1. ", "dataType": "Double; Field"}, {"name": "max_horizon_radius", "isOptional": true, "description": "The maximum distance for which a horizon should be sought from the observer location. A value of zero indicates that there should be no limit imposed. The default is 0. ", "dataType": "Double"}, {"name": "segment_skyline", "isOptional": true, "description": "Instead of only one line being generated to represent the skyline from each observer point, the output is split into multiple lines. Each of these lines represents a different feature, or a stretch of horizon between features. If silhouettes are being generated, then this parameter will indicate whether divergent rays should be used; for sun shadows, this should generally be no or unchecked. NO_SEGMENT_SKYLINE \u2014 The output 3D polyline results will not be segmented. This is the default. SEGMENT_SKYLINE \u2014 The output line results will be segmented according to the various features that are encountered during the analysis.", "dataType": "Boolean"}, {"name": "scale_to_percent", "isOptional": true, "description": "Indicates to what percent of the original vertical angle (angle above the horizon, or angle of elevation) or elevation each skyline vertex should be placed. If either 0 or 100 is entered, then no scaling will occur. The default is 100. ", "dataType": "Double"}, {"name": "scale_according_to", "isOptional": true, "description": "The values according to which the scaling should be determined. VERTICAL_ANGLE \u2014 Scaling is done by considering the vertical angle of each vertex relative to the observer point. This is the default. ELEVATION \u2014 Scaling is done by considering the elevation of each vertex relative to the observer point.", "dataType": "String"}, {"name": "scale_method", "isOptional": true, "description": " The vertex to be used to calculate against. SKYLINE_MAXIMUM \u2014 Vertices will be scaled relative to the vertical angle (or the elevation) of the vertex with the highest vertical angle (or elevation). This is the default. EACH_VERTEX \u2014 Vertices will be scaled relative to the original vertical angle (or elevation) of each vertex.", "dataType": "String"}, {"name": "use_curvature", "isOptional": true, "description": "Indicates whether the earth's curvature should be taken into consideration when generating the ridgeline from a functional surface. CURVATURE \u2014 The earth's curvature will be taken into consideration. NO_CURVATURE \u2014 The earth's curvature will not be taken into consideration. This is the default.", "dataType": "Boolean"}, {"name": "use_refraction", "isOptional": true, "description": "Indicates whether atmospheric refraction should be taken into consideration when generating the ridgeline from a functional surface. If no actual surface is specified, then checking this check box (or passing REFRACTION in Python) will cause silhouettes to be created instead of skylines. NO_REFRACTION \u2014 Atmospheric refraction will not be taken into consideration. This is the default. REFRACTION \u2014 Atmospheric refraction will be taken into consideration.", "dataType": "Boolean"}, {"name": "refraction_factor", "isOptional": true, "description": "If atmospheric refraction is considered, then apply a scalar value. The default is 0.13. ", "dataType": "Double"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "If a terrain is specified for a functional surface, then this parameter is populated with the resolutions resident in the terrain. One of the resolutions may be chosen for the purpose of generating the horizon line. The default is 0, or full resolution. ", "dataType": "Double"}, {"name": "create_silhouettes", "isOptional": true, "description": "Specify whether output features will represent silhouettes that are visible from the observer point. NO_CREATE_SILHOUETTES \u2014 The resulting polylines will represent the skyline. This is the default. CREATE_SILHOUETTES \u2014 The resulting polylines will represent silhouettes.", "dataType": "Boolean"}]},
{"syntax": "TinTriangle_3d (in_tin, out_feature_class, {units}, {z_factor}, {hillshade}, {tag_field})", "name": "TIN Triangle (3D Analyst)", "description": "Exports triangle faces from a triangulated irregular network (TIN) dataset to a polygon feature class  and provides slope, aspect, and optional attributes of hillshade and tag value for each triangle.", "example": {"title": "TinTriangle example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.TinTriangle_3d ( \"tin\" , \"tin_triangle.shp\" , \"DEGREE\" , 1 , \"HILLSHADE 310,45\" , \"tag\" )"}, "usage": ["Slope and aspect calculations are based on the plane of the\r\ntriangle. Slope cannot be calculated properly if the linear unit of the TIN's coordinate system uses angular measures, like decimal degrees.", "XY and Z linear units should be in the same unit of measure in order for slope and hillshade calculations to provide accurate results. If the units differ but the TIN has its vertical and horizontal coordinate systems defined, an appropriate Z-factor is automatically determined. Otherwise, the ", "Z Factor", " parameter can be used to explicitly define the conversion factor to be applied on the elevation values.\r\n", "Aspect values are expressed in degrees and assume that North is 0\u00b0. The values increase clockwise and are recorded in the ", "Aspect", " field. An aspect value of -1 is assigned for any flat triangle in the TIN.", "Slope can be returned in degrees or percent, and the field name that the values are recorded in depends on the selection made in the ", "Slope Units", " parameter:", "Hillshade values reflect the localized relief that is produced from a light source that assumes the azimuth and vertical angle specified in the ", "Hillshade", " parameter.  0\u00b0 is assumed to be North for the azimuth, and the brightness value is expressed in between 0 to 255, where the lower the number, the darker the shade.", "Tag Value Field", " parameter will only be active if the TIN has tag values that were explicitly defined.\r\n"], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "units", "isOptional": true, "description": "The units of measure to be used in calculating slope. PERCENT \u2014 Slope is expressed as a percentage value. This is the default. DEGREE \u2014 Slope is expressed as the angle of inclination from a horizontal plane.", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}, {"name": "hillshade", "isOptional": false, "description": "Specifies the azimuth and altitude angles of the light source when applying a hillshade effect for the feature layer output. Azimuth can range from 0 to 360 degrees, whereas altitude can range from 0 to 90. An azimuth of 45 degrees and altitude of 30 degrees would be entered as \"HILLSHADE 45, 30\" . ", "dataType": "String"}, {"name": "tag_field", "isOptional": true, "description": "The field name in the output feature that will store the triangle tag value. This parameter is empty by default, which will result in tag values not being written to the output. ", "dataType": "String"}]},
{"syntax": "SurfaceContour_3d (in_surface, out_feature_class, interval, {base_contour}, {contour_field}, {contour_field_precision}, {index_interval}, {index_interval_field}, {z_factor}, {pyramid_level_resolution})", "name": "Surface Contour (3D Analyst)", "description": "Creates contour lines derived using a terrain or TIN surface.  \r\n Learn more about how Surface Contour works \r\n", "example": {"title": "SurfaceContour example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.SurfaceContour_3d ( \"sample.gdb/featuredataset/terrain\" , \"contour.shp\" , 10 )"}, "usage": ["The output feature class is 2D and contains an attribute with contour values.", "Use the interval and base contour options to tailor the extent and resolution of the output feature class.", "Use the out contour field data to convert the feature class to 3D.", "In certain instances, the last valid contour line may not be produced when creating contours using TIN surfaces. This is an algorithmic limitation common to computer contouring software. To ensure that all valid contours are generated, add a very small negative value to the ", "Base Contour", " field to slightly shift the data.", "The ", "Z factor", " parameter only affects results for rasters and TINs, not terrain datasets.     When working with terrain datasets you can  specify a contour interval that has the z factor built in to it. For example, if you want a one-foot contour interval and your terrain dataset surface is in meters, specify a contour interval of 0.3048.  You can also convert the terrain dataset to a raster or TIN using either the ", "Terrain To Raster", " or ", "Terrain To TIN", " geoprocessing tools."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input terrain or TIN dataset. ", "dataType": "Terrain Layer; TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "interval", "isOptional": false, "description": "The interval between the contours. ", "dataType": "Double"}, {"name": "base_contour", "isOptional": true, "description": "Along with the index interval, the base height is used to determine what contours are produced. The base height is a starting point from which the index interval is either added or subtracted. By default, the base contour is 0.0. ", "dataType": "Double"}, {"name": "contour_field", "isOptional": true, "description": "The field that stores the contour value associated with each line in the output feature class. ", "dataType": "String"}, {"name": "contour_field_precision", "isOptional": true, "description": "The precision of the contour field. Zero specifies an integer, and the numbers 1\u20139 indicate how many decimal places the field will contain. By default, the field will be an integer (0). ", "dataType": "Long"}, {"name": "index_interval", "isOptional": true, "description": "An optional value that specifies the difference in elevation between index contours. This value is typically five times larger than the contour interval. Use of this parameter adds an integer field defined by the Index Interval Field to the attribute table of the output feature class, where a value of 1 defines index contours. ", "dataType": "Double"}, {"name": "index_interval_field", "isOptional": true, "description": "The name of the field that specifies whether an isoline is an index contour. This will only be used if the Index Interval is defined. By default, the field name is Index . ", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "Specifies a factor by which to multiply the surface heights for converting Z units to match XY units. The Z Factor parameter only affects results for rasters and TINs, not terrain datasets. ", "dataType": "Double"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "InterpolatePolyToPatch_3d (in_surface, in_feature_class, out_feature_class, {max_strip_size}, {z_factor}, {area_field}, {surface_area_field}, {pyramid_level_resolution})", "name": "Interpolate Polygon To Multipatch (3D Analyst)", "description": "Creates surface-conforming  multipatch features from a polygon feature class using a raster, terrain, or TIN surface. Each polygon feature has its boundary profiled along the surface. Heights are obtained using linear interpolation by sampling at each input vertex and wherever the boundary line intersects surface triangle edges and nodes. This natural densification captures the full definition of the linear surface using a minimal number of samples. Then, all nodes that fall within the polygon are extracted. The nodes are retriangulated in a new memory-based TIN, and the 3D polygon boundary is enforced as a clip polygon. The triangles of this new TIN are then extracted in a series of strips that are used to define a multipatch-based feature. ", "example": {"title": "InterpolatePolygonToMultipatch example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.InterpolatePolyToPatch_3d ( \"sample.gdb/featuredataset/terrain\" , \"polygon.shp\" , \"out_multipatch.shp\" , 1024 , 1 , \"Area\" , \"SArea\" , 5 )"}, "usage": ["Resulting multipatch will capture the 3D surface representation in its geometry.\r\nPlanimetric and surface area calculations are included in the output alongside other attributes from the input polygon.", "Consider converting polygons to multipatches if you experience display problems with three-dimensional rendering of polygons draped on a surface.", "The ", "Maximum Triangle Strip Size", " value must be 3 or larger. This parameter specifies the maximum number of\r\nvertices allowed in any triangle strip used in constructing the multipatch. ArcGIS does not have a particular size limit\r\nor preference, but some 3D graphic cards might, as triangle strips are\r\ndirectly loaded to the 3D graphics application program interface\r\n(API) for rendering.\r\nThe recommended range is between 128 and 2048."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input triangulated irregular network (TIN) or terrain dataset surface. ", "dataType": "Terrain Layer; TIN Layer"}, {"name": "in_feature_class", "isInputFile": true, "isOptional": false, "description": "The input polygon feature class. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output multipatch feature class. ", "dataType": "Feature Class"}, {"name": "max_strip_size", "isOptional": true, "description": "Controls the maximum number of points used to create an individual triangle strip. Note that each multipatch is usually composed of multiple strips. The default value is 1,024. ", "dataType": "Long"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}, {"name": "area_field", "isOptional": true, "description": "The name of the output field containing the planimetric, or 2D, area of the resulting multipatches. ", "dataType": "String"}, {"name": "surface_area_field", "isOptional": true, "description": "The name of the output field containing the 3D area of the resulting multipatches. This area takes the surface undulations into consideration and is always larger than the planimetric area unless the surface is flat, in which case, the two are equal. ", "dataType": "String"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "AddTerrainPyramidLevel_3d (in_terrain, pyramid_level_definition, {pyramid_type})", "name": "Add Terrain Pyramid Level (3D Analyst)", "description": "Adds one or more pyramid levels to an existing terrain dataset. \r\n Learn more about how Terrain Pyramid Level Definition works \r\n", "example": {"title": "AddTerrainPyramidLevel example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.AddTerrainPyramidLevel_3d ( 'test.gdb/featuredataset/terrain' , 'WINDOWSIZE' , '2.5 10000; 5 25000; 10 50000' )"}, "usage": ["Each pyramid level is entered as a space-delimited pair of the pyramid level resolution and reference scale (for example, \"20 24000\" for a window size of 20 and reference scale of 1:24000, or \"1.5 10000\" for a z-tolerance of 1.5 and reference scale of 1:10000).  ", "The pyramid level resolution refers to the z-tolerance or window size value that will be used for the pyramid.", "The reference scale represents the largest map scale in which the pyramid level will be displayed.", "Adding a new pyramid level to a terrain invalidates it because the terrain needs to populate the pyramid with elevation points sampled from its preceding pyramid level. Use ", "Build Terrain", " after adding a pyramid level.", "When used in an SDE database, the input terrain cannot be registered as versioned."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "pyramid_level_definition", "isOptional": false, "description": "The z-tolerance or window size and its associated reference scale for each pyramid level being added to the terrain. Each pyramid level is entered as a space-delimited pair of the pyramid level resolution and reference scale (for example, \"20 24000\" for a window size of 20 and reference scale of 1:24000, or \"1.5 10000\" for a z-tolerance of 1.5 and reference scale of 1:10000). The pyramid level resolution can be provided as a floating-point value, while the reference scale must be entered as a whole number. The z-tolerance value represents the maximum deviation that can occur from the elevation of the terrain at full resolution, whereas the window size value defines the area of the terrain tile used in thinning elevation points by selecting one or two points from the area based on the window size method defined during the creation of the terrain. The reference scale represents the largest map scale at which the pyramid level is enforced. When the terrain is displayed at a scale larger than this value, the next highest pyramid level is displayed. ", "dataType": "String"}, {"name": "pyramid_type", "isOptional": true, "description": "The pyramid type used by the terrain dataset. This parameter is not used in ArcGIS 9.3 and beyond, as its purpose is to ensure backward-compatibility with scripts and models written using ArcGIS 9.2. ", "dataType": "String"}]},
{"syntax": "Minus_3d (in_raster_or_constant1, in_raster_or_constant2, out_raster)", "name": "Minus (3D Analyst)", "description": "Subtracts the value of the second input raster from the value of the first input raster on a cell-by-cell basis.", "example": {"title": "Minus example 1 (Python window)", "description": "This example subtracts the values of the second input raster from the first.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Minus_3d ( \"elevation\" , 100 , \"c:/output/outminus\" )"}, "usage": ["The order of inputs is relevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input from which to subtract the values in the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input values to subtract from the values in the first input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The cell values are the product of the first input multiplied by the second. ", "dataType": "Raster Dataset"}]},
{"syntax": "BuildTerrain_3d (in_terrain, {update_extent})", "name": "Build Terrain (3D Analyst)", "description": "Performs tasks required for analyzing and displaying a terrain dataset.", "example": {"title": "BuildTerrain example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.BuildTerrain_3d ( 'test.gdb/featuredataset/terrain' )"}, "usage": ["Use this tool after initially defining the features that participate in a terrain or making subsequent modifications to its schema or participating features.", "Edits to the terrain schema consist of the following modifications:", "It is more efficient to make a collection of edits followed by one build rather than running a build after each individual edit."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "update_extent", "isOptional": true, "description": " Update Extent is used to recalculate the data extent for a window size based terrain dataset when the data area has been reduced through editing. It is not needed if the data extent has increased or if the terrain dataset is z-tolerance based. It will scan through all the terrain data to determine the new extent. If the terrain is large it may be expensive in terms of I/O to run this option. NO_UPDATE_EXTENT \u2014 The extent of the terrain dataset will not be recalculated. This is the default. UPDATE_EXTENT \u2014 The extent of the terrain dataset will be recalculated.", "dataType": "String"}]},
{"syntax": "Lookup_3d (in_raster, lookup_field, out_raster)", "name": "Lookup (3D Analyst)", "description": "Creates a new raster by looking up values found in another field in the table of the input raster.", "example": {"title": "Lookup example 1 (Python window)", "description": "This example creates a new raster determined by the specified field of the input raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Lookup_3d ( \"mycity\" , \"land_code\" , \"C:/output/mylandcode.img\" )"}, "usage": ["Lookup", " supports fields that are  numeric (integer or floating point) or string.  If the field is  integer or string, the output will be an integer raster; otherwise, the output raster will be a floating-point raster.", "If the lookup field is of  integer type, the values of that field will be written to the output raster attribute table as Value. Other items in the input raster attribute table will not be transferred to the output raster attribute table.", "For example, an attribute table of input raster with numeric field ", "Attr1", ":", "Output attribute table from ", "Lookup", " on ", "Attr1", " field:", "If the lookup field is a string type, the lookup field will appear in the output raster attribute table, and the value field will be the same as for input raster. Any other items in the input raster's attribute table will not be transferred to the output raster's attribute table.", "For example, consider the attribute table of an input raster with string field ", "Text1", ":", "The attribute table of the output raster from running ", "Lookup", " on the ", "Text1", " field would be:"], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster that contains a field from which to create a new raster. ", "dataType": "Raster Layer"}, {"name": "lookup_field", "isOptional": false, "description": "Field containing the desired values for the new raster. It can be a numeric or string type. ", "dataType": "Field"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster whose values are determined by the specified field of the input raster. ", "dataType": "Raster Dataset"}]},
{"syntax": "Intersect3DLineWithMultiPatch_3d (in_line_features, in_multipatch_features, {join_attributes}, {out_point_feature_class}, {out_line_feature_class})", "name": "Intersect 3D Line With Multipatch (3D Analyst)", "description": "Determines and returns the number of geometric intersections between  line and multipatch features. Point features representing the intersection and line features representing the input lines divided at such points can optionally be written to output feature classes.", "example": {"title": "Intersect3DLineWithMultipatch example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.Intersect3DLineWithMultiPatch_3d ( 'inLine.shp' , 'inMultipatch.shp' , 'IDS_ONLY' , 'outPts.shp' , 'outLine.shp' )"}, "usage": ["2D line features are not supported by this tool. Z-enabled 3D line features are required because intersection calculations are performed on 3D features in 3D Euclidian space. 2D line features with height definitions stored in an attribute field can be converted to 3D by using ", "Feature To 3D By Attribute", ".", "The intersection count will be returned as an integer result in the message window and can be used in a model or script to establish preconditions for subsequent operations.", "The optional point output represents points of intersection between the input line and multipatch features and contains the following attributes:", "The optional line output divides input line features at the points of intersection and contains the following attributes:", "If\r\na line does not intersect with a multipatch, it is directly copied to\r\nthe output and its ", "FROM_MP_ID", " and ", "TO_MP_ID", " fields will be attributed with a value of -1.", "Attribute values from the original lines can be referenced  in the optional output features through the use of output line feature class through the ", "Join Attributes", " parameter.", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them."], "parameters": [{"name": "in_line_features", "isInputFile": true, "isOptional": false, "description": "The line features that will be intersected with multipatch features.. ", "dataType": "Feature Layer"}, {"name": "in_multipatch_features", "isInputFile": true, "isOptional": false, "description": "The multipatch features that the lines will be intersected against. ", "dataType": "Feature Layer"}, {"name": "join_attributes", "isOptional": true, "description": "Defines whether attributes from the input line features will be preserved in the output line feature class. IDS_ONLY \u2014 No attributes from the input line features will be written to the output line feature class. This is the default. ALL \u2014 All attributes from the input line features will be written to the output line feature class.", "dataType": "String"}, {"name": "out_point_feature_class", "isOutputFile": true, "isOptional": true, "description": "Optional features that represent points of intersection between the 3D line and multipatch. ", "dataType": "Feature Class"}, {"name": "out_line_feature_class", "isOutputFile": true, "isOptional": true, "description": " Optional line features that divide input 3D lines at points of intersection with multipatch features. ", "dataType": "Feature Class"}]},
{"syntax": "EditTin_3d (in_tin, in_features, {constrained_delaunay})", "name": "Edit TIN (3D Analyst)", "description": "Adds  features from one or more input feature classes  that define the surface area of a triangulated irregular network (TIN). \r\n Learn more about how Edit TIN works \r\n", "example": {"title": "EditTIN example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.ddd.EditTin ( \"my_tin\" , \"clip_polygon.shp <None> <None> hardclip false; \" \"new_points.shp Shape <None> masspoints true\" , \"Delaunay\" )"}, "usage": ["Multiple feature classes can be added at the same time.", "You can resize the Input Feature Class properties dialog box to increase its width in case the default column width is too small."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "in_features", "isInputFile": true, "isOptional": false, "description": "Specify features that will be included in the TIN. Each input can have the following properties set to define the manner in which its features contribute to the surface: height_field \u2014The field supplying elevation values for the features. Valid options include any numeric field and the Shape field, if the feature geometry supports Z-values. The <None> keyword is also available for features that do not have any qualifying fields or do not require elevation values. Z-less features have their values interpolated from the surrounding surface. tag_value \u2014The fill polygons that assign integer values to triangles as a basic form of attribution. Their boundaries are enforced in the triangulation as breaklines. Triangles inside these polygons are attributed with the tag values. Specify the name of the feature class attribute to use as tag values in the TIN. If tag values are not to be used, specify <none>. SF_type \u2014The surface feature type that defines how the feature geometry gets incorporated into the triangulation for the surface. Point features can only be used as masspoints, whereas line features can be defined as breaklines, and polygons can be defined as the masspoints option, whereas lines can be defined as clip, erase, replace, and value fill features. Breaklines and polygon surface types have 'hard' and 'soft' qualifiers that indicate whether the features represent smooth or sharp discontinuties on the surface. use_z \u2014Specifies whether Z values are used when the SHAPE field of the input feature is indicated as the height source. Selecting true will result in Z values being used, and false will result in M, or measure values, being used. Z's are used by default. ", "dataType": "Value Table"}, {"name": "constrained_delaunay", "isOptional": true, "description": "A constrained Delaunay triangulation conforms to Delaunay rules everywhere except along breaklines. When using conforming Delaunay triangulation, breaklines are densified by the software; therefore, one input breakline segment can result in multiple triangle edges. When using constrained Delaunay triangulation no densication occurs and each breakline segment is added as a single edge. DELAUNAY \u2014 The triangulation will fully conform to the Delaunay rules. This is the default. CONSTRAINED_DELAUNAY \u2014 The Delaunay triangulation will be constrained.", "dataType": "Boolean"}]},
{"syntax": "SkylineGraph_3d (in_observer_point_features, in_line_features, {base_visibility_angle}, {additional_fields}, {out_angles_table}, {out_graph})", "name": "Skyline Graph (3D Analyst)", "description": "Calculates sky visibility and generates an optional table and polar graph. The table and graph represent the horizontal and vertical angles going from the observer point to each of the vertices on the skyline. ", "example": {"title": "SkylineGraph example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.SkylineGraph_3d ( \"observers.shp\" , \"skyline_outline.shp\" , 0 , \"ADDITIONAL_FIELDS\" , \"table.dbf\" )"}, "usage": ["The fields that will always appear in the table are:", "The additional fields are:", " The additional fields are not necessary for generating the graph.", "The arithmetic horizontal angle is equal to 90 minus the azimuth, and the zenith angle is 90 minus the vertical angle. (An arithmetic horizontal angle of 0 is due east, and 90 is due north; a zenith angle of 90 is horizontal, and 0 is straight up.)", "The graph option will only display a graph with an ArcGIS Desktop installation."], "parameters": [{"name": "in_observer_point_features", "isInputFile": true, "isOptional": false, "description": " The input features containing one or more observer points. ", "dataType": "Feature Layer"}, {"name": "in_line_features", "isInputFile": true, "isOptional": false, "description": "The line feature class representing the skyline. ", "dataType": "Feature Layer"}, {"name": "base_visibility_angle", "isOptional": true, "description": "The vertical angle which is used as the baseline for calculating percentage of visible sky; 0 is the horizon, 90 is straight up; -90 is straight down. The default is 0. ", "dataType": "Double"}, {"name": "additional_fields", "isOptional": true, "description": "Determines whether to output additional fields to the table, rather than just the two angle values. NO_ ADDITIONAL_FIELDS \u2014 The additional fields will not be output. This is the default. ADDITIONAL_FIELDS \u2014 The additional fields will be output.", "dataType": "Boolean"}, {"name": "out_angles_table", "isOutputFile": true, "isOptional": true, "description": "The table to be created for outputting the angles. The default is blank, meaning no table. ", "dataType": "Table"}, {"name": "out_graph", "isOutputFile": true, "isOptional": true, "description": "The name of the optional graph. A table has to be generated in order for the graph to be made. The graph will be displayed and can be saved and/or edited. The default is blank, meaning no graph. ", "dataType": "Graph"}]},
{"syntax": "ChangeLasClassCodes_3d (in_las_dataset, class_codes, {compute_stats})", "name": "Change LAS Class Codes (3D Analyst)", "description": "Modifies the class code values of LAS files referenced by a LAS dataset.", "example": {"title": "ChangeLasClassCodes example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.ChangeLasClassCodes_3d ( 'test.lasd' , [[ 5 , 2 ], [ 3 , 1 ], [ 4 , 6 ]], 'COMPUTE_STATS' )"}, "usage": ["\r\nAll LAS files referenced by the LAS dataset\r\nwill be evaluated by this tool, but only the files that contain the provided class codes will be modified.", "Consider using this tool to update the classification of data points in LAS files generated prior to the introduction of  standardized class codes in the LAS 1.1 specifications.  ", "LAS points can be classified into a number of categories that describe the material encountered by the lidar return, such as  ground, building, or water. The American Society for Photogrammetry and Remote Sensing (ASPRS) defined the following class codes for LAS file versions  1.1, 1.2, and 1.3:", "Classification Value ", "Classification Type", "0", "Never Classified", "1", "Unassigned", "2", "Ground", "3", "Low Vegetation", "4", "Medium Vegetation", "5", "High Vegetation", "6", "Building", "7", "Noise", "8", "Model Key", "9", "Water", "10", "Reserved for ASPRS Definition", "11", "Reserved for ASPRS Definition", "12", "Overlap", "13\u201331", "Reserved for ASPRS Definition"], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "class_codes", "isOptional": false, "description": "Specify each pair of current and new class code values as a space-delimited string or a list of integers. For example, a current class code of 5 can be changed to 2 by specifying \"5 2\" or [5, 2] . Multiple class codes can be entered as a semicolon-delimited string ( \"5 2; 8 3; 1 4\" ), a list of strings (for example, [[5, 2], [8, 3], [1, 4]] ). ", "dataType": "Value Table"}, {"name": "compute_stats", "isOptional": true, "description": " Specifies whether statistics should be computed for the LAS files referenced by the LAS dataset. The presence of statistics allows the LAS dataset layer's filtering and symbology options to only show LAS attribute values that exist in the LAS files. COMPUTE_STATS \u2014 Statistics will be computed. NO_COMPUTE_STATS \u2014 Statistics will not be computed. This is the default.", "dataType": "Boolean"}]},
{"syntax": "SetLasClassCodesUsingFeatures_3d (in_las_dataset, feature_class, {compute_stats})", "name": "Set LAS Class Codes Using Features (3D Analyst)", "description": "Classifies data points in LAS files referenced by a LAS dataset using point, line, and polygon features.", "example": {"title": "SetLasClassCodesUsingFeatures example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.SetLasClassCodesUsingFeatures_3d ( \"test.lasd\" , [[ \"lake.shp 0 9\" ], [ \"outliers.shp\" , 5 , \"NO_CHANGE\" , \"NO_CHANGE\" , \"NO_CHANGE\" , \"SET\" ]], \"COMPUTE_STATS\" )"}, "usage": ["LAS points can be classified into a number of categories that describe the material encountered by the lidar return, such as  ground, building, or water. The American Society for Photogrammetry and Remote Sensing (ASPRS) defined the following class codes for LAS file versions  1.1, 1.2, and 1.3:", "Classification Value ", "Classification Type", "0", "Never Classified", "1", "Unassigned", "2", "Ground", "3", "Low Vegetation", "4", "Medium Vegetation", "5", "High Vegetation", "6", "Building", "7", "Noise", "8", "Model Key", "9", "Water", "10", "Reserved for ASPRS Definition", "11", "Reserved for ASPRS Definition", "12", "Overlap", "13\u201331", "Reserved for ASPRS Definition", "While the LAS 1.0 specifications provide class codes ranging  from 0 to 255, it does not have a standardized classification scheme. Any class codes used in 1.0 files would typically be defined by the data vendor and provided through auxiliary information.", "The LAS dataset layer can be used to filter LAS points by class code or return values. The layer can be created by using the ", "Make LAS Dataset Layer", " tool, or by loading the LAS dataset in ArcMap or ArcScene and specifying the desired class codes and return values through the layer properties dialog box.", "LAS data points that fall within the 2-dimensional area of the buffer specified for the input features will be classified.", "\r\nConsider using the points obtained from ", "Locate Outliers", "\r\nto classify LAS points as Noise."], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "feature_class", "isOptional": false, "description": " Specify one or more feature classes that will be used to define class code values for the lidar files referenced by a LAS dataset. Each feature will have the following options that can be specified: features \u2014The feature layer or full path to the input feature class. buffer_distance \u2014The selection tolerance used in determining which lidar points will be modified by the input features. new_class \u2014The class code to be assigned to the lidar files that intersect with the features and the associated buffer distance. synthetic \u2014Specifies whether to flag or remove a Synthetic designation, which implies that the point was not created with lidar, but an alternate technique (for example, digitized from photogrammetric stereo model). key_point \u2014Specifies whether to flag or remove a Key Point designation for the data point. A model key point is typically treated as an anchor point that does not get removed by any thinning algorithm. withheld \u2014Specifies whether to flag or remove a Withheld designation for the data point, which is generally used to signify erroneous data.", "dataType": "Value Table"}, {"name": "compute_stats", "isOptional": true, "description": " Specifies whether statistics should be computed for the LAS files referenced by the LAS dataset. The presence of statistics allows the LAS dataset layer's filtering and symbology options to only show LAS attribute values that exist in the LAS files. COMPUTE_STATS \u2014 Statistics will be computed. NO_COMPUTE_STATS \u2014 Statistics will not be computed. This is the default.", "dataType": "Boolean"}]},
{"syntax": "SunShadowVolume_3d (in_features, start_date_and_time, out_feature_class, {adjusted_for_dst}, {time_zone}, {end_date_and_time}, {iteration_interval}, {iteration_unit})", "name": "Sun Shadow Volume (3D Analyst)", "description": "\r\nCreates a feature class that models shadows cast by each input feature using sunlight for a given date and time.", "example": {"title": "SunShadowVolume example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.SunShadowVolume_3d ([ 'sample.fgdb/buildings_1' , 'buildings_2.shp' ], '12/25/2011 10:00 AM' , 'shadows_dec25.shp' , 'ADJUSTED_FOR_DST' , 'Eastern_Standard_Time' , '12/25/2011 3:00 PM' , 'HOURS' , 1 )"}, "usage": ["Polygon and line features can be used as input if they are extruded 3D layers. Extrusion properties can be applied to a feature layer in ArcScene or ArcGlobe, and have the effect of transforming the feature into a multipatch. ", "Learn more about using extrusion as 3D symbology", ".", "Sunrise and sunset shadow calculations can be made by providing only a date in the ", "Start Date and Time", " and ", "End Date and Time", " parameters respectively. This can be particularly useful when desiring shadow volume calculations for all daylight hours on a given time interval.", "All input features should reside in the same locale, as relative sun position calculations are based on the position of the first feature in the first feature class and are accurate to 0.01 degrees. ", "Shadow volumes will not be created if the sun is not visible for a given date and time or if the relative position of the sun is at a vertical angle of 90 degrees from the input features.", " Shadow volumes are\r\ncreated as a closed multipatch by extruding the input features in the direction of the light from the sun. Light rays are considered to be parallel and travel in a direction specified in the attributes of the input feature.", "The shadow volume always starts from a vertical plane and ends\r\nat a vertical plane. Both planes are perpendicular to the\r\nhorizontal project of the direction of the sun's rays.", "The output shadow volume following fields will be added to the output feature class\r\nwhich contains the shadow volumes:", "Typically, each shadow volume is created such that it appears to hug, or be cast tightly against, its originating feature. In the event that at least one  shadow volume does not do so, a HUGS_FEATR field will be added to the resulting table."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The multipatch features that will be used to model shadows. Polygon and line features can also be used if they are added as an extruded 3D layer. ", "dataType": "Feature Layer"}, {"name": "start_date_and_time", "isOptional": false, "description": " The date and time that the trajectory of sunlight will be calculated for modeling the shadows. ", "dataType": "Date"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The multipatch feature class that will store the resulting shadow volumes. ", "dataType": "Feature Class"}, {"name": "adjusted_for_dst", "isOptional": true, "description": "Specifies if time value is adjusted for Daylight Savings Time (DST). ADJUSTED_FOR_DST \u2014 DST is observed. NOT_ADJUSTED_FOR_DST \u2014 DST is not observed. This is the default.", "dataType": "Boolean"}, {"name": "time_zone", "isOptional": true, "description": " The time zone in which the participating input is located. The default setting is the time zone to which the operating system is set. ", "dataType": "String"}, {"name": "end_date_and_time", "isOptional": true, "description": " The final date and time for calculating sun position. If only a date is provided, the final time is presumed to be sunset. ", "dataType": "Date"}, {"name": "iteration_interval", "isOptional": true, "description": " The value used to define the iteration of time from the start date. ", "dataType": "Double"}, {"name": "iteration_unit", "isOptional": true, "description": " The unit that defines the iteration value applied to the Start Date and Time . DAYS \u2014 Iteration value will represent days. This is the default. HOURS \u2014 Iteration value will represent one or more hours. MINUTES \u2014 Iteration value will represent one or more minutes", "dataType": "String"}]},
{"syntax": "LasDatasetToTin_3d (in_las_dataset, out_tin, {thinning_type}, {thinning_method}, {thinning_value}, {max_nodes}, {z_factor})", "name": "LAS Dataset To TIN (3D Analyst)", "description": "Exports a triangulated irregular network (TIN) from a  LAS dataset.", "example": {"title": "LasDatasetToTin example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.LasDatasetToTin_3d ( 'se_baltimore.lasd' , 'se_bmore' , 'RANDOM' , 15 , 3.28 )"}, "usage": ["When a LAS dataset is specified as input, all the data points in the LAS files it references will be processed.", "The LAS dataset layer can be used to filter LAS points by class code or return values. The layer can be created by using the ", "Make LAS Dataset Layer", " tool, or by loading the LAS dataset in ArcMap or ArcScene and specifying the desired class codes and return values through the layer properties dialog box.", "The ", "MINIMUM", ", ", "MAXIMUM", ", and ", "AVERAGE", " point selection options utilize an automatically calculated window size area that is designed to produce a TIN dataset whose number of nodes can be adequately handled by the system.\r\n"], "parameters": [{"name": "in_las_dataset", "isInputFile": true, "isOptional": false, "description": " The input LAS dataset. ", "dataType": "LAS Dataset Layer"}, {"name": "out_tin", "isOutputFile": true, "isOptional": false, "description": "The output TIN dataset. ", "dataType": "TIN"}, {"name": "thinning_type", "isOptional": true, "description": "The type of thinning used for reducing the LAS data points saved as the nodes in the resulting TIN. NONE \u2014No thinning is applied, and no thinning method or thinning value is required. This is the default. RANDOM \u2014Randomly selects LAS data points based on the corresponding Thinning Method selection and Thinning Value entry. WINDOW_SIZE \u2014Reduces LAS data points by evaluating each square area defined by the Thinning Value and selecting LAS points using the Thinning Method .", "dataType": "String"}, {"name": "thinning_method", "isOptional": true, "description": " The thinning method defines the specific technique used for reducing the LAS data points, and impacts the way the Thinning Value gets interpretted. The available options depend on the selected Thinning Type . For RANDOM : For WINDOW_SIZE : PERCENT \u2014 Thinning value will reflect a percentage of nodes in the full resolution of the LAS dataset. NODE_COUNT \u2014 Thinning value will reflect the total number of nodes that are allowed in the output. MINIMUM \u2014 Selects LAS data point with the lowest elevation in each of the automatically determined window size areas. MAXIMUM \u2014 Selects LAS data point with the highest elevation in each of the automatically determined window size areas. CLOSEST_TO_MEAN \u2014 Selects LAS data point whose elevation is closest to the average value found in the automatically determined window size areas.", "dataType": "String"}, {"name": "thinning_value", "isOptional": true, "description": "The value associated with the selected Thinning Type and Thinning Method . For thinning methods available with the RANDOM point selection method: For any WINDOW_SIZE thinning methods, the value represents the area that the extent of the LAS dataset is divided into for sampling data points. PERCENT \u2014value will represent percentage of data points from the full resolution of the LAS dataset. NODE_COUNT \u2014value will represent the total number of nodes allowed in the output TIN.", "dataType": "Double"}, {"name": "max_nodes", "isOptional": true, "description": " The maximum number of nodes permitted in the output TIN. The default is 5 million. ", "dataType": "Double"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}]},
{"syntax": "Intersect3DLineWithSurface_3d (in_surfaces, in_line_features, out_line_feature_class, {out_point_feature_class})", "name": "Intersect 3D Line With Surface (3D Analyst)", "description": "\r\nComputes the geometric intersection of 3D line features and one or more surfaces to return the intersection as segmented line features and points.", "example": {"title": "Intersect3DLineWithSurface example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.Intersect3DLineWithSurface_3d ( 'lines.shp' , 'dtm_tin; elev.tif' , 'intersect_lines.shp' , 'intersect_pts.shp' )"}, "usage": ["\r\nPoints (of intersection) and/or lines (resulting from input lines being broken at intersection points) can optionally be written to output feature classes.\r\n\r\n", "All of the input line features will be in the output line feature class even if they do not intersect a surface.", "The following fields are present in the output line\r\nfeature class:", "The following fields are present in the optional output point\r\nfeature class:"], "parameters": [{"name": "in_surfaces", "isInputFile": true, "isOptional": false, "description": "Specify one or more input raster or TIN surfaces to construct the geometric intersections. ", "dataType": "Raster Layer; TIN Layer"}, {"name": "in_line_features", "isInputFile": true, "isOptional": false, "description": " The input 3D line features. ", "dataType": "Feature Layer"}, {"name": "out_line_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output line feature class that will contain a copy of the input lines split at the points of intersection. ", "dataType": "Feature Class"}, {"name": "out_point_feature_class", "isOutputFile": true, "isOptional": true, "description": "The optional point feature class that will contain the points of intersection. ", "dataType": "Feature Class"}]},
{"syntax": "EncloseMultipatch_3d (in_features, out_feature_class, {grid_size})", "name": "Enclose Multipatch (3D Analyst)", "description": "\r\nCreates  closed multipatch features in the output feature class using the features of the input multipatch.\r\n", "example": {"title": "EncloseMultipatch example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.EncloseMultiPatch_3d ( 'unclosed_features.shp' , 'enclosed_features.shp' , 0.5 )"}, "usage": ["\r\n\r\nUse this tool for multipatch features that were designed to be closed but have geometric gaps that keep them open, such as buildings.", "All multipatch features will be evaluated to determine which ones are closed. Features that are already closed will be copied to the output multipatch. ", "The ", "Is Closed 3D", " tool can be used to determine whether a multipatch feature class contains any unclosed features.", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The multipatch features that will be used to construct closed multipatches. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output closed multipatch features. ", "dataType": "Feature Class"}, {"name": "grid_size", "isOptional": true, "description": " The resolution used to construct the closed multipatch features. This value is defined using the linear units of the input feature's spatial reference. ", "dataType": "Double"}]},
{"syntax": "StackProfile_3d (in_line_features, profile_targets, out_table, {out_graph})", "name": "Stack Profile (3D Analyst)", "description": "Creates a table and optional graph denoting the  profile of line features over one or more multipatch, raster, TIN, or terrain surfaces.", "example": {"title": "StackProfile example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.StackProfile_3d ( 'profile_line.shp' , 'dem.tif; buildings.shp' , 'profile_values.dbf' , 'Surface Profile' )"}, "usage": ["If the ", "Output Graph Name", " parameter is populated and the tool is executed in ArcMap, ArcScene, or ArcGlobe, the resulting graph will be displayed on-screen.", "The graph will be stored in memory, but can be saved as a graph file using the ", "Save Graph", " tool.", "The output table provides the information needed for generating the profile graph. Each line feature is densified along its overlapping profile targets in a manner that captures the profile's characteristics by introducing new vertices along the line.  The elevation and distance along the input lines created by this densification gets stored in the output table along with the additional information about the line features  and profile targets. The values in its fields can be used to create graphs in a wide variety of external applications. The fields represent:"], "parameters": [{"name": "in_line_features", "isInputFile": true, "isOptional": false, "description": " The line features that will be profiled over the surface inputs. ", "dataType": "Feature Layer"}, {"name": "profile_targets", "isOptional": false, "description": " The one or more multipatch features and raster, terrain, or TIN datasets that contribute to the surface model being profiled. ", "dataType": "Feature Layer; Raster Layer; Terrain Layer; TIN Layer"}, {"name": "out_table", "isOutputFile": true, "isOptional": false, "description": "The output table containing the values obtained from interpolating the input line features over the stack targets. ", "dataType": "Table"}, {"name": "out_graph", "isOutputFile": true, "isOptional": true, "description": " The in-memory name of the optional output graph that can be saved to disk with the Save Graph tool. ", "dataType": "Graph"}]},
{"syntax": "Buffer3D_3d (in_features, out_feature_class, buffer_distance_or_field, {buffer_joint_type}, {buffer_quality}, {simplification_tolerance})", "name": "Buffer 3D (3D Analyst)", "description": "\r\n\r\nCreates a 3D buffer around point or line features.", "example": {"title": "Buffer3D example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.Buffer3D_3d ( 'lineFC.shp' , 'buffer3d.shp' , '15 Meters' , 'Round' , 30 , '1 Meters' )"}, "usage": ["Generates spheres for point inputs and cylindrical features for line inputs.", "Outputs closed multipatch features which can be used for volumetric computations, including those provided by the 3D set operator tools. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them.", ".", "Increasing ", "Buffer Quality", " produces smoother 3D features at the expense of extending processing time.", "Consider specifying a ", "Simplification", " value to improve performance with complex line features, such as curved lines with a large number of vertices.", "This tool may not be able to generate a closed multipatch for certain line features due to the geometry of the line and the buffer distance being used. Avoid buffer distances that may cause overlapping buffers within the same feature, and consider specifying a smaller buffer distance for features that fail to generate an output."], "parameters": [{"name": "in_features", "isInputFile": true, "isOptional": false, "description": " The line or point features to be buffered. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output multipatch features that represent the 3D buffers. ", "dataType": "Feature Class"}, {"name": "buffer_distance_or_field", "isOptional": false, "description": "The distance of the buffer around the input features, which can be provided as either a linear distance or be derived from a numeric field in the input feature's attribute table. If the Distance linear units are not specified or are entered as Unknown, the linear unit of the input features' spatial reference is used. ", "dataType": "Linear Unit; Field"}, {"name": "buffer_joint_type", "isOptional": true, "description": "The shape of the buffer between the vertices of the line segments. This parameter is only valid for input line features. STRAIGHT \u2014 The shape of connections between vertices will be straight. This is the default. ROUND \u2014 The shape of connections between vertices will be round.", "dataType": "String"}, {"name": "buffer_quality", "isOptional": true, "description": "The number of segments used to represent the resulting multipatch features. The default is 20, but any number between the range of 6 to 60 can be entered. ", "dataType": "Long"}, {"name": "simplification_tolerance", "isOptional": true, "description": "Simplification eliminates vertices from the input lines by maintaining critical vertex points that define the shape of the original lines within the maximum allowable offset. By default, simplification will not take place unless a tolerance value is specified. The simplification tolerance can be defined as a string containing the numeric value and its desired linear unit of measure (for example, 1.5 Meters) or a numeric value without an associated unit of measure, in which case, it would default to the linear unit of the input's horizontal spatial reference. ", "dataType": "Linear Unit"}]},
{"syntax": "Difference3D_3d (in_features_minuend, in_features_subtrahend, out_feature_class, {out_table})", "name": "Difference 3D (3D Analyst)", "description": "Eliminates portions  of multipatch features in a target feature class that overlap with enclosed volumes of multipatch features in the subtraction feature class.", "example": {"title": "Difference3D example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.Difference3D_3d ( 'input_mp.shp' , 'erase_mp.shp' , 'difference_mp.shp' )"}, "usage": ["Closed multipatch geometry is required for this analysis. The ", "Is Closed 3D", " tool can be used to determine if a multipatch feature class contains closed features, and the ", "Enclose Multipatch", " tool can be used to eliminate gaps in multipatch features.", "Input features that are fully enclosed by a subtract feature will be completely removed in the output", "This tool's execution can be very time consuming, and care should be taken when selecting input datasets.", "The output features will not have any of the attributes of the\r\ninput features.\r\nIf necessary, a spatial join to the source features or a relationship class to the optional output table can be constructed to map attributes from the input features to the output dataset.", "This tool is a 3D set operator that provides analytical functions on 3D features. See ", "Working with 3D set operators", " for more information on what set operators are and how to use them."], "parameters": [{"name": "in_features_minuend", "isInputFile": true, "isOptional": false, "description": "The multipatch features that will have its features removed by the subtrahend features. ", "dataType": "Feature Layer"}, {"name": "in_features_subtrahend", "isInputFile": true, "isOptional": false, "description": "The multipatch features that will be subtracted from the input. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": " The output multipatch feature class that will contain the resulting features. ", "dataType": "Feature Class"}, {"name": "out_table", "isOutputFile": true, "isOptional": true, "description": " An optional table that stores information about the relationship between the input features and the difference output. The following fields are present in this table: Output_ID\u2014The unique ID of the output feature class. Minuend_ID\u2014The unique ID of the primary multipatch. Subtrahend\u2014The unique ID of the multipatch feature that was subtracted from the primary multipatch.", "dataType": "Table"}]},
{"syntax": "LASToMultipoint_3d (input, out_feature_class, average_point_spacing, {class_code}, {return}, {attribute}, {input_coordinate_system}, {file_suffix}, {z_factor}, {folder_recursion})", "name": "LAS To Multipoint (3D Analyst)", "description": "Creates multipoint features using  one or more lidar files.", "example": {"title": "LASToMultipoint example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.LASToMultipoint_3d ( \"001.las\" , \"Test.gdb/feature_dataset/sample_1\" , 1.5 , \"2\" , \"ANY_RETURNS\" , \"INTENSITY\" , \"Coordinate Systems\" \"/Projected Coordinate Systems/UTM/NAD 1983/NAD 1983 \" \"UTM Zone 17N.prj\" , \"las\" , 1 )"}, "usage": ["Supported LAS file versions are 1.0, 1.1, 1.2, and 1.3.", "LAS points can be classified into a number of categories that describe the material encountered by the lidar return, such as  ground, building, or water. The American Society for Photogrammetry and Remote Sensing (ASPRS) defined the following class codes for LAS file versions  1.1, 1.2, and 1.3:", "Classification Value ", "Classification Type", "0", "Never Classified", "1", "Unassigned", "2", "Ground", "3", "Low Vegetation", "4", "Medium Vegetation", "5", "High Vegetation", "6", "Building", "7", "Noise", "8", "Model Key", "9", "Water", "10", "Reserved for ASPRS Definition", "11", "Reserved for ASPRS Definition", "12", "Overlap", "13\u201331", "Reserved for ASPRS Definition", "While the LAS 1.0 specifications provide class codes ranging  from 0 to 255, it does not have a standardized classification scheme. Any class codes used in 1.0 files would typically be defined by the data vendor and provided through auxiliary information.", "If you are not interested in importing points based on their return number, or if all returns specified in the file or files are set to 0 because the points have been filtered or classified, select ANY_RETURN.", "When loading multiple LAS attributes into an Oracle database, you'll need to make sure all DBTUNE keywords for parameter attribute_binary are set to use binary large objects (BLOBs), not LONGRAW. This is because LAS attributes are loaded as BLOBs, and Oracle does not support multiple BLOBs in LONGRAW tables. See your Oracle database administrator for assistance."], "parameters": [{"name": "input", "isOptional": false, "description": "One or more files or folders with data in the LAS version 1.0, 1.1, and 1.2 format. The LAS format is the industry standard for lidar data. ", "dataType": "Folder or File"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "average_point_spacing", "isOptional": false, "description": "The average 2D distance between points in the input file or files. This can be an approximation. If areas have been sampled at different densities, specify the smaller spacing. The value needs to be provided in the projection units of the output coordinate system. ", "dataType": "Double"}, {"name": "class_code", "isOptional": false, "description": "The classification codes to use as a query filter for LAS data points. Valid values range from 1 to 32. No filter is applied by default. ", "dataType": "Long"}, {"name": "return", "isOptional": false, "description": "The return values used as a query filter. Valid return values are 1\u20135, LAST_RETURNS, and ANY_RETURNS. The default is ANY_RETURNS. ", "dataType": "String"}, {"name": "attribute", "isOptional": false, "description": "One or more LAS attributes to load and store and, optionally, the field names to use. The default is none. Supported attribute keywords are INTENSITY, RETURN_NUMBER, NUMBER_OF_RETURNS, SCAN_DIRECTION_FLAG, EDGE_OF_FLIGHTLINE, CLASSIFICATION, SCAN_ANGLE_RANK, FILE_MARKER, USER_BIT_FIELD, and GPS_TIME. ", "dataType": "Value Table"}, {"name": "input_coordinate_system", "isInputFile": true, "isOptional": true, "description": "The coordinate system of the input LAS file. This defaults to that specified in the LAS file. If for some reason it's not defined in the file but you know what it is, specify it here. ", "dataType": "Coordinate System"}, {"name": "file_suffix", "isOptional": true, "description": "The suffix of the files to import from an input folder. This parameter is required when a folder is specified as input. ", "dataType": "String"}, {"name": "z_factor", "isOptional": true, "description": "The factor by which elevation values will be multiplied. This is typically used to convert Z linear units that match those of the XY linear units. The default is 1, which leaves elevation values unchanged. ", "dataType": "Double"}, {"name": "folder_recursion", "isOptional": true, "description": "Scans through subfolders when an input folder is selected containing data in a subfolders directory. The output feature class will be generated with a row for each file encountered in the directory structure. NO_RECURSION \u2014 Only LAS files found in an input folder will be converted to multipoint features. This is the default. RECURSION \u2014 All LAS files residing in the subdirectories of an input folder will be converted to multipoint features.", "dataType": "Boolean"}]},
{"syntax": "TerrainToPoints_3d (in_terrain, out_feature_class, {pyramid_level_resolution}, {source_embedded_feature_class}, {out_geometry_type})", "name": "Terrain To Points (3D Analyst)", "description": "Converts a terrain dataset into a new point or multipoint feature class.", "example": {"title": "TerrainToPoints example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.TerrainToPoints_3d ( \"sample.gdb/featuredataset/terrain\" , \"terrain_points.shp\" , \"6\" , \"<NONE>\" , \"POINT\" )"}, "usage": ["If an embedded feature contains lidar attributes, such as RGB, classification, or return values, the attributes will be written to the output feature class. However, the way the attributes are written will depend on the geometry type that is specified:", "For more information on embedded features, read ", "Embedded feature classes", "."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}, {"name": "source_embedded_feature_class", "isOptional": true, "description": "The name of the terrain dataset's embedded points to be exported. If an embedded feature is specified, only the points from the feature will be written to the output. Otherwise, all points from all data sources in the terrain will be exported. ", "dataType": "String"}, {"name": "out_geometry_type", "isOutputFile": true, "isOptional": true, "description": "The geometry type of the output feature class. MULTIPOINT \u2014 The output point features will be written to a multipoint feature class. This is the default. POINT \u2014 The output point features will be written to a point feature class. ", "dataType": "String"}]},
{"syntax": "TinLine_3d (in_tin, out_feature_class, {code_field})", "name": "TIN Line (3D Analyst)", "description": "Exports breaklines from a triangulated irregular network (TIN) dataset to a 3D line feature class.", "example": {"title": "TinLine example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.TinLine_3d ( 'tin' , 'tin_line.shp' )"}, "usage": ["The output lines are attributed with integer values that identify each breakline's type. These codes are stored in a field whose name is defined by the ", "Code Field", " parameter, and the meaning of the values are defined below:", "The TIN must have breaklines for this tool to produce  line features.  If triangle edges are desired regardless of the breaklines, consider using the ", "TIN Edge", " tool."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "code_field", "isOptional": true, "description": "The name of the field in the output feature class that defines the breakline type. The default field name is Code . ", "dataType": "String"}]},
{"syntax": "Import3DFiles_3d (in_files, out_featureClass, {root_per_feature}, {spatial_reference}, {y_is_up}, {file_suffix}, {in_featureClass}, {symbol_field})", "name": "Import 3D Files (3D Analyst)", "description": "Imports one or more 3D models into a multipatch feature class.", "example": {"title": "Import3DFiles example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.Import3DFiles_3d ( \"AddisSheraton.skp\" , \"Test.gdb/AddisSheraton\" , False , \"\" , False )"}, "usage": ["The 3D Studio Max (", "*.3ds", "), VRML and GeoVRML 2.0 (", "*.wrl", "), SketchUp 6.0 (", "*.skp", "), OpenFlight 15.8 (", "*.flt", "), COLLADA (*.dae), and billboards (PNG, JPEG, BMP, TIFF, GIF, and so on) formats are supported.", "Preserve textured images in the 3D models by storing the output multipatch in a geodatabase. Shapefiles do not support the retention of textures.", "If the top side of the resulting multipatch features are oriented sideways, try adjusting the orientation by using this tool again with  the ", "Y_Is_Up", " parameter enabled.", "GeoVRML is the only format that has a defined coordinate system. Many formats tend to be generated using local coordinate systems that center the XYZ axis on 0,0,0.  Such features can be gereferenced to real world coordinates in one of the following methods:provided In the latter case, the output shapes will need to be georeferenced.", "Point and line geometries that may exist in a 3D file are not maintained in the output multipatch feature class, as multipatches do not support them.", "Unsupported geometry types for VRML files include Box, Cone, Cylinder, Extrusion, PointSet, Sphere, and Text."], "parameters": [{"name": "in_files", "isInputFile": true, "isOptional": false, "description": "One or more 3D models or folders containing such files in the supported formats, which include 3D Studio Max ( *.3ds ), SketchUp ( *.skp ), VRML and GeoVRML ( *.wrl ), OpenFlight ( *.flt ), and COLLADA ( *.dae ). ", "dataType": "File; Folder"}, {"name": "out_featureClass", "isOutputFile": true, "isOptional": false, "description": "The output multipatch that will be created. ", "dataType": "Feature Class"}, {"name": "root_per_feature", "isOptional": true, "description": "Indicates whether to produce one feature per file or one feature for every root node in the file. This option only applies to VRML models. ONE_ROOT_ONE_FEATURE \u2014 The generated output will contain one feature for each root node in the file. ONE_FILE_ONE_FEATURE \u2014 The generated output will contain one file for each feature. This is the default.", "dataType": "Boolean"}, {"name": "spatial_reference", "isOptional": true, "description": "The coordinate system of the input data. For the majority of formats, this is unknown. Only the GeoVRML format stores its coordinate system, and its default will be obtained from the first file in the list unless a spatial reference is specified here. ", "dataType": "Spatial Reference"}, {"name": "y_is_up", "isOptional": true, "description": "Identifies the axis that defines the vertical orientation of the input files. Z_IS_UP \u2014 Indicates that z is up. This is the default. Y_IS_UP \u2014 Indicated that y is up. ", "dataType": "Boolean"}, {"name": "file_suffix", "isOptional": true, "description": "The suffix of the files to import from an input folder. This parameter is required when a folder is specified as input. ", "dataType": "String"}, {"name": "in_featureClass", "isInputFile": true, "isOptional": true, "description": " The point features whose coordinates define the real-world position of the input files. Each input file will be matched to its corresponding point based on the file names stored in the Symbol Field . The Coordinate System parameter should be defined to match the spatial reference of the points. ", "dataType": "Feature Class"}, {"name": "symbol_field", "isOptional": true, "description": " The field in the point features containing the name of the 3D file associated with each point. ", "dataType": "Field"}]},
{"syntax": "SurfaceAspect_3d (in_surface, out_feature_class, {class_breaks_table}, {aspect_field}, {pyramid_level_resolution})", "name": "Surface Aspect (3D Analyst)", "description": "Calculates the aspect, or direction of the steepest downhill slope, of each triangle in a TIN or terrain dataset and writes the output as a polygon feature class.", "example": {"title": "SurfaceAspect example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.SurfaceVolume_3d ( \"sample.gdb/featuredataset/terrain\" , \"surf_vol.txt\" , \"ABOVE\" , 300 , 1 , 5 )"}, "usage": ["Each surface triangle's aspect is determined in units of degrees, then assigned an aspect code based on the cardinal or ordinal direction of its slope. The default classification scheme is defined as follows:", " Code", "Slope Direction", "Slope Angle Range", "-1", "Flat", "No Slope", "1", "North", "0\u201322.5", "2", "Northeast", "22.5\u201345", "3", "East", "45\u2013135", "4", "Southeast", "135\u2013180", "5", "South", "180\u2013225", "6", "Southwest", "225\u2013270", "7", "West", "270\u2013315", "8", "Northwest", "315\u2013337.5", "9", "North", "337.5\u2013360", "Contiguous triangles of the same code are merged into one feature.", "Customized class definitions can be provided through a ", "Class Breaks Table", ".  The table must have two columns where the first indicates the aspect break point in degrees and the second defines its code value. Consider the following example:", "Break", "Aspect_Code", "90.0", "1", "180.0", "2", "270.0", "3", "360.0", "4", "The table can be in any supported format (", ".dbf", ", ", ".txt", ", or geodatabase table). The name of the fields are irrelevant, as the first will always be used for the class breaks and the second for the aspect codes."], "parameters": [{"name": "in_surface", "isInputFile": true, "isOptional": false, "description": "The input terrain or TIN surface. ", "dataType": "Terrain Layer; TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "class_breaks_table", "isOptional": true, "description": "A table containing the classification breaks that will be used to define the aspect ranges in the output feature class. ", "dataType": "Table"}, {"name": "aspect_field", "isOptional": true, "description": "The field containing aspect code values. ", "dataType": "String"}, {"name": "pyramid_level_resolution", "isOptional": true, "description": "The z-tolerance or window size resolution of the terrain pyramid level that will be used by this tool. The default is 0, or full resolution. ", "dataType": "Double"}]},
{"syntax": "Divide_3d (in_raster_or_constant1, in_raster_or_constant2, out_raster)", "name": "Divide (3D Analyst)", "description": "Divides the values of two rasters on a cell-by-cell basis.", "example": {"title": "Divide example 1 (Python window)", "description": "This example divides the values of the first input raster by the second.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Divide_3d ( \"degs\" , \"negs\" , \"C:/output/outdivide2\" )"}, "usage": ["The order of inputs is relevant for this tool.", "When a number is divided by zero, the output result is NoData.", "The data types of the inputs to determine the data type of the output:"], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input whose values will be divided by the second input. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input whose values the first input are to be divided by. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The cell values are the quotient of the first input raster (dividend) divided by the second input (divisor). ", "dataType": "Raster Dataset"}]},
{"syntax": "ReclassByTable_3d (in_raster, in_remap_table, from_value_field, to_value_field, output_value_field, out_raster, {missing_values})", "name": "Reclass by Table (3D Analyst)", "description": "Reclassifies (or changes) the values of the input cells of a raster using a remap table.", "example": {"title": "ReclassByTable example 1 (Python window)", "description": "This example uses a remap table to reclassify the input raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.ReclassByTable_3d ( \"slope\" , \"remapslope\" , \"C:/sapyexamples/output/recslope\" , \"FROM\" , \"TO\" , \"OUT\" , \"NODATA\" )"}, "usage": ["The input raster must have valid statistics. If the statistics do\r\nnot exist, they can be created using the ", "Calculate Statistics", " tool in the Data Management Tools toolbox.", "The ", "From value field", ", ", "To value field", ", and ", "Output value field", " are the field names in the table that define the remapping.", "To reclassify individual values, use a simple remap table of two items. The first item identifies the value to reclassify, and the other item identifies the value to assign to it. Set the 'to value field' the same as the 'from value field'. The value to assign to the output is 'output value field'.", "To reclassify ranges of values, the remap table must have items defining the start and end of each range, along with the value to assign the range. The item defining the start of the range is the ", "From value field", ", and the value defining the end of the range is the ", "To value field", ". The value to assign to the output is ", "Output value field", ".", "The remap table can be an INFO table, a .dbf file, an Access table, or a text file.", "The values in the from and to fields can be any numerical item. The assignment values in the output field must be integers.", "Values in the from field for .dbf, INFO, and Geodatabase tables do not need to be sorted. For text-file based tables, they must be sorted in ascending order. The values should not overlap in either case."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input raster to be reclassified. ", "dataType": "Raster Layer"}, {"name": "in_remap_table", "isInputFile": true, "isOptional": false, "description": "Table holding fields defining value ranges to be reclassified and the values they will become. ", "dataType": "Table View"}, {"name": "from_value_field", "isOptional": false, "description": "Field holding the beginning value for each value range to be reclassified. This is a numeric field of the input remap table. ", "dataType": "Field"}, {"name": "to_value_field", "isOptional": false, "description": "Field holding the ending value for each value range to be reclassified. This is a numeric field of the input remap table. ", "dataType": "Field"}, {"name": "output_value_field", "isOutputFile": true, "isOptional": false, "description": "Field holding the integer values to which each range should be changed. This is an integer field of the input remap table. ", "dataType": "Field"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output reclassified raster. The output will always be of integer type. ", "dataType": "Raster Dataset"}, {"name": "missing_values", "isOptional": true, "description": "Denotes whether missing values in the reclass table retain their value or get mapped to NoData. DATA \u2014 Signifies that if any cell location on the input raster contains a value not present or reclassed in a remap table, the value should remain intact and be written for that location to the output raster. This is the default. NODATA \u2014 Signifies that if any cell location on the input raster contains a value not present or reclassed in a remap table, the value will be reclassed to NoData for that location on the output raster.", "dataType": "Boolean"}]},
{"syntax": "Times_3d (in_raster_or_constant1, in_raster_or_constant2, out_raster)", "name": "Times (3D Analyst)", "description": "Multiplies the values of two rasters on a cell-by-cell basis.", "example": {"title": "Times example 1 (Python window)", "description": "This example multiplies the values of an input elevation raster by a constant value to convert the elevation values from feet to meters.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Times_3d ( \"elevation\" , 0.3048 , \"C:/output/outtimes\" )"}, "usage": ["The order of inputs is irrelevant for this tool.", "If both inputs are integer, the output will be an integer raster; otherwise, it will be a floating-point raster."], "parameters": [{"name": "in_raster_or_constant1", "isInputFile": true, "isOptional": false, "description": "The input containing the values to be multiplied. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "in_raster_or_constant2", "isInputFile": true, "isOptional": false, "description": "The input containing the values by which the first input will be multiplied. A number can be used as an input for this parameter, provided a raster is specified for the other parameter. To be able to specify a number for both inputs, the cell size and extent must first be set in the environment. ", "dataType": "Raster Layer | Constant"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output raster. The cell values are the product of the first input multiplied by the second. ", "dataType": "Raster Dataset"}]},
{"syntax": "Aspect_3d (in_raster, out_raster)", "name": "Aspect (3D Analyst)", "description": "Derives aspect from a raster surface. The aspect identifies the downslope direction of the maximum rate of change in value from each cell to its neighbors. Aspect can be thought of as the slope direction. The values of the output raster will be the compass direction of the aspect. \r\n Learn more about how Aspect works", "example": {"title": "Aspect example 1 (Python window)", "description": "This example creates an aspect raster from an input surface raster.", "code": "import arcpy from arcpy import env env.workspace = \"C:/data\" arcpy.Aspect_3d ( \"elevation\" , \"C:/output/outaspect.img\" )"}, "usage": ["Aspect is the direction of the maximum rate of change in the z-value from each cell in a raster surface.", "Aspect is expressed in positive degrees from 0 to 359.9, measured clockwise from north.", "Cells in the input raster that are flat\u2014with zero slope\u2014are assigned an aspect of -1.", "If the center cell in the immediate neighborhood (3 x 3 window) is NoData, the output is NoData.", "If any neighborhood cells are NoData, they are first assigned the value of the center cell, then the aspect is computed."], "parameters": [{"name": "in_raster", "isInputFile": true, "isOptional": false, "description": "The input surface raster. ", "dataType": "Raster Layer"}, {"name": "out_raster", "isOutputFile": true, "isOptional": false, "description": "The output aspect raster. ", "dataType": "Raster Dataset"}]},
{"syntax": "ReplaceTerrainPoints_3d (in_terrain, terrain_feature_class, in_point_features, {polygon_features_or_extent})", "name": "Replace Terrain Points (3D Analyst)", "description": " Replaces point features used in a terrain dataset with points from a specified feature class.", "example": {"title": "ReplaceTerrainPoints example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.ReplaceTerrainPoints_3d ( \"sample.gdb/featuredataset/terrain\" , \"points_old\" , \"sample.gdb/featuredataset/terrain/pts_new\" )"}, "usage": ["The terrain data source can be points, multipoints, or embedded points.", "The replacement points can come from single-point or multipoint features.", " Replacing points in a terrain dataset will invalidate it. Run ", "Build Terrain", " after adding points or multipoints.", "If the terrain is in SDE, it must be registered as versioned."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "terrain_feature_class", "isOptional": false, "description": " The terrain point features that will have its source data replaced. ", "dataType": "Feature Layer"}, {"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": " The point or multipoint features that will replace the terrain point features. ", "dataType": "Feature Layer"}, {"name": "polygon_features_or_extent", "isOptional": true, "description": " An optional area of interest can be used to define the extent of the area in which the terrain points would be replaced. ", "dataType": "Feature Layer; Extent"}]},
{"syntax": "AppendTerrainPoints_3d (in_terrain, terrain_feature_class, in_point_features, {polygon_features_or_extent})", "name": "Append Terrain Points (3D Analyst)", "description": " Appends points to a point  feature referenced by a terrain dataset.", "example": {"title": "AppendTerrainPoints example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.AppendTerrainPoints_3d ( 'sample.gdb/featuredataset/terrain' , 'existing_points' , 'new_points.shp' )"}, "usage": ["This tool will invalidate a terrain dataset. Run ", "Build Terrain", " after adding points or multipoints.", "If the terrain is in SDE, it must be registered as versioned."], "parameters": [{"name": "in_terrain", "isInputFile": true, "isOptional": false, "description": "The input terrain dataset. ", "dataType": "Terrain Layer"}, {"name": "terrain_feature_class", "isOptional": false, "description": " The feature class that contributes to the terrain dataset into which the points or multipoints will be added. This parameter only requires the name of the feature class and not its full path. ", "dataType": "String"}, {"name": "in_point_features", "isInputFile": true, "isOptional": false, "description": " The feature class of points or multipoints to add as an additional data source for the terrain dataset. ", "dataType": "Feature Layer"}, {"name": "polygon_features_or_extent", "isOptional": true, "description": " Specify a polygon feature class or arcpy.Extent object to define the area where point features will be added. This parameter is empty by default, which results in all the points from the input feature class being loaded to the terrain feature. ", "dataType": "Extent; Feature Layer"}]},
{"syntax": "DelineateTinDataArea_3d (in_tin, max_edge_length, {method})", "name": "Delineate TIN Data Area (3D Analyst)", "description": "Redefines the data area, or interpolation zone, of a triangulated irregular network (TIN) based on its triangle edge length.", "example": {"title": "DelineateTinDataArea example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.DelineateTinDataArea_3d ( \"elevation\" , 10 , \"PERIMETER_ONLY\" )"}, "usage": ["The source TIN will be modified by this tool. If you prefer not to modify the input, consider creating a duplicate dataset using the ", "Copy TIN", " tool.", "All triangles are marked as inside before edge length classification begins. This will effectively undo any preexisting data area classification. If the result obtained from one iteration is unsatisfactory, the tool can be executed again without having to obtain the original data.", "Extreme edge lengths produced by concave characteristics in the TIN's source measurements can be removed from the TIN's valid data area through this tool. Triangles that have an edge exceeding the ", "Maximum Edge Length", " will be masked as NoData areas.", "The ", "Maximum Edge Length", " value is best determined from the average spacing of nodes in the TIN within areas that are considered valid data zones. Provide a value that is larger than the average spacing. The most successful use of this tool is based on knowledge of the\r\ndata used to create the TIN.\r\n", "The ", "Method", " determines which triangles are evaluated.", "Use ", "PERIMETER_ONLY", " if the concave data portions are relegated to the outer extremities of the TIN."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "max_edge_length", "isOptional": false, "description": "The two-dimensional distance that defines the maximum length of a TIN triangle edge in the TIN's data area. Triangles with one or more edges that exceed this value will be considered outside the TIN's interpolation zone and will not be rendered in maps or used in surface analysis. ", "dataType": "Double"}, {"name": "method", "isOptional": true, "description": "The method used for delineating the TIN's data area. PERIMETER_ONLY \u2014 Iterates through triangles from the TIN's outer extent inward and will stop when the current iteration of boundary triangle edges does not exceed the Maximum Edge Length . This is the default. ALL \u2014 Classifies the entire collection of TIN triangles by edge length.", "dataType": "String"}]},
{"syntax": "TINDomain_3d (in_tin, out_feature_class, out_geometry_type)", "name": "TIN Domain (3D Analyst)", "description": "Creates a line or polygon feature class representing the interpolation zone of a triangulated irregular network (TIN) dataset.", "example": {"title": "TINDomain example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( '3D' ) env.workspace = 'C:/data' arcpy.TinDomain_3d ( 'tin' , 'tin_domain.shp' )"}, "usage": ["This tool can be used to generate a convex hull (the minimum bounding polygon) around a set of points. If there aren't any clip or erase polygons used to define the TIN, the domain is equivalent to the convex hull.", "The output geometry is placed in one feature record and may be either a single or multipart geometry, depending on the nature of the interpolation zone. For example, if the interpolation zone is composed of islands or contains holes, the resulting geometry will be multipart.", "3D polygons only contain elevation values along the perimeter of the features, as interior portions of the polygon will not contain any vertices. When drawn in 3D with an area fill, the boundary vertices are arbitrarily connected into triangles for rendering. Unless the polygon is planar, either sloped or horizontal, it's unlikely the fill will accurately represent the interior surface. For this reason, it is recommended that nonplanar 3D polygons are  drawn without fill symbology."], "parameters": [{"name": "in_tin", "isInputFile": true, "isOptional": false, "description": "The input TIN. ", "dataType": "TIN Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output feature class. ", "dataType": "Feature Class"}, {"name": "out_geometry_type", "isOutputFile": true, "isOptional": false, "description": "The geometry of the output feature class. LINE \u2014 The output will be a z-enabled line feature class. POLYGON \u2014 The output will be a z-enabled polygon feature class.", "dataType": "String"}]},
{"syntax": "Layer3DToFeatureClass_3d (in_feature_layer, out_feature_class, {group_field})", "name": "Layer 3D To Feature Class (3D Analyst)", "description": "Exports feature layers that have 3D properties defined to a multipatch feature class. ", "example": {"title": "Layer3DToFeatureClass example 1 (Python window)", "description": "The following sample demonstrates the use of this tool in the Python window:", "code": "import arcpy from arcpy import env arcpy.CheckOutExtension ( \"3D\" ) env.workspace = \"C:/data\" arcpy.Layer3DToFeatureClass_3d ( \"Points_3D.lyr\" , \"Test.gdb/trees\" )"}, "usage": ["Only features whose rendering can be persisted as a multipatch will be converted, such as points symbolized with 3D markers. Texture fill symbols are not supported, nor are  ArcMap layers because they do not retain any 3D properties.", "Only certain 3D properties are applied, for example:", "Draped layers in ArcGlobe are not supported. Due to the dynamic nature of draped surfaces, height values will not be maintained.", "Textured fill symbols are not supported.  If a feature in the layer uses 3D marker symbols with restricted properties, like textures, the feature will not be added to the output.", "A feature layer with 3D properties handles well for most visualization applications and will not need to be converted to a multipatch.  However, converting the layer to a multipatch would be particularly useful if the resulting multipatch will be used for additional editing in third-party modeling software or if the multipatch is large and would be consumed in ArcGlobe as a cached layer."], "parameters": [{"name": "in_feature_layer", "isInputFile": true, "isOptional": false, "description": "The input feature layer that has 3D properties defined. ", "dataType": "Feature Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output multipatch feature class. ", "dataType": "Feature Class"}, {"name": "group_field", "isOptional": true, "description": "The field in the input feature class that identifies the features that will be combined into the same multipatch feature. The resulting attributes will be set to one of the input records. ", "dataType": "Field"}]},
{"syntax": "TinDifference_3d (in_tin1, in_tin2, out_feature_class)", "name": "TIN Difference (3D Analyst)", "description": "Calculates the volumetric difference between two TINs.", "example": {"title": "TIN Difference Example (Python window)", "description": "The following Python Window script demonstrates how to use the ", "code": "Missing source code file"}, "usage": ["The input TINs need to overlap in horizontal extent.", "It's best if the horizontal and vertical coordinate systems of the input TINs are the same."], "parameters": [{"name": "in_tin1", "isInputFile": true, "isOptional": false, "description": "The first input TIN. ", "dataType": "Tin Layer"}, {"name": "in_tin2", "isInputFile": true, "isOptional": false, "description": "The second input TIN. ", "dataType": "Tin Layer"}, {"name": "out_feature_class", "isOutputFile": true, "isOptional": false, "description": "The output polygon feature class. ", "dataType": "Feature Class"}]}
]